Question,Topic
find all solutions to the following system of equations by row-reducing the coefficient matrix: \[\begin{array}{l}\frac{1}{5}x_{1}+2x_{2}-\phantom{-}6x_{3}=0\\ -4x_{1}\phantom{-}+\phantom{-}5x_{3}=0\\ -3x_{1}+6x_{2}-13x_{3}=0\\ -\frac{1}{5}x_{1}+2x_{3}-\phantom{-}\frac{1}{5}x_{3}=0\end{array}\] ,linear equations
find a row-reduced echelon matrix which is row-equivalent to \[a=\begin{bmatrix}1&-i\\ 2&2\\ i&1+i\end{bmatrix}.\] what are the solutions of \(ax=0\)? ,linear equations
let \[a=\begin{bmatrix}1&2&1&0\\ -1&0&3&5\\ 1&-2&1&1\end{bmatrix}.\] find a row-reduced echelon matrix \(r\) which is row-equivalent to \(a\) and an invertible \(3\times 3\) matrix \(p\) such that \(r=pa\). ,linear equations
"do exercise 1, but with \[a=\begin{bmatrix}2&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad i\\ 1&-3&-i\\ i&1&1\end{bmatrix}.\] ",linear equations
"for each of the two matrices \[\begin{bmatrix}2&5&-1\\ 4&-1&2\\ 6&4&1\end{bmatrix},\quad\quad\begin{bmatrix}1&-1&2\\ 3&2&4\\ 0&1&-2\end{bmatrix}\] use elementary row operations to discover whether it is invertible, and to find the inverse in case it is. ",linear equations
let \[a=\begin{bmatrix}5&0&0\\ 1&5&0\\ 0&1&5\end{bmatrix}.\] ,linear equations
"if \(f\) is a field, verify that \(f^{n}\) (as defined in example 1) is a vector space over the field \(f\).",vector spaces
"if \(v\) is a vector space over the field \(f\), verify that  \[(\alpha_{1}+\alpha_{2})+(\alpha_{3}+\alpha_{4})=[\alpha_{2}+(\alpha_{3}+ \alpha_{1})]+\alpha_{4}\]  for all vectors \(\alpha_{1},\alpha_{3},\alpha_{4},\) and \(\alpha_{4}\) in \(v\).  ",vector spaces
"if \(c\) is the field of complex numbers, which vectors in \(c^{3}\) are linear combinations of \((1,0,-1)\), \((0,1,1)\), and \((1,1,1)\)? ",vector spaces
"prove that if two vectors are linearly dependent, one of them is a scalar multiple of the other.",vector spaces
"are the vectors \[\begin{array}{l}\alpha_{1}=(1,1,2,4),\quad\ \alpha_{2}=(2,-1,-5,2)\\ \alpha_{3}=(1,-1,-4,0),\quad\ \alpha_{4}=(2,1,1,6)\end{array}\] linearly independent in \(r^{4}\)?",vector spaces
find a basis for the subspace of \(r^{4}\) spanned by the four vectors of exercise 2.,vector spaces
"show that the vectors \[\alpha_{1}=(1,0,-1),\quad\ \ \alpha_{2}=(1,2,1),\quad\ \ \alpha_{3}=(0,-3,2)\] form a basis for \(r^{3}\). express each of the standard basis vectors as linear combinations of \(\alpha_{1}\), \(\alpha_{2}\) and \(\alpha_{3}\).",vector spaces
" find three vectors in \(r^{2}\) which are linearly dependent, and are such that any two of them are linearly independent.",vector spaces
let \(v\) be the vector space of all \(2\times 2\) matrices over the field \(f\). prove that \(v\) has dimension \(4\) by exhibiting a basis for \(v\) which has four elements.,vector spaces
let \(v\) be the vector space of exercise 6. let \(w_{1}\) be the set of matrices of the form \[\begin{bmatrix}x&-x\\ y&z\end{bmatrix}\] and let \(w_{2}\) be the set of matrices of the form \[\begin{bmatrix}a&b\\ -a&c\end{bmatrix}.\] ,vector spaces
let \(s<n\) and \(\iota\) an \(s\times n\) matrix with entries in the field \(f\). use theorem 4 (not its proof) to show that there is a non-zero \(x\) in \(f^{\mathsf{w}\times\mathsf{i}}\) such that \(ax=0\). ,vector spaces
"let \[\alpha_{1}=(1,1,-2,1),\qquad\alpha_{2}\simeq(3,0,4,-1),\qquad\alpha_{3}=(-1,2, 5,2).\] let \[\alpha=(4,-5,9,-7),\qquad\beta=(3,1,-4,4),\qquad\gamma=(-1,1,0,1).\] 1. which of the vectors \(\alpha,\beta,\gamma\) are in the subspace of \(r^{4}\) spanned by the \(\alpha_{i}\)? 2. which of the vectors \(\alpha,\beta,\gamma\) are in the subspace ",vector spaces
"which of the following functions \(t\) from \(r^{2}\) into \(r^{2}\) are linear transformations? 1. \(t(x_{1},x_{2})=(1+x_{1},x_{2})\); 2. \(t(x_{1},x_{2})=(x_{2},x_{1})\); 3. \(t(x_{1},x_{2})=(x_{1}^{2},x_{2})\); 4. \(t(x_{1},x_{2})=(\sin x_{1},x_{2})\); 5. \(t(x_{1},x_{ ",linear transformations
"let \(t\) and \(u\) be the linear operators on \(r^{2}\) defined by \[t(x_{1},x_{2})=(x_{2},x_{1})\quad\text{and}\quad u(x_{1},x_{2})=(x_{1},0).\] (1) how would you describe \(t\) and \(u\) geometrically? (2) give rules like the ones defining \(t\) and \(u\) for each of the transformations \((u+t)\), \(ut\), \(tu\), \(t^{1}\), \(u^{2}\). ",linear transformations
"let \(t\) be the (unique) linear operator on \(c^{2}\) for which \[t\epsilon_{1}=(1,0,i),\qquad t\epsilon_{2}=(0,1,1),\qquad t\epsilon_{3}=(i,1,0).\]  is \(t\) invertible? ",linear transformations
"let \(t\) be the linear operator on \(r^{3}\) defined by \[t(x_{1},x_{2},x_{3})=(3x_{1},x_{1}-x_{3},2x_{1}+x_{2}+x_{3}).\]  is \(t\) invertible? if so, find a rule for \(t^{-1}\) like the one which defines \(t\). ",linear transformations
"for the linear operator \(t\) of exercise 3, prove that \[(t^{2}-i)(t-3i)=0.\] ",linear transformations
let \(c^{2x2}\) be the complex vector space of \(2\times 2\) matrices with complex entries. let \[b=\left[\begin{array}{cc}1&-1\\ -4&4\end{array}\right]\]  and let \(t\) be the linear operator on \(c^{2x2}\) defined by \(t(a)=ba\). what is the rank of \(t\)? can you describe \(t\)? ,linear transformations
"let \(t\) be a linear transformation from \(r^{2}\) into \(r^{2}\), and let \(u\) be a linear transformation from \(r^{2}\) into \(r^{2}\). prove that the transformation \(ut\) is not invertible. generalize the theorem.   ",linear transformations
"in \(r^{3}\), let \(\alpha_{1}=(1,\,0,\,1)\), \(\alpha_{2}=(0,\,1,\,-2)\), \(\alpha_{3}=(-1,\,-1,\,0)\).  ",linear transformations
"if \(f\) is a linear functional on \(r^{3}\) such that \[f(\alpha_{1})=1,\ \ \ \ \ f(\alpha_{3})=-1,\ \ \ \ \ f(\alpha_{3})=3,\] and if \(\alpha=(a,\,b,\,c)\), find \(f(\alpha)\).   ",linear transformations
"let \(q\) be the field of rational numbers. determine which of the following subsets of \(q[x]\) are ideals. when the set is an ideal, find its monic generator. 1. all \(f\) of even degree; 2. all \(f\) of degree \(\geq\) 5; 3. all \(f\) such that \(f(0)=0\); 4. all \(f\) such that \(f(2)=f(4)=0\); 5. all \(f\) in the range of the linear operator \(t\) defined by \[t\bigl{(}\sum\limits_{i=0}^{n}c_{i}x^{i}\bigr{)}=\sum\limits_{i=0}^{n}\frac{c_{ i}}{i+1}\,x^{i+1}.\] ",polynomials
"find the g.c.d. of each of the following pairs of polynomials 1. \(2x^{4}-x^{3}-3x^{2}-6x+4\), \(x^{4}+x^{3}-x^{2}-2x-2\); 2. \(3x^{4}+8x^{2}-3\), \(x^{3}+2x^{2}+3x+6\); 3. \(x^{4}-2x^{3}-2x^{2}-2x-3\), \(x^{3}+6x^{2}+7\,x+1\). ",polynomials
let \(a\) be an \(n\times n\) matrix over a field \(f\). show that the set of all polynomials \(f\) in \ ,polynomials
"let \(p\) be a monic polynomial over the field \(f\), and let \(f\) and \(g\) be relatively prime polynomials over \(f\). prove that the g.e.d. of \(pf\) and \(pg\) is \(p\).  ",polynomials
"assuming the fundamental theorem of algebra, prove the following. if \(f\) and \(g\) are polynomials over the field of complex numbers, then g.e.d. \((f,g)=1\) if and only if \(f\) and \(g\) have no common root.  ",polynomials
"let \(d\) be the differentiation operator on the space of polynomials over the field of complex numbers. let \(f\) be a monic polynomial over the field of complex numbers. prove that  \[f=(x-c_{k})\,\cdots\,(x-c_{k})\]  where \(c_{k}\), ..., \(c_{k}\) are _distinct_ complex numbers if and only if \(f\) and \(df\) are relatively prime. in other words, \(f\) has no repeated root if and only if \(f\) and \(df\) have no common root. (assume the fundamental theorem of algebra.)  ",polynomials
"prove the following generalization of taylor's formula. let \(f\), \(g\), and \(h\) be polynomials over a subfield of the complex numbers, with \(\deg f\leq n\). then  \[f(g)\,=\,\sum\limits_{h\,=\,0}^{n}\frac{1}{k!}f^{(h)}(h)(g-h)^{k}.\]  (here \(f(g)\) denotes '\(f\) of \(g\).')  for the remaining exercises, we shall need the following definition. if \(f\), \(g\), and \(p\) are polynomials over the field \(f\) with \(p\neq 0\), we say that \(f\) is **congruent to \(g\) modulo \(p\)** if \((f-g)\) is divisible by \(p\). if \(f\) is congruent to \(g\) modulo \(p\), we write  \[f\equiv g\bmod p.\]  ",polynomials
"prove, for any non-zero polynomial \(p\), that congruence modulo \(p\) is an equivalence relation. it is reflexive; \(f\equiv f\bmod p\). it is symmetric: if \(f\equiv g\bmod p\), then \(g\equiv f\bmod p\). it is transitive: if \(f\equiv g\bmod p\) and \(g\equiv h\bmod p\), then \(f\equiv h\bmod p\).",polynomials
suppose \(f\equiv g\bmod p\) and \(f_{1}\equiv g_{1}\bmod p\). prove that \(f+f_{1}=g+g_{1}\bmod p\). prove that \(f\!f_{1}=gg_{1}\bmod p\).  ,polynomials
"use exercise 7 to prove the following. if \(f\), \(g\), \(h\), and \(p\) are polynomials over the field \(f\) and \(p\neq 0\), and if \(f\equiv g\bmod p\), then \(h(f)\equiv h(g)\bmod p\).  ",polynomials
"if \(p\) is an irreducible polynomial and \(fg\equiv 0\bmod p\), prove that either \(f\equiv 0\bmod p\) or \(g\equiv 0\bmod p\). give an example which shows that: this is false if \(p\) is not irreducible.   ",polynomials
each of the following expressions defines a function \(d\) on the set of \(3\times 3\) matrices over the field of real numbers. in which of these cases is \(d\) a 3-linear function? \(d(a)=a_{11}+a_{22}+a_{31}\);\(d(a)=(a_{11})^{2}+3a_{11}a_{22}\);\(d(a)=a_{11}a_{12}a_{32}\);\(d(a)=a_{12}a_{22}+5a_{12}a_{22}a_{22}\);\(d(a)=0\);\(d(a)=1\).  ,determinants
"verify directly that the three functions \(e_{1}\), \(e_{2}\), \(e_{3}\) defined by (5-6), (5-7), and (5-8) are identical.  ",determinants
"let \(k\) be a commutative ring with identity. if \(a\) is a \(2\times 2\) matrix over \(k\), the **classical adjoint** of \(a\) is the \(2\times 2\) matrix adj \(a\) defined by  \[\text{adj}\ a=\begin{bmatrix}a_{22}&-a_{11}\\ -a_{21}&a_{11}\end{bmatrix}.\]  if det denotes the unique determinant function on \(2\times 2\) matrices over \(k\), show that ",determinants
"use the classical adjoint formula to compute the inverses of each of the following \(3\times 3\) real matrices.  \[\left[\begin{array}{rrr}-2&3&2\\ 6&0&3\\ 4&1&-1\end{array}\right],\qquad\left[\begin{array}{rrr}\cos\theta&0&-\sin \theta\\ 0&1&0\\ \sin\theta&0&\cos\theta\end{array}\right]\]  ",determinants
use cramer's rule to solve each of the following systems of linear equations over the field of rational numbers.  \begin{tabular}{l l l l} (a) & \(x+\) & \(y+\) & \(z=11\) \\  & \(2x-\) & \(6y-\) & \(z=\) & \(0\) \\  & \(3x+\) & \(4y+\) & \(2z=\) & \(0\). \\  & \(3x-\) & \(2y=\) & \(7\) \\  & \(3y-\) & \(2z=\) & \(6\) \\  & \(3z-\) & \(2x=\) & \(-1\). \\ \end{tabular}  ,determinants
"an \(n\times n\) matrix \(a\) over a field \(f\) is **skew-symmetric** if \(a^{i}=-a\). if \(a\) is a skew-symmetric \(n\times n\) matrix with complex entries and \(n\) is odd, prove that \(\det a=0\).  ",determinants
" an \(n\times n\) matrix \(a\) over a field \(f\) is called **orthogonal** if \(aa^{i}=i\). if \(a\) is orthogonal, show that \(\det a=\pm 1\). give an example of an orthogonal matrix for which \(\det a=-1\).   ",determinants
"let \(t\) be the linear operator on \(r^{2}\), the matrix of which in the standard ordered basis is \[a\ =\begin{bmatrix}1&-1\\ 2&2\end{bmatrix}.\] (a) prove that the only subspaces of \(r^{2}\) invariant under \(t\) are \(r^{2}\) and the zero subspace. (b) if \(u\) is the linear operator on \(c^{2}\), the matrix of which in the standard ordered basis is \(a\), show that \(u\) has 1-dimensional invariant subspaces.",elementary canonical forms
"let \(w\) be an invariant subspace for \(t\). prove that the minimal polynomial for the restriction operator \(t_{w}\) divides the minimal polynomial for \(t\), without referring to matrices.",elementary canonical forms
let \(c\) be a characteristic value of \(t\) and let \(w\) be the space of characteristic vectors associated with the characteristic value \(c\). what is the restriction operator \(t_{w}\)?,elementary canonical forms
"let \[a\ =\begin{bmatrix}0&1&0\\ 2&-2&2\\ 2&-3&2\end{bmatrix}.\] is \(a\) similar over the field of real numbers to a triangular matrix? if so, find such a triangular matrix.",elementary canonical forms
every matrix \(a\) such that \(a^{2}=a\) is similar to a diagonal matrix.,elementary canonical forms
"let \(t\) be a diagonalizable linear operator on the \(n\)-dimensional vector space \(v\), and let \(w\) be a subspace which is invariant under \(t\). prove that the restriction operator \(t_{w}\) is diagonalizable.",elementary canonical forms
let \(t\) be a linear operator on a finite-dimensional vector space over the field of complex numbers. prove that \(t\) is diagonalizable if and only if \(t\) is annihilated by some polynomial over \(c\) which has distinct roots.,elementary canonical forms
"let \(t\) be a linear operator on \(v\). if every subspace of \(v\) is invariant under \(t\), then \(t\) is a scalar multiple of the identity operator.",elementary canonical forms
let \(t\) be the indefinite integral operator \[(t\!f)(x)=\int_{0}^{x}f(t)\ dt\],elementary canonical forms
"find an invertible real matrix \(p\) such that \(p^{-1}ap\) and \(p^{-1}bp\) are both diagonal, where \(a\) and \(b\) are the real matrices (a) \[a=\begin{bmatrix}1&2\\ 0&2\end{bmatrix},\qquad b=\begin{bmatrix}3&-8\\ 0&-1\end{bmatrix}\] (b) \[a=\begin{bmatrix}1&1\\ 1&1\end{bmatrix},\qquad b=\begin{bmatrix}1&a\\ a&1\end{bmatrix}.\] ",elementary canonical forms
let \(\mathfrak{f}\) be a commuting family of \(3\times 3\) complex matrices. how many linearly independent matrices can \(\mathfrak{f}\) contain? what about the \(n\times n\) case? ,elementary canonical forms
"let \(t\) be a linear operator on an \(n\)-dimensional space, and suppose that \(t\) has \(n\) distinct characteristic values. prove that any linear operator which commutes with \(t\) is a polynomial in \(t\). ",elementary canonical forms
"let \(a\), \(b\), \(c\), and \(d\) be \(n\times n\) complex matrices which commute. let \(e\) be the \(2n\times 2n\) matrix \[e=\begin{bmatrix}a&b\\ c&\b\end{bmatrix}.\] prove that \(\det e=\det\,(ad-bc)\). ",elementary canonical forms
"let \(f\) be a field, \(n\) a positive integer, and let \(v\) be the space of \(n\times n\) matrices over \(f\). if \(a\) is a fixed \(n\times n\) matrix over \(f\), let \(t_{a}\) be the linear operator on \(v\) defined by \(t_{a}(b)=ab-ba\). consider the family of linear operators \(t_{a}\) obtained by letting \(a\) vary over all diagonal matrices. prove that the operators in that family are simultaneously diagonalizable.   ",elementary canonical forms
"let \(t\) be a linear operator on \(f^{2}\). prove that any non-zero vector which is not a characteristic vector for \(t\) is a cyclic vector for \(t\). hence, prove that either \(t\) has a cyclic vector or \(t\) is a scalar multiple of the identity operator.  ",the rational and jordan forms
"let \(t\) be the linear operator on \(r^{3}\) which is represented in the standard ordered basis by the matrix  \[\begin{bmatrix}2&0&0\\ 0&2&0\\ 0&0&-1\end{bmatrix}\]  prove that \(t\) has no cyclic vector. what is the \(t\)-cyclic subspace generated by the vector (1, \(-1\), 3)?  ",the rational and jordan forms
let \(t\) be the linear operator on \(c^{3}\) which is represented in the standard ordered basis by the matrix  \[\begin{bmatrix}1&i&0\\ -1&2&-i\\ 0&1&1\end{bmatrix}.\] ,the rational and jordan forms
"let \(v\) be the space \(c^{\natural}\), with the standard inner product. let \(t\) be the linear operator defined by \(t\epsilon_{1}=(1,\,-2)\), \(t\epsilon_{2}=(i,\,-1)\). if \(\alpha=(x_{1},x_{2})\), find \(t^{*}\alpha\).  ",inner product spaces
"let \(t\) be the linear operator on \(c^{\natural}\) defined by \(t\epsilon_{1}=(1+i,\,2)\), \(t\epsilon_{2}=(i,\,i)\). using the standard inner product, find the matrix of \(t^{*}\) in the standard ordered basis. does \(t\) commute with \(t^{*}\)?  ",inner product spaces
"let \(v\) be \(c^{\natural}\) with the standard inner product. let \(t\) be the linear operator on \(v\) whose matrix in the standard ordered basis is defined by  \[a_{ik}=i^{\prime+k},\qquad(i^{\natural}=-1).\]  find a basis for the null space of \(t^{*}\).  ",inner product spaces
let \(v\) be a finite-dimensional inner product space and \(t\) a linear operator on \(v\). show that the range of \(t^{*}\) is the orthogonal complement of the null space of \(t\).  ,inner product spaces
"let \(v\) be a finite-dimensional inner product space and \(t\) a linear operator on \(v\). if \(t\) is invertible, show that \(t^{*}\) is invertible and \((t^{*})^{-1}=(t^{-1})^{*}\).  ",inner product spaces
"let \(v\) be an inner product space and \(\beta\), \(\gamma\) fixed vectors in \(v\). show that \(t\alpha=(\alpha|\beta)\gamma\) defines a linear operator on \(v\). show that \(t\) has an adjoint, and describe \(t^{*}\) explicitly.  now suppose \(v\) is \(c^{n}\) with the standard inner product, \(\beta=(y_{1},\,.\,.\,.\,,\,y_{n})\), and \(\gamma=(x_{1},\,.\,.\,.\,,\,x_{n})\). what is the \(j\), \(k\) entry of the matrix of \(t\) in the standard ordered basis? what is the rank of this matrix?  ",inner product spaces
show that the product of two self-adjoint operators is self-adjoint if and only if the two operators commute.   ,inner product spaces
"for each of the following real symmetric matrices \(a\), find a real orthogonal matrix \(p\) such that \(p^{t}ap\) is diagonal.  \[\begin{bmatrix}1&1\\ 1&1\end{bmatrix},\quad\begin{bmatrix}1&2\\ 2&1\end{bmatrix},\quad\begin{bmatrix}\cos\theta&\sin\theta\\ \sin\theta&-\cos\theta\end{bmatrix}\]  ",inner product spaces
is a complex symmetric matrix self-adjoint? is it normal?  ,inner product spaces
for  \[a=\begin{bmatrix}1&2&3\\ 2&3&4\\ 3&4&5\end{bmatrix}\]  there is a real orthogonal matrix \(p\) such that \(p^{t}ap=d\) is diagonal. find such a diagonal matrix \(d\).  ,inner product spaces
"let \(v\) be \(c^{2}\), with the standard inner product. let \(t\) be the linear operator on \(v\) which is represented in the standard ordered basis by the matrix  \[a=\begin{bmatrix}1&i\\ i&1\end{bmatrix}.\]  show that \(t\) is normal, and find an orthonormal basis for \(v\), consisting of characteristic vectors for \(t\).  ",inner product spaces
"give an example of a \(2\times 2\) matrix \(a\) such that \(a^{2}\) is normal, but \(a\) is not normal.  ",inner product spaces
"let \(t\) be a normal operator on a finite-dimensional complex inner product space. prove that \(t\) is self-adjoint, positive, or unitary according as every characteristic value of \(t\) is real, positive, or of absolute value \(1\). (use theorem 22 to reduce to a similar question about diagonal matrices.)  ",inner product spaces
"let \(t\) be a linear operator on the finite-dimensional inner product space \(v\), and suppose \(t\) is both positive and unitary. prove \(t=i\).  ",inner product spaces
prove \(t\) is normal if and only if \(t=t_{1}+it_{2}\) where \(t_{1}\) and \(t_{2}\) are self-adjoint operators which commute.  ,inner product spaces
"prove that a real symmetric matrix has a real symmetric cube root; i.e., if \(a\) is real symmetric, there is a real symmetric \(b\) such that \(b^{2}=a\).  ",inner product spaces
prove that every positive matrix is the square of a positive matrix.  ,inner product spaces
"which of the following functions \(f\), defined on vectors \(\alpha=(x_{1},\,x_{2})\) and \(\beta=(y_{1},\,y_{2})\) in \(c^{2}\), are (sesqui-linear) forms on \(c^{2}\)?  (a) \(f(\alpha,\,\beta)\,=\,1\).  (b) \(f(\alpha,\,\beta)\,=\,(x_{1}-\bar{y}_{1})^{2}+x_{2}\bar{y}_{2}\).  (c) \(f(\alpha,\,\beta)\,=\,(x_{1}+\bar{y}_{1})^{2}-(x_{1}-\bar{y}_{1})^{2}\).  (d) \(f(\alpha,\,\beta)\,=\,x_{1}\bar{y}_{2}-\,\bar{x}",operatorsoninner product spaces
"let \(v\) be \(c^{\sharp}\), with the standard inner product. for which vectors \(\alpha\) in \(v\) is there a positive linear operator \(t\) such that \(\alpha\,=\,t\epsilon_{1}\)?  ",operatorsoninner product spaces
"let \(v\) be \(r^{\sharp}\), with the standard inner product. if \(\theta\) is a real number, let \(t\) be the linear operator 'rotation through \(\theta\),'  \[t\theta(x_{1},x_{2})\,=\,(x_{1}\cos\theta-x_{2}\sin\theta,x_{1}\sin\theta+x_ {2}\cos\theta).\]  for which values of \(\theta\) is \(t\theta\) a positive operator?  ",operatorsoninner product spaces
"let \(v\) be the space of \(n\,\times\,1\) matrices over \(c\), with the inner product \((x|y)\,=\,y^{*}gx\) (where \(g\) is an \(n\,\times\,n\) matrix such that this is an inner product). let \(a\) be an \(n\,\times\,n\) matrix and \(t\) the linear operator \(t(x)\,=\,ax\). find \(t^{*}\). if \(y\) is a fixed element of \(v\), find the element \(z\) of \(v\) which determines the linear functional \(x\,\xrightarrow{}\,y^{*}x\). in other words, find \(z\) such that \(y^{*}x\,=\,(x|z)\) for all \(x\) in \(v\).   ",operatorsoninner product spaces
"give a reasonable definition of a non-negative \(n\times n\) matrix, and then prove that such a matrix has a unique non-negative square root.  ",operatorsoninner product spaces
"let \(a\) be an \(n\times n\) matrix with complex entries such that \(a^{*}=-a\), and let \(b=e^{a}\). show that \(\det b=e^{\mathrm{tr}\,a}\); \(b^{*}=e^{-a}\);\(b\) is unitary. ",operatorsoninner product spaces
"if \(u\) and \(t\) are normal operators which commute, prove that \(u+t\) and \(ut\) are normal.  ",operatorsoninner product spaces
"let \(t\) be a linear operator on the finite-dimensional complex inner product space \(v\). prove that the following ten statements about \(t\) are equivalent. \(t\) is normal. \(\|t\alpha\|=\|t^{*}\alpha\|\) for every \(\alpha\) in \(v\). \(t=t_{1}+it_{2}\), where \(t_{1}\) and \(t_{2}\) are self-adjoint and \(t_{1}t_{2}=t_{2}t_{1}\). if \(\alpha\) is a vector and \(c\) a scalar such that \(t\alpha=c\alpha\), then \(t^{*}\alpha=\varepsilon\alpha\). there is an orthonormal basis for \(v\) consisting of characteristic vectors [for \(t\). there is an orthonormal basis \(\otimes\) such that \([t]_{\otimes}\) is diagonal. there is a polynomial \(g\) with complex coefficients such that \(t^{*}=g(t)\). every subspace which is invariant under \(t\) is also invariant under \(t^{*}\). \(t=nu\), where \(n\) is non-negative, \(u\) is unitary, and \(n\) commutes with \(u\). \(t=e_{1}e_{1}+\cdots+e_{k}e_{k}\), where \(i=e_{1}+\cdots+e_{k}\), \(e_{i}e_{j}=0\) for \(i\neq j\), and \(e_{i}^{2}=e_{i}=e_{i}^{*}\).",operatorsoninner product spaces
use exercise 3 to show that any commuting family of normal operators (not necessarily diagonalizable ones) on a finite-dimensional inner product space generates a commutative self-adjoint algebra of normal operators.  ,operatorsoninner product spaces
"let \(v\) be a finite-dimensional complex inner product space and \(u\) a unitary operator on \(v\) such that \(u\alpha=\alpha\) implies \(\alpha=0\). let  \[f(z)=i\,\frac{(1+z)}{(1-z)},\qquad z\neq 1\] and show that \(f(u)=i(i+u)(i-u)^{-1}\); \(f(u)\) is self-adjoint; for every self-adjoint operator \(t\) on \(v\), the operator \[u=(t-ii)(t+ii)^{-1}\] is unitary and such that \(t=f(u)\).",operatorsoninner product spaces
let \(v\) be the space of complex \(n\times n\) matrices equipped with the inner product  \[(a|b)=\mathrm{tr}\ (ab^{*}).\],operatorsoninner product spaces
"which of the following functions \(f\), defined on vectors \(\alpha=(x_{1},x_{2})\) and \(\beta=(y_{1},y_{2})\) in \(r^{\natural}\), are bilinear forms? \(f(\alpha,\,\beta)\,=\,1\). 2. \(f(\alpha,\,\beta)\,=\,(x_{1}-y_{1})^{\natural}+x_{2}y_{2}\). 3. \(f(\alpha,\,\beta)\,=\,(x_{1}+y_{1})^{\natural}-(x_{1}-y_{1})^{\natural}\). 4. \(f(\alpha,\,\beta)\,=\,x_{1}y_{2}-x_{2}y_{1}\).",bilinear forms
"let \(f\) be the bilinear form on \(r^{\natural}\) defined by \[f((x_{1},y_{1}),\,(x_{2},y_{2}))\,=\,x_{1}y_{1}+x_{2}y_{2}.\] find the matrix of \(f\) in each of the following bases: \[\{(1,\,0),\,(0,\,1)\},\qquad\{(1,\,-1),\,(1,\,1)\},\qquad\{(1,\,2),\,(3,\,4)\}.\] ",bilinear forms
"let \(v\) be the space of all \(2\times 3\) matrices over \(r_{\natural}\) and let \(f\) be the bilinear form on \(v\) defined by \(f(x,\,y)\,=\,\text{trace}\,\,(x^{\,\prime}ay)\), where \[a\,=\,\begin{bmatrix}1&2\\ 3&4\end{bmatrix}.\] find the matrix of \(f\) in the ordered basis \[\{e^{\natural},e^{\natural},e^{\natural},e^{\natural},e^{\natural},e^{\natural \natural},e^{\natural\natural}\}\] where \(e^{\natural}\) is the matrix whose only non-zero entry is a \(1\) in row \(i\",bilinear forms
"let \(v\) be a vector space over a field \(f\). show that the set of all skew-symmetric bilinear forms on \(v\) is a subspace of \(l(v,\,v,\,f)\). ",bilinear forms
find all skew-symmetric bilinear forms on \(r^{3}\). ,bilinear forms
find a basis for the space of all skew-symmetric bilinear forms on \(r^{n}\). ,bilinear forms
let \(f\) be a symmetric bilinear form on \(c^{n}\) and \(g\) a skew-symmetric bilinear form on \(c^{n}\). suppose \(f+g=0\). show that \(f=g=0\). ,bilinear forms
"let \(v\) be an \(n\)-dimensional vector space over a subfield \(f\) of \(c\). prove the following. 1. the equation \((p\!f)(\alpha,\beta)=\frac{1}{2}f(\alpha,\beta)-\frac{1}{2}f(\beta,\alpha)\) defines a linear operator \(p\) on \(l(v,\,v,\,f)\).",bilinear forms
"let \(v\) be an \(n\)-dimensional vector space over a subfield \(f\) of \(c\). prove the following. 1. \(p^{\intercal}=p_{i}\), i.e., \(p\) is a projection.",bilinear forms
let \(v\) be an \(n\)-dimensional vector space over a subfield \(f\) of \(c\). prove the following. 1. rank \(p=\frac{n(n-1)}{2}\); nullity \(p=\frac{n(n+1)}{2}\).,bilinear forms
"let \(v\) be an \(n\)-dimensional vector space over a subfield \(f\) of \(c\). prove the following. 1. if \(u\) is a linear operator on \(v\), the equation \((u\!f)(\alpha,\beta)=f(u\alpha,\,u\beta)\) defines a linear operator \(u\!t\) on \(l(v,\,v,\,f)\).",bilinear forms
"let \(v\) be an \(n\)-dimensional vector space over a subfield \(f\) of \(c\). prove the following. 1. for every linear operator \(u\), the projection \(p\) commutes with \(u^{\intercal}\).",bilinear forms
"prove an analogue of exercise 11 in section 10.2 for non-degenerate, skew-symmetric bilinear forms.",bilinear forms
let \(f\) be a bilinear form on a vector space \(v\). let \(l_{f}\) and \(r_{f}\) be the mappings of \(v\) into \(v^{*}\) associated with \(f\) in section 10.1. prove that \(f\) is skew-symmetric if and only if \(l_{f}=-r_{f}\). ,bilinear forms
prove an analogue of exercise 17 in section 10.2 for skew-symmetric forms. ,bilinear forms
"let \(v\) be a finite-dimensional vector space and \(l_{1}\), \(l_{2}\) linear functionals on \(v\). show that the equation \[f(\alpha,\,\beta)=l_{1}(\alpha)l_{2}(\beta)=l_{1}(\beta)l_{2}(\alpha)\] defines a skew-symmetric bilinear form on \(v\). show that \(f=0\) if and only if \(l_{1}\), \(l_{2}\) are linearly dependent. ",bilinear forms
let \(v\) be a finite-dimensional vector space over a subfield of the complex numbers and \(f\) a skew-symmetric bilinear form on \(v\). show that \(f\) has rank \(2\) if,bilinear forms
"let \(m\) be a member of the complex orthogonal group, \(o(n,c)\). show that \(m^{\prime}\), \(\overline{m}\), and \(m^{*}=\overline{m}^{\prime}\) also belong to \(o(n,c)\).  ",bilinear forms
"suppose \(m\) belongs to \(o(n,c)\) and that \(m^{\prime}\) is similar to \(m\). does \(m^{\prime}\) also belong to \(o(n,c)\)?  ",bilinear forms
"let  \[y_{j}=\sum_{k=1}^{n}m_{jk}x_{k}\]  where \(m\) is a member of \(o(n,c)\). show that  \[\sum_{j}y_{j}^{2}=\sum_{j}x_{j}^{2}.\]  ",bilinear forms
"let \(m\) be an \(n\times n\) matrix over \(c\) with columns \(m_{1}\), \(m_{1}\), ..., \(m_{n}\). show that \(m\) belongs to \(o(n,c)\) if and only if  \[m^{\prime}_{j}m_{k}=\delta_{jk}.\]  ",bilinear forms
"let \(x\) be an \(n\times 1\) matrix over \(c\). under what conditions does \(o(n,c)\) contain a matrix \(m\) whose first column is \(x\)?  ",bilinear forms
"find a matrix in \(o(3,c)\) whose first row is \((2i,2i,3)\).  ",bilinear forms
"let \(v\) be the space of all \(n\times 1\) matrices over \(c\) and \(f\) the bilinear form on \(v\) given by \(f(x,y)=x^{\prime}y\). let \(m\) belong to \(o(n,c)\). what is the matrix of \(f\) in the basis of \(v\) consisting of the columns \(m_{1}\), \(m_{2}\), ..., \(m_{n}\) of \(m\)?  ",bilinear forms
"let \(x\) be an \(n\times 1\) matrix over \(c\) such that \(x^{\prime}x=1\), and \(i_{i}\) be the \(j\)th column of the identity matrix. show there is a matrix \(m\) in \(o(n,c)\) such that \(mx=i_{i}\). if \(x\) has real entries, show there is an \(m\) in \(o(n,r)\) with the property that \(mx=i_{i}\).  ",bilinear forms
"let \(v\) be the space of all \(n\times 1\) matrices over \(c\), \(a\) an \(n\times n\) matrix over \(c\), and \(f\) the bilinear form on \(v\) given by \(f(x,y)=x^{\prime}ay\). show that \(f\) is invariant under \(o(n,c)\), i.e., \(f(mx,my)=f(x,y)\) for all \(x\), \(y\) in \(v\) and \(m\) in \(o(n,c)\), if and only if \(a\) commutes with each member of \(o(n,c)\).  ",bilinear forms
let \(s\) be any set of \(n\times n\) matrices over \(c\) and \(s^{\prime}\) the set of all \(n\times n\) matrices over \(c\) which commute with each element of \(s\). show that \(s^{\prime}\) is an algebra over \(c\),bilinear forms
"let \(f\) be a subfield of \(c\), \(v\) a finite-dimensional vector space over \(f\), and \(f\) a non-singular bilinear form on \(v\). if \(t\) is a linear operator on \(v\) preserving \(f\), prove that \(\det t=\pm 1\).",bilinear forms
"let \(f\) be a subfield of \(c\), \(v\) the space of \(n\times 1\) matrices over \(f\), \(a\) a n invertible \(n\times n\) matrix over \(f\), and \(f\) the bilinear form on \(v\) given by \(f(x,\,y)=x^{t}a\,y\). if \(m\) is an \(n\times n\) matrix over \(f\), show that \(m\) preserves \(f\ ",bilinear forms
"for the equations x+y = 4, 2x−2y = 4, draw the row picture (two intersecting lines) and the column picture (combination of two columns equal to the column vector (4,4) on the right side).",matrices and gaussian elimination
solve to find a combination of the columns that equals b: u − v − w = b 1 triangular system v + w = b 2 w = b . 3,matrices and gaussian elimination
describe the intersection of the three planes u+v+w+z = 6 and u+w+z=4 and u+w=2 (all in four-dimensional space). is it a line or a point or an empty set? what is the intersection if the fourth plane u = −1 is included? find a fourth equation that leaves us with no solution.,matrices and gaussian elimination
sketch these three lines and decide if the equations are solvable: x + 2y = 2 3 by 2 system x − y = 2 y = 1. what happens if all right-hand sides are zero? is there any nonzero choice of right- hand sides that allows the three lines to intersect at the same point?,matrices and gaussian elimination
find two points on the line of intersection of the three planes t = 0 and z = 0 and x+y+z+t =1 in four-dimensional space.,matrices and gaussian elimination
"when b=(2,5,7), find a solution (u,v,w) to equation (4) different from the solution (1,0,1) mentioned in the text.",matrices and gaussian elimination
"give two more right-hand sides in addition to b = (2,5,7) for which equation (4) can be solved. give two more right-hand sides in addition to b=(2,5,6) for which it cannot be solved.",matrices and gaussian elimination
explain why the system u + v + w = 2 u + 2v + 3w = 1 v + 2w = 0 is singular by finding a combination of the three equations that adds up to 0 = 1. what value should replace the last zero on the right side to allow the equations to have solutions—and what is one of the solutions?,matrices and gaussian elimination
"under what condition on y , y , y do the points (0,y ), (1,y ), 1 2 3 1 2 (2,y ) lie on a straight line? 3",matrices and gaussian elimination
these equations are certain to have the solution x = y = 0. for which values of a is there a whole line of solutions? ax + 2y = 0 2x + ay = 0,matrices and gaussian elimination
"starting with x+4y=7, find the equation for the parallel line through x=0, y=0. find the equation of another line that meets the first at x=3, y=1. problems 13–15 are a review of the row and column pictures.",matrices and gaussian elimination
"draw the two pictures in two planes for the equations x−2y=0, x+y=6.",matrices and gaussian elimination
"for two linear equations in three unknowns x, y, z, the row picture will show (2 or 3) (lines or planes) in (two or three)-dimensional space. the column picture is in (two or three)-dimensional space. the solutions normally lie on a .",matrices and gaussian elimination
"for four linear equations in two unknowns x and y, the row picture shows four . the column picture is in -dimensional space. the equations have no solution unless the vector on the right-hand side is a combination of .",matrices and gaussian elimination
find a point with z = 2 on the intersection line of the planes x+y+3z = 6 and x−y+z=4. find the point with z=0 and a third point halfway between.,matrices and gaussian elimination
"the first of these equations plus the second equals the third: x + y + z = 2 x + 2y + z = 3 2x + 3y + 2z = 5. the first two planes meet along a line. the third plane contains that line, because if x, y, z satisfy the first two equations then they also . the equations have infinitely many solutions (the whole line l). find three solutions.",matrices and gaussian elimination
"move the third plane in problem 17 to a parallel plane 2x+3y+2z = 9. now the three equations have no solution—why not? the first two planes meet along the line l, but the third plane doesn’t that line.",matrices and gaussian elimination
"in problem 17 the columns are (1,1,2) and (1,2,3) and (1,1,2). this is a “singular case” because the third column is . find two combinations of the columns that give b=(2,3,5). this is only possible for b=(4,6,c) if c= .",matrices and gaussian elimination
"normally 4 “planes” in four-dimensional space meet at a . normally 4 col- umn vectors in four-dimensional space can combine to produce b. what combina- tion of (1,0,0,0), (1,1,0,0), (1,1,1,0), (1,1,1,1) produces b=(3,3,3,2)? what 4 equations for x, y, z,t are you solving?",matrices and gaussian elimination
" when equation 1 is added to equation 2, which of these are changed: the planes in the row picture, the column picture, the coefficient matrix, the solution?",matrices and gaussian elimination
"if (a,b) is a multiple of (c,d) with abcd =0, show that (a,c) is a multiple of (b,d). this is surprisingly important: call it a challenge question. you could use numbers first to see how a, b, c, and d are related. the question will lead to: if a= a b has dependent rows then it has dependent columns. c d",matrices and gaussian elimination
"in these equations, the third column (multiplying w) is the same as the right side b. the column form of the equations immediately gives what solution for (u,v,w)? 6u + 7v + 8w = 8 4u + 5v + 9w = 9 2u − 2v + 7w = 7.",matrices and gaussian elimination
"what multiple of equation 1 should be subtracted from equation 2? 2x + 3y = 1 10x + 9y = 11. after this elimination step, write down the upper triangular system and circle the two pivots. the numbers 1 and 11 have no influence on those pivots.",matrices and gaussian elimination
"solve the triangular system of problem 1 by back-substitution, y before x. verify that x times (2,10) plus y times (3,9) equals (1,11). if the right-hand side changes to (4,44), what is the new solution?",matrices and gaussian elimination
"what multiple of equation 2 should be subtracted from equation 3? 2x − 4y = 6 −x + 5y = 0. after this eliminations tep, solve thet riangular system. if the right-hand side changes to (−6,0), what is the new solution?",matrices and gaussian elimination
what multiple of equation 1 should be subtracted from equation 2? ax + by = f cx + dy = g. the first pivot is a (assumed nonzero). elimination produces what formula for the second pivot? what is y? the second pivot is missing when ad=bc.,matrices and gaussian elimination
choose a right-hand side which gives no solution and another right-hand side which gives infinitely many solutions. what are two of those solutions? 3x + 2y = 10 6x + 4y = .,matrices and gaussian elimination
choose a coefficient b that makes this system singular. then choose a right-hand side g that makes it solvable. find two solutions in that singular case. 2x + by = 16 4x + 8y = g.,matrices and gaussian elimination
"for which numbers a does elimination break down (a) permanently, and (b) temporarily? ax + 3y = −3 4x + 6y = 6. solve for x and y after fixing the second breakdown by a row exchange.",matrices and gaussian elimination
"for which three numbers k does elimination break down? which is fixed by a row exchange? in each case, is the number of solutions 0 or 1 or ∞? kx + 3y = 6 3x + ky = −6.",matrices and gaussian elimination
what test on b and b decides whether these two equations allow a solution? how 1 2 many solutions will they have? draw the column picture. 3x − 2y = b 1 6x − 4y = b . 2 problems 10–19 study elimination on 3 by 3 systems (and possible failure).,matrices and gaussian elimination
"reduce this system to upper triangular form by two row operations: 2x + 3y + z = 8 4x + 7y + 5z = 20 − 2y + 2z = 0. circle the pivots. solve by back-substitution for z, y, x.",matrices and gaussian elimination
apply elimination (circle the pivots) and back-substitution to solve 2x − 3y = 3 4x − 5y + z = 7 2x − y − 3z = 5. list the three row operations: subtract times row from row .,matrices and gaussian elimination
"12. which number d forces a row exchange, and what is the triangular system (not sin- gular) for that d? which d makes this system singular (no third pivot)? 2x + 5y + z = 0 4x + dy + z = 2 y − z = 3.",matrices and gaussian elimination
"13. which number b leads later to a row exchange? which b leads to a missing pivot? in that singular case find a nonzero solution x, y, z. x + by = 0 x − 2y − z = 0 y + z = 0.",matrices and gaussian elimination
"14. construct a 3 by 3 system that needs two row exchanges to reach a triangular form and a solution. (b) construct a 3 by 3 system that needs a row exchange to keep going, but breaks down later.",matrices and gaussian elimination
"15. if rows 1 and 2 are the same, how far can you get with elimination (allowing row exchange)? if columns 1 and 2 are the same, which pivot is missing? 2x−y+z=0 2x+2y+z=0 2x−y+z=0 4x+4y+z=0 4x+y+z=2 6x+6y+z=2.",matrices and gaussian elimination
"16. construct a 3 by 3 example that has 9 different coefficients on the left-hand side, but rows 2 and 3 become zero in elimination. how many solutions to your system with b=(1,10,100) and how many with b=(0,0,0)?",matrices and gaussian elimination
17. which number q makes this system singular and which right-hand side t gives it infinitely many solutions? find the solution that has z=1. x + 4y − 2z = 1 x + 7y − 6z = 6 3y + qz = t.,matrices and gaussian elimination
"18. it is impossible for a system of linear equations to have exactly two solutions. explain why. (a) if (x,y,z) and (x,y,z) are two solutions, what is another one? (b) if 25 planes meet at two points, where else do they meet?",matrices and gaussian elimination
"19. three planes can fail to have an intersection point, when no two planes are parallel. the system is singular if row 3 of a is a of the first two rows. find a third equation that can’t be solved if x+y+z=0 and x−2y−z=1. problems 20–22 move up to 4 by 4 and n by n.",matrices and gaussian elimination
20. find the pivots and the solution for these four equations: 2x + y = 0 x + 2y + z = 0 y + 2z + t = 0 z + 2t = 5.,matrices and gaussian elimination
"21. if you extend problem 20 following the 1, 2, 1 pattern or the −1, 2, −1 pattern, what is the fifth pivot? what is the nth pivot?",matrices and gaussian elimination
22. apply elimination and back-substitution to solve 2u + 3v = 0 4u + 5v + w = 3 2u − v − 3w = 5. what are the pivots? list the three operations in which a multiple of one row is subtracted from another.,matrices and gaussian elimination
"23. for the system u + v + w = 2 u + 3v + 3w = 0 u + 3v + 5w = 2, what is the triangular system after forward elimination, and what is the solution?",matrices and gaussian elimination
"24. solve the system and find the pivots when 2u − v = 0 −u + 2v − w = 0 − v + 2w − z = 0 − w + 2z = 5. you may carry the right-hand side as a fifth column (and omit writing u, v, w, z until the solution at the end).",matrices and gaussian elimination
"25. apply elimination to the system u + v + w = −2 3u + 3v − w = 6 u − v + w = −1. when a zero arises in the pivot position, exchange that equation for the one below it and proceed. what coefficient of v in the third equation, in place of the present −1, would make it impossible to proceed—and force elimination to break down?",matrices and gaussian elimination
"26. solve by elimination the system of two equations x − y = 0 3x + 6y = 18. draw a graph representing each equation as a straight line in the x-y plane; the lines intersect at the solution. also, add one more line—the graph of the new second equation which arises after elimination.",matrices and gaussian elimination
"27. find three values of a for which elimination breaks down, temporarily or perma- nently, in au + u = 1 4u + av = 2. breakdown at the first step can be fixed by exchanging rows—but not breakdown at the last step.",matrices and gaussian elimination
"28. true or false: (a) if the third equation starts with a zero coefficient (it begins with 0u) then no multiple of equation 1 will be subtracted from equation 3. (b) if the third equation has zero as its second coefficient (it contains 0v) then no multiple of equation 2 will be subtracted from equation (c) if the third equation contains 0u and 0v, then no multiple of equation 1 or equa- tion 2 will be subtracted from equation 3.",matrices and gaussian elimination
"29. normally the multiplication of two complex numbers (a+ib)(c+id)=(ac−bd)+i(bc+ad) involves the four separate multiplications ac, bd, be, ad. ignoring i, can you compute ac−bd and bc+ad with only three multiplications? (you may do additions, such as forming a+b before multiplying, without any penalty.)",matrices and gaussian elimination
30. use elimination to solve u + v + w = 6 u + v + w = 7 u + 2v + 2w = 11 and u + 2v + 2w = 10 2u + 3v − 4w = 3 2u + 3v − 4w = 3.,matrices and gaussian elimination
31. for which three numbers a will elimination fail to give three pivots? ax+2y+3z=b 1 ax+ay+4z=b 2 ax+ay+az=b . 3,matrices and gaussian elimination
"32. find experimentally the average size (absolute value) of the first and second and third pivots for matlab ’slu(rand(3,3)). the average of the first pivot from abs(a(1,1)) should be 0.5.",matrices and gaussian elimination
"4. if an m by n matrix a multiplies an n-dimensional vector x, how many separate multiplications are involved? what if a multiplies an n by p matrix b?",matrices and gaussian elimination
6. write down the 2 by 2 matrices a and b that have entries a = i + j and b = (−1) i + j. ij ij multiply them to find ab and ba.,matrices and gaussian elimination
7. give 3 by 3 examples (not just the zero matrix) of (a) a diagonal matrix: a=0 if i= j. ij (b) a symmetric matrix: a =a for all i and j. ij ji (c) an upper triangular matrix: a =0 if i> j. ij (d) a skew-symmetric matrix: a =−a for all i and j. ij ji,matrices and gaussian elimination
"8. do these subroutines multiply ax by rows or columns? start with b(i)=0: do 10 i = 1, n do 10 j = 1, n do 10 j = 1, n do 10 i = 1, n 10 b(i) = b(i) + a(i,j) * x(j) 10 b(i) = b(i) + a(i,j) * x(j) the outputs bx = ax are the same. the second code is slightly more efficient in fortran and much more efficient on a vector machine (the first changes single entries b(i), the second can update whole vectors).",matrices and gaussian elimination
"9. if the entries of a are a , use subscript notation to write ij (a) the first pivot. (b) the multiplier  of row 1 to be subtracted from row i. i1 (c) the new entry that replaces a after that subtraction. ij (d) the second pivot.",matrices and gaussian elimination
"10. true or false? give a specific counterexample when false. (a) if columns 1 and 3 of b are the same, so are columns 1 and 3 of ab. (b) if rows 1 and 3 of b are the same, so are rows 1 and 3 of ab. (c) if rows 1 and 3 of a are the same, so are rows 1 and 3 of ab. (d) (ab)2 =a2b2.",matrices and gaussian elimination
"12. the product of two lower triangular matrices is again lower triangular (all its entries above the main diagonal are zero). confirm this with a 3 by 3 example, and then explain how it follows from the laws of matrix multiplication.",matrices and gaussian elimination
"13. by trial and error find examples of 2 by 2 matrices such that (a) a2 =−i, a having only real entries. (b) b2 =0, although b=0. (c) cd=−dc, not allowing the casecd=0. (d) ef =0, although no entries of e or f are zero.",matrices and gaussian elimination
14. describe the rows of ea and the columns of ae if 1 7 e = . 0 1,matrices and gaussian elimination
"15. suppose a commutes with every 2 by 2 matrix (ab=ba), and in particular a b 1 0 0 1 a= commutes with b = and b = . 1 2 c d 0 0 0 0 show that a = d and b = c = 0. if ab = ba for all matrices b, then a is a multiple of the identity.",matrices and gaussian elimination
"16. let x be the column vector (1,0,...,0). show that the rule (ab)x=a(bx) forces the first column of ab to equal a times the first column of b.",matrices and gaussian elimination
"17. which of the following matrices are guaranteed to equal (a+b)2? a2+2ab+b2, a(a+b)+b(a+b), (a+b)(b+a), a2+ab+ba+b2.",matrices and gaussian elimination
"18. if a and b are n by n matrices with all entries equal to 1, find (ab) . summation ij notation turns the product ab, and the law (ab)c =a(bc), into (ab) =∑a b ∑ ∑a b c =∑a ∑b c . ij ik kj ik kj jl ik kj jl k j k k j compute both sides ifc is also n by n, with every c =2. jl",matrices and gaussian elimination
19. a fourth way to multiply matrices is columns of a times rows of b: ab=(column 1)(row 1)+···+(column n)(row n)=sum of simple matrices. give a 2 by 2 example of this important rule for matrix multiplication.,matrices and gaussian elimination
20. the matrix that rotates the x-y plane by an angleθis cosθ −sinθ a(θ)= . sinθ cosθ verifythata(θ )a(θ )=a(θ +θ )fromtheidentitiesforcos(θ +θ )andsin(θ + 1 2 1 2 1 2 1 θ ). what is a(θ) times a(−θ)? 2,matrices and gaussian elimination
"21. find the powers a2, a3 (a2 times a), and b2, b3,c2,c3. what are ak, bk, andck? 1 1 1 0 1 −1 a= 2 2 and b= and c =ab= 2 2 1 1 0 −1 1 −1 2 2 2 2 problems 22–31 are about elimination matrices.",matrices and gaussian elimination
"22. write down the 3 by 3 matrices that produce these elimination steps: (a) e subtracts 5 times row 1 from row 2. 21 (b) e subtracts −7 times row 2 from row 3. 32 (c) p exchanges rows 1 and 2, then rows 2 and 3.",matrices and gaussian elimination
"23. inproblem22,applyinge andthene tothecolumnb=(1,0,0)givese e b= 21 32 32 21 . applying e before e gives e e b = . when e comes first, row 32 21 21 32 32 feels no effect from row .",matrices and gaussian elimination
"25. suppose a = 7 and the third pivot is 5. if you change a to 11, the third pivot is 33 33 . if you change a to , there is zero in the pivot position. 33",matrices and gaussian elimination
"26. if every column of a is a multiple of (1,1,1), then a is always a multiple of (1,1,1). do a 3 by 3 example. how many pivots are produced by elimination?",matrices and gaussian elimination
"27. what matrix e subtracts 7 times row 1 from row 3? to reverse that step, r should 31 31 7 times row to row . multiply e by r . 31 31",matrices and gaussian elimination
28. (a) e subtracts row 1 from row 2 and then p exchanges rows 2 and 3. what 21 23 matrix m =p e does both steps at once? 23 21 (b) p exchanges rows 2 and 3 and then e subtracts row i from row 3. what 23 31 matrix m = e p does both steps at once? explain why the m’s are the same 31 23 but the e’s are different.,matrices and gaussian elimination
29. (a) what 3 by 3 matrix e will add row 3 to row 1? 13 (b) what matrix adds row 1 to row 3 and at the same time adds row 3 to row 1? (c) what matrix adds row 1 to row 3 and then adds row 3 to row 1?,matrices and gaussian elimination
"32. write these ancient problems in a 2 by 2 matrix form ax=b and solve them: (a) x is twice as old asy and their ages add to 39, (b) (x,y)=(2,5) and (3,7) lie on the line y=mx+c. find m and c.",matrices and gaussian elimination
"33. the parabola y = a+bx+cx2 goes through the points (x,y) = (1,4) and (2,8) and (3,14). find and solve a matrix equation for the unknowns (a,b,c).",matrices and gaussian elimination
"35. (a) suppose all columns of b are the same. then all columns of eb are the same, because each one is e times . (b) suppose all rows of b are [1 2 4]. show by example that all rows of eb are not [1 2 4]. it is true that those rows are .",matrices and gaussian elimination
"36. if e adds row 1 to row 2 and f adds row 2 to row 1, does ef equal fe?",matrices and gaussian elimination
"37. the first component of ax is ∑a x = a x +···+a x . write formulas for the 1j j 11 1 1n n third component of ax and the (1,1) entry of a2.",matrices and gaussian elimination
"38. if ab=i and bc =i, use the associative law to prove a=c.",matrices and gaussian elimination
"39. a is 3 by 5, b is 5 by 3, c is 5 by 1, and d is 3 by 1. all entries are 1. which of these matrix operations are allowed, and what are the results? ba ab abd dba a(b+c).",matrices and gaussian elimination
"40. what rows or columns or matrices do you multiply to find (a) the third column of ab? (b) the first row of ab? (c) the entry in row 3, column 4 of ab? (d) the entry in row 1, column 1 ofcde?",matrices and gaussian elimination
"41. (3 by 3 matrices) choose the only b so that for every matrix a, (a) ba=4a. (b) ba=4b. (c) ba has rows 1 and 3 of a reversed and row 2 unchanged. (d) all rows of ba are the same as row 1 of a.",matrices and gaussian elimination
42. true or false? (a) if a2 is defined then a is necessarily square. (b) if ab and ba are defined then a and b are square. (c) if ab and ba are defined then ab and ba are square. (d) if ab=b then a=i.,matrices and gaussian elimination
"43. if a is m by n, how many separate multiplications are involved when (a) a multiplies a vector x with n components? (b) a multiplies an n by p matrix b? then ab is m by p. (c) a multiplies itself to produce a2? here m=n.",matrices and gaussian elimination
"44. to prove that (ab)c =a(bc), use the column vectors b ,...,b of b. first suppose 1 n thatc has only one column c with entries c ,...,c : 1 n ab has columns ab ,...,ab , and bc has one column c b +···+c b . 1 n 1 1 n n then (ab)c=c ab +···+c ab , equals a(c b +···+c b )=a(bc). 1 1 n n 1 1 n n linearity gives equality of those two sums, and (ab)c=a(bc). the same is true for all other ofc. therefore (ab)c =a(bc). problems 45–49 use column-row multiplication and block multiplication.",matrices and gaussian elimination
47. draw the cuts in a and b and ab to show how each of the four multiplication rules is really a block multiplication to find ab: (a) matrix a times columns of b. (b) rows of a times matrix b. (c) rows of a times columns of b. (d) columns of a times rows of b.,matrices and gaussian elimination
48. block multiplication says that elimination on column 1 produces  1 0 a b a b ea= = . −c/a i c d 0,matrices and gaussian elimination
"49. elimination for a 2 by 2 block matrix: when a−1a =i, multiply the first block row byca−1 and subtract from the second row, to find the “schur complement” s:  i 0 a b a b = . −ca−1 i c d 0 s",matrices and gaussian elimination
"50. with i2 = −1, the product (a+ib)(x+iy) is ax+ibx+iay−by. use blocks to separate the real part from the imaginary part that multiplies i:  a −b x ax−by real part = ? ? y ? imaginary part",matrices and gaussian elimination
"52. if the three solutions in question 51 are x = (1,1,1) and x = (0,1,1) and x = 1 2 3 (0,0,1), solve ax=b when b=(3,5,8). challenge problem: what is a?",matrices and gaussian elimination
53. find all matrices a b 1 1 1 1 a= that satisfy a = a. c d 1 1 1 1,matrices and gaussian elimination
"54. if you multiply a northwest matrix a and a southeast matrix b, what type of matrices are ab and ba? “northwest” and “southeast” mean zeros below and above the antidiagonal going from (1,n) to (n,1).",matrices and gaussian elimination
"55. write 2x+3y+z+5t =8 as a matrix a (how many rows?) multiplying the column vector (x,y,z,t) to produce b. the solutions fill a plane in four-dimensional space. the plane is three-dimensional with no 4d volume.",matrices and gaussian elimination
"56. what 2 by 2 matrix p projects the vector (x,y) onto the x axis to produce (x,0)? 1 what matrix p projects onto the y axis to produce (0,y)? if you multiply (5,7) by 2 p and then multiply by p , you get ( ) and ( ). 1 2",matrices and gaussian elimination
"57. write the inner product of (1,4,5) and (x,y,z) as a matrix multiplication ax. a has onerow. the solutions to ax=0 lie on a plane perpendicular to the vector. the columns of a are only in -dimensional space.",matrices and gaussian elimination
"58. in matlab notation, write the commands that define the matrix a and the column vectors x and b. what command would test whether or not ax=b? 1 2 5 1 a= x= b= 3 4 −2 7",matrices and gaussian elimination
"59. the matlab commands a = eye(3) and v = [3:5]’ produce the 3 by 3 identity matrix and the column vector (3,4,5). what are the outputs from a ∗ v and v’ ∗ v? (computer not needed!) if you ask for v ∗ a, what happens?",matrices and gaussian elimination
"60. if you multiply the 4 by 4 all-ones matrix a = ones(4,4) and the column v = ones(4,1), what is a ∗ v? (computer not needed.) if you multiply b = eye(4) + ones(4,4) times w = zeros(4,1) + 2 ∗ ones(4,1), what is b ∗ w?",matrices and gaussian elimination
"61. invent a 3 by 3 magic matrix m with entries 1,2,...,9. all rows and columns and diagonals add to 15. the first row could be 8, 3, 4. what is m times (1,1,1)? what is the row vector 1 1 1 times m?",matrices and gaussian elimination
2. solve (from l and u and b find the solution x). the separation into factor and solve means that a series of b’s can be processed.,matrices and gaussian elimination
1. when is an upper triangular matrix nonsingular (a full set of pivots)?,matrices and gaussian elimination
4. apply elimination to produce the factors l andu for ... a= and a=[1 3 1] and a=[1 4 4]. 8 7 1 1 3 1 4 8,matrices and gaussian elimination
"5. factor a into lu, and write down the upper triangular systemux=c which appears after elimination, for  2 3 3 u 2 ax=[0 5 7] [v]=[2]. 6 9 8 w 5",matrices and gaussian elimination
6. find e2 and e8 and e−1 if 1 0 e = . 6 1,matrices and gaussian elimination
7. find the products fgh and hgf if (with upper triangular zeros omitted) [ ] [ ] [ ] 1 1 1 [ ] [ ] [ ] [2 1 ] [0 1 ] [0 1 ] f =[ ] g=[ ] h =[ ]. [0 0 1 ] [0 2 1 ] [0 0 1 ] 0 0 0 1 0 0 0 1 0 0 2 1,matrices and gaussian elimination
8. (second proof of a = lu) the third row of u comes from the third row of a by subtracting multiples of rows 1 and 2 (of u!): row 3 ofu =row 3 of a− (row 1 ofu)− (row 2 ofu). 31 32 (a) why are rows of u subtracted off and not rows of a? (b) the equation above is the same as row 3 of a= (row 1 ofu)+ (row 2 ofu)+1(row 3 ofu). 31 32 which rule for matrix multiplication makes this row 3 of l timesu? the other rows of lu agree similarly with the rows of a.,matrices and gaussian elimination
9. (a) under what conditions is the following product nonsingular? [ ][ ][ ] 1 0 0 d 1 −1 0 1 [ ][ ][ ] a=[−1 1 0][ d ][0 1 −1]. 2 0 −1 1 d 0 0 1 3 (b) solve the system ax=b starting with lc=b: [ ][ ] [ ] 1 0 0 c 0 1 [ ][ ] [ ] [−1 1 0][c ]=[0]=b. 2 0 −1 1 c 1 3,matrices and gaussian elimination
10. (a) why does it take approximately n2/2 multiplication-subtraction steps to solve each of lc=b andux=c? (b) howmanystepsdoeseliminationuseinsolving10systemswith thesame60by 60 coefficient matrix a?,matrices and gaussian elimination
"11. solve as two triangular systems, without multiplying lu to find a: [ ][ ][ ] [ ] 1 0 0 2 4 4 u 2 [ ][ ][ ] [ ] lux=[1 1 0][0 1 2][v]=[0]. 1 0 1 0 0 1 w 2",matrices and gaussian elimination
"12. how could you factor a into a productul, upper triangular times lower triangular? would they be the same factors as in a=lu?",matrices and gaussian elimination
"13. solve by elimination, exchanging rows when necessary: u + 4v + 2w = −2 v + w = 0 −2u − 8v + 3w = 32 and u + v = 0 v + w = 1 u + v + w = 1. which permutation matrices are required?",matrices and gaussian elimination
"14. writedownallsixofthe3by3permutationmatrices,includingp=i. identifytheir inverses,whicharealsopermutationmatrices. theinversessatisfypp−1 =i andare on the same list.",matrices and gaussian elimination
15. find the pa=ldu factorizations (and check them) for [ ] [ ] 0 1 1 1 2 1 [ ] [ ] a=[1 0 1] and a=[2 4 2]. 2 3 4 1 1 1,matrices and gaussian elimination
16. find a 4 by 4 permutation matrix that requires three row exchanges to reach the end of elimination (which isu =i).,matrices and gaussian elimination
"17. the less familiar form a=lpu exchanges rows only at the end: [ ] [ ] [ ][ ] 1 1 1 1 1 1 1 0 0 1 1 1 [ ] [ ] [ ][ ] a=[1 1 3]→l−1a=[0 0 2]=pu =[0 0 1][0 3 6]. 2 5 8 0 3 6 0 1 0 0 0 2 whatisl isthiscase? comparingwith pa=lu inbox1j,themultipliers nowstay in place ( is 1 and  is 2 when a=lpu). 21 31",matrices and gaussian elimination
"18. decide whether the following systems are singular or nonsingular, and whether they have no solution, one solution, or infinitely many solutions: v − w = 2 v − w = 0 v + w = 1 u − v = 2 and u − v = 0 and u + v = 1 u − w = 2 u − w = 0 u + w = 1.",matrices and gaussian elimination
1.5 triangularfactorsandrowexchanges 47,matrices and gaussian elimination
"19. which numbers a, b, c lead to row exchanges? which make the matrix singular? [ ] 1 2 0 [ ] c 2 a=[a 8 3] and a= . 6 4 0 b 5 problems 20–31 compute the factorization a=lu (and also a=ldu).    ",matrices and gaussian elimination
"20. forward elimination changes 1 1 x=b to a triangular 1 1 x=c: 1 2 0 1 x + y = 5 x + y = 5 1 1 5 1 1 5 → → . x + 2y = 7 y = 2 1 2 7 0 1 2 that step subtracted  = times row 1 from row 2. the reverse step adds  21 21 times row 1 to row 2. the matrix for that reverse step is l = . multiply this l     times the triangular system 1 1 x= 5 to get = . in letters, l multiplies 0 1 2 ux=c to give .",matrices and gaussian elimination
21. (move to 3 by 3) forward elimination changes ax=b to a triangularux=c: x+y+z=5 x+y+z=5 x+y+z=5 x+2y+3z=7 y+2z=2 y+2z=2 x+3y+6z=11 2y+5z=6 z=2. the equation z=2 inux=c comes from the original x+3y+6z=11 in ax=b by subtracting  = times equation 1 and  = times the final equation 2. 31 32 reverse that to recover [1 3 6 11] in [a b] from the final [1 1 1 5] and [0 1 2 2] and [0 0 1 2] in [u c]:     row 3 of a b =( row 1 + row 2+1 row 3) of u c . 31 32 in matrix notation this is multiplication by l. so a=lu and b=lc.,matrices and gaussian elimination
"22. what are the 3 by 3 triangular systems lc=b andux=c from problem 21? check that c=(5,2,2) solves the first one. which x solves the second one?",matrices and gaussian elimination
23. whattwoeliminationmatricese ande putaintouppertriangularforme e a= 21 32 32 21 u? multiply by e−1 and e−1 to factor a into lu =e−1e−1u: 31 21 21 32 [ ] 1 1 1 [ ] a=[2 4 5]. 0 4 0,matrices and gaussian elimination
"24. what three elimination matrices e , e , e put a into upper triangular form 21 31 32 e e e a =u? multiply by e−1, e−1 and e−1 to factor a into lu where l = 32 31 21 32 31 21 e−1e−1e−1. find l andu: 21 31 32 [ ] 1 0 1 [ ] a=[2 2 2]. 3 4 5",matrices and gaussian elimination
"25. when zero appears in a pivot position, a = lu is not possible! (we need nonzero pivots d, f, i inu.) show directly why these are both impossible: [ ] [ ][ ] 1 1 0 1 d e g 0 1 1 0 d e [ ] [ ][ ] = [1 1 2]=[ 1 ][ f h]. 2 3  1 0 f 1 2 1 m n 1 i",matrices and gaussian elimination
26. which number c leads to zero in the second pivot position? a row exchange is neededanda=lu isnotpossible. whichcproduceszerointhethirdpivotposition? then a row exchange can’t help and elimination fails: [ ] 1 c 0 [ ] a=[2 4 1]. 3 5 1,matrices and gaussian elimination
27. what are l and d for this matrix a? what isu in a=lu and what is the newu in a=ldu? [ ] 2 4 8 [ ] a=[0 3 9]. 0 0 7,matrices and gaussian elimination
28. a and b are symmetric across the diagonal (because 4 =4). find their triple factor- izations ldu and say howu is related to l for these symmetric matrices: [ ] 1 4 0 2 4 [ ] a= and b=[4 12 4]. 4 11 0 4 0,matrices and gaussian elimination
"29.  compute l andu for the symmetric matrix [ ] a a a a [ ] [a b b b] a=[ ]. [a b c c] a b c d find four conditions on a, b, c, d to get a=lu with four pivots.",matrices and gaussian elimination
"30. find l andu for the nonsymmetric matrix [ ] a r r r [ ] [a b s s] a=[ ]. [a b c t] a b c d find the four conditions on a, b, c, d, r, s,t to get a=lu with four pivots.",matrices and gaussian elimination
1.5 triangularfactorsandrowexchanges 49,matrices and gaussian elimination
31. tridiagonal matrices have zero entries except on the main diagonal and the two adjacent diagonals. factor these into a=lu and a=ldv: [ ] [ ] 1 1 0 a a 0 [ ] [ ] a=[1 2 1] and a=[a a+b b ]. 0 1 2 0 b b+c,matrices and gaussian elimination
32. solve the triangular system lc=b to find c. then solveux=c to find x: 1 0 2 4 2 l= and u = and b= . 4 1 0 1 11 for safety find a=lu and solve ax=b as usual. circle c when you see it.,matrices and gaussian elimination
33. solve lc=b to find c. then solveux=c to find x. what was a? [ ] [ ] [ ] 1 0 0 1 1 1 4 [ ] [ ] [ ] l=[1 1 0] and u =[0 1 1] and b=[5]. 1 1 1 0 0 1 6,matrices and gaussian elimination
"34. if a and b have nonzeros in the positions marked by x, which zeros are still zero in their factors l andu? [ ] [ ] x x x x x x x 0 [ ] [ ] [x x x 0] [x x 0 x] a=[ ] and b=[ ]. [0 x x x] [x 0 x x] 0 0 x x 0 x x x",matrices and gaussian elimination
"35.  if a has pivots 2, 7, 6 with no row exchanges, what are the pivots for the upper left 2 by 2 submatrix b (without row 3 and column 3)? explain why.",matrices and gaussian elimination
"36. starting from a 3 by 3 matrix a with pivots 2, 7, 6, add a fourth row and column to produce m. what are the first three pivots for m, and why? what fourth row and column are sure to produce 9 as the fourth pivot?",matrices and gaussian elimination
"37. use chol(pascal(5)) to find the triangular factors of matlab’s pascal(5). row exchanges in [l, u] = lu(pascal(5)) spoil pascal’s pattern!",matrices and gaussian elimination
38. for which numbers c is a=lu impossible—with three pivots? [ ] 1 2 0 [ ] a=[3 c 1]. 0 1 1,matrices and gaussian elimination
"39. estimate the time difference for each new right-hand side b when n=800. create a = rand(800) and b = rand(800,1) and b = rand(800,9). compare the times from tic; a\b; toc and tic; a\b; toc (which solves for 9 right sides). problems 40–48 are about permutation matrices.",matrices and gaussian elimination
"40. there are 12 “even” permutations of (1,2,3,4), with an even number of exchanges. two of them are (1,2,3,4) with no exchanges and (4,3,2,1) with two exchanges. list the other ten. instead of writing each 4 by 4 matrix, use the numbers 4, 3, 2, 1 to give the position of the 1 in each row.",matrices and gaussian elimination
"41. how many exchanges will permute (5,4,3,2,1) back to (1,2,3,4,5)? how many exchanges to change (6,5,4,3,2,1) to (1,2,3,4,5,6)? one is even and the other is odd. for (n,...,1) to (1,...,n), show that n = 100 and 101 are even, n = 102 and 103 are odd.",matrices and gaussian elimination
"42. if p and p are permutation matrices, so is p p . this still has the rows of i in some 1 2 1 2 order. give examples with p p =p p and p p =p p . 1 2 2 1 3 4 4 3",matrices and gaussian elimination
43. (try this question.) which permutation makes pa upper triangular? which permu- tations make p ap lower triangular? multiplying a on the right by p exchanges 1 2 2 the of a. [ ] 0 0 6 [ ] a=[1 2 3] 0 4 5,matrices and gaussian elimination
44. find a 3 by 3 permutation matrix with p3 =i (but not p=i). find a 4 by 4 permu- tation p with p 4 =i.,matrices and gaussian elimination
"45. if you take powers of a permutation, why is some pk eventually equal to i? find a 5 by 5 permutation p so that the smallest power to equal i is p6. (this is a challenge question. combine a 2 by 2 block with a 3 by 3 block.)",matrices and gaussian elimination
"46. the matrix p that multiplies (x,y,z) to give (z,x,y) is also a rotation matrix. find p and p3. the rotation axis a=(1,1,1) doesn’t move, it equals pa. what is the angle of rotation from v=(2,3,−5) to pv=(−5,2,3)?",matrices and gaussian elimination
"47. if p is any permutation matrix, find a nonzero vector x so that (i−p)x = 0. (this will mean that i−p has no inverse, and has determinant zero.)",matrices and gaussian elimination
"48. if p has 1s on the antidiagonal from (1,n) to (n,1), describe pap.",matrices and gaussian elimination
"1. find the inverses (no special system required) of 0 2 2 0 cosθ −sinθ a = , a = , a = . 1 2 3 3 0 4 2 sinθ cosθ",matrices and gaussian elimination
2. (a) find the inverses of the permutation matrices [ ] [ ] 0 0 1 0 0 1 [ ] [ ] p=[0 1 0] and p=[1 0 0]. 1 0 0 0 1 0 (b) explain for permutations why p−1 is always the same as pt. show that the 1s are in the right places to give ppt =i.,matrices and gaussian elimination
3. from ab=c find a formula for a−1. also find a−1 from pa=lu.,matrices and gaussian elimination
"4. (a) if a is invertible and ab=ac, prove quickly that b=c. (b) if a=[1 0], find an example with ab=ac but b=c. 0 0",matrices and gaussian elimination
"5. if the inverse of a2 is b, show that the inverse of a is ab. (thus a is invertible whenever a2 is invertible.)",matrices and gaussian elimination
"6. use the gauss-jordan method to invert [ ] [ ] [ ] 1 0 0 2 −1 0 0 0 1 [ ] [ ] [ ] a =[1 1 1], a =[−1 2 −1], a =[0 1 1]. 1 2 3 0 0 1 0 −1 2 1 1 1",matrices and gaussian elimination
"7. find three 2 by 2 matrices, other than a=i and a=−i, that are their own inverses: a2 =i.",matrices and gaussian elimination
"8. show that a=[1 1] has no inverse by solving ax=0, and by failing to solve 3 3  1 1 a b 1 0 = . 3 3 c d 0 1",matrices and gaussian elimination
9. suppose elimination fails because there is no pivot in column 3: [ ] 2 1 4 6 [ ] [0 3 8 5] missing pivot a=[ ]. [0 0 0 7] 0 0 0 9,matrices and gaussian elimination
"show that a cannot be invertible. the third row of a−1, multiplying a, should give the third row [0 0 1 0] of a−1 a=i. why is this impossible?",matrices and gaussian elimination
"10. find the inverses (in any legal way) of [ ] [ ] [ ] 0 0 0 1 1 0 0 0 a b 0 0 [ ] [ ] [ ] [0 0 2 0] [−1 1 0 0] [c d 0 0] a =[ ], a =[ 2 ], a =[ ]. 1 [0 3 0 0] 2 [ 0 −2 1 0] 3 [0 0 a b] 3 4 0 0 0 0 0 −3 1 0 0 c d 4",matrices and gaussian elimination
"11. give examples of a and b such that (a) a+b is not invertible although a and b are invertible. (b) a+b is invertible although a and b are not invertible. (c) all of a, b, and a+b are invertible. (d) in the last case use a−1(a+b)b−1 =b−1+a−1 to show thatc =b−1+a−1 is also invertible—and find a formula for c−1.",matrices and gaussian elimination
"12. if a is invertible, which properties of a remain true for a−1? (a) a is triangular. (b) a is symmetric. (c) a is tridiagonal. (d) all entries are whole numbers. (e) all entries are fractions (including numbers like 3).",matrices and gaussian elimination
"13. if a=[3] and b=[2], compute atb, bta, abt, and bat.",matrices and gaussian elimination
"14. if b is square, show that a=b+bt is always symmetric and k =b−bt is always skew-symmetric—which means that kt = −k. find these matrices a and k when b = [1 3], and write b as the sum of a symmetric matrix and a skew-symmetric 1 1 matrix.",matrices and gaussian elimination
15. (a) how many entries can be chosen independently in a symmetric matrix of order n? (b) how many entries can be chosen independently in a skew-symmetric matrix (kt =−k) of order n? the diagonal of k is zero!,matrices and gaussian elimination
"16. (a) if a = ldu, with 1s on the diagonals of l and u, what is the corresponding factorization of at? note that a and at (square matrices with no row exchanges) share the same pivots. (b) what triangular systems will give the solution to aty=b?",matrices and gaussian elimination
"17. if a=l d u and a=l d u , prove that l =l , d =d , andu =u . if a is 1 1 1 2 2 2 1 2 1 2 1 2 invertible, the factorization is unique. (a) derive the equation l−1l d = d u u−1, and explain why one side is lower 1 2 2 1 1 2 triangular and the other side is upper triangular. (b) compare the main diagonals and then compare the off-diagonals.",matrices and gaussian elimination
18. under what conditions on their entries are a and b invertible? [ ] [ ] a b c a b 0 [ ] [ ] a=[d e 0] b=[c d 0]. f 0 0 0 0 e,matrices and gaussian elimination
19. compute the symmetric ldlt factorization of [ ] 1 3 5 [ ] a b a=[3 12 18] and a= . b d 5 18 30,matrices and gaussian elimination
20. find the inverse of [ ] 1 0 0 0 [ ] [1 1 0 0] a=[4 ]. [1 1 1 0] 3 3 1 1 1 1 2 2 2,matrices and gaussian elimination
"21. if a and b are square matrices, show that i−ba is invertible if i−ab is invertible. start from b(i−ab)=(1−ba)b.",matrices and gaussian elimination
"22. find the inverses (directly or from the 2 by 2 formula) of a, b,c: 0 3 a b 3 4 a= and b= and c = . 4 6 b 0 5 7 x t",matrices and gaussian elimination
23. solve for the columns of a−1 = : y z   10 20 x 1 10 20 t 0 = and = . 20 50 y 0 20 50 z 1,matrices and gaussian elimination
"24. show that [1 2] has no inverse by trying to solve for the column (x,y): 3 6   1 2 x t 1 0 1 2 x 1 = must include = . 3 6 y z 0 1 3 6 y 0",matrices and gaussian elimination
"25.  if a has row 1 + row 2 = row 3, show that a is not invertible: (a) explain why ax=(1,0,0) cannot have a solution. (b) which right-hand sides (b ,b ,b ) might allow a solution to ax=b? 1 2 3 (c) what happens to row 3 in elimination?",matrices and gaussian elimination
"26. if a has column 1 + column 2 = column 3, show that a is not invertible:",matrices and gaussian elimination
(a) find a nonzero solution x to ax=0. the matrix is 3 by 3. (b) elimination keeps column 1 + column 2 = column 3. explain why there is no third pivot.,matrices and gaussian elimination
27. suppose a is invertible and you exchange its first two rows to reach b. is the new matrix b invertible? how would you find b−1 from a−1?,matrices and gaussian elimination
"28. if the product m = abc of three square matrices is invertible, then a, b, c are invertible. find a formula for b−1 that involves m−1 and a andc.",matrices and gaussian elimination
29. prove that a matrix with a column of zeros cannot have an inverse.,matrices and gaussian elimination
30. multiply [a b] times [ d −b]. what is the inverse of each matrix if ad =bc? c d −c a,matrices and gaussian elimination
"31. (a) what matrix e has the same effect as these three steps? subtract row 1 from row 2, subtract row 1 from row 3, then subtract row 2 from row 3. (b) what single matrix l has the same effect as these three reverse steps? add row 2 to row 3, add row 1 to row 3, then add row 1 to row 2.",matrices and gaussian elimination
"32. find the numbers a and b that give the inverse of 5 ∗ eye(4) − ones(4,4): [ ] [ ] −1 4 −1 −1 −1 a b b b [ ] [ ] [−1 4 −1 −1] [b a b b] [ ] =[ ]. [−1 −1 4 −1] [b b a b] −1 −1 −1 4 b b b a what are a and b in the inverse of 6 ∗ eye(5) − ones(5,5)?",matrices and gaussian elimination
"33. show that a = 4 ∗ eye(4) − ones(4,4) is not invertible: multiply a ∗ ones(4,1).",matrices and gaussian elimination
34. there are sixteen 2 by 2 matrices whose entries are 1s and 0s. how many of them are invertible?,matrices and gaussian elimination
35. change i into a−1 as you reduce a to i (by row operations):     1 3 1 0 1 4 1 0 a i = and a i = . 2 7 0 1 3 9 0 1,matrices and gaussian elimination
36. follow the 3 by 3 text example but with plus signs in a. eliminate above and below the pivots to reduce [a i] to [i a−1]: [ ] 2 1 0 1 0 0   [ ] a i =[1 2 1 0 1 0]. 0 1 2 0 0 1,matrices and gaussian elimination
37. use gauss-jordan elimination on [a i] to solve aa−1 =i: [ ] [ ] 1 a b 1 0 0   [ ] [ ] [0 1 c] x x x =[0 1 0]. 1 2 3 0 0 1 0 0 1,matrices and gaussian elimination
38. invert these matrices a by the gauss-jordan method starting with [a i]: [ ] [ ] 1 0 0 1 1 1 [ ] [ ] a=[2 1 3] and a=[1 2 2]. 0 0 1 1 2 3,matrices and gaussian elimination
39. exchange rows and continue with gauss-jordan to find a−1:   0 2 1 0 a i = . 2 2 0 1,matrices and gaussian elimination
40. true or false (with a counter example if false and a reason if true): (a) a 4 by 4 matrix with a row of zeros is not invertible. (b) a matrix with is down the main diagonal is invertible. (c) if a is invertible then a−1 is invertible. (d) if at is invertible then a is invertible.,matrices and gaussian elimination
"41. for which three numbers c is this matrix not invertible, and why not? [ ] 2 c c [ ] a=[c c c]. 8 7 c",matrices and gaussian elimination
42. prove that a is invertible if a=0 and a=b (find the pivots and a−1): [ ] a b b [ ] a=[a a b]. a a a,matrices and gaussian elimination
43. this matrix has a remarkable inverse. find a−1 by elimination on [a i]. extend to a 5 by 5 “alternating matrix” and guess its inverse: [ ] 1 −1 1 −1 [ ] [0 1 −1 1 ] a=[ ]. [0 0 1 −1] 0 0 0 1,matrices and gaussian elimination
"44. if b has the columns of a in reverse order, solve (a−b)x =0 to show that a−b is not invertible. an example will lead you to x.",matrices and gaussian elimination
45. find and check the inverses (assuming they exist) of these block matrices: i 0 a 0 0 i . c i c d i d,matrices and gaussian elimination
"46. use inv(s) to invert matlab’s 4 by 4 symmetric matrix s = pascal(4). create pascal’s lower triangular a = abs(pascal(4,1)) and test inv(s) = inv(a’) ∗ inv(a).",matrices and gaussian elimination
"47. if a = ones(4,4) and b = rand(4,1), how does matlab tell you that ax = b has no solution? if b = ones(4,1), which solution to ax=b is found by a\b?",matrices and gaussian elimination
"48. m−1 shows the change in a−1 (useful to know) when a matrix is subtracted from a. check part 3 by carefully multiplying mm−1 to get i: 1. m =i−uvt and m−1 =i+uvt/(1−vtu). 2. m =a−uvt and m−1 =a−1+a−1uvta−1/(1−vta−1u). 3. m =i−uv and m−1 =i +u(i −vu)−1v. n m 4. m =a−uw−1v and m−1 =a−1+a−1u(w −va−1u)−1va−1. the four identities come from the 1, 1 block when inverting these matrices: i u a u i u a u n . vt 1 vt 1 v i v w m problems 49–55 are about the rules for transpose matrices.",matrices and gaussian elimination
49. find at and a−1 and (a−1)t and (at)−1 for 1 0 1 c a= and also a= . 9 3 c 0,matrices and gaussian elimination
"50. verify that (ab)t equals btat but those are different from atbt: 1 0 1 3 1 3 a= b= ab= . 2 1 0 1 2 7 in case ab=ba (not generally true!), how do you prove that btat =atbt? ",matrices and gaussian elimination
51. (a) the matrix (ab)−1 t comes from (a−1)t and (b−1)t. in what order? (b) ifu is upper triangular then (u−1)t is triangular.,matrices and gaussian elimination
52. show that a2 =0 is possible but ata=0 is not possible (unless a= zero matrix).,matrices and gaussian elimination
"53. (a) the row vector xt times a times the column y produces what number? [ ] 0   1 2 3 [ ] xtay= 0 1 [1]= . 4 5 6 0 (b) this is the row xta= times the column y=(0,1,0). (c) this is the row xt =[0 1] times the column ay= .",matrices and gaussian elimination
"54. when you transpose a block matrix m = [a b] the result is mt = . test it. c d under what conditions on a, b,c, d is the block matrix symmetric?",matrices and gaussian elimination
"55. explain why the inner product of x and y equals the inner product of px and py. then (px)t(py)=xty says that ptp=i for any permutation. with x=(1,2,3) and y=(1,4,2), choose p to show that (px)ty is not always equal to xt(pty).",matrices and gaussian elimination
"56. if a=at and b=bt, which of these matrices are certainly symmetric? (a) a2−b2 (b) (a+b)(a−b) (c) aba (d) abab.",matrices and gaussian elimination
"57. if a=at needs a row exchange, then it also needs a column exchange to stay sym- metric. in matrix language, pa loses the symmetry of a but recovers the symmetry.",matrices and gaussian elimination
"58. (a) how many entries of a can be chosen independently, if a=at is 5 by 5? (b) how do l and d (5 by 5) give the same number of choices in ldlt?",matrices and gaussian elimination
59. suppose r is rectangular (m by n) and a is symmetric (m by m). (a) transpose rtar to show its symmetry. what shape is this matrix? (b) show why rtr has no negative numbers on its diagonal.,matrices and gaussian elimination
60. factor these symmetric matrices into a=ldlt. the matrix d is diagonal: [ ] 2 −1 0 1 3 1 b [ ] a= and a= and a=[−1 2 −1]. 3 2 b c 0 −1 2 the next three problems are about applications of (ax)ty=xt(aty).,matrices and gaussian elimination
"61. wires go between boston, chicago, and seattle. those cities are at voltages x , x , b c x . with unit resistances between cities, the three currents are in y: s [ ] [ ][ ] y 1 −1 0 x bc b [ ] [ ][ ] y=ax is [y ]=[0 1 −1][x ]. cs c y 1 0 −1 x bs s",matrices and gaussian elimination
(a) find the total currents aty out of the three cities. (b) verify that (ax)ty agrees with xt(aty)—six terms in both.,matrices and gaussian elimination
"62. producing x trucks and x planes requires x +50x tons of steel, 40x +1000x 1 2 1 2 1 2 pounds of rubber, and 2x +50x months of labor. if the unit costs y , y , y are $700 1 2 1 2 3 per ton, $3 per pound, and $3000 per month, what are the values of one truck and one plane? those are the components of aty.",matrices and gaussian elimination
"63. ax gives the amounts of steel, rubber, and labor to produce x in problem 62. find a. then (ax)ty is the of inputs while xt(aty) is the value of .",matrices and gaussian elimination
64. here is a new factorization of a into triangular times symmetric: start from a=ldu. then a equals l(ut)−1 times utdu. why is l(ut)−1 triangular? its diagonal is all 1s. why is utdu symmetric?,matrices and gaussian elimination
"65. a group of matrices includes ab and a−1 if it includes a and b. “products and inverses stay in the group.” which of these sets are groups? lower triangular matrices l with is on the diagonal, symmetric matrices s, positive matrices m, diagonal invertible matrices d, permutation matrices p. invent two more matrix groups.",matrices and gaussian elimination
"66. if every row of a 4 by 4 matrix contains the numbers 0, 1, 2, 3 in some order, can the matrix be symmetric? can it be invertible?",matrices and gaussian elimination
67. prove that no reordering of rows and reordering of columns can transpose a typical matrix.,matrices and gaussian elimination
"68. a square northwest matrix b is zero in the southeast corner, below the antidiagonal that connects (1,n) to (n,1). will bt and b2 be northwest matrices? will b−1 be northwest or southeast? what is the shape of bc = northwest times southeast? you are allowed to combine permutations with the usual l and u (southwest and northeast).",matrices and gaussian elimination
69. compare tic; inv(a); toc for a = rand(500) and a = rand(1000). the n3 count says that computing time (measured by tic; toc) should multiply by 8 when n is doubled. do you expect these random a to be invertible?,matrices and gaussian elimination
"70. i =eye(1000); a=rand(1000); b=triu(a); produces a random triangular matrix b. compare the times for inv(b) and b\i. backslash is engineered to use the zeros in b, while inv uses the zeros in i when reducing [b i] by gauss-jordan. (compare also with inv(a) and a\i for the full matrix a.)",matrices and gaussian elimination
"71. show that l−1 has entries j/i for i≤ j (the −1, 2, −1 matrix has this l): [ ] [ ] 1 0 0 0 1 0 0 0 [ ] [ ] [−1 1 0 0] [1 1 0 0] l=[ 2 ] and l−1 =[2 ]. [ 0 −2 1 0] [1 2 1 0] 3 3 3 0 0 −3 1 1 2 3 1 4 4 4 4 test this pattern for l= eye(5) − diag(1:5)\diag(1:4,−1) and inv(l).",matrices and gaussian elimination
1. write out the ldu =ldlt factors of a in equation (6) when n=4. find the deter- minant as the product of the pivots in d.,matrices and gaussian elimination
"2. modify a in equation (6) from a =2 to a =1, and find the ldu factors of this 11 11 11 new tridiagonal matrix.",matrices and gaussian elimination
"3. find the 5 by 5 matrix a (h= 1) that approximates 0 6 d2u du du − = f(x), (0)= (1)=0, dx2 dx dx replacing these boundary conditions by u = u and u = u . check that your a 0 1 6 5 0 times the constant vector (c,c,c,c,c), yields zero; a is singular. analogously, if 0 u(x) is a solution of the continuous problem, then so is u(x)+c.",matrices and gaussian elimination
"4. write down the 3 by 3 finite-difference matrix equation (h= 1) for 4 d2u − +u=x, u(0)=u(1)=0. dx2",matrices and gaussian elimination
"5. with h= 1 and f(x)=4π2sin2πx, the difference equation (5) is 4 [ ][ ] [ ] 2 −1 0 u 1 [ ][ 1 ] π2 [ ] [−1 2 −1][u ]= [ 0 ]. 2 4 0 −1 2 u −1 3 solve for u , u , u and find their error in comparison with the true solution u = 1 2 3 sin2πx at x= 1, x= 1, and x= 3. 4 2 4",matrices and gaussian elimination
"6. what 5 by 5 system replaces (6) if the boundary conditions are changed to u(0)=1, u(1)=0? problems 7–11 are about roundoff error and row exchanges.",matrices and gaussian elimination
"7. compute h−1 in two ways for the 3 by 3 hilbert matrix [ ] 1 1 1 2 3 [ ] h =[1 1 1 ], 2 3 4 1 1 1 3 4 5 first by exact computation and second by rounding off each number to three figures. this matrix h is ill-conditioned and row exchanges don’t help.",matrices and gaussian elimination
"8. for the same matrix h, compare the right-hand sides of hx = b when the solutions are x=(1,1,1) and x=(0,6,−3.6).",matrices and gaussian elimination
"9. solve hx=b=(1,0,...,0) for the 10 by 10 hilbert matrix with h =1/(i+ j−1), ij using any computer code for linear equations. then change an entry of b by .0001 and compare the solutions.",matrices and gaussian elimination
10. compare the pivots in direct elimination to those with partial pivoting for .001 0 a= . 1 1000 (this is actually an example that needs rescaling before elimination.),matrices and gaussian elimination
"11. explain why partial pivoting produces multipliers  in l that satisfy | | ≤ 1. can ij ij you construct a 3 by 3 example with all |a | ≤ 1 whose last pivot is 4? this is the ij worst possible, since each entry is at most doubled when | |≤1. ij",matrices and gaussian elimination
1.1 (a) write down the 3 by 3 matrices with entries i a =i− j and b = . ij ij j (b) compute the products ab and ba and a2.,matrices and gaussian elimination
"1.2 for the matrices 1 0 1 2 a= and b= , 2 1 0 1 compute ab and ba and a−1 and b−1 and (ab)−1.",matrices and gaussian elimination
1.3 find examp1es of 2 by 2 matrices with a = 1 for which (a) a2 = i. (b) 12 2 a−1 =at. (c) a2 =a.,matrices and gaussian elimination
1.4 solve by elimination and back-substitution: u + w = 4 v + w = 0 u + v = 3 and u + w = 0 u + v + w = 6 u + v = 6.,matrices and gaussian elimination
1.5 factor the preceding matrices into a=lu or pa=lu.,matrices and gaussian elimination
"1.6 (a) there are sixteen 2 by 2 matrices whose entries are 1s and 0s. how many are invertible? (b) (much harder!) if you put 1s and 0s at random into the entries of a 10 by 10 matrix, is it more likely to be invertible or singular?",matrices and gaussian elimination
1.7 there are sixteen 2 by 2 matrices whose entries are 1s and −1s. how many are invertible?,matrices and gaussian elimination
1.8 how are the rows of ea related to the rows of a in the following cases? [ ] [ ] 1 0 0 0 0 1 [ ] 1 1 1 [ ] e =[0 2 0] or e = or e =[0 1 0]. 0 0 0 4 0 1 1 0 0,matrices and gaussian elimination
1.9 write down a 2 by 2 system with infinitely many solutions.,matrices and gaussian elimination
"1.10 find inverses if they exist, by inspection or by gauss-jordan: [ ] [ ] [ ] 1 0 1 2 1 0 1 1 −2 [ ] [ ] [ ] a=[1 1 0] and a=[1 2 1] and a=[ 1 −2 1 ] 0 1 1 0 1 2 −2 1 1",matrices and gaussian elimination
"1.11 if e is 2 by 2 and it adds the first equation to the second, what are e2 and e8 and 8e?",matrices and gaussian elimination
"1.12 true or false, with reason if true or counter example if false: (1) if a is invertible and its rows are in reverse order in b, then b is invertible. (2) if a and b are symmetric then ab is symmetric. (3) if a and b are invertible then ba is invertible. (4) every nonsingular matrix can be factored into the product a = lu of a lower triangular l and an upper triangularu.",matrices and gaussian elimination
"1.13 solve ax=b by solving the triangular systems lc=b and ux=c: [ ][ ] [ ] 1 0 0 2 2 4 0 [ ][ ] [ ] a=lu =[4 1 0][0 1 3], b=[0]. 1 0 1 0 0 1 1 what part of a−1 have you found, with this particular b?",matrices and gaussian elimination
"1.14 if possible, find 3 by 3 matrices b such that (1) ba=2a for every a. (2) ba=2b for every a. (3) ba has the first and last rows of a reversed. (4) ba has the first and last columns of a reversed.",matrices and gaussian elimination
1.15 find the value for c in the following n by n inverse: [ ] [ ] n −1 · −1 c 1 · 1 [ ] [ ] [−1 n · −1] 1 [1 c · 1] if a=[ ] then a−1 = [ ]. [ · · · −1] n+1[· · · 1] −1 −1 −1 n 1 1 1 c,matrices and gaussian elimination
"1.16 for which values of k does kx + y = 1 x + ky = 1 have no solution, one solution, or infinitely many solutions?",matrices and gaussian elimination
1.17 find the symmetric factorization a=ldlt of [ ] 1 2 0 [ ] a b a=[2 6 4 ] and a= . b c 0 4 11,matrices and gaussian elimination
"1.18 suppose a is the 4 by 4 identity matrix except for a vector v in column 2: [ ] 1 v 0 0 1 [ ] [0 v 0 0] 2 a=[ ]. [0 v 1 0] 3 0 v 0 1 4 (a) factor a into lu, assuming v =0. 2 (b) find a−1, which has the same form as a.",matrices and gaussian elimination
"1.19 solve by elimination, or show that there is no solution: u + v + w = 0 u + v + w = 0 u + 2v + 3w = 0 and u + u + 3w = 0 3u + 5v + 7w = 1 3u + 5v + 7w = 1.",matrices and gaussian elimination
1.20 the n by n permutation matrices are an important example of a “group.” if you multiply the m you stay inside the group;they have inverses in the group; the identity is in the group; and the law p (p p ) = (p p )p is true because it is true for all 1 2 3 1 2 3 matrices. (a) how many members belong to the groups of 4 by 4 and n by n permutation matrices? (b) find a power k so that all 3 by 3 permutation matrices satisfy pk =i.,matrices and gaussian elimination
1.21 describe the rows of da and the columns of ad if d=[2 0]. 0 5,matrices and gaussian elimination
1.22 (a) if a is invertible what is the inverse of at? (b) if a is also symmetric what is the transpose of a−1? (c) illustrate both formulas when a=[2 1]. 1 1,matrices and gaussian elimination
"1.23 by experiment with n=2 and n=3, find n n −1 2 3 2 3 2 3 , , . 0 0 0 1 0 1",matrices and gaussian elimination
"1.24 starting with a first plane u+2v−w=6, find the equation for (a) the parallel plane through the origin. (b) a second plane that also contains the points (6,0,0) and (2,2,0). (c) a third plane that meets the first and second in the point (4,1,0).",matrices and gaussian elimination
"1.25 what multiple of row 2 is subtracted from row 3 in forward elimination of a? [ ][ ] 1 0 0 1 2 0 [ ][ ] a=[2 1 0][0 1 5]. 0 5 1 0 0 1 how do you know (without multiplying those factors) that a is invertible, symmet- ric, and tridiagonal? what are its pivots?",matrices and gaussian elimination
"1.26 (a) what vector x will make ax = column 1 of a+2 (column3), for a 3 by 3 matrix a? (b) construct a matrix that has column 1 + 2(column 3) = 0. check that a is singular (fewer than 3 pivots) and explain why that must be the case.",matrices and gaussian elimination
"1.27 true or false, with reason if true and counter example if false: (1) if l u = l u (upper triangular u’s with nonzero diagonal, lower triangular 1 1 2 2 l’s with unit diagonal), then l = l and u = u . the lu factorization is 1 2 1 2 unique. (2) if a2+a=i then a−1 =a+i. (3) if all diagonal entries of a are zero, then a is singular.",matrices and gaussian elimination
"1.28 by experiment or the gauss-jordan method compute [ ] [ ] [ ] n −1 −1 1 0 0 1 0 0 1 0 0 [ ] [ ] [ ] [ 1 0] , [ 1 0] , [ 1 0] . m 0 1 m 0 1 0 m 1",matrices and gaussian elimination
1.29 write down the 2 by 2 matrices that (a) reverse the direction of every vector. (b) project every vector onto the x axis. 2 (c) turn every vector counter clockwise through 90°. (d) reflect every vector through the 45° line x =x,matrices and gaussian elimination
"1. construct a subset of the x-y plane r2 that is (a) closed under vector addition and subtraction, but not scalar multiplication. (b) closed under scalar multiplication but not under vector addition. hint: starting with u and v, add and subtract for (a). try cu and cv for (b).",vector spaces
"2. which of the following subsets of r3 are actually subspaces? (a) the plane of vectors (b ,b ,b ) with first component b =0. 1 2 3 1 (b) the plane of vectors b with b =1. 1 (c) the vectors b with b b =0 (this is the union of two subspaces, the plane b =0 2 3 2 and the plane b =0). 3 (d) all combinations of two given vectors (1,1,0) and (2,0,1). (e) the plane of vectors (b ,b ,b ) that satisfy b −b +3b =0. 1 2 3 3 2 1",vector spaces
3. describe the column space and the nullspace of the matrices 1 −1 0 0 3 0 0 0 a= and b= and c = . 0 0 1 2 3 0 0 0,vector spaces
4. what is the smallest subspace of 3 by 3 matrices that contains all symmetric matrices and all lower triangular matrices? what is the largest subspace that is contained in both of those subspaces?,vector spaces
"5. addition and scalar multiplication are required to satisfy these eight rules: 1. x+y=y+x. 2. x+(y+z)=(x+y)+z. 3. there is a unique “zero vector” such that x+0=x for all x. 4. for each x there is a unique vector −x such that x+(−x)=0. 5. 1x=x. 6. (c c )x=c (c x). 1 2 1 2 7. c(x+y)=cx+cy. 8. (c +c )x=c x+c x. 1 2 1 2 (a) suppose addition in r2 adds an extra 1 to each component, so that (3,1)+(5,0) equals(9,2)insteadof(8,1). withscalarmultiplicationunchanged, whichrules are broken? (b) showthatthesetofallpositiverealnumbers,withx+yandcxredefinedtoequal the usual xy and xc, is a vector space. what is the “zero vector”? (c) suppose (x ,x )+(y ,y ) is defined to be (x +y ,x +y ). with the usual 1 2 1 2 1 2 2 1 cx=(cx ,cx ), which of the eight conditions are not satisfied? 1 2",vector spaces
6. let p be the plane in 3-space with equation x+2y+z = 6. what is the equation of the plane p through the origin parallel to p? are p and p subspaces of r3? 0 0,vector spaces
"7. which of the following are subspaces of r∞? (a) all sequences like (1,0,1,0,...) that include infinitely many zeros. (b) all sequences (x1,x2,...) with x =0 from some point onward. j (c) all decreasing sequences: x ≤x for each j. j+1 j (d) all convergent sequences: the x have a limit as j →∞. j (e) all arithmetic progressions: x −x is the same for all j. j+1 j (f) all geometric progressions (x ,kx ,k2x ,...) allowing all k and x . 1 1 1 1",vector spaces
8. which of the following descriptions are correct? the solutions x of [ ] x 1 1 1 1 [ ] 0 ax= [x ]= 2 1 0 2 0 x 3 form (a) a plane. (b) a line. (c) a point. (d) a subspace. (e) the nullspace of a. (f) the column space of a.,vector spaces
9. show that the set of nonsingular 2 by 2 matrices is not a vector space. show also that the set of singular 2 by 2 matrices is not a vector space.,vector spaces
"10. the matrix a= 2 −2 is a “vector” in the space m of all 2 by 2 matrices. write the 2 −2 zero vector in this space, the vector 1a, and the vector −a. what matrices are in the 2 smallest subspace containing a?    ",vector spaces
"11. (a) describe a subspace of m that contains a= 1 0 but not b= 0 0 . 0 0 0 −1 (b) if a subspace of m contains a and b, must it contain i? (c) describe a subspace of m that contains no nonzero diagonal matrices.",vector spaces
12. the functions f(x)=x2 and g(x)=5x are “vectors” in the vector space f of all real functions. the combination 3f(x)−4g(x) is the function h(x) = . which rule is broken if multiplying f(x) by c gives the function f(cx)?,vector spaces
"13. if the sum of the “vectors” f(x) and g(x) in f is defined to be f(g(x)), then the “zero vector” is g(x) = x. keep the usual scalar multiplication cf(x), and find two rules that are broken.",vector spaces
"14. describe the smallest subspace of the 2 by 2 matrix space m that contains 1 0 0 1 1 0 1 0 (a) and . (b) and . 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 (c) . (d) , , . 0 0 0 0 0 1 0 1",vector spaces
"15. let p be the plane in r3 with equation x+y−2z = 4. the origin (0,0,0) is not in p! find two vectors in p and check that their sum is not in p.",vector spaces
"16. p is the plane through (0,0,0) parallel to the plane p in problem 15. what is the 0 equation for p ? find two vectors in p and check that their sum is in p . 0 0 0",vector spaces
"17. the four types of subspaces of r3 are planes, lines, r3 itself, or z containing only (0,0,0). (a) describe the three types of subspaces of r2. (b) describe the five types of subspaces of r4.",vector spaces
"18. (a) the intersection of two planes through (0,0,0) is probably a but it could be a . it can’t be the zero vector z! (b) the intersection of a plane through (0,0,0) with a line through (0,0,0) is probably a but it could be a . (c) if s and t are subspaces of r5, their intersection s∩t (vectors in both sub- spaces) is a subspace of r5. check the requirements on x+y and cx.",vector spaces
"19. suppose p is a plane through (0,0,0) and l is a line through (0,0,0). the smallest vector space containing both p and l is either or .",vector spaces
"20. true or false for m= all 3 by 3 matrices (check addition using an example)? (a) the skew-symmetric matrices in m (with at =−a) form a subspace. (b) the unsymmetric matrices in m (with at =a) form a subspace. (c) the matrices that have (1,1,1) in their nullspace form a subspace. problems 21–30 are about column spaces c(a) and the equation ax=b.",vector spaces
21. describe the column spaces (lines or planes) of these particular matrices: [ ] [ ] [ ] 1 2 1 0 1 0 [ ] [ ] [ ] a=[0 0] and b=[0 2] and c =[2 0]. 0 0 0 0 0 0,vector spaces
"22. for which right-hand sides (find a condition on b ,b ,b ) are these systems solvable? 1 2 3 [ ][ ] [ ] [ ] [ ] 1 4 2 x b 1 4 b 1 1 1 [ ][ ] [ ] [ ] x [ ] 1 (a) [ 2 8 4 ][x ]=[b ]. (b) [ 2 9 ] =[b ]. 2 2 2 x 2 −1 −4 −2 x b −1 −4 b 3 3 3",vector spaces
23. adding row 1 of a to row 2 produces b. adding column 1 to column 2 produces c. a combination of the columns of is also a combination of the columns of a. which two matrices have the same column ? 1 2 1 2 1 3 a= and b= and c = . 2 4 3 6 2 6,vector spaces
"24. for which vectors (b ,b ,b ) do these systems have a solution? 1 2 3 [ ][ ] [ ] [ ][ ] [ ] 1 1 1 x b 1 1 1 x b 1 1 1 1 [ ][ ] [ ] [ ][ ] [ ] [0 1 1][x ]=[b ] and [0 1 1][x ]=[b ]. 2 2 2 2 0 0 1 x b 0 0 0 x b 3 3 3 3",vector spaces
"25.  if we add an extra column b to a matrix a, then the column space gets larger unless . give an example in which the column space gets larger and an example in which it doesn’t. why is ax = b solvable exactly when the column space doesn’t get larger by including b?",vector spaces
26. the columns of ab are combinations of the columns of a. this means: the column space of ab is contained in (possibly equal to) the column space of a. give an example where the column spaces of a and ab are not equal.,vector spaces
"27. if a is any 8 by 8 invertible matrix, then its column space is . why?",vector spaces
"28. true or false (with a counter example if false)? (a) the vectors b that are not in the column space c(a) form a subspace. (b) if c(a) contains only the zero vector, then a is the zero matrix. (c) the column space of 2a equals the column space of a. (d) the column space of a−i equals the column space of a.",vector spaces
"29. construct a 3 by 3 matrix whose column space contains (1,1,0) and (1,0,1) but not (1,1,1). construct a 3 by 3 matrix whose column space is only a line.",vector spaces
"30. if the 9 by 12 system ax=b is solvable for every b, then c(a)= .",vector spaces
31. why isn’t r2 a subspace of r3?,vector spaces
"1. anyvectorx inthenullspacecanbeaddedtoaparticularsolutionx . thesolutions n p to all linear equations have this form, x=x +x : p n complete solution ax =b and ax =0 produce a(x +x )=b. p n p n",vector spaces
"1. after reaching rx=0, identify the pivot variables and free variables.",vector spaces
"2. give one free variable the value 1, set the other free variables to 0, and solve rx=0 for the pivot variables. this x is a special solution.",vector spaces
1. reduce ax=b to ux=c.,vector spaces
2.2 solving ax=0 and ax=b,vector spaces
"2. with free variables = 0, find a particular solution to ax=b and ux=c.",vector spaces
"3. find the special solutions to ax = 0 (or ux = 0 or rx = 0). each free variable, in turn, is 1.",vector spaces
"1. reduce [a b] to [u c], to reach a triangular system ux=c.",vector spaces
"2. find the condition on b , b , b to have a solution. 1 2 3",vector spaces
3. describe the column space of a: which plane in r3?,vector spaces
4. describe the nullspace of a: which special solutions in r4?,vector spaces
"5. find a particular solution to ax=(0,6,−6) and the complete x +x . p n",vector spaces
6. reduce [u c] to [r d]: special solutions from r and x from d. p solution.,vector spaces
"1. construct a system with more unknowns than equations, but no solution. change the right-hand side to zero and find all solutions x",vector spaces
"2. reduce a and b to echelon form, to find their ranks. which variables are free? [ ] [ ] 1 2 0 1 1 2 3 [ ] [ ] a=[0 1 1 0] b=[4 5 6]. 1 2 0 1 7 8 9 find the special solutions to ax=0 and bx=0. find all solutions.",vector spaces
"3. find the echelon form u, the free variables, and the special solutions: 0 1 0 3 b 1 a= , b= . 0 2 0 6 b 2 ax = b is consistent (has a solution) when b satisfies b = . find the complete 2 solution in the same form as equation (4).",vector spaces
"4. carry out the same steps as in the previous problem to find the complete solution of mx=b: [ ] [ ] 0 0 b 1 [ ] [ ] [1 2] [b ] 2 m =[ ], b=[ ]. [0 0] [b ] 3 3 6 b 4",vector spaces
"5. write the complete solutions x=x +x to these systems, as in equation (4): p n [ ] [ ] u u 1 2 2 [ ] 1 1 2 2 [ ] 1 [v]= [v]= . 2 4 5 4 2 4 4 4 w w",vector spaces
"6. describe the set of attainable right-hand sides b (in the column space) for [ ] [ ] 1 0 b 1 [ ] u [ ] [0 1] =[b ], 2 v 2 3 b 3 by finding the constraints on b that turn the third equation into 0=0 (after elimina- tion). what is the rank, and a particular solution?",vector spaces
"7. find the value of c that makes it possible to solve ax=b, and solve it: u + v + 2w = 2 2u + 3v − w = 5 3u + 4v + w = c.",vector spaces
"8. under what conditions on b and b (if any) does ax=b have a solution? 1 2 1 2 0 3 b 1 a= , b= . 2 4 0 7 b 2 find two vectors in the nullspace of a, and the complete solution to ax=b.",vector spaces
"9. (a) find the special solutions toux=0. reduceu to r and repeat: [ ] [ ] [ ] x 1 1 2 3 4 [ ] 0 [ ][x ] [ ] 2 ux=[0 0 1 2][ ]=[0]. [x ] 3 0 0 0 0 0 x 4 (b) if the right-hand side is changed from (0,0,0) to (a,b,0), what are all solutions?",vector spaces
10. find a 2 by 3 system ax=b whose complete solution is [ ] [ ] 1 1 [ ] [ ] x=[2]+w[3]. 0 1 solvingax=0andax=b 97 find a 3 by 3 system with these solutions exactly when b +b =b . 1 2 3,vector spaces
11. write a 2 by 2 system ax=b with many solutions x but no solution x . (therefore n p the system has no solution.) which b’s allow an x ? p,vector spaces
12. which of these rules give a correct definition of the rank of a? (a) the number of nonzero rows in r. (b) the number of columns minus the total number of rows. (c) the number of columns minus the number of free columns. (d) the number of 1s in r.,vector spaces
13. find the reduced row echelon forms r and the rank of these matrices: (a) the 3 by 4 matrix of all 1s. (b) the 4 by 4 matrix with a =(−1)ij. ij (c) the 3 by 4 matrix with a =(−1)j. ij,vector spaces
"14. find r for each of these (block) matrices, and the special solutions: [ ] 0 0 0   [ ] a a a=[0 0 3] b= a a c = . a 0 2 4 6",vector spaces
"15. if the r pivot variables come first, the reduced r must look like i f i is r by r r= 0 0 f is r by n−r what is the nullspace matrix n containing the special solutions?",vector spaces
16. suppose all r pivot variables come last. describe the four blocks in the m by n reduced echelon form (the block b should be r by r): a b r= . c d what is the nullspace matrix n of special solutions? what is its shape?,vector spaces
"17. describe all 2 by 3 matrices a and a with row echelon forms r 1 2 1 and r , such that r+r is the row echelon form of a +a . is it true that r =a 2 1 2 1 2 1 1 and r =a in this case? 2 2",vector spaces
"18. if a has r pivot columns, then at has r pivot columns. give a 3 by 3 example for which the column numbers are different for a and at.",vector spaces
19. what are the special solutions to rx=0 and rty=0 for these r? [ ] [ ] 1 0 2 3 0 1 2 [ ] [ ] r=[0 1 4 5] r=[0 0 0]. 0 0 0 0 0 0 0,vector spaces
"20. if a has rank r, then it has an r by r submatrix s that is invertible. find that submatrix s from the pivot rows and pivot columns of each a: [ ] 0 1 0 1 2 3 1 2 3 [ ] a= a= a=[0 0 0]. 1 2 4 2 4 6 0 0 1",vector spaces
21. explain why the pivot rows and pivot columns of a (not r) always give an r by r invertible submatrix of a.,vector spaces
22. find the ranks of ab and am (rank 1 matrix times rank 1 matrix): 1 2 2 1 4 1 b a= and b= and m = . 2 4 3 1.5 6 c bc,vector spaces
23. multiplying the rank 1 matrices a = uvt and b = wzt gives uzt times the number . ab has rank 1 unless =0.,vector spaces
24. every column of ab is a combination of the columns of a. then the dimensions of the column spaces give rank(ab)≤rank(a). problem: prove also that rank(ab)≤rank(b).,vector spaces
"25. suppose a and b are n by n matrices, and ab=i. prove from rank(ab)≤ rank(a) that the rank of a is n. so a is invertible and b must be its two-sided inverse. therefore ba=i.",vector spaces
"26. if a is 2 by 3 and c is 3 by 2, show from its rank that ca = i. give an example in which ac =i. for m<n, a right inverse is not a left inverse.",vector spaces
27. suppose a and b have the same reduced-row echelon form r. explain how to change a to b by elementary row operations. so b equals an matrix times a.,vector spaces
28. every m by n matrix of rank r reduces to (m by r) times (r by n): a=(pivot columns of a)(first r rows of r)=(col)(row). write the 3 by 4 matrix a at the start of this section as the product of the 3 by 2 matrix from the pivot columns and the 2 by 4 matrix from r: [ ] 1 3 3 2 [ ] a=[ 2 6 9 7] −1 −3 3 4 solvingax=0andax=b,vector spaces
29. suppose a is an m by n matrix of rank r. its reduced echelon form is r. describe exactly the reduced row echelon form of rt (not at).,vector spaces
30.  execute the six steps following equation (6) to find the column space and nullspace of a and the solution to ax=b: [ ] [ ] [ ] 2 4 6 4 b 4 1 [ ] [ ] [ ] a=[2 5 7 6] b=[b ]=[3]. 2 2 3 5 2 b 5 3,vector spaces
"31. for every c, find r and the special solutions to ax=0: [ ] 1 1 2 2 [ ] 1−c 2 a=[2 2 4 4] and a= . 0 2−c 1 c 2 2",vector spaces
"32. what is the nullspace matrix n (of special solutions) for a, b,c?     i i a= i i and b= and c = i i i . 0 0 problems 33–36 are about the solution of ax=b. follow the steps in the text to x and x . reduce the augmented matrix [a b]. p n",vector spaces
33. find the complete solutions of [ ] [ ] [ ] x x+3y+3z=1 1 3 1 2 [ ] 1 [ ][y] [ ] 2x+6y+9z=5 and [2 6 4 8][ ]=[3]. [z] −x−3y+3z=5 0 0 2 4 1 t,vector spaces
"34. under what condition on b , b , b is the following system solvable? include b as a 1 2 3 fourth column in [a b]. find all solutions when that condition holds: x+2y−2z=b 1 2x+5y−4z=b 2 4x+9y−8z=b . 3",vector spaces
"35. what conditions on b , b , b , b make each system solvable? solve for x: 1 2 3 4 [ ] [ ] [ ] [ ] [ ] 1 2 b 1 2 3 b 1 1 [ ] [ ] [ ] x 1 [ ] [2 4] x [b ] [2 4 6 ][ ] [b ] 1 2 2 [ ] =[ ] [ ][x ]=[ ]. 2 [2 5] x [b ] [2 5 7 ] [b ] 2 3 3 x 3 3 9 b 3 9 12 b 4 4",vector spaces
"36. which vectors (b ,b ,b ) are in the column space of a? which combinations of the 1 2 3 rows of a give zero? [ ] [ ] 1 2 1 1 1 1 [ ] [ ] (a) a=[2 6 3] (b) a=[1 2 4]. 0 2 5 2 4 8",vector spaces
"37. why can’t a 1 by 3 system have x =(2,4,0) and x = any multiple of (1,1,1)? p n",vector spaces
"38. (a) if ax=b has two solutions x and x , find two solutions to ax=0. 1 2 (b) then find another solution to ax=b.",vector spaces
39. explain why all these statements are false: (a) the complete solution is any linear combination of x and x . p n (b) a system ax=b has at most one particular solution. (c) the solution x with all free variables zero is the shortest solution (minimum p length (cid:107)x(cid:107)). (find a 2 by 2 counterexample.) (d) if a is invertible there is no solution x in the nullspace. n,vector spaces
"40. suppose column 5 of u has no pivot. then x is a variable. the zero vector 5 (is) (is not) the only solution to ax = 0. if ax = b has a solution, then it has solutions.",vector spaces
"41. if you know x (free variables = 0) and all special solutions for ax = b, find x and p p all special solutions for these systems:     x a b ax=2b a a =b x = . x a b",vector spaces
"42. if ax = b has infinitely many solutions, why is it impossible for ax = b (new right- hand side) to have only one solution? could ax=b have no solution?",vector spaces
"43. choose the number q so that (if possible) the ranks are (a) 1, (b) 2, (c) 3: [ ] 6 4 2 [ ] 3 1 3 a=[−3 −2 −1] and b= . q 2 q 9 6 q",vector spaces
"44. give examples of matrices a for which the number of solutions to ax=b is (a) 0 or 1, depending on b. (b) ∞, regardless of b. (c) 0 or ∞, depending on b. (d) 1, regardless of b.",vector spaces
2.2 solvingax=0andax=b 101,vector spaces
"45. write all known relations between r and m and n if ax=b has (a) no solution for some b. (b) infinitely many solutions for every b. (c) exactly one solution for some b, no solution for other b. (d) exactly one solution for every b.",vector spaces
46. apply gauss-jordan elimination (right-hand side becomes extra column) toux = 0 andux=c. reach rx=0 and rx=d:     1 2 3 0 1 2 3 5 u 0 = and u c = . 0 0 4 0 0 0 4 8 solve rx =0 to find x (its free variable is x =1). solve rx =d to find x (its free n 2 p variable is x =0). 2,vector spaces
47. apply elimination with the extra column to reach rx=0 and rx=d: [ ] [ ] 3 0 6 0 3 0 6 9     [ ] [ ] u 0 =[0 0 2 0] and u c =[0 0 2 4]. 0 0 0 0 0 0 0 5 solve rx=0 (free variable =1). what are the solutions to rx=d?,vector spaces
48. reduce toux=c (gaussian elimination) and then rx=d: [ ] [ ] [ ] x 1 1 0 2 3 [ ] 2 [ ][x ] [ ] 2 ax=[1 3 2 0][ ]=[ 5 ]=b. [x ] 3 2 0 4 9 10 x 4 find a particular solution x and all nullspace solutions x . p n,vector spaces
49. find a and b with the given property or explain why you can’t.     1 (a) the only solution to ax= 2 is x= 0 . 1 3     1 (b) the only solution to bx= 0 is x= 2 . 1 3      ,vector spaces
50. the complete solution to ax= 1 is x= 1 +c 0 . find a. 3 0 1,vector spaces
"51. the nullspace of a 3 by 4 matrix a is the line through (2,3,1,0). (a) what is the rank of a and the complete solution to ax=0? (b) what is the exact row reduced echelon form r of a?",vector spaces
"52. reduce these matrices a and b to their ordinary echelon formsu: [ ] [ ] 1 2 2 4 6 2 4 2 [ ] [ ] (a) a=[1 2 3 6 9] (b) b=[0 4 4]. 0 0 1 2 3 0 8 8 find a special solution for each free variable and describe every solution to ax = 0 and bx = 0. reduce the echelon forms u to r, and draw a box around the identity matrix in the pivot rows and pivot columns.",vector spaces
"53. true or false? (give reason if true, or counterexample to show it is false.) (a) a square matrix has no free variables. (b) an invertible matrix has no free variables. (c) an m by n matrix has no more than n pivot variables. (d) an m by n matrix has no more than m pivot variables.",vector spaces
54. is there a 3 by 3 matrix with no zero entries for whichu =r=i?,vector spaces
"55. put as many 1s as possible in a 4 by 7 echelon matrix u and in a reduced form r whose pivot columns are 2, 4, 5.",vector spaces
56. suppose column 4 of a 3 by 5 matrix is all 0s. then x is certainly a variable. 4 the special solution for this variable is the vector x= .,vector spaces
57. suppose the first and last columns of a 3 by 5 matrix are the same (nonzero). then is a free variable. find the special solution for this variable.,vector spaces
"58. the equation x−3y−z = 0 determines a plane in r3. what is the matrix a in this equation? which are the free variables? the special solutions are (3,1,0) and . the parallel plane x−3y−z=12 contains the particular point (12,0,0). all points on this plane have the following form (fill in the first components): [ ] [ ] [ ] [ ] x [ ] [ ] [ ] [ ] [y]=[0]+y[1]+z[0]. z 0 0 1",vector spaces
59. suppose column 1 + column 3 + column 5 = 0 in a 4 by 5 matrix with four pivots. which column is sure to have no pivot (and which variable is free)? what is the special solution? what is the nullspace? problems 60–66 ask for matrices (if possible) with specific properties.,vector spaces
"60. construct a matrix whose nullspace consists of all combinations of (2,2,1,0) and (3,1,0,1).",vector spaces
"61. construct a matrix whose nullspace consists of all multiples of (4,3,2,1).",vector spaces
"2.3 linearindependence,basis,anddimension 103",vector spaces
"62. construct a matrix whose column space contains (1,1,5) and (0,3.1) and whose nullspace contains (1,1,2).",vector spaces
"63. construct a matrix whose column space contains (1,1,0) and (0,1,1) and whose nullspace contains (1,0,1) and (0,0,1).",vector spaces
"64. construct a matrix whose column space contains (1,1,1) and whose nullspace is the line of multiples of (1,1,1,1).",vector spaces
65. construct a 2 by 2 matrix whose nullspace equals its column space.,vector spaces
66. why does no 3 by 3 matrix have a nullspace that equals its column space?,vector spaces
67. the reduced form r of a 3 by 3 matrix with randomly chosen entries is almost sure to be . what r is virtually certain if the random a is 4 by 3?,vector spaces
68. show by example that these three statements are generally false: (a) a and at have the same nullspace. (b) a and at have the same free variables. (c) if r is the reduced form rref(a) then rt is rref(at).,vector spaces
"69. if the special solutions to rx=0 are in the columns of these n, go backward to find the nonzero rows of the reduced matrices r: [ ] [ ] [ ] 2 3 0 [ ] [ ] [ ] n =[1 0] and n =[0] and n =[ ] (empty 3 by 1). 0 1 1",vector spaces
70. explain why a and −a always have the same reduced echelon form r.,vector spaces
"1. show that v , v , v are independent but v , v , v , v are dependent: 1 2 3 1 2 3 4 [ ] [ ] [ ] [ ] 1 1 1 2 [ ] [ ] [ ] [ ] v =[0] v =[1] v =[1] v =[3]. 1 2 3 4 0 0 1 4 solve c v +···+c v =0 or ac=0. the v’s go in the columns of a. 1 1 4 4",vector spaces
2. find the largest possible number of independent vectors among [ ] [ ] [ ] [ ] [ ] [ ] 1 1 1 0 0 0 [ ] [ ] [ ] [ ] [ ] [ ] [−1] [ 0 ] [ 0 ] [ 1 ] [ 1 ] [ 0 ] v =[ ] v =[ ] v =[ ] v =[ ] v =[ ] v =[ ]. 1 2 3 4 5 6 [ 0 ] [−1] [ 0 ] [−1] [ 0 ] [ 1 ] 0 0 −1 0 −1 −1 this number is the of the space spanned by the v’s.,vector spaces
"3. prove that if a=0, d =0, or f =0 (3 cases), the columns ofu are dependent: [ ] a b c [ ] u =[0 d e]. 0 0 f",vector spaces
"4. ifa,d, f inproblem3areallnonzero,showthattheonlysolutiontoux=0isx=0. thenu has independent columns.",vector spaces
"5. decide the dependence or independence of (a) the vectors (1,3,2), (2,1,3), and (3.2,1). (b) the vectors (1,−3,2), (2,1,−3), and (−3,2,1).",vector spaces
"2.3 linearindependence,basis,anddimension 111",vector spaces
6. choosethreeindependentcolumnsofu. thenmaketwootherchoices. dothesame for a. you have found bases for which spaces? [ ] [ ] 2 3 4 1 2 3 4 1 [ ] [ ] [0 6 7 0] [0 6 7 0] u =[ ] and a=[ ]. [0 0 0 9] [0 0 0 9] 0 0 0 0 4 6 8 2,vector spaces
"7. if w , w , w are independent vectors, show that the differences v = w −w , v = 1 2 3 1 2 3 2 w −w , and v = w −w are dependent. find a combination of the v’s that gives 1 3 3 1 2 zero.",vector spaces
"8. ifw ,w ,w areindependentvectors,showthatthesumsv =w +w ,v =w +w , 1 2 3 1 2 3 2 1 3 and v =w +w are independent. (write c v +c v +c v =0 in terms of the w’s. 3 1 2 1 1 2 2 3 3 find and solve equations for the c’s.)",vector spaces
"9. suppose v , v , v , v are vectors in r3. 1 2 3 4 (a) these four vectors are dependent because . (b) the two vectors v and v will be dependent if . 1 2 (c) the vectors v and (0,0,0) are dependent because . 1",vector spaces
10. find two independent vectors on the plane x+2y−3z−t =0 in r4. then find three independent vectors. why not four? this plane is the nullspace of what matrix? problems 11–18 are about the space spanned by a set of vectors. take all linear combinations of the vectors,vector spaces
"11. describe the subspace of r3 (is it a line or a plane or r3?) spanned by (a) the two vectors (1,1,−1) and (−1,−1,1). (b) the three vectors (0,1,1) and (1,1,0) and (0,0,0). (c) the columns of a 3 by 5 echelon matrix with 2 pivots. (d) all vectors with positive components.",vector spaces
"12. thevectorbisinthesubspace spannedbythecolumns of awhenthereisa solution to . the vector c is in the row space of a when there is a solution to . true or false: if the zero vector is in the row space, the rows are dependent.",vector spaces
"13. find the dimensions of (a) the column space of a, (b) the column space ofu, (c) the row space of a, (d) the row space ofu. which two of the spaces are the same? [ ] [ ] 1 1 0 1 1 0 [ ] [ ] a=[1 3 1 ] and u =[0 2 1]. 3 1 −1 0 0 0",vector spaces
"14. choose x = (x ,x ,x ,x ) in r4. it has 24 rearrangements like (x ,x ,x ,x ) and 1 2 3 4 2 1 3 4 (x ,x ,x ,x ). those 24 vectors, including x itself, span a subspace s. find specific 4 3 1 2 vectors x so that the dimension of s is: (a) 0, (b) 1, (c) 3, (d) 4.",vector spaces
15. v+wandv−warecombinationsofvandw. writevandwascombinationsofv+w and v−w. the two pairs of vectors the same space. when are they a basis for the same space?,vector spaces
"16. decide whether or not the following vectors are linearly independent, by solving c v +c v +c v +c v =0: 1 1 2 2 3 3 4 4 [ ] [ ] [ ] [ ] 1 1 0 0 [ ] [ ] [ ] [ ] [1] [0] [0] [1] v =[ ], v =[ ], v =[ ], v =[ ]. 1 2 3 4 [0] [1] [1] [0] 0 0 1 1 decide also if they span r4, by trying to solve c v +···+c v =(0,0,0,1). 1 1 4 4",vector spaces
"17. suppose the vectors to be tested for independence are placed into the rows instead of the columns of a, how does the elimination process from a to u decide for or against independence?",vector spaces
"18. to decide whether b is in the sub space spanned by w ,...,w , let the vectors w be 1 n the columns of a and try to solve ax=b. what is the result for (a) w =(1,1,0), w =(2,2,1), w =(0,0,2), b=(3,4,5)? 1 2 3 (b) w =(1,2,0), w =(2,5,0), w =(0,0,2), w =(0,0,0), and any b? 1 2 3 4 problems 19–37 are about the requirements for a basis.",vector spaces
"19. ifv ,...,v arelinearlyindependent,thespacetheyspanhasdimension . these 1 n vectors are a for that space. if the vectors are the columns of an m by n matrix, then m is than n.",vector spaces
"20. find a basis for each of these subspaces of r4: (a) all vectors whose components are equal. (b) all vectors whose components add to zero. (c) all vectors that are perpendicular to (1,1,0,0) and (1,0,1,1).   (d) the column space (in r2) and nullspace (in r5) ofu = 1 0 1 0 1 . 0 1 0 1 0",vector spaces
21. find three different bases for the column space ofu above. then find two different bases for the row space ofu.,vector spaces
"22. suppose v ,v ,...,v are six vectors in r4. 1 2 6 (a) those vectors (do)(do not)(might not) span r4.",vector spaces
"2.3 linearindependence,basis,anddimension 113 (b) those vectors (are)(are not)(might be) linearly independent. (c) any four of those vectors (are)(are not)(might be) a basis for r4. (d) if those vectors are the columns of a, then ax = b (has) (does not have) (might not have) a solution.",vector spaces
"23. the columns of a are n vectors from rm. if they are linearly independent, what is the rank of a? if they span rm, what is the rank? if they are a basis for rm, what then?",vector spaces
24. findabasisfortheplanex−2y+3z=0inr3. thenfindabasisfortheintersection of that plane with the xy-plane. then find a basis for all vectors perpendicular to the plane.,vector spaces
25. suppose the columns of a 5 by 5 matrix a are a basis for r5. (a) the equation ax=0 has only the solution x=0 because . (b) if b is in r5 then ax=b is solvable because . conclusion: a is invertible. its rank is 5.,vector spaces
26. suppose s is a five-dimensional subspace of r6. true or false? (a) every basis for s can be extended to a basis for r6 by adding one more vector. (b) every basis for r6 can be reduced to a basis for s by removing one vector.,vector spaces
27. u comes from a by subtracting row 1 from row 3: [ ] [ ] 1 3 2 1 3 2 [ ] [ ] a=[0 1 1] and u =[0 1 1]. 1 3 2 0 0 0 findbasesforthetwocolumnspaces. findbasesforthetworowspaces. findbases for the two nullspace.,vector spaces
"28. true or false (give a good reason)? (a) if the columns of a matrix are dependent, so are the rows. (b) the column space of a 2 by 2 matrix is the same as its row space. (c) the column space of a 2 by 2 matrix has the same dimension as its row space. (d) the columns of a matrix are a basis for the column space.",vector spaces
29. for which numbers c and d do these matrices have rank 2? [ ] 1 2 5 0 5 [ ] c d a=[0 0 c 2 2] and b= . d c 0 0 0 d 2,vector spaces
"30. by locating the pivots, find a basis for the column space of [ ] 0 5 4 3 [ ] [0 0 2 1] u =[ ]. [0 0 0 0] 0 0 0 0 express each column that is not in the basis as a combination of the basic columns, find also a matrix a with this echelon formu, but a different column space.",vector spaces
"31. find a counterexample to the following statement: if v , v , v , v is a basis for the 1 2 3 4 vector space r4, and if w is a subspace, then some subset of the v’s is a basis for w.",vector spaces
32. find the dimensions of these vector spaces: (a) the space of all vectors in r4 whose components add to zero. (b) the nullspace of the 4 by 4 identity matrix. (c) the space of all 4 by 4 matrices.,vector spaces
"33. suppose v is known to have dimension k. prove that (a) any k independent vectors in v form a basis; (b) any k vectors that span v form a basis. in other words, if the number of vectors is known to be correct, either of the two properties of a basis implies the other.",vector spaces
"34. prove that if v and w are three-dimensional subspaces of r5, then v and w must have a nonzero vector in common. hint: start with bases for the two subspaces, making six vectors in all.",vector spaces
"35. true or false? (a) ifthecolumnsofaarelinearlyindependent,thenax=bhasexactlyonesolution for every b. (b) a 5 by 7 matrix never has linearly independent columns,",vector spaces
"36. if a is a 64 by 17 matrix of rank 11, how many independent vectors satisfy ax =0? how many independent vectors satisfy aty=0?",vector spaces
37. find a basis for each of these subspaces of 3 by 3 matrices: (a) all diagonal matrices. (b) all symmetric matrices (at =a). (c) all skew-symmetric matrices (at =−a). problems 38–42 are about spaces in which the “vectors” are functions.,vector spaces
2.4 thefourfundamentalsubspaces 115 dy,vector spaces
38. (a) find all functions that satisfy =0. dx dy (b) choose a particular function that satisfies =3. dx dy (c) find all functions that satisfy =3. dx,vector spaces
39. the cosine space f contains all combinations y(x) = acosx+bcos2x+ccos3x. 3 find a basis for the subspace that has y(0)=0.,vector spaces
40. find a basis for the space of functions that satisfy dy (a) −2y=0. dx dy y (b) − =0. dx x,vector spaces
"41. suppose y (x), y (x), y (x) are three different functions of x. the vector space they 1 2 3 span could have dimension 1, 2, or 3. give an example of y , y , y to show each 1 2 3 possibility.",vector spaces
42. find a basis for the space of polynomials p(x) of degree ≤ 3. find a basis for the subspace with p(1)=0.,vector spaces
"43. writethe3by3identitymatrixasacombinationoftheotherfivepermutationmatri- ces! then show that those five matrices are linearly independent. (assume a combi nation gives zero, and check entries to prove each term is zero.) the five permuta- tions are a basis for the subspace of 3 by 3 matrices with row and column sums all equal.",vector spaces
"44. review: which of the following are bases for r3? (a) (1,2,0) and (0,1,−1). (b) (1,1,−1), (2,3,4), (4,1,−1), (0,1,−1). (c) (1,2,2), (−1,2,1), (0,8,0). (d) (1,2,2), (−1,2,1), (0,8,6).",vector spaces
45. review: supposeais5by4withrank4. showthatax=bhasnosolutionwhenthe 5by5matrix[a b]isinvertible. showthatax=bissolvablewhen[a b]issingular.,vector spaces
"1. true or false: if m = n, then the row space of a equals the column space. if m < n, then the nullspace has a larger dimension than .",vector spaces
2. find the dimension and construct a basis for the four subspaces associated with each of the matrices 0 1 4 0 0 1 4 0 a= and u = . 0 2 8 0 0 0 0 0,vector spaces
3. find the dimension and a basis for the four fundamental subspaces for [ ] [ ] 1 2 0 1 1 2 0 1 [ ] [ ] a=[0 1 1 0] and u =[0 1 1 0]. 1 2 0 1 0 0 0 0,vector spaces
4. describe the four subspaces in three-dimensional space associated with [ ] 0 1 0 [ ] a=[0 0 1]. 0 0 0,vector spaces
"5. if the product ab is the zero matrix, ab = 0, show that the column space of b is contained in the nullspace of a. (also the row space of a is in the left nullspace of b, since each row of a multiplies b to give a zero row.)",vector spaces
6. suppose a is an m by n matrix of rank r. under what conditions on those numbers does (a) a have a two-sided inverse: aa−1 =a−1a=i? (b) ax=b have infinitely many solutions for every b?,vector spaces
"7. why is there no matrix whose row space and nullspace both contain (1,1,1)?",vector spaces
8. suppose the only solution to ax = 0 (m equations in n unknowns) is x = 0. what is the rank and why? the columns of a are linearly .,vector spaces
9. find a 1 by 3 matrix whose nullspace consists of all vectors in r3 such that x + 1 2x +4x =0. find a 3 by 3 matrix with that same nullspace. 2 3,vector spaces
"10. if ax =b always has at least one solution, show that the only solution to aty =0 is y=0. hint: what is the rank?",vector spaces
"11. if ax = 0 has a nonzero solution, show that aty = f fails to be solvable for some right-hand sides f. construct an example of a and f.",vector spaces
12. find the rank of a and write the matrix as a=uvt: [ ] 1 0 0 3 [ ] 2 −2 a=[0 0 0 0] and a= . 6 −6 2 0 0 6,vector spaces
"13. if a, b, c are given with a=0, choose d so that a b a= =uvt c d has rank 1. what are the pivots?",vector spaces
14. find a left-inverse and/or a right-inverse (when they exist) for [ ] 1 0 1 1 0 [ ] a b a= and m =[1 1] and t = . 0 1 1 0 a 0 1,vector spaces
"15. if the columns of a are linearly independent (a is m by n), then the rank is , the nullspace is , the row space is , and there exists a -inverse.",vector spaces
16. (a paradox) suppose a has a right-inverse b. then ab = i leads to atab = at or b(ata)−1at. butthatsatisfiesba=i;itisaleft-inverse. whichstepisnotjustified?,vector spaces
"17. findamatrixathathasvasitsrowspace,andamatrixbthathasvasitsnullspace, if v is the subspace spanned by [ ] [ ] [ ] 1 1 1 [ ] [ ] [ ] [1], [2], [5]. 0 0 0",vector spaces
18. find a basis for each of the four subspaces of [ ] [ ][ ] 0 1 2 3 4 1 0 0 0 1 2 3 4 [ ] [ ][ ] a=[0 1 2 4 6]=[1 1 0][0 0 0 1 2]. 0 0 0 1 2 0 1 1 0 0 0 0 0,vector spaces
"19. if a has the same four fundamental subspaces as b, does a=cb?",vector spaces
"20. (a) if a 7 by 9 matrix has rank 5, what are the dimensions of the four subspaces? what is the sum of all four dimensions? (b) if a 3 by 4 matrix has rank 3, what are its column space and left nullspace?",vector spaces
"21. construct a matrix with the required property, or explain why you can’t.         1 0 (a) column space contains 1 , 0 , row space contains 1 , 2 . 2 5 0  1   1 3 (b) column space has basis 2 , nullspace has basis 2 . 3 1 (c) dimension of nullspace =1+ dimension of left nullspace.     (d) left nullspace contains 1 , row space contains 3 . 3 1 (e) row space = column space, nullspace = left nullspace.",vector spaces
"22. without elimination, find dimensions and bases for the four subspaces for [ ] [ ] 0 3 3 3 1 1 [ ] [ ] a=[0 0 0 0] and b=[4 4]. 0 1 0 1 5 5",vector spaces
"23. suppose the 3 by 3 matrix a is invertible. write bases for the four subspaces for a, and also for the 3 by 6 matrix b=[a a].",vector spaces
"24. what are the dimensions of the four subspaces for a, b, and c, if i is the 3 by 3 identity matrix and 0 is the 3 by 2 zero matrix?     i i a= i 0 and b= and c = 0 . 0t 0t",vector spaces
2.4 thefourfundamentalsubspaces 127,vector spaces
25. which subspaces are the same for these matrices of different sizes?   a a a a (a) a and . (b) and . a a a a prove that all three matrices have the same rank r.,vector spaces
"26. if the entries of a 3 by 3 matrix are chosen randomly between 0 and 1, what are the most likely dimensions of the four subspaces? what if the matrix is 3 by 5?",vector spaces
"27.  a is an m by n matrix of rank r. suppose there are right-hand sides b for which ax=b has no solution. (a) what inequalities (< or ≤) must be true between m, n, and r? (b) how do you know that aty=0 has a nonzero solution?",vector spaces
"28. construct a matrix with (1,0,1) and (1,2,0) as a basis for its row space and its column space. why can’t this be a basis for the row space and nullspace?",vector spaces
"29. without computing a, find bases for the four fundamental subspaces: [ ][ ] 1 0 0 1 2 3 4 [ ][ ] a=[6 1 0][0 1 2 3]. 9 8 1 0 0 1 2",vector spaces
"30. if you exchange the first two rows of a matrix a, which of the four subspaces stay the same? if y = (1,2,3,4) is in the left nullspace of a, write down a vector in the left nullspace of the new matrix.",vector spaces
"31. explain why v=(1,0,−1) cannot be a row of a and also be in the nullspace.",vector spaces
32. describe the four subspaces of r3 associated with [ ] [ ] 0 1 0 1 1 0 [ ] [ ] a=[0 0 1] and i+a=[0 1 1]. 0 0 0 0 0 1,vector spaces
33. (left nullspace) add the extra column b and reduce a to echelon form: [ ] [ ] 1 2 3 b 1 2 3 b   1 1 [ ] [ ] a b =[4 5 6 b ]→[0 −3 −6 b −4b ]. 2 2 1 7 8 9 b 0 0 0 b −2b +b 3 3 2 1 a combination of the rows of a has produced the zero row. what combination is it? (look at b −2b +b on the right-hand side.) which vectors are in the nullspace of 3 2 1 at and which are in the nullspace of a?,vector spaces
"34. following the method of problem 33, reduce a to echelon form and look at zero rows. the b column tells which combinations you have taken of the rows: [ ] [ ] 1 2 b 1 1 2 b 1 [ ] [ ] [2 3 b ] 2 (a) [3 4 b ]. (b) [ ]. 2 [2 4 b ] 3 4 6 b 3 2 5 b 4 fromthebcolumnafterelimination,readoffm−r basisvectorsintheleftnullspace of a (combinations of rows that give zero).",vector spaces
"35. suppose a is the sum of two matrices of rank one: a=uvt+wzt. (a) which vectors span the column space of a? (b) which vectors span the row space of a? (c) the rank is less than 2 if or if . (d) compute a and its rank if u=z=(1,0,0) and v=w=(0,0,1).",vector spaces
"36. without multiplying matrices, find bases for the row and column spaces of a: [ ] 1 2 [ ] 3 0 3 a=[4 5] . 1 1 2 2 7 how do you know from these shapes that a is not invertible?",vector spaces
37. true or false (with a reason or a counterexample)? (a) a and at have the same number of pivots. (b) a and at have the same left nullspace. (c) if the row space equals the column space then at =a. (d) if at =−a then the row space of a equals the column space.,vector spaces
"38. if ab = 0, the columns of b are in the nullspace of a. if those vectors are in rn, prove that rank(a)+rank(b)≤n.",vector spaces
39. cantic-tac-toebecompleted(5onesand4zerosina)sothatrank(a)=2butneither side passed up a winning move?,vector spaces
40. construct any 2 by 3 matrix of rank 1. copy figure 2.5 and put one vector in each subspace (two in the nullspace). which vectors are orthogonal?,vector spaces
41. redraw figure 2.5 for a 3 by 2 matrix of rank r = 2. which subspace is z (zero vector only)? the nullspace part of any vector x in r2 is x = . n,vector spaces
"1. for the 3-node triangular graph in the figure following, write the 3 by 3 incidence matrix a. find a solution to ax=0 and describe all other vectors in the nullspace of a. find a solution to aty = 0 and describe all other vectors in the left nullspace of a. node 1 x1 b b y5 edge 1 edge 3 y1 y2 y4 xb 4 y6 b b b b node 2 edge 2 node 3 x2 y3 x3",vector spaces
"2. for the same 3 by 3 matrix, show directly from the columns that every vector b in the column space will satisfy b +b −b =0. derive the same thing from the three 1 2 3 rows—the equations in the system ax = b. what does that mean about potential differences around a loop?",vector spaces
3. show directly from the rows that every vector f in the row space will satisfy f + 1 f + f =0. derivethesamethingfromthethreeequationsaty= f. whatdoesthat 2 3 mean when the f’s are currents into the nodes?,vector spaces
"4. compute the 3 by 3 matrix ata, and show that it is symmetric but singular—what vectors are in its nullspace? removing the last column of a (and last row of at) leaves the 2 by 2 matrix in the upper left corner; show that it is not singular.",vector spaces
"5. put the diagonal matrix c with entries c , c , c in the middle and compute atca. 1 2 3 show again that the 2 by 2 matrix in the upper left corner is invertible.",vector spaces
"6. write the 6 by 4 incidence matrix a for the second graph in the figure. the vector (1,1,1,1) is in the nullspace of a, but now there will be m−n+1 = 3 independent vectors that satisfy aty = 0. find three vectors y and connect them to the loops in the graph.",vector spaces
"7. if that second graph represents six games between four teams, and the score dif- ferences are b ,...,b , when is it possible to assign potentials x ,...,x so that the 1 6 1 4 potential differences agree with the b’s? you are finding (from kirchhoff or from elimination) the conditions that make ax=b solvable.",vector spaces
"8. write down the dimensions of the four fundamental subspaces for this 6 by 4 inci- dence matrix, and a basis for each subspace.",vector spaces
"9. compute ata and atca, where the 6 by 6 diagonal matrixc has entries c ,...,c . 1 6 how can you tell from the graph where the c’s will appear on the main diagonal of atca?",vector spaces
10. draw a graph with numbered and directed edges (and numbered nodes) whose inci- dence matrix is [ ] −1 1 0 0 [ ] [−1 0 1 0 ] a=[ ]. [ 0 1 0 −1] 0 0 −1 1 is this graph a tree? (are the rows of a independent?) show that removing the last edge produces a spanning tree. then the remaining rows are a basis for ?,vector spaces
"11. with the last column removed from the preceding a, and with the numbers 1. 2, 2, 1 on the diagonal ofc, write out the 7 by 7 system c−1y + ax = 0 aty = f. eliminating y , y , y , y leaves three equations atcax = −f for x , x , x . solve 1 2 3 4 1 2 3 the equations when f = (1,1,6). with those currents entering nodes 1, 2, 3 of the network, what are the potentials at the nodes and currents on the edges?",vector spaces
"12. if a is a 12 by 7 incidence matrix from a connected graph, what is its rank? how many free variables are there in the solution to ax = b? how many free variables are there in the solution to aty = f? how many edges must be removed to leave a spanning tree?",vector spaces
"13. in the graph above with 4 nodes and 6 edges, find all 16 spanning trees.",vector spaces
"14. if mit beats harvard 35-0, yale ties harvard, and princeton beats yale 7-6, what score differences in the other 3 games (h-p mit-p, mit-y) will allow potential dif- ferences that agree with the score differences? if the score differences are known for the games in a spanning tree, they are known for all games.",vector spaces
"15. in our method for football rankings, should the strength of the opposition be consid- ered — or is that already built in?",vector spaces
"16. if there is an edge between every pair of nodes (a complete graph), how many edges are there? the graph has n nodes, and edges from a node to itself are not allowed.",vector spaces
"17. for both graphs drawn below, verify euler’s formula: (# of nodes) − (# of edges) + (# of loops) = 1.",vector spaces
"18. multiply matrices to find ata, and guess how its entries come from the graph: (a) the diagonal of ata tells how many into each node. (b) the off-diagonals −1 or 0 tell which pairs of nodes are .",vector spaces
"19. why does the nullspace of ata contain (1,1,1,1)? what is its rank?",vector spaces
20. why does a complete graph with n = 6 nodes have m = 15 edges? a spanning tree connecting all six nodes has edges. there are nn−2 =64 spanning trees!,vector spaces
"21. the adjacency matrix of a graph has m = 1 if nodes i and j are connected by an ij edge (otherwise m = 0). for the graph in problem 6 with 6 nodes and 4 edges, ij write down m and also m2. why does (m2) count the number of 2-step paths from ij node i to node j?",vector spaces
1. what matrix has the effect of rotating every vector through 90° and then projecting theresultontothex-axis? whatmatrixrepresentsprojectionontothex-axisfollowed by projection onto the y-axis?,vector spaces
2. does the product of 5 reflections and 8 rotations of the x-y plane produce a rotation or a reflection?  ,vector spaces
"3. the matrix a = 2 0 produces a stretching in the x-direction. draw the circle x2+ 0 1 y2 = 1 and sketch around it the points (2x,y) that result from multiplication by a. what shape is that curve?",vector spaces
"4. every straight line remains straight after a linear transformation. if z is halfway between x and y, show that az is halfway between ax and ay.  ",vector spaces
"5. the matrix a = 1 0 yields a shearing transformation, which leaves the y-axis un- 3 1 changed. sketch its effect on the x-axis, by indicating what happens to (1,0) and (2,0) and (−1,0)—and how the whole axis is transformed.",vector spaces
"6. what 3 by 3 matrices represent the transformations that (a) project every vector onto the x-y plane? (b) reflect every vector through the x-y plane? (c) rotate the x-y plane through 90°, leaving the z-axis alone? (d) rotate the x-y plane, then x-z, then y-z, through 90°? (e) carry out the same three rotations, but each one through 180°?",vector spaces
"7. on the space p of cubic polynomials, what matrix represents d2/dt2? construct 3 the 4 by 4 matrix from the standard basis 1, t, t2, t3. find its nullspace and column space. what do they mean in terms of polynomials?",vector spaces
"8. from the cubics p to the fourth-degree polynomials p , what matrix represents 3 4 multiplication by 2+3t? the columns of the 5 by 4 matrix a come from applying the transformation to 1,t,t2,t3.",vector spaces
"9. the solutions to the linear differential equation d2u/dt2 = u form a vector space (sincecombinationsofsolutionsarestillsolutions). findtwoindependentsolutions, to give a basis for that solution space.",vector spaces
"10. with initial values u = x and du/dt = y at t = 0, what combination of basis vectors in problem 9 solves u = u? this transformation from initial values to solution is linear. what is its 2 by 2 matrix (using x =1, y=0 and x =0, y=1 as basis for v, and your basis for w)?",vector spaces
11. verify directly from c2+s2 =1 that reflection matrices satisfy h2 =1.,vector spaces
"12. suppose a is a linear transformation from the x-y plane to itself. why does a−1(x+ y) = a−1x+a−1y? if a is represented by the matrix m, explain why a−1 is repre- sented by m−1.",vector spaces
13. the product (ab)c of linear transformations starts with a vector x and produces u=cx. then rule 2v applies ab to u and reaches (ab)cx. (a) is this result the same as separately applyingc then b then a? (b) istheresultthesameasapplyingbcfollowedbya? parenthesesareunnecessary and the associative law (ab)c =a(bc) holds for linear transformations. this is the best proof of the same law for matrices.,vector spaces
14. prove that t2 is a linear transformation if t is linear (from r3 to r3).,vector spaces
"15. the space of all 2 by 2 matrices has the four basis “vectors” 1 0 0 1 0 0 0 0 , , , . 0 0 0 0 1 0 0 1 for the linear transformation of transposing, find its matrix a with respect to this basis. why is a2 =i?",vector spaces
"16. find the 4 by 4 cyclic permutation matrix: (x ,x ,x ,x ) is transformed to ax = 1 2 3 4 (x ,x ,x ,x ). what is the effect of a2? show that a3 =a−1. 2 3 4 1",vector spaces
"17. find the 4 by 3 matrix a that represents a right shift: (x ,x ,x ) is transformed to 1 2 3 (0,x ,x ,x ). find also the left shift matrix b from r4 back to r3, transforming 1 2 3 (x ,x ,x ,x ) to (x ,x ,x ). what are the products ab and ba? 1 2 3 4 2 3 4",vector spaces
2.6 lineartransformations 151,vector spaces
"18. in the vector space p of all p(x) = a +a x+a x2+a x3, let s be the subset of 3 0 1 2 3 (cid:82) 1 polynomials with p(x)dx=0. verify that s is a subspace and find a basis. 0",vector spaces
"19. a nonlinear transformation is invertible if t(x) = b has exactly one solution for every b. the example t(x) = x2 is not invertible because x2 = b has two solutions forpositivebandnosolutionfornegativeb. whichofthefollowingtransformations (from the real numbers r1 to the real numbers r1) are invertible? none are linear, not even (c). (a) t(x)=x3. (b) t(x)=ex. (c) t(x)=x+11. (d) t(x)=cosx.",vector spaces
"20. what is the axis and the rotation angle for the transformation that takes (x ,x ,x ) 1 2 3 into (x ,x ,x )? 2 3 1",vector spaces
21. a linear transformation must leave the zero vector fixed: t(0)=0. prove this from t(v+w)=t(v)+t(w) by choosing w= . prove it also from the requirement t(cv)=ct(v) by choosing c=,vector spaces
"22. which of these transformations is not linear? the input is v=(v ,v ). 1 2 (a) t(v)=(v ,v ). (b) t(v)=(v ,v ). 2 1 1 1 (c) t(v)=(0,v ). (d) t(v)=(0,1). 1",vector spaces
"23. if s and t are linear with s(v)=t(v)=v, then s(t(v))=v or v2?",vector spaces
"24. suppose t(v)=v, except that t(0,v )=(0,0). show that this transformation satis- 2 fies t(cv)=ct(v) but not t(v+w)=t(v)+t(w).",vector spaces
"25. which of these transformations satisfy t(v+w) = t(v)+t(w), and which satisfy t(cv)=ct(v)? (a) t(v)=v/(cid:107)v(cid:107). (b) t(v)=v +v +v . 1 2 3 (c) t(v)=(v ,2v ,3v ). (d) t(v)=largest component of v. 1 2 3",vector spaces
"26. for these transformations of v=r2 to w=r2, find t(t(v)). (a) t(v)=−v. (b) t(v)=v+(1,1). (c) t(v)=90° rotation=(−v ,v ). 2 1  v +v v +v 1 2 1 2 (d) t(v)=projection= , . 2 2",vector spaces
"27. the “cyclic” transformation t is defined by t(v ,v ,v ) = (v ,v ,v ). what is 1 2 3 2 3 1 t(t(t(v)))? what is t100(v)?",vector spaces
"28. find the range and kernel (those are new words for the column space and nullspace) of t. (a) t(v ,v )=(v ,v ). (b) t(v ,v ,v )=(v ,v ). 1 2 2 1 1 2 3 1 2 (c) t(v ,v )=(0,0). (d) t(v ,v )=(v ,v ). 1 2 1 2 1 1",vector spaces
"29. a linear transformation from v to w has an inverse from w to v when the range is all of w and the kernel contains only v = 0. why are these transformations not invertible? (a) t(v ,v )=(v ,v ) w=r2. 1 2 2 2 (b) t(v ,v )=(v ,v ,v +v ) w=r3. 1 2 1 2 1 2 (c) t(v ,v )=v w=r1. 1 2 1",vector spaces
"30. suppose a linear t transforms (1,1) to (2,2) and (2,0) to (0,0). find t(v) when (a) v=(2,2). (b) v=(3,1). (c) v=(−1,1). (d) v=(a,b). problems 31–35 may be harder. the input space v contains all 2 by 2 matrices m.  ",vector spaces
31. m is any 2 by 2 matrix and a = 1 2 . the linear transformation t is defined by 3 4 t(m)=am. what rules of matrix multiplication show that t is linear?  ,vector spaces
32. suppose a = 1 2 . show that the identity matrix i is not in the range of t. find a 3 6 nonzero matrix m such that t(m)=am is zero.,vector spaces
33. suppose t transposes every matrix m. try to find a matrix a that gives am = mt for every m. show that no matrix a will do it. to professors: is this a linear transformation that doesn’t come from a matrix?,vector spaces
34. thetransformationt thattransposeseverymatrixisdefinitelylinear. whichofthese extra properties are true? (a) t2 = identity transformation. (b) the kernel of t is the zero matrix. (c) every matrix is in the range of t. (d) t(m)=−m is impossible.    ,vector spaces
35. suppose t(m) = 1 0 [m] 0 0 . find a matrix with t(m) = 0. describe all ma- 0 0 0 1 trices with t(m) = 0 (the kernel of t) and all output matrices t(m) (the range of t). problems 36–40 are about changing the basis,vector spaces
"36. (a) what matrix transforms (1,0) into (2,5) and transforms (0,1) to (1,3)? (b) what matrix transforms (2,5) to (1,0) and (1,3) to (0,1)? (c) why does no matrix transform (2,6) to (1,0) and (1,3) to (0,1)?",vector spaces
"37. (a) what matrix m transforms (1,0) and (0,1) to (r,t) and (s,u)? (b) what matrix n transforms (a,c) and (b,d) to (1,0) and (0,1)? (c) what condition on a, b, c, d will make part (b) impossible?",vector spaces
"38. (a) how do m and n in problem 37 yield the matrix that transforms (a,c) to (r,t) and (b,d) to (s,u)? (b) what matrix transforms (2,5) to (1,1) and (1,3) to (0,2)?",vector spaces
"39. if you keep the same basis vectors but put them in a different order, the change-of- basis matrix m is a matrix. if you keep the basis vectors in order but change their lengths, m is a matrix.",vector spaces
"40. the matrix that transforms (1,0) and (0,1) to (1,4) and (1,5) is m = . the combination a(1,4)+b(1,5) that equals (1,0) has (a,b) = ( , ). how are those new coordinates of (1,0) related to m or m−1?",vector spaces
"41. what are the three equations for a, b, c if the parabola y = a+bx+cx2 equals 4 at x = a, 5 at x = b, and 6 at x = c? find the determinant of the 3 by 3 matrix. for which numbers a, b, c will it be impossible to find this parabolay?",vector spaces
"42. suppose v , v , v are eigenvectors for t. this means t(v ) = λv for i = 1,2,3. 1 2 3 i i i what is the matrix for t when the input and output bases are the v’s?",vector spaces
43. every invertible linear transformation can have i as its matrix. for the output basis just choose w =t(v ). why must t be invertible? i i,vector spaces
"44. suppose t is reflection across the x-axis and s is reflection across the y-axis. the domain v is the x-y plane. if v =(x,y) what is s(t(v))? find a simpler description of the product st.",vector spaces
"45. suppose t is reflection across the 45° line, and s is reflection across the y-axis, if v = (2,1) then t(v) = (1,2). find s(t(v)) and t(s(v)). this shows that generally st =ts.",vector spaces
46. show that the product st of two reflections is a rotation. multiply these reflection matrices to find the rotation angle: cos2θ sin2θ cos2α sin2α . sin2θ −cos2θ sin2α −cos2α,vector spaces
"47. the 4 by 4 hadamard matrix is entirely +1 and −1: [ ] 1 1 1 1 [ ] [1 −1 1 −1] h =[ ]. [1 1 −1 −1] 1 −1 −1 1 find h−1 and write v=(7,5,3,1) as a combination of the columns of h.",vector spaces
"48. suppose we have two bases v ,...,v and w ,...,w for rn. if a vector has coeffi- 1 n 1 n cients b in one basis and c in the other basis, what is the change-of-basis matrix in i i b=mc? start from b v +···+b v =vb=c w +···+c w =wc. 1 1 n n 1 1 n n your answer represents t(v) = v with input basis of v’s and output basis of w’s. because of different bases, the matrix is not i.",vector spaces
"49. true or false: if we know t(v) for n different nonzero vectors in r2, then we know t(v) for every vector in rn.",vector spaces
"50.  suppose all vectors x in the unit square 0≤x ≤1, 0≤x ≤1 are 1 2 transformed to ax (a is 2 by 2). (a) what is the shape of the transformed region (all ax)? (b) for which matrices a is that region a square? (c) for which a is it a line? (d) for which a is the new area still 1? review exercises",vector spaces
"1.1 find a basis for the following subspaces of r4: (a) the vectors for which x1=2x . 4 (b) the vectors for which x +x +x =0 and x +x =0. 1 2 3 3 4 (c) the subspace spanned by (1,1,1,1), (1,2,3,4), and (2,3,4,5).",vector spaces
"1.2 by giving a basis, describe a two-dimensional subspace of r3 that contains none of the coordinate vectors (1,0,0), (0,1,0), (0,0,1).",vector spaces
"1.3 true or false, with counterexample if false: (a) if the vectors x ,...,x span a subspace s, then dims=m. 1 m (b) the intersection of two subspaces of a vector space cannot be empty. (c) if ax=ay, then x=y. (d) the row space of a has a unique basis that can be computed by reducing a to echelon form. (e) if a square matrix a has independent columns, so does a2.",vector spaces
1.4 what is the echelon formu of a? [ ] 1 2 0 2 1 [ ] a=[−1 −2 1 1 0 ]. 1 2 −3 −7 −2 what are the dimensions of its four fundamental subspaces?,vector spaces
1.5 find the rank and the nullspace of [ ] [ ] 0 0 1 0 0 1 2 [ ] [ ] a=[0 0 1] and b=[0 0 1 2]. 1 1 1 1 1 1 0,vector spaces
"1.6 find bases for the four fundamental subspaces associated with 1 2 0 0 1 1 0 0 a= , b= , c = . 3 6 1 2 0 1 0 1",vector spaces
"1.7 what is the most general solution to u+v+w=1, u−w=2?",vector spaces
"1.8 (a) construct a matrix whose nullspace contains the vector x=(1,1,2). (b) construct a matrix whose left nullspace contains y=(1,5). (c) construct a matrix whose column space is spanned by (1,1,2) and whose row space is spanned by (1,5). (d) if you are given any three vectors in r6 and any three vectors in r5, is there a 6 by 5 matrix whose column space is spanned by the first three and whose row space is spanned by the second three?",vector spaces
"1.9 in the vector space of 2 by 2 matrices, (a) is the set of rank 1 matrices a subspace? (b) what subspace is spanned by the permutation matrices? (c) what subspace is spanned by the positive matrices (all a >0)? ij (d) what subspace is spanned by the invertible matrices?",vector spaces
1.10 invent a vector space that contains all linear transformations from rn to rn. you have to decide on a rule for addition. what is its dimension?,vector spaces
"1.11 (a) find the rank of a, and give a basis for its nullspace. [ ][ ] 1 1 2 0 1 2 1 [ ][ ] [2 1 ][0 0 2 2 0 0] a=lu =[ ][ ]. [2 1 2 ][0 0 0 0 0 1] 3 2 4 1 0 0 0 0 0 0 (b) the first 3 rows ofu are a basis for the row space of a—true or false? columns 1, 3, 6 ofu are a basis for the column space of a—true or false? the four rows of a are a basis for the row space of a—true or false? (c) find as many linearly independent vectors b as possible for which ax=b has a solution. (d) in elimination on a, what multiple of the third row is subtracted to knock out the fourth row?",vector spaces
"1.12 if a is an n by n−1 matrix, and its rank is n−2, what is the dimension of its nullspace?",vector spaces
"1.13 use elimination to find the triangular factors in a=lu, if [ ] a a a a [ ] [a b b b] a=[ ]. [a b c c] a b c d under what conditions on the numbers a, b, c, d are the columns linearly indepen- dent?",vector spaces
"1.14 do the vectors (1,1,3), (2,3,6), and (1,4,3) form a basis for r3?",vector spaces
"1.15 what do you know about c(a) when the number of solutions to ax=b is (a) 0 or 1, depending on b. (b) ∞, independent of b. (c) 0 or ∞, depending on b. (d) 1, regardless of b.",vector spaces
"1.16 in the previous exercise, how is r related to m and n in each example?",vector spaces
"1.17 if x is a vector in rn, and xty=0 for every y, prove that x=0.",vector spaces
"1.18 if a is an n by n matrix such that a2 =a and ranka=n, prove that a=i.",vector spaces
"1.19 what subspace of 3 by 3 matrices is spanned by the elementary matrices e , with ij 1s on the diagonal and at most one nonzero entry below?",vector spaces
1.20 how many 5 by 5 permutation matrices are there? are they linearly independent? do they span the space of all 5 by 5 matrices? no need to write them all down.,vector spaces
"1.21 what is the rank of the n by n matrix with every entry equal to 1? how about the “checkerboard matrix,” with a =0 when i+ j is even, a =1 when i+ j is odd? ij ij",vector spaces
"1.22 (a) ax=b has a solution under what conditions on b, for the following a and b? [ ] [ ] 1 2 0 3 b 1 [ ] [ ] a=[0 0 0 0] and b=[b ]. 2 2 4 0 1 b 3 (b) find a basis for the nullspace of a. (c) find the general solution to ax=b, when a solution exists. (d) find a basis for the column space of a. (e) what is the rank of at?",vector spaces
"1.23 howcanyouconstructamatrixthattransformsthecoordinatevectorse ,e ,e into 1 2 3 three given vectors v ,v ,v ? when will that matrix be invertible? 1 2 3",vector spaces
"1.24 if e ,e ,e are in the column space of a 3 by 5 matrix, does it have a left-inverse? 1 2 3 does it have a right-inverse?",vector spaces
"1.25 suppose t is the linear transformation on r3 that takes each point (u,v,w) to (u+ v+w,u+v,u), describe what t−1 does to the point (x,y,z).",vector spaces
"1.26 true or false? (a) every subspace of r4 is the nullspace of some matrix. (b) if a has the same nullspace as at, the matrix must be square. (c) the transformation that takes x to mx+b is linear (from r1 to r1).",vector spaces
1.27 find bases for the four fundamental subspaces of [ ] [ ] 1 2 0 3 [ ] 1   [0 2 2 2] [ ] a =[ ] and a =[1] 1 4 . 1 2 [0 0 0 0] 1 0 0 0 4,vector spaces
"1.28 (a) iftherowsofaarelinearlyindependent(aismbyn)thentherankis , the column space is , and the left nullspace is . (b) ifais8by10withatwo-dimensionalnullspace,showthatax=bcanbesolved for every b.",vector spaces
"1.29 describe the linear transformations of the x-y plane that are represented with stan- dard basis (1,0) and (0,1) by the matrices 1 0 1 0 0 1 a = , a = , a = . 1 2 3 0 −1 2 1 −1 0",vector spaces
"1.30 (a) if a is square, show that the nullspace of a2 contains the nullspace of a. (b) show also that the column space of a2 is contained in the column space of a.",vector spaces
1.31 when does the rank-1 matrix a=uvt have a2 =0?,vector spaces
1.32 (a) find a basis for the space of all vectors in r6 with x +x =x +x =x +x . 1 2 3 4 5 6 (b) find a matrix with that subspace as its nullspace. (c) find a matrix with that subspace as its column space.,vector spaces
"1.33 suppose the matrices in pa=lu are [ ][ ] [ ][ ] 0 1 0 0 0 0 1 −3 2 1 0 0 0 2 −1 4 2 1 [ ][ ] [ ][ ] [1 0 0 0][2 −1 4 2 1] [0 1 0 0][0 0 1 −3 2] [ ][ ]=[ ][ ]. [0 0 0 1][4 −2 9 1 4] [1 1 1 0][0 0 0 0 2] 0 0 1 0 2 −1 5 −1 5 2 1 0 1 0 0 0 0 0 (a) what is the rank of a? (b) what is a basis for the row space of a? (c) true or false: rows 1, 2, 3 of a are linearly independent. (d) what is a basis for the column space of a? (e) what is the dimension of the left nullspace of a? (f) what is the general solution to ax=0? 3 chapter orthogonality",vector spaces
"1. find the lengths and the inner product of x=(1,4,0,2) and y=(2,−2,1,3).",orthogonality
"2. giveanexamplein r2 oflinearlyindependentvectorsthatarenot orthogonal. also, give an example of orthogonal vectors that are not independent.",orthogonality
"3. two lines in the plane are perpendicular when the product of their slopes is −1. apply this to the vectors x = (x ,x ) and y = (y ,y ), whose slopes are x /x and 1 2 1 2 2 1 y /y , to derive again the orthogonality condition xty=0. 2 1",orthogonality
"4. how do we know that the ith row of an invertible matrix b is orthogonal to the jth column of b−1, if i= j?",orthogonality
"5. which pairs are orthogonal among the vectors v , v , v , v ? 1 2 3 4 [ ] [ ] [ ] [ ] 1 4 1 1 [ ] [ ] [ ] [ ] [ 2 ] [0] [−1] [1] v =[ ], v =[ ], v =[ ], v =[ ]. 1 2 3 4 [−2] [4] [−1] [1] 1 0 −1 1",orthogonality
"6. find all vectors in r3 that are orthogonal to (1,1,1) and (1,−1,0). produce an orthonormal basis from these vectors (mutually orthogonal unit vectors).",orthogonality
"7. find a vector x orthogonal to the row space of a, and a vector y orthogonal to the column space, and a vector z orthogonal to the nullspace: [ ] 1 2 1 [ ] a=[2 4 3]. 3 6 4",orthogonality
"8. ifvandwareorthogonalsubspaces,showthattheonlyvectortheyhaveincommon is the zero vector: v∩w={0}.",orthogonality
"9. find the orthogonal complement of the plane spanned by the vectors (1,1,2) and (1,2,3), by taking these to be the rows of a and solving ax=0. remember that the complement is a whole line.",orthogonality
"10. constructahomogeneousequationinthreeunknownswhosesolutionsarethelinear combinations of the vectors (1,1,2) and (1,2,3). this is the reverse of the previous exercise, but the two problems are really the same.",orthogonality
"11. the fundamental theorem is often stated in the form of fredholm’s alternative: for any a and b, one and only one of the following systems has a solution: (i) ax=b. (ii) aty=0, ytb=0. either b is in the column space c(a) or there is a y in n(at) such that ytb = 0. show that it is contradictory for (i) and (ii) both to have solutions.",orthogonality
"12. find a basis for the orthogonal complement of the row space of a: 1 0 2 a= . 1 1 4 split x=(3,3,3) into a row space component x and a nullspace component x . r n",orthogonality
"13. illustrate the action of at by a picture corresponding to figure 3.4, sending c(a) back to the row space and the left nullspace to zero.",orthogonality
14. show that x−y is orthogonal to x+y if and only if (cid:107)x(cid:107)=(cid:107)y(cid:107).,orthogonality
"15. findamatrixwhoserowspacecontains(1,2,1)andwhosenullspacecontains(1,−2,1), or prove that there is no such matrix.",orthogonality
"16. find all vectors that are perpendicular to (1,4,4,1) and (2,9,8,2).",orthogonality
"17. if v is the orthogonal complement of w in rn, is there a matrix with row space v and nullspace w? starting with a basis for v, construct such a matrix.",orthogonality
"18. if s={0} is the subspace of r4 containing only the zero vector, what is s⊥? if s is spanned by (0,0,0,1), what is s⊥? what is (s⊥)⊥?",orthogonality
"19. why are these statements false? (a) if v is orthogonal to w, then v⊥ is orthogonal to w⊥. (b) v orthogonal to w and w orthogonal to z makes v orthogonal to z.",orthogonality
20. let s be a subspace of rn. explain what (s⊥)⊥ =s means and why it is true.,orthogonality
"21. let p be the plane in r2 with equation x+2y−z = 0. find a vector perpendicular to p. what matrix has the plane p as its nullspace, and what matrix has p as its row space?",orthogonality
"22. let s be the subspace of r4 containing all vectors with x +x +x +x =0. find a 1 2 3 4 basis for the space s⊥, containing all vectors orthogonal to s.",orthogonality
23. construct an unsymmetric 2 by 2 matrix of rank 1. copy figure 3.4 and put one vector in each subspace. which vectors are orthogonal?,orthogonality
24. redraw figure 3.4 for a 3 by 2 matrix of rank r = 2. which subspace is z (zero vector only)? the nullspace part of any vector x in r2 is x = . n,orthogonality
"25. construct a matrix with the required property or say why that is impossible.       1 2 1 (a) column space contains 2 and −3 , nullspace contains 1 . −3 5 1       1 2 1 (b) row space contains 2 and −3 , nullspace contains 1 .   −3 5    1 1 1 0 (c) ax= 1 has a solution and at 0 = 0 . 1 0 0 (d) every row is orthogonal to every column (a is not the zero matrix). (e) the columns add up to a column of 0s, the rows add to a row of 1s.",orthogonality
26. if ab=0 then the columns of b are in the of a. the rows of a are in the of b. why can’t a and b be 3 by 3 matrices of rank 2?,orthogonality
"27. (a) if ax=b has a solution and aty=0, then y is perpendicular to . (b) if aty=c has a solution and ax=0, then x is perpendicular to .",orthogonality
"28. this is a system of equations ax=b with no solution: x+2y+2z=5 2x+2y+3z=5 3x+4y+5z=9. find numbers y , y , y to multiply the equations so they add to 0 = 1. you have 1 2 3 found a vector y in which subspace? the inner product ytb is 1.",orthogonality
"29. in figure 3.4, how do we know that ax is equal to ax? how do we know that this  r    vector is in the column space? if a= 1 1 and x= 1 what is x ? 1 1 0 r",orthogonality
30. if ax is in the nullspace of at then ax =0. reason: ax is also in the of a and the spaces are . conclusion: ata has the same nullspace as a.,orthogonality
"31. suppose a is a symmetric matrix (at =a). (a) why is its column space perpendicular to its nullspace? (b) if ax = 0 and az = 5z, which subspaces contain these “eigenvectors” x and z? symmetric matrices have perpendicular eigenvectors (see section 5.5).",orthogonality
32.  draw figure 3.4 to show each subspace for 1 2 1 0 a= and b= . 3 6 3 0,orthogonality
"33. find the pieces x and x , and draw figure 3.4 properly, if r n [ ] 1 −1 [ ] 2 a=[0 0 ] and x= . 0 0 0 problems 34–44 are about orthogonal subspaces.",orthogonality
34. put bases for the orthogonal subspaces v and w into the columns of matricesv and w. why doesvtw = zero matrix? this matches vtw=0 for vectors.,orthogonality
35. the floor and the wall are not orthogonal subspaces because they share a nonzero vector (along the line where they meet). two planes in r3 cannot be orthogonal! find a vector in both column spaces c(a) and c(b): [ ] [ ] 1 2 5 4 [ ] [ ] a=[1 3] and b=[6 3]. 1 2 5 1 this will be a vector ax and also bx . think 3 by 4 with the matrix [a b].,orthogonality
36. extend problem 35 to a p-dimensional subspace v and a q-dimensional subspace w of rn. what inequality on p+q guarantees that v intersects w in a nonzero vector? these subspaces cannot be orthogonal.,orthogonality
"37. prove that every y in n(at) is perpendicular to every ax in the column space, using the matrix shorthand of equation (8). start from aty=0.",orthogonality
"38. ifsisthesubspaceofr3 containingonlythezerovector,whatiss⊥? ifsisspanned by (1,1,1), what is s⊥? if s is spanned by (2,0,0) and (0,0,3), what is s⊥?",orthogonality
"39. suppose s only contains (1,5,1) and (2,2,2) (not a subspace). then s⊥ is the nullspace of the matrix a= . s⊥ is a subspace even if s is not.",orthogonality
40. suppose l is a one-dimensional subspace (a line) in r3. its orthogonal complement l⊥ is the perpendicular to l. then (l⊥)⊥ is a perpendicular to l⊥. in fact (l⊥)⊥ is the same as .,orthogonality
41. suppose v is the whole space r4. then v⊥ contains only the vector . then (v⊥)⊥ is . so (v⊥)⊥ is the same as .,orthogonality
"42. suppose s is spanned by the vectors (1,2,2,3) and (1,3,3,2). find two vectors that span s⊥. this is the same as solving ax=0 for which a?",orthogonality
"43. if p is the plane of vectors in r4 satisfying x +x +x +x = 0, write a basis for 1 2 3 4 p⊥. construct a matrix that has p as its nullspace.",orthogonality
"44. if a subspace s is contained in a subspace v, prove that s⊥ contains v⊥. problems 45–50 are about perpendicular columns and rows.",orthogonality
45. suppose an n by n matrix is invertible: aa−1 = i. then the first column of a−1 is orthogonal to the space spanned by which rows of a?,orthogonality
"46. find ata if the columns of a are unit vectors, all mutually perpendicular.",orthogonality
47. construct a 3 by 3 matrix a with no zero entries whose columns are mutually per- pendicular. compute ata. why is it a diagonal matrix?,orthogonality
"48. the lines 3x+y=b and 6x+2y=b are . they are the same line if . in 1 2 that case (b ,b ) is perpendicular to the vector . the nullspace of the matrix is 1 2 the line 3x+y= . one particular vector in that nullspace is .",orthogonality
"49. why is each of these statements false? (a) (1,1,1)isperpendicularto(1,1,−2),sotheplanesx+y+z=0andx+y−2z= 0 are orthogonal subspaces. (b) the subspace spanned by (1,1,0,0,0) and (0,0,0,1,1) is the orthogonal com- plement of the subspace spanned by (1,−1,0,0,0) and (2,−2,3,4,−4). (c) two subspaces that meet only in the zero vector are orthogonal.",orthogonality
"50. find a matrix with v = (1,2,3) in the row space and column space. find another matrixwithvinthenullspaceandcolumnspace. whichpairsofsubspacescanvnot be in?",orthogonality
"51. suppose a is 3 by 4, b is 4 by 5, and ab=0. prove rank(a)+rank(b)≤4.",orthogonality
52. the command n = null(a) will produce a basis for the nullspace of a. then the command b = null(n’) will produce a basis for the of a.,orthogonality
"1. (a) given any two positive numbers x and y, choose the vector b equal to ( x, y), √ √ and choose a = ( y, x). apply the schwarz inequality to compare the arith- √ metic mean 1(x+y) with the geometric mean xy. 2 (b) suppose we start with a vector from the origin to the point x, and then add a vector of length (cid:107)y(cid:107) connecting x to x+y. the third side of the triangle goes from the origin to x+y. the triangle inequality asserts that this distance cannot be greater than the sum of the first two: (cid:107)x+y(cid:107)≤(cid:107)x(cid:107)+(cid:107)y(cid:107). after squaring both sides, and expanding (x+y)t(x+y), reduce this to the schwarz inequality.",orthogonality
"2. verifythatthelengthoftheprojectioninfigure3.7is(cid:107)p(cid:107)=(cid:107)b(cid:107)cosθ,usingformula (5).",orthogonality
"3. what multiple of a=(1,1,1) is closest to the point b=(2,4,4)? find also the point closest to a on the line through b.",orthogonality
"4. explain why the schwarz inequality becomes an equality in the case that a and b lie on the same line through the origin, and only in that case. what if they lie on opposite sides of the origin?",orthogonality
"5. in n dimensions, what angle does the vector (1,1,...,1) make with the coordinate axes? what is the projection matrix p onto that vector?",orthogonality
6. the schwarz inequality has a one-line proof if a and b are normalized ahead of time to be unit vectors: |a |2+|b |2 1 1 |atb|∑a b ≤∑|a ||b |≤∑ j j = + =(cid:107)a(cid:107)(cid:107)b(cid:107). j j j j 2 2 2 which previous problem justifies the middle step?,orthogonality
"7. by choosing the correct vector b in the schwarz inequality, prove that (a +···+a )2 ≤n(a2+···+a2). 1 n 1 n when does equality hold?",orthogonality
"8. the methane molecule ch is arranged as if the carbon atom were at the center of a 4 regular tetrahedron with four hydrogen atoms at the vertices. if vertices are placed √ at (0,0,0), (1,1,0), (1,0,1), and (0,1,1)—note that all six edges have length 2, so the tetrahedron is regular—what is the cosine of the angle between the rays going from the center (1,1,1) to the vertices? (the bond angle itself is about 109.5°, an 2 2 2 old friend of chemists.)",orthogonality
"9. square the matrix p = aat/ata, which projects onto a line, and show that p2 = p. (note the number ata in the middle of the matrix aataat!)",orthogonality
10. is the projection matrix p invertible? why or why not?,orthogonality
11. (a) findtheprojectionmatrixp ontothelinethrougha=[1]andalsothematrixp 1 3 2 that projects onto the line perpendicular to a. (b) compute p +p and p p and explain. 1 2 1 2,orthogonality
12. find the matrix that projects every point in the plane onto the line x+2y=0.,orthogonality
13. prove that the trace of p = aat/ata—which is the sum of its diagonal entries— always equals 1.,orthogonality
14. what matrix p projects every point in r3 onto the line of intersection of the planes x+y+t =0 and x−t =0?,orthogonality
15. show that the length of ax equals the length of atx if aat =ata.,orthogonality
"16. suppose p is the projection matrix onto the line through a. (a) why is the inner product of x with py equal to the inner product of px with y? (b) are the two angles the same? find their cosines if a = (1,1,−1), x = (2,0,1), y=(2,1,2). (c) why is the inner product of px with py again the same? what is the angle between those two? problems 17–26 ask for projections onto lines. also errors e=b−p and matri- ces p.",orthogonality
17. project the vector b onto the line through a. check that e is perpendicular to a:,orthogonality
3.2 cosinesandprojectionsontolines 179 [ ] [ ] [ ] [ ] 1 1 1 −1 [ ] [ ] [ ] [ ] (a) b=[2] and a=[1]. (b) b=[3] and a=[−3]. 2 1 1 −1,orthogonality
18. draw the projection of b onto a and also compute it from p=x a: cosθ 1 1 1 (a) b= and a= . (b) b= and a= . sinθ 0 1 −1,orthogonality
"19. in problem 17, find the projection matrix p = aat/ata onto the line through each vector a. verify in both cases that p2 =p. multiply pb in each case to compute the projection p.",orthogonality
20. constructtheprojectionmatrices p andp ontothelinesthroughthe a’sinproblem 1 2,orthogonality
"18. is it true that (p +p )2 =p +p ? this would be true if p p =0. 1 2 1 2 1 2 for problems 21–26, consult the accompanying figures.",orthogonality
"21. compute the projection matrices aat/ata onto the lines through a =(−1,2,2) and 1 a = (2,2,−1), multiply those projection matrices and explain why their product 2 p p is what it is. 1 2",orthogonality
"22. project b = (1,0,0) onto the lines through a and a in problem 21 and also onto 1 2 a =(2,−1,2). add the three projections p +p +p . 3 1 2 3",orthogonality
"23. continuing problems 21–22, find the projection matrix p onto a =(2,−1,2). ver- 3 3 ify that p +p +p =i. the basis a , a , a is orthogonal! 1 2 3 1 2 3",orthogonality
"24. project the vector b=(1,1) onto the lines through a =(1,0) and a =(1,2). draw 1 2 the projections p and p and add p +p . the projections do not add to b because 1 2 1 2 the a’s are not orthogonal.",orthogonality
"25. in problem 24, the projection of b onto the plane of a and a will equal b. find   1 2 p=a(ata)−1at for a=[a a ] 1 1 . 1 2 0 2",orthogonality
"26. projecta =(1,0)ontoa =(1,2). thenprojecttheresultbackontoa . drawthese 1 2 1 projections and multiply the projection matrices p p : is this a projection? 1 2",orthogonality
"1. findthebestleast-squaressolutionx to3x=10,4x=5. whaterrore2isminimized? check that the error vector (10−3x ,5−4x ) is perpendicular to the column (3,4).",orthogonality
"2. suppose the values b = 1 and b = 7 at times t = 1 and t = 2 are fitted by a line 1 2 1 2 b=dt through the origin. solve d=1 and 2d=7 by least squares, and sketch the best line.",orthogonality
"3. solve ax=b by least squares, and find p=ax if [ ] [ ] 1 0 1 [ ] [ ] a=[0 1], b=[1]. 1 1 0 verify that the error b−p is perpendicular to the columns of a.",orthogonality
"4. write out e2 =(cid:107)ax−b(cid:107)2 and set to zero its derivatives with respect to u and v, if [ ] [ ] 1 0 1 [ ] u [ ] a=[0 1], x= , b=[3]. v 1 1 4 compare the resulting equations with atax =atb, confirming that calculus as well as geometry gives the normal equations. find the solution x and the projection p = ax . why is p=b?",orthogonality
5. the following system has no solution: [ ] [ ] 1 −1 4 [ ] c [ ] ax=[1 0 ] =[5]=b. d 1 1 9 sketch and solve a straight-line fit that leads to the minimization of the quadratic (c−d−4)2+(c−5)2+(c+d−9)2? whatistheprojectionofbontothecolumn space of a?,orthogonality
"6. find the projection of b onto the column space of a: [ ] [ ] 1 1 1 [ ] [ ] a=[ 1 −1], b=[2]. −2 4 7 split b into p+q, with p in the column space and q perpendicular to that space. which of the four subspaces contains q?",orthogonality
"7. find the projection matrix p onto the space spanned by a = (1,0,1) and a = 1 2 (1,1,−1).",orthogonality
"8. if p is the projection matrix onto a k-dimensional subspace s of the whole space rn, what is the column space of p and what is its rank?",orthogonality
"9. (a) if p=ptp, show that p is a projection matrix. (b) what subspace does the matrix p=0 project onto?",orthogonality
"10. if the vectors a , a , and b are orthogonal, what are ata and atb? what is the 1 2 projection of b onto the plane of a and a ? 1 2",orthogonality
11. suppose p is the projection matrix onto the subspace s and q is the projection onto theorthogonalcomplements⊥. whatarep+qandpq? showthatp−qisitsown inverse.,orthogonality
"12. if v is the subspace spanned by (1,1,0,1) and (0,0,1,0), find (a) a basis for the orthogonal complement v⊥. (b) the projection matrix p onto v. (c) the vector in v closest to the vector b=(0,1,0,−1) in v⊥.",orthogonality
"13. find the best straight-line fit (least squares) to the measurements b=4 at t =−2, b=3 at t =−1, b=1 at t =0, b=0 at t =2. then find the projection of b=(4,3,1,0) onto the column space of [ ] 1 −2 [ ] [1 −1] a=[ ]. [1 0 ] 1 2",orthogonality
"14. the vectors a = (1,1,0) and a = (1,1,1) span a plane in r3. find the projection 1 2 matrix p onto the plane, and find a nonzero vector b that is projected to zero.",orthogonality
"15. if p is the projection matrix onto a line in the x-y plane, draw a figure to describe the effect of the “reflection matrix” h = i −2p. explain both geometrically and algebraically why h2 =i.",orthogonality
"16. show that if u has unit length, then the rank-1 matrix p=uut is a projection matrix: ithasproperties(i)and(ii)in3n.bychoosingu=a/(cid:107)a(cid:107),pbecomestheprojection onto the line through a, and pb is the point p = x a. rank-1 projections correspond exactly to least-squares problems in one unknown.",orthogonality
17. what 2 by 2 matrix projects the x-y plane onto the −45° line x+y=0?,orthogonality
"18. we want to fit a plane y=c+dt+ez to the four points y=3 at t =1,z=1 y=6 at t =0,z=3 y=5 at t =2,z=1 y=0 at t =0,z=0. (a) find 4 equations in 3 unknowns to pass a plane through the points (if there is such a plane). (b) find 3 equations in 3 unknowns for the best least-squares solution.",orthogonality
"19. if p = a(ata)−1at is the projection onto the column space of a, what is the pro- c jection p onto the row space? (it is not pt!) r c",orthogonality
3.3 projectionsandleastsquares 193,orthogonality
"20. if p is the projection onto the column space of a, what is the projection onto the left nullspace?",orthogonality
"21. suppose l is the line through the origin in the direction of a and l is the line 1 1 2 through b in the direction of a . to find the closest points x a and b+x a on the 2 1 1 2 2 two lines, write the two equations for the x and x that minimize (cid:107)x a −x a −b(cid:107). 1 2 1 1 2 2 solve for x if a =(1,1,0), a =(0,1,0), b=(2,1,4). 1 2",orthogonality
"22. find the best linec+dt to fit b=4,2,−1,0,0 at timest =−2,−1,0,1,2.",orthogonality
"23. showthatthebestleast-squaresfittoasetofmeasurementsy ,...,y byahorizontal 1 m line (a constant function y=c) is their average y +···+y 1 m c = . m",orthogonality
"24. find the best straight-line fit to the following measurements, and sketch your solu- tion: y=2 at t =−1, y=0 at t =0, y=−3 at t =1, y=−5 at t =2.",orthogonality
"25. suppose that instead of a straight line, we fit the data in problem 24 by a parabola: y =c+dt+et2. in the inconsistent system ax = b that comes from the four mea- surements, what are the coefficient matrix a, the unknown vector x, and the data vector b? you need not compute x .",orthogonality
"26. a middle-aged man was stretched on a rack to lengths l = 5, 6, and 7 feet under applied forces of f = 1, 2, and 4 tons. assuming hooke’s law l = a+bf, find his normal length a by least squares. problems 27–31 introduce basic ideas of statistics—the foundation for least squares.",orthogonality
"27.  this problem projects b = (b ,...,b ) onto the line through a = 1 m (1,...,1). we solve m equations ax=b in 1 unknown (by least squares). (a) solve atax =atb to show that is the mean (the average) of the b’s, (b) find e=b−ax , the variance (cid:107)e(cid:107)2, and the standard deviation (cid:107)e(cid:107).   (c) the horizontal line b = 3 is closest to b = (1,2,6), check that p = (3,3,3) is perpendicular to e and find the projection matrix p.",orthogonality
"28. firstassumptionbehindleastsquares: eachmeasurementerrorhasmeanzero. mul- tiply the 8 error vectors b−ax = (±1,±1,±1) by (ata)−1at to show that the 8 vectors x −x also average to zero. the estimate x is unbiased.",orthogonality
"29. second assumption behind least squares: the m errors e are independent with i variance σ2, so the average of (b−ax)(b−ax)t is σ2i. multiply on the left by (ata)−1at andontherightby a(ata)−1 toshowthattheaverageof (x −x)(x −x)t isσ2(ata)−1. this is the all-important covariance matrix for the error in x .",orthogonality
"30. adoctortakesfourreadingsofyourheartrate. thebestsolutiontox=b ,...,x=b 1 4 is the average x of b ,...,b . the matrix a is a column of 1s. problem 29 gives the 1 4 expectederror(x −x)2 asσ2(ata)−1= . byaveraging,thevariancedropsfrom σ2 toσ2/4.",orthogonality
"31. if you know the average x  of 9 numbers b ,...,b , how can you quickly find the 9 1 9 average x  with one more number b ? the idea of recursive least squares is to 10 10 avoid adding 10 numbers. what coefficient of x  correctly gives x  ? 9 10 x  = 1   b + x  = 1 (b +···+b ). 10 10 10 9 10 1 10 problems 32–37 use four points b=(0,8,8,20) to bring out more ideas.",orthogonality
"32. withb=0,8,8,20att =0,1,3,4,setupandsolvethenormalequationsatax =atb. for the best straight line as in figure 3.9a, find its four heights p and four errors e . i i what is the minimum value e2 =e2+e2+e2+e2? 1 2 3 4",orthogonality
"33. (line c+dt does go through p’s) with b = 0,8,8,20 at times t = 0,1,3,4, write the four equations ax=b (unsolvable). change the measurements to p=1,5,13,17 and find an exact solution to ax = p.",orthogonality
"34. check that e = b−p = (−1,3,−5,3) is perpendicular to both columns of a. what is the shortest distance (cid:107)e(cid:107) from b to the column space of a?",orthogonality
"35. for the closest parabola b=c+dt+et2 to the same four points, write the unsolv- able equations ax = b in three unknowns x = (c,d,e). set up the three normal equationsatax =atb(solutionnotrequired). youarenowfittingaparabolatofour points—what is happening in figure 3.9b?",orthogonality
"36. for the closest cubic b =c+dt+et2+ft3 to the same four points, write the four equations ax = b. solve them by elimination, this cubic now goes exactly through the points. what are p and e?",orthogonality
"37. theaverageofthefourtimesis t = 1(0+1+3+4)=2. theaverageofthefourb’s 4 is  b= 1(0+8+8+20)=9. 4 (a) verify that the best line goes through the center point ( t,  b)=(2,9). (b) explain whyc+d t =  b comes from the first equation in atax =atb.",orthogonality
38. what happens to the weighted average x  = (w2b +w2b )/(w2+w2) if the first w 1 1 2 2 1 2 weight w approaches zero? the measurement b is totally unreliable. 1 1,orthogonality
"39. frommindependentmeasurementsb ,...,b ofyourpulserate,weightedbyw ,...,w , 1 m 1 m what is the weighted average that replaces equation (9)? it is the best estimate when the statistical variances areσ2 ≡1/w2. i i  ",orthogonality
"40. ifw = 2 0 , find thew-inner product of x=(2,3) and y=(1,1), and thew-length 0 1 of x. what line of vectors isw-perpendicular to y?",orthogonality
41. find the weighted least-squares solution x  to ax=b: w [ ] [ ] [ ] 1 0 0 2 0 0 [ ] [ ] [ ] a=[1 1] b=[1] w =[0 1 0]. 1 2 1 0 0 1 check that the projection ax  is still perpendicular (in thew-inner product!) to the w error b−ax  . w,orthogonality
"42. (a) supposeyou guess yourprofessor’sage, making errors e=−2,−1,5 withprob- abilities 1,1,1. check that the expected error e(e) is zero and find the variance 2 4 4 e(e2). (b) if the professor guesses too (or tries to remember), making errors −1, 0, 1 with probabilities 1,6,1, what weights w and w give the reliability of your guess 8 8 8 1 2 and the professor’s guess?",orthogonality
"1. solve [1 x] c =x5 by least squares. the equation atax =atb is d   (1,1) (1,x) c (1,x5) 1 1 c 1 = or 2 = 6 . (x,1) (x,x) d (x,x5) 1 1 d 1 2 3 17 (cid:82)",orthogonality
"2. minimizee2 = 1 (x5−c−dx)2dx= 1 −2c−2d+c2+cd+1d2. thederiva- 0 11 6 7 3 tives with respect toc and d, after dividing by 2, bring back the normal equations of method 1 (and the solution isc  = 1− 5 , d  = 5 ): 6 14 17 1 1 1 1 1 − +c+ d=0 and − + c+ d=0. 6 2 7 2 3",orthogonality
"1. (a) write the four equations for fitting y=c+dt to the data y=−4 at t =−2, y=−3 at t =−1 y=−1 at t =1, y=0 at t =2. show that the columns are orthogonal. (b) find the optimal straight line, draw its graph, and write e2. (c) interpret the zero error in terms of the original system of four equations in two unknowns: the right-hand side (−4,−3,−1,0) is in the space.",orthogonality
"2. project b = (0,3,0) onto each of the orthonormal vectors a = (2,2,−1) and a = 1 3 3 3 2 (−1,2,2), and then find its projection p onto the plane of a and a . 3 3 3 1 2",orthogonality
"3. find also the projection of b = (0,3,0) onto a = (2,−1,2), and add the three pro- 3 3 3 3 jections. why is p=a at+a at+a at equal to i? 1 1 2 2 3 3",orthogonality
"4. if q and q are orthogonal matrices, so that qtq = i, show that q q is also 1 2 1 2 orthogonal. if q is rotation throughθ, and q is rotation throughφ, what is q q ? 1 2 1 2 canyoufindthetrigonometricidentitiesforsin(θ+φ)andcos(θ+φ)inthematrix multiplication q q ? 1 2",orthogonality
"5. if u is a unit vector, show that q = i−2uut is a symmetric orthogonal matrix. (it is a reflection, also known as a householder transformation.) compute q when   ut = 1 1 −1 −1 . 2 2 2 2",orthogonality
6. find a third column so that the matrix [ √ √ ] 1/ 3 1/ 14 √ √ [ ] q=[1/ 3 2/ 14 ] √ √ 1/ 3 −3/ 14 is orthogonal. it must be a unit vector that is orthogonal to the other columns; how much freedom does this leave? verify that the rows automatically become orthonor- mal at the same time.,orthogonality
"7. show, by forming btb directly, that pythagoras’s law holds for any combination b=x q +···+x q of orthonormal vectors: (cid:107)b(cid:107)2 =x2+···+x2. in matrix terms, 1 1 n n 1 n b=qx, so this again proves that lengths are preserved: (cid:107)qx(cid:107)2 =(cid:107)x(cid:107)2.",orthogonality
"8. project the vector b = (1,2) onto two vectors that are not orthogonal, a = (1,0) 1 and a = (1,1). show that, unlike the orthogonal case, the sum of the two one- 2 dimensional projections does not equal b.",orthogonality
"9. if the vectors q , q , q are orthonormal, what combination of q and q is closest to 1 2 3 1 2 q ? 3",orthogonality
3.4 orthogonalbasesandgram-schmidt 209,orthogonality
"10. ifq andq aretheoutputsfromgram-schmidt,whatwerethepossibleinputvectors 1 2 a and b?",orthogonality
11. show that an orthogonal matrix that is upper triangular must be diagonal.    ,orthogonality
12. what multiple of a = 1 should be subtracted from a2 = 4 to make the result 1 1  0 orthogonal to a ? factor 1 4 into qr with orthonormal vectors in q. 1 1 0,orthogonality
"13. apply the gram-schmidt process to [ ] [ ] [ ] 0 0 1 [ ] [ ] [ ] a=[0], b=[1], c=[1] 1 1 1 and write the result in the form a=qr.",orthogonality
"14. from the nonorthogonal a, b, c, find orthonormal vectors q , q , q : 1 2 3 [ ] [ ] [ ] 1 1 0 [ ] [ ] [ ] a=[1], b=[0], c=[1]. 0 1 1",orthogonality
"15. find an orthonormal set q , q , q for which q , q span the column space of 1 2 3 1 2 [ ] 1 1 [ ] a=[ 2 −1]. −2 4 which fundamental subspace contains q ? what is the least-squares solution of 3 ax=b if b=[1 2 7]t?",orthogonality
"16. express the gram-schmidt orthogonalization of a , a as a=qr: 1 2 [ ] [ ] 1 1 [ ] [ ] a =[2], a =[3]. 1 2 2 1 given n vectors a with m components, what are the shapes of a, q, and r? i",orthogonality
"17. with the same matrix a as in problem 16, and with b = [1 1 1]t, use a = qr to solve the least-squares problem ax=b.",orthogonality
"18. if a =qr, find a simple formula for the projection matrix p onto the column space of a.",orthogonality
"19. show that these modified gram-schmidt steps produce the same c as in equation (10): c∗ =c−(qtc)q and c =c∗−(qtc∗)q . 1 1 2 2 this is much more stable, to subtract the projections one at a time. √ √ √",orthogonality
"20. in hilbert space, find the length of the vector v = (1/ 2,1/ 4,1/ 8,...) and the length of the function f(x) = ex (over the interval 0 ≤ x ≤ 1). what is the inner product over this interval of ex and e−x?",orthogonality
21. what is the closest function acosx+bsinx to the function f(x) = sin2x on the in- terval from −πtoπ? what is the closest straight line c+dx?,orthogonality
"22. by setting the derivative to zero, find the value of b that minimizes 1 2π (cid:107)b sinx−cosx(cid:107)2 = (b sinx−cosx)2dx. 1 1 0 compare with the fourier coefficient b . 1",orthogonality
"23. find the fourier coefficients a , a , b of the step function y(x), which equals 1 on 0 1 1 the interval 0≤x≤πand 0 on the remaining intervalπ<x<2π: (y,1) (y,cosx) (y,sinx) a = a = b = . 0 1 1 (1,1) (cosx,cosx) (sinx,sinx)",orthogonality
"24. findthefourthlegendrepolynomial. itisacubicx3+ax2+bx+cthatisorthogonal to 1, x, and x2−1 over the interval −1≤x≤1. 3",orthogonality
25. what is the closest straight line to the parabola y=x2 over −1≤x≤1?,orthogonality
"26. in the gram-schmidt formula (10), verify thatc is orthogonal to q and q . 1 2",orthogonality
"27. find an orthonormal basis for the subspace spanned by a = (1,−1,0,0), a = 1 2 (0,1,−1,0), a =(0,0,1,−1). 3",orthogonality
"28. apply gram-schmidt to (1,−1,0), (0,1,−1), and (1,0,−1), to find an orthonormal basis on the plane x +x +x =0. what is the dimension of this subspace, and how 1 2 3 many nonzero vectors come out of gram-schmidt?",orthogonality
"29.  find orthogonal vectors a, b,c by gram-schmidt from a, b, c: a=(1,−1,0,0) b=(0,1,−1,0) c=(0,0,1,−1). a, b,c and a, b, c are bases for the vectors perpendicular to d =(1,1,1,1).",orthogonality
"30. if a=qr then ata=rtr= triangular times triangular. gram-schmidt on a corresponds to elimination on ata. compare [ ] [ ] 1 0 0 [ ] 2 −1 0 [−1 1 0 ] [ ] a=[ ] with ata=[−1 2 −1]. [ 0 −1 1 ] 0 −1 2 0 0 −1 for ata, the pivots are 2, 3, 4 and the multipliers are −1 and −2. 2 3 2 3",orthogonality
"3.5 thefastfouriertransform 211 (a) using those multipliers in a, show that column 1 of a and b = column 2− 1(column 1) andc =column 3−2(column 2) are orthogonal. 2 3 (b) check that (cid:107)column 1(cid:107)2 =2, (cid:107)b(cid:107)2 = 3, and (cid:107)c(cid:107)2 = 4, using the pivots. 2 3",orthogonality
31. true or false (give an example in either case): (a) q−1 is an orthogonal matrix when q is an orthogonal matrix. (b) if q (3 by 2) has orthonormal columns then (cid:107)qx(cid:107) always equals (cid:107)x(cid:107).,orthogonality
"32. (a) find a basis for the subspace s in r4 spanned by all solutions of x +x +x −x =0. 1 2 3 4 (b) find a basis for the orthogonal complement s⊥. (c) find b in s and b in s⊥ so that b +b =b=(1,1,1,1). 1 2 1 2",orthogonality
1. what are f2 and f4 for the 4 by 4 fourier matrix f?,orthogonality
"2. find a permutation p of the columns of f that produces fp=f (n by n), combine with ff =ni to find f2 and f4 for the n by n fourier matrix.",orthogonality
"3. if you form a 3 by 3 submatrix of the 6 by 6 matrix f , keeping only the entries in 6 its first, third, and fifth rows and columns, what is that submatrix?",orthogonality
4. mark all the sixth roots of 1 in the complex plane. what is the primitive root w ? 6 (find its real and imaginary part.) which power of w is equal to 1/w ? what is 6 6 1+w+w2+w3+w4+w5?,orthogonality
"5. find all solutions to the equation eix =−1, and all solutions to eiθ =i.",orthogonality
"6. what are the square and the square root of w , the primitive 128th root of 1? 128",orthogonality
"7. solve the 4 by 4 system (6) if the right-hand sides are y =2, y =0, y =2, y =0. 0 1 2 3 in other words, solve f c=y. 4",orthogonality
"8. solve the same system with y = (2,0,−2,0) by knowing f−1 and computing c = 4 f−1y. verify that c +c eix+c e2ix+c e3ix takes the values 2, 0, −2, 0 at the points 4 0 1 2 3 x=0,π/2,π,3π/2.",orthogonality
"9. (a) if y=(1,1,1,1), show that c=(1,0,0,0) satisfies f c=y. 4 (b) now suppose y=(1,0,0,0), and find c.",orthogonality
"10. for n = 2, write y from the first line of equation (13) and y from the second line. 0 1 for n = 4, use the first line to find y and y , and the second to find y and y , all in 0 1 2 3 terms of y and y.",orthogonality
"11. compute y=f c by the three steps of the fast fourier transform if c=(1,0,1,0). 4",orthogonality
"12. computey=f cbythethreestepsofthefastfouriertransformifc=(1,0,1,0,1,0,1,0). 8 repeat the computation with c=(0,1,0,1,0,1,0,1).",orthogonality
"13. for the 4 by 4 matrix, write out the formulas for c , c , c , c and verify that if f is 0 1 2 3 odd then c is odd. the vector f is odd if f = −f ; for n = 4 that means f = 0, n−j j 0 f =−f , f =0 as in sin0, sinπ/2, sinπ, sin3π/2. this is copied by c and it leads 3 1 2 to a fast sine transform.",orthogonality
14. multiplythethreematricesinequation(16)andcomparewithf. inwhichsixentries do you need to know that i2 =−1?,orthogonality
15. invert the three factors in equation (14) to find a fast factorization of f−1.,orthogonality
16. f is symmetric. so transpose equation (14) to find a new fast fourier transform!,orthogonality
"17. all entries in the factorization of f involve powers of w= sixth root of 1: 6    i d f 3 f = p . 6 i −d f 3 write these factors with 1, w, w2 in d and 1, w2, w4 in f . multiply! 3 problems18–20introducetheideaofaneigenvectorandeigenvalue,whenamatrix times a vector is a multiple of that vector. this is the theme of chapter 5.",orthogonality
18. the columns of the fourier matrix f are the eigenvectors of the cyclic permutation p. multiply pf to find the eigenvaluesλ toλ : 0 3 [ ][ ] [ ][ ] 0 1 0 0 1 1 1 1 1 1 1 1 λ 0 [ ][ ] [ ][ ] [0 0 1 0][1 i i2 i3 ] [1 i i2 i3 ][ λ ] 1 [ ][ ]=[ ][ ]. [0 0 0 1][1 i2 i4 i6] [1 i2 i4 i6][ λ ] 2 1 0 0 0 1 i3 i6 i9 1 i3 i6 i9 λ 3 this is pf =fλ or p=fλf−1.,orthogonality
"19. two eigenvectors of this circulant matrixc are (1,1,1,1) and (1,i,i2,i3). what are the eigenvalues e and e ? 0 1 [ ][ ] [ ] [ ] [ ] c c c c 1 1 1 1 0 1 2 3 [ ][ ] [ ] [ ] [ ] [c c c c ][1] [1] [i] [i] 3 0 1 2 [ ][ ]=e [ ] and c[ ]=e [ ]. [c c c c ][1] 0 [1] [i2] 1 [i2] 2 3 0 1 c c c c 1 1 i3 i3 1 2 3 0",orthogonality
3.5 thefastfouriertransform 221,orthogonality
"20. find the eigenvalues of the “periodic” −1, 2, −1 matrix c. the −1s in the corners ofc make it periodic (a circulant matrix): [ ] 2 −1 0 −1 [ ] [−1 2 −1 0 ] c =[ ] has c =2, c =−1, c =0, c =−1. 0 1 2 3 [ 0 −1 2 −1] −1 0 −1 2",orthogonality
"21. tomultiplyc timesx,whenc=fef−1,wecanmultiplyf(e(f−1x))instead. the direct cx uses n2 separate multiplications. knowing e and f, the second way uses only nlog n+n multiplications. how many of those come from e, how many from 2 f, and how many from f−1?",orthogonality
"22. how could you quickly compute these four components of fc starting from c +c , 0 2 c −c , c +c , c −c ? you are finding the fast fourier transform! 0 2 1 3 1 3 [ ] c +c +c +c 0 1 2 3 [ ] [c +ic +i2c +i3c ] 0 1 2 3 fc=[ ]. [c +i2c +i4c +i6c ] 0 1 2 3 c +i3c +i6c +i9c 0 1 2 3 review exercises",orthogonality
"3.1 find the length of a = (2,−2,1), and write two independent vectors that are per- pendicular to a.",orthogonality
"3.2 find all vectors that are perpendicular to (1,3,1) and (2,7,2), by making those the rows of a and solving ax=0.",orthogonality
"3.3 what is the angle between a=(2,−2,1) and b=(1,2,2)?",orthogonality
"3.4 what is the projection p of b=(1,2,2) onto a=(2,−2,1)?",orthogonality
"3.5 find the cosine of the angle between the vectors (3,4) and (4,3),",orthogonality
"3.6 where is the projection of b = (1,1,1) onto the plane spanned by (1,0,0) and (1,1,0)?",orthogonality
3.7 the system ax=b has a solution if and only if b is orthogonal to which of the four fundamental subspaces?,orthogonality
"3.8 which straight line gives the best fit to the following data: b = 0 at t = 0, b = 0 at t =1, b=12 att =3?",orthogonality
"3.9 construct the projection matrix p onto the space spanned by (1,1,1) and (0,1,3).",orthogonality
3.10 which constant function is closest to y = x4 (in the least-squares sense) over the interval 0≤x≤1?,orthogonality
"3.11 if q is orthogonal, is the same true of q3?",orthogonality
3.12 find all 3 by 3 orthogonal matrices whose entries are zeros and ones.,orthogonality
"3.13 what multiple of a should be subtracted from a , to make the result orthogonal to 1 2 a ? sketch a figure. 1",orthogonality
"3.14 factor cosθ sinθ sinθ 0 into qr, recognizing that the first column is already a unit vector.",orthogonality
"3.15 if every entry in an orthogonal matrix is either 1 or −1, how big is the matrix? 4 4",orthogonality
"3.16 suppose the vectors q ,...,q are orthonormal. if b = c q +···+c q , give a 1 n 1 1 n n formula for the first coefficient c in terms of b and the q’s. 1",orthogonality
"3.17 what words describe the equation atax = atb, the vector p = ax = pb, and the matrix p=a(ata)−1at?",orthogonality
"3.18 if the orthonormal vectors q = (2,2,−1) and q = (−1,2,2) are the columns of 1 3 3 3 2 3 3 3 q, what are the matrices qtq and qqt? show that qqt is a projection matrix (onto the plane of q and q ). 1 2",orthogonality
"3.19 if v ,...,v is an orthonormal basis for rn, show that v vt+···+v vt =i. 1 n 1 1 n n",orthogonality
"3.20 true or false: if the vectors x and y are orthogonal, and p is a projection, then px and py are orthogonal.",orthogonality
"3.21 trytofitalineb=c+dt throughthepointsb=0,t =2,andb=6,t =2,andshow that the normal equations break down. sketch all the optimal lines, minimizing the sum of squares of the two errors.",orthogonality
"3.22 what point on the plane x+y−z=0 is closest to b=(2,1,0)?",orthogonality
"3.23 find an orthonormal basis for r3 starting with the vector (1,1,1).",orthogonality
"3.24 ct scanners examine the patient from different directions and produce a matrix giving the densities of bone and tissue at each point. mathematically, the problem is to recover a matrix from its projections. in the 2 by 2 case, can you recover the matrix a if you know the sum along each row and down each column?",orthogonality
"3.25 can you recover a 3 by 3 matrix if you know its row sums and column sums, and also the sums down the main diagonal and the four other parallel diagonals?",orthogonality
"3.26 find an orthonormal basis for the plane x−y+z = 0, and find the matrix p that projects onto the plane. what is the nullspace of p?",orthogonality
"3.27 let a=[3 1 1], and let v be the nullspace of a. (a) find a basis for v and a basis for v⊥. (b) writeanorthonormalbasisforv⊥,andfindtheprojectionmatrixp thatprojects 1 vectors in r3 onto v⊥. (c) find the projection matrix p that projects vectors in r3 onto v. 2",orthogonality
"3.28 use gram-schmidt to construct an orthonormal pair q , q from a = (4,5,2,2) 1 2 1 and a = (1,2,0,0), express a and a as combinations of q and q , and find the 2 1 2 1 2 triangular r in a=qr.",orthogonality
"3.29 for any a, b, x, and y, show that (a) if ax=b and yta=0, then ytb=0. (b) if ax=0 and aty=b, then xtb=0. what theorem does this prove about the fundamental subspaces?",orthogonality
"3.30 is there a matrix whose row space contains (1,1,0) and whose nullspace contains (0,1,1)?",orthogonality
"3.31 thedistancefromaplaneatx=c(inm-dimensionalspace)totheoriginis|c|/(cid:107)a(cid:107). how far is the plane x +x −x −x = 8 from the origin, and what point on it is 1 2 3 4 nearest?",orthogonality
"3.32 in the parallelogram with corners at 0, v, w, and v+w, show that the sum of the squared lengths of the four sides equals the sum of the squared lengths of the two diagonals.",orthogonality
"3.33 (a) find an orthonormal basis for the column space of a. [ ] 1 −6 [ ] [3 6 ] [ ] a=[4 8 ]. [ ] [5 0 ] 7 8 (b) write a as qr, where q has orthonormal columns and r is upper triangular. (c) find the least-squares solution to ax=b, if b=(−3,7,1,0,4).  ",orthogonality
"3.34 withweightingmatrixw = 2 1 ,whatisthew-innerproductof(1,0)with(0,1)? 1 0",orthogonality
"3.35 to solve a rectangular system ax = b, we replace a−1 (which doesn’t exist) by (ata)−1at (which exists if a has independent columns). show that this is a left- inverseofabutnotaright-inverse. ontheleftofaitgivestheidentity;ontheright it gives the projection p.",orthogonality
"3.36 find the straight line c+dt that best fits the measurements b = 0,1,2,5 at times t =0,1,3,4.",orthogonality
"3.37 find the curve y =c+d2t which gives the best least-squares fit to the measure- ments y = 6 at t = 0, y = 4 at t = 1, y = 0 at t = 2. write the three equations that are solved if the curve goes through the three points, and find the bestc and d.",orthogonality
"3.38 if the columns of a are orthogonal to each other what can you say about the form of ata? if the columns are orthonormal, what can you say then?",orthogonality
3.39 under what condition on the columns of a (which may be rectangular) is ata in- vertible?,orthogonality
"1. if a 4 by 4 matrix has deta= 1, find det(2a), det(−a), det(a2), and det(a−1). 2",determinants
"2. if a 3 by 3 matrix has deta=−1, find det(1a), det(−a), det(a2), and det(a−1). 2",determinants
"3. row exchange: add row 1 of a to row 2, then subtract row 2 from row 1. then add row 1 to row 2 and multiply row 1 by −1 to reach b. which rules show the following?c a  detb equals −deta=.a c  those rules could replace rule 2 in the definition of the determinant.",determinants
"4. by applying row operations to produce an upper triangularu, compute [ ] [ ] 1 2 −2 0 2 −1 0 0 [ ] [ ] [ 2 3 −4 1] [−1 2 −1 0 ] det[ ] and det[ ]. [−1 −2 0 2] [ 0 −1 2 −1] 0 2 5 3 0 0 −1 −2 exchange rows 3 and 4 of the second matrix and recompute the pivots and determi- nant. note. some readers will already know a formula for 3 by 3 determinants. it has six terms(equation(2)ofthenextsection),threegoingparalleltothemaindiagonaland three others going the opposite way with minus signs. there is a similar formula for 4 by 4 determinants, but it contains 4!=24 terms (not just eight). you cannot even be sure that a minus sign goes with the reverse diagonal, as the next exercises show.",determinants
5. count row exchanges to find these determinants: [ ] [ ] 0 0 0 1 0 1 0 0 [ ] [ ] [0 0 1 0] [0 0 1 0] det[ ]=±1 and det[ ]=−1. [0 1 0 0] [0 0 0 1] 1 0 0 0 1 0 0 0,determinants
"6. for each n, how many exchanges will put (row n, row n−1,..., row 1) into the normal order (row 1, ... , row n−1, row n)? find detp for the n by n permutation with 1s on the reverse diagonal. problem 5 had n=4.",determinants
7. find the determinants of: (a) a rank one matrix [ ] 1   [ ] a=[4] 2 −1 2 . 2 (b) the upper triangular matrix [ ] 4 4 8 8 [ ] [0 1 2 2] u =[ ]. [0 0 2 6] 0 0 0 2 (c) the lower triangular matrixut. (d) the inverse matrixu−1.,determinants
"4.2 propertiesofthedeterminant 233 (e) the “reverse-triangular” matrix that results from row exchanges, [ ] 0 0 0 2 [ ] [0 0 2 6] m =[ ]. [0 1 2 2] 4 4 8 8",determinants
8. show how rule 6 (det=0 if a row is zero) comes directly from rules 2 and 3.,determinants
"9. suppose you do two row operations at once, going from a b a−mc b−md to . c d c−a d−b find the determinant of the new matrix, by rule 3 or by direct calculation.",determinants
"10. if q is an orthogonal matrix, so that qtq = i, prove that detq equals +1 or −1. what kind of box is formed from the rows (or columns) of q?",determinants
11. prove again that detq = 1 or −1 using only the product rule. if |detq| > 1 then detqn blows up. how do you know this can’t happen to qn?,determinants
12. use row operations to verify that the 3 by 3 “vandermonde determinant” is [ ] 1 a a2 [ ] det[1 b b2]=(b−a)(c−a)(c−b). 1 c c2,determinants
"13. (a) a skew-symmetric matrix satisfies kt =−k, as in [ ] 0 a b [ ] k =[−a 0 c]. −b −c 0 in the 3 by 3 case, why is det(−k) = (−1)3detk? on the other hand detkt = detk (always). deduce that the determinant must be zero. (b) write down a 4 by 4 skew-symmetric matrix with detk not zero.",determinants
"14. true or false, with reason if true and counterexample if false: (a) if a and b are identical except that b =2a , then detb=2deta. 11 11 (b) the determinant is the product of the pivots. (c) if a is invertible and b is singular, then a+b is invertible. (d) if a is invertible and b is singular, then ab is singular. (e) the determinant of ab−ba is zero.",determinants
"15. if every row of a adds to zero, prove that deta = 0. if every row adds to 1, prove that det(a−i)=0. show by example that this does not imply deta=1.",determinants
16. find these 4 by 4 determinants by gaussian elimination: [ ] [ ] 11 12 13 14 1 t t2 t3 [ ] [ ] [21 22 23 24] [t 1 t t2 ] det[ ] and det[ ]. [31 32 33 34] [t2 t 1 t ] 41 42 43 44 t3 t2 t 1,determinants
"17. find the determinants of 4 2 1 3 −2 4−λ 2 a= , a−1 = , a−λi = . 1 3 10 −1 4 1 3−λ for which values ofλ is a−λi a singular matrix?",determinants
"18. evaluate deta by reducing the matrix to triangular form (rules 5 and 7). [ ] [ ] [ ] 1 1 3 1 1 3 1 1 3 [ ] [ ] [ ] a=[0 4 6], b=[0 4 6], c =[0 4 6]. 1 5 8 0 0 1 1 5 9 what are the determinants of b,c, ab, ata, andct?",determinants
"19. suppose that cd = −dc, and find the flaw in the following argument: taking de- terminants gives (detc)(detd) = −(detd)(detc), so either detc = 0 or detd = 0. thuscd=−dc is only possible ifc or d is singular.",determinants
"20. do these matrices have determinant 0, 1, 2, or 3? [ ] [ ] [ ] 0 0 1 0 1 1 1 1 1 [ ] [ ] [ ] a=[1 0 0] b=[1 0 1] c =[1 1 1]. 0 1 0 1 1 0 1 1 1",determinants
21. the inverse of a 2 by 2 matrix seems to have determinant =1: 1 d −b ad−bc deta−1 =det = =1. ad−bc −c a ad−bc what is wrong with this calculation? what is the correct deta−1? problems 22–28 use the rules to compute specific determinants.,determinants
22. reduce a tou and find deta= product of the pivots: [ ] [ ] 1 1 1 1 2 3 [ ] [ ] a=[1 2 2] and a=[2 2 3]. 1 2 3 3 3 3,determinants
4.2 propertiesofthedeterminant 235,determinants
"23. by applying row operations to produce an upper triangularu, compute [ ] [ ] 1 2 3 0 2 1 1 1 [ ] [ ] [ 2 6 6 1] [1 2 1 1] det[ ] and det[ ]. [−1 0 0 3] [1 1 2 1] 0 2 0 7 1 1 1 2",determinants
24. use row operations to simplify and compute these determinants: [ ] [ ] 101 201 301 1 t t2 [ ] [ ] det[102 202 302] and det[t 1 t ]. 103 203 303 t2 t 1,determinants
"25. elimination reduces a tou. then a=lu: [ ] [ ][ ] 3 3 4 1 0 0 3 3 4 [ ] [ ][ ] a=[ 6 8 7 ]=[ 2 1 0][0 2 −1]=lu. −3 5 −9 −1 4 1 0 0 −1 find the determinants of l,u, a,u−1l−1, andu−1l−1a.",determinants
"26. if a is i times j, show that deta=0. (exception when a=[1].) ij",determinants
"27. if a is i+ j, show that deta=0. (exception when n=1 or 2.) ij",determinants
"28. compute the determinants of these matrices by row operations: [ ] [ ] [ ] 0 a 0 0 0 a 0 [ ] a a a [ ] [0 0 b 0] [ ] a=[0 0 b], b=[ ], and c =[a b b]. [0 0 0 c] c 0 0 a b c d 0 0 0",determinants
29. what is wrong with this proof that projection matrices have detp=1? 1 p=a(ata)−1at so |p|=|a| |at|=1. |at||a|,determinants
"30. (calculus question) show that the partial derivatives of ln(deta) give a−1: ∂f/∂a ∂f/∂c f(a,b,c,d)=ln(ad−bc) leads to =a−1. ∂f/∂b ∂f/∂d",determinants
"31. (matlab) the hilbert matrix hilb(n) has i, j entry equal to 1/(i+ j−1). print ti determinants of hilb(1),hilb(2),...,hilb(10). hilbert matrices are hard to work with! what are the pivots?",determinants
"32. (matlab)whatisatypicaldeterminant(experimentally)ofrand(n)andrandn(n) for n=50,100,200,400? (and what does “inf” mean in matlab?)",determinants
"33. using matlab, find the largest determinant of a 4 by 4 matrix of 0s and 1s.",determinants
"34. if you know that deta=6, what is the determinant of b?row row 1+row  detarow =6 detbrow 2+row =row row 3+row ",determinants
"35. suppose the 4 by 4 matrix m has four equal rows all containing a, b, c, d. we know that det(m)=0. the problem is to find det(i+m) by any method: 1+a b c d a 1+b c d det(i+m). a b 1+c d a b c 1+d partial credit if you find this determinant when a = b = c = d = 1. sudden death if you say that det(i+m)=deti+detm.",determinants
"1. for these matrices, find the only nonzero term in the big formula (6): [ ] [ ] 0 1 0 0 0 0 1 2 [ ] [ ] [1 0 1 0] [0 3 4 5] a=[ ] and b=[ ]. [0 1 0 1] [6 7 8 9] 0 0 1 0 0 0 0 1 there is only one way of choosing four nonzero entries from different rows and different columns. by deciding even or odd, compute deta and detb.",determinants
2. expand those determinants in cofactors of the first row. find the cofactors (they include the signs (−1)i+j) and the determinants of a and b.,determinants
"3. true or false? (a) the determinant of s−1as equals the determinant of a. (b) if deta=0 then at least one of the cofactors must be zero. (c) a matrix whose entries are 0s and 1s has determinant 1, 0, or −1.",determinants
"4. (a) find the lu factorization, the pivots, and the determinant of the 4 by 4 matrix whose entries are a = smaller of i and j. (write out the matrix.) ij (b) find the determinant if a = smaller of n and n , where n =2, n =6, n =8, ij i j 1 2 3 n =10. can you give a general rule for any n ≤n ≤n ≤n ? 4 1 2 3 4",determinants
"5. let f be the determinant of the 1, 1, −1 tridiagonal matrix (n by n): n [ ] 1 −1 [ ] [1 1 −1 ] [ ] f =det[ 1 1 −1 ]. n [ ] [ · · ·] 1 1 by expanding in cofactors along row 1, show that f =f +f . this yields the n n−1 n−2 fibonacci sequence 1,2,3,5,8,13,... for the determinants.",determinants
"6. suppose a is the n by n tridiagonal matrix with is on the three diagonals: n [ ] 1 1 0   1 1 [ ] a = 1 , a = , a =[1 1 1], ... 1 2 3 1 1 0 1 1 let d be the determinant of a ; we want to find it. n n (a) expand in cofactors along the first row to show that d =d −d . n n−1 n−2 (b) starting from d = 1 and d = 0, find d ,d ,...,d . by noticing how these 1 2 3 4 8 numbers cycle around (with what period?) find d . 1000",determinants
7. (a) evaluate this determinant by cofactors of row 1: 4 4 4 41 2 0 .2 0 1  1 1 0 2 (b) check by subtracting column 1 from the other columns and recomputing.,determinants
"8. compute the determinants of a , a , a . can you predict a ? 2 3 4 n [ ] [ ] 0 1 1 1 0 1 1 [ ] 0 1 [ ] [1 0 1 1] a = a =[1 0 1] a =[ ]. 2 3 4 1 0 [1 1 0 1] 1 1 0 1 1 1 0 use row operations to produce zeros, or use cofactors of row 1.",determinants
"9. how many multiplications to find an n by n determinant from (a) the big formula (6)? (b) the cofactor formula (10), building from the count for n−1? (c) the product of pivots formula (including the elimination steps)?",determinants
"10. ina5by5matrix,doesa+signor−signgowitha a a a a downthereverse 15 24 33 42 51 diagonal? inotherwords,isp=(5,4,3,2,1)evenorodd? thecheckerboardpattern of ± signs for cofactors does not give detp.",determinants
"11. if a is m by n and b is n by m, explain why  0 a i 0 det =detab. hint: postmultiply by . −b i b i do an example with m < n and an example with m > n. why does your second example automatically have detab=0?",determinants
"12. supposethematrixaisfixed,exceptthata variesfrom−∞to+∞. giveexamples 11 in which deta is always zero or never zero. then show from the cofactor expansion (8) that otherwise deta=0 for exactly one value of a . 11 problems 13–23 use the big formula with n! terms: |a|=∑±a a ···a . 1α 2β nv",determinants
"13. compute the determinants of a, b,c from six terms. independent rows? [ ] [ ] [ ] 1 2 3 1 2 3 1 1 1 [ ] [ ] [ ] a=[3 1 2] b=[4 4 4] c =[1 1 0]. 3 2 1 5 6 7 1 0 0",determinants
"14. compute the determinants of a, b,c. are their columns independent? [ ] [ ] 1 1 0 1 2 3 [ ] [ ] a 0 a=[1 0 1] b=[4 5 6] c = . 0 b 0 1 1 7 8 9",determinants
"15. show that deta=0, regardless of the five nonzeros marked by x’s: [ ] x x x [ ] a=[0 0 x]. (what is the rank of a?) 0 0 x",determinants
16. this problem shows in two ways that deta=0 (the x’s are any numbers): [ ] x x x x x [ ] [x x x x x] 5 by 5 matrix [ ] a=[0 0 0 x x]. 3 by 3 zero matrix [ ] [0 0 0 x x] always singular 0 0 0 x x (a) how do you know that the rows are linearly dependent? (b) explain why all 120 terms are zero in the big formula for deta.,determinants
17. find two ways to choose nonzeros from four different rows and columns: [ ] [ ] 1 0 0 1 1 0 0 2 [ ] [ ] [0 1 1 1] [0 3 4 5] a=[ ] b=[ ]. (b has the same zeros as a.) [1 1 0 1] [5 4 0 3] 1 0 0 1 2 0 0 1 is deta equal to 1+1 or 1−1 or −1−1? what is detb?,determinants
18. place the smallest number of zeros in a 4 by 4 matrix that will guarantee deta = 0. place as many zeros as possible while still allowing deta=0.,determinants
"19. (a) if a =a =a =0, how many of the six terms in deta will be zero? 11 22 33 (b) ifa =a =a =a =0, howmanyofthe24products a a a a aresure 11 22 33 44 1j 2k 3 4m to be zero?",determinants
20. how many 5 by 5 permutation matrices have detp = +1? those are even permuta- tions. find one that needs four exchanges to reach the identity matrix.,determinants
"21. if deta = 0, at least one of the n! terms in the big formula (6) is not zero. deduce that some ordering of the rows of a leaves no zeros on the diagonal. (don’t use p from elimination; that pa can have zeros on the diagonal.)",determinants
22. prove that 4 is the largest determinant for a 3 by 3 matrix of 1s and −1s.,determinants
"23. how many permutations of (1,2,3,4) are even and what are they? extra credit: what are all the possible 4 by 4 determinants of i+p ? even problems 24–33 use cofactorsc =(−1)i+jdetm . delete row i, column j. ij ij",determinants
24. find cofactors and then transpose. multiplyct andct by a and b! a b [ ] 1 2 3 2 1 [ ] a= b=[4 5 6]. 3 6 7 0 0,determinants
25. find the cofactor matrixc and compare act with a−1: [ ] [ ] 2 −1 0 3 2 1 [ ] 1[ ] a=[−1 2 −1] a−1 = [2 4 2]. 4 0 −1 2 1 2 3,determinants
4.3 formulasforthedeterminant 245,determinants
"26. the matrix b is the −1, 2, −1 matrix a except that b = 1 instead of a = 2. n n 11 11 using cofactors of the last row of b , show that |b |=2|b |−|b |=1: 4 4 3 2 [ ] [ ] 1 −1 [ ] 1 −1 [−1 2 −1 ] [ ] b =[ ] b =[−1 2 −1]. 4 3 [ −1 2 −1] −1 2 −1 2 the recursion |b |=2|b |−|b | is the same as for the a’s. the difference is in n n−1 n−2 the starting values 1, 1, 1 for n=1,2,3. what are the pivots?",determinants
"27. b is still the same as a except for b = 1. so use linearity in the first row, where n n 11 [1 −1 0] equals [2 −1 0] minus [1 0 0]: 1 −1 0 2 −1 0 1 0 0−1−1−1 |b |. n a a a n−1 n−1 n−1 0 0 0 linearity in row 1 gives |b |=|a |−|a |= . n n n−1",determinants
"28. the n by n determinantc has 1s above and below the main diagonal: n 0 1 0 00 1 0 1 0 1  c  c  c 1 0  c . 1 2 3 41 0 1 0 0 1  0 0 1 0 (a) what are the determinants ofc ,c ,c ,c ? 1 2 3 4 (b) by cofactors find the relation betweenc andc andc . findc . n n−1 n−2 10",determinants
"29. problem 28 has 1s just above and below the main diagonal. going down the matrix, which order of columns (if any) gives all 1s? explain why that permutation is even for n=4,8,12,... and odd for n=2,6,10,... c =0 (odd n) c =1 (n=4,8,...) c =−1 (n=2,6,...). n n n",determinants
"30. explain why this vandermonde determinant contains x3 but not x4 or x5: [ ] 1 a a2 a3 [ ] [1 b b2 b3 ] v =det[ ]. 4 [1 c c2 c3] 1 x x2 x3 the determinant is zero at x = , , and . the cofactor of x3 is v = 3 (b−a)(c−a)(c−b). thenv =(x−a)(x−b)(x−c)v . 4 3",determinants
"31. compute the determinants s , s , s of these 1, 3, 1 tridiagonal matrices: 1 2 33 1 3  s s  s 1 3 . 1 2 31 0 1  make a fibonacci guess for s and verify that you are right. 4",determinants
"32. cofactors of those 1, 3, 1 matrices give s = 3s −s . challenge: show that n n−1 n−2 s is the fibonacci number f by proving f = 3f −f . keep using n 2n+2 2n+2 2n 2n−2 fibonacci’s rule f =f +f . k k−1 k−2",determinants
"33. change 3 to 2 in the upper left corner of the matrices in problem 32. why does that subtract s from the determinant s ? show that the determinants become the n−1 n fibonacci numbers 2, 5, 13 (always f ). 2n+1 problems 34–36 are about block matrices and block determinants.",determinants
"34. with 2 by 2 blocks, you cannot always use block determinants!a a =|a||d| but=|a||d|−|c||b|.0 c  (a) why is the first statement true? somehow b doesn’t enter. (b) show by example that equality fails (as shown) whenc enters. (c) show by example that the answer det(ad−cb) is also wrong.",determinants
"35. with block multiplication, a=lu has a =l u in the upper left corner: k k k  a ∗ l 0 u ∗ k k k a= = . ∗ ∗ ∗ ∗ 0 ∗ (a) suppose the first three pivots of a are 2, 3, −1. what are the determinants of l , 1 l , l (with diagonal 1s),u ,u ,u , and a , a , a ? 2 3 1 2 3 1 2 3 (b) if a , a , a have determinants 5, 6, 7, find the three pivots. 1 2 3",determinants
36. block elimination subtracts ca−1 times the first row [a b] from the second row [c d]. this leaves the schur complement d−ca−1b in the corner:  i 0 a b a b = . −ca−1 i c d 0 d−ca−1b take determinants of these matrices to prove correct rules for square blocks:a =|a d−ca−1 =|ad−cb|.c  ifa−1exists ifac=ca,determinants
37. a 3 by 3 determinant has three products “down to the right” and three “down to the left” with minus signs. compute the six terms in the figure to find d. then explain without determinants why this matrix is or is not invertible:,determinants
"38. for a in problem 6, five of the 4! = 24 terms in the big formula (6) are nonzero. 4 find those five terms to show that d =−1. 4",determinants
"39. for the 4 by 4 tridiagonal matrix (entries −1, 2, −1), find the five terms in the big formula that give deta=16−4−4−4+1.",determinants
"40. find the determinant of this cyclic p by cofactors of row 1. how many exchanges reorder 4, 1, 2, 3 into 1, 2, 3, 4? is |p2|=+1 or −1? [ ] [ ] 0 0 0 1 0 0 1 0 [ ] [ ] [1 0 0 0] [0 0 0 1] 0 i p=[ ] p2 =[ ]= . [0 1 0 0] [1 0 0 0] i 0 0 0 1 0 0 1 0 0",determinants
"41. a=2∗eye(n)−diag(ones(n−1, 1),1)−diag(ones(n−1, 1),−1) is the −1, 2, −1 matrix. change a(1,1) to 1 so deta = 1. predict the entries of a−1 based on n = 3 and test the prediction for n=4.",determinants
"42. (matlab) the −1, 2, −1 matrices have determinant n+1. compute (n+1)a−1 for n = 3 and 4, and verify your guess for n = 5. (inverses of tridiagonal matrices have the rank-1 form uvt above the diagonal.)",determinants
"43. allpascalmatriceshavedeterminant1. ifisubtract1fromthen,nentry,whydoes the determinant become zero? (use rule 3 or a cofactor.) [ ] [ ] 1 1 1 1 1 1 1 1 [ ] [ ] [1 2 3 4 ] [1 2 3 4 ] det[ ]=1(known) det[ ]=0(explain). [1 3 6 10] [1 3 6 10] 1 4 10 20 1 4 10 19",determinants
1. find the determinant and all nine cofactorsc of this triangular matrix: ij [ ] 1 2 3 [ ] a=[0 4 0]. 0 0 5 formct and verify that act =(deta)i. what is a−1?,determinants
2. use the cofactor matrixc to invert these symmetric matrices: [ ] [ ] 2 −1 0 1 1 1 [ ] [ ] a=[−1 2 −1] and b=[1 2 2]. 0 −1 2 1 2 3,determinants
"3. find x, y, and z by cramer’s rule in equation (4): x + 4y − z = 1 ax + by = 1 and x + y + z = 0 cx + dy = 0 2x + 3z = 0.",determinants
"4. (a) find the determinant when a vector x replaces column j of the identity (consider x =0 as a separate case): j [ ] 1 x 1 [ ] [ 1 · ] [ ] if m =[ x ] then detm = . j [ ] [ · 1 ] x 1 n (b) if ax=b, show that am is the matrix b in equation (4), with b in column j. j (c) derive cramer’s rule by taking determinants in am =b . j",determinants
"5. (a) draw the triangle with vertices a = (2,2), b = (−1,3), and c = (0,0). by regarding it as half of a parallelogram, explain why its area equals 1 2 2 area(abc)= det . 2 −1 3 (b) move the third vertex toc =(1,−4) and justify the formula [ ] [ ] x y 1 2 2 1 1 1 1 [ ] 1 [ ] area(abc)= det[x y 1]= det[−1 3 1]. 2 2 2 2 x y 1 1 −4 1 3 3 hint: subtracting the last row from each of the others leaves [ ] [ ] 2 2 1 1 6 0 [ ] [ ] 1 6 det[−1 3 1]=det[−2 7 0]=det . −2 7 1 −4 1 1 −4 1 sketch a =(1,6), b =(−2,7),c =(0,0) and their relation to a, b,c.",determinants
6. explain in terms of volumes why det3a=3ndeta for an n by n matrix a.,determinants
"7. predict in advance, and confirm by elimination, the pivot entries of [ ] [ ] 2 1 2 2 1 2 [ ] [ ] a=[4 5 0] and b=[4 5 3]. 2 7 0 2 7 0",determinants
"8. find all the odd permutations of the numbers {1,2,3,4}. they come from an odd number of exchanges and lead to detp=−1.",determinants
"9. suppose the permutation p takes (1,2,3,4,5) to (5,4,1,2,3). (a) what does p2 do to (1,2,3,4,5)? (b) what does p−1 do to (1,2,3,4,5)?",determinants
"10. if p is an odd permutation, explain why p2 is even but p−1 is odd.",determinants
"11. prove that if you keep multiplying a by the same permutation matrix p, the first row eventually comes back to its original place.",determinants
"12. if a is a 5 by 5 matrix with all |a | ≤ 1, then deta ≤ . volumes or the big ij formula or pivots should give some upper bound on the determinant. problems 13–17 are about cramer’s rule for x=a−1b.",determinants
13. solve these linear equations by cramer’s rule x =detb /deta: j j 2x + x = 1 1 2 2x + 5x = 1 1 2 (a) (b) x + 2x + x = 70 1 2 3 x + 4x = 2. 1 2 x + 2x = 0. 2 3,determinants
14. use cramer’s rule to solve for y (only). call the 3 by 3 determinant d: ax + by + cz = 1 ax + by = 1 (a) (b) dx + ey − fz = 0 cx + dy = 0. gx + hy + iz = 0.,determinants
"15. cramer’s rule breaks down when deta = 0. example (a) has no solution, whereas (b) has infinitely many. what are the ratios x =detb /deta? j j 2x +3x =1 2x +3x =1 1 2 1 2 (a) (parallel lines) (b) (same line) 4x +6x =1. 4x +6x =2. 1 2 1 2",determinants
"16. quick proof of cramer’s rule. the determinant is a linear function of column 1. it is zeroiftwocolumnsareequal. whenb=ax=x a +x a +x a goesintocolumn 1 1 2 2 3 3 1 to produce b , the determinant is 1b a ax a +x a +x a a a=xa a a=x deta. 2 3 1 1 2 2 3 3 2 3 1 1 2 3 1 (a) what formula for x comes from left side = right side? 1 (b) what steps lead to the middle equation?",determinants
"17. if the right side b is the last column of a, solve the 3 by 3 system ax = b. explain how each determinant in cramer’s rule leads to your solution x. problems 18–26 are about a−1 =ct/deta. remember to transposec.",determinants
18. find a−1 from the cofactor formulact/deta. use symmetry in part (b): [ ] [ ] 1 2 0 2 −1 0 [ ] [ ] (a) a=[0 3 0]. (b) a=[−1 2 −1]. 0 4 1 0 −1 2,determinants
"19. if all the cofactors are zero, how do you know that a has no inverse? if none of the cofactors are zero, is a sure to be invertible?",determinants
"20. find the cofactors of a and multiply act to find deta: [ ] [ ] 1 1 4 6 −3 0 [ ] [ ] a=[1 2 2], c =[· · ·], and act = . 1 2 5 · · · if you change that corner entry from 4 to 100, why is deta unchanged?",determinants
21. suppose deta=1 and you know all the cofactors. how can you find a?,determinants
22. from the formula act =(deta)i show that detc =(deta)n−1.,determinants
"23. (for professors only) if you know all 16 cofactors of a 4 by 4 invertible matrix a, how would you find a?",determinants
"24. if all entries of a are integers, and deta = 1 or −1, prove that all entries of a−1 are integers. give a 2 by 2 example.",determinants
25. l is lower triangular and s is symmetric. assume they are invertible: [ ] [ ] a 0 0 a b d [ ] [ ] l=[b c 0] s=[b c e]. d e f d e f (a) which three cofactors of l are zero? then l−1 is lower triangular. (b) which three pairs of cofactors of s are equal? then s−1 is symmetric.,determinants
26. for n = 5 the matrix c contains cofactors and each 4 by 4 cofactor contains terms and each term needs multiplications. compare with 53 = 125 for the gauss-jordan computation of a−1. problems 27–36 are about area and volume by determinants.,determinants
"27. (a) find the area of the parallelogram with edges v=(3,2) and w=(1,4). (b) find the area of the triangle with sides v, w, and v+w. draw it. (c) find the area of the triangle with sides v, w, and w−v. draw it.",determinants
"28. a box has edges from (0,0,0) to (3,1,1), (1,3,1), and (1,1,3). find its volume and also find the area of each parallelogram face.",determinants
"29. (a) the corners of a triangle are (2,1), (3,4), and (0,5). what is the area? (b) a new corner at (−1,0) makes it lopsided (four sides). find the area.",determinants
"30. the parallelogram with sides (2,1) and (2,3) has the same area as the parallelogram with sides (2,2) and (1,3). find those areas from 2 by 2 determinants and say why they must be equal. (i can’t see why from a picture. please write to me if you do.)",determinants
31. the hadamard matrix h has orthogonal rows. the box is a hypercube! 1 1 1 11 1 −1 − what is deth =volume of a hypercube in r4?1 −1 −1 1 1 −1 1 −1,determinants
"32. if the columns of a 4 by 4 matrix have lengths l , l , l , l , what is the largest 1 2 3 4 possible value for the determinant (based on volume)? if all entries are 1 or −1, what are those lengths and the maximum determinant?",determinants
33. show by a picture how a rectangle with area x y minus a rectangle with area x y 1 2 2 1 produces the area x y −x y of a parallelogram. 1 2 2 1,determinants
"34. when the edge vectors a, b, c are perpendicular, the volume of the box is (cid:107)a(cid:107) times (cid:107)b(cid:107) times (cid:107)c(cid:107). the matrix ata is . find detata and deta.",determinants
35. an n-dimensional cube has how many corners? how many edges? how many (n− 1)-dimensional faces? the n-cube whose edges are the rows of 2i has volume . a hypercube computer has parallel processors at the corners with connections along the edges.,determinants
"36. the triangle with corners (0,0), (1,0), (0,1) has area 1. the pyramid with four 2 corners (0,0,0), (1,0,0), (0,1,0), (0,0,1) has volume . the pyramid in r4 with five corners at (0,0,0,0) and the rows of i has what volume? problems 37–40 are about areas da and volumes dv in calculus.",determinants
37. polar coordinates satisfy x=rcosθand y=rsinθ. polar area j dr dθincludes j:∂x/∂r ∂x/∂cosθ −rsin j .∂y/∂r ∂y/∂sinθ rcosθ the two columns are orthogonal. their lengths are . thus j = .,determinants
"38. spherical coordinates ρ, φ, θ give x = ρsinφcosθ, y = ρsinφsinθ, z = ρcosφ. find the jacobian matrix of 9 partial derivatives: ∂x/∂ρ,∂x/∂φ,∂x/∂θ are in row. 1. simplify its determinant to j =ρ2sinφ. then dv =ρ2sinφdρdφdθ.",determinants
"39. the matrix that connects r,θto x, y is in problem 37. invert that matrix:∂r/∂x ∂r/∂cosθ  j−1 =?∂θ/∂x ∂θ/∂ ?  it is surprising that∂r/∂x=∂x/∂r. the product jj−1 =i gives the chain rule ∂x ∂x∂r ∂x ∂θ = + =1. ∂x ∂r∂x ∂θ∂x",determinants
"40. the triangle with corners (0,0), (6,0), and (1,4) has area . when you rotate it byθ=60° the area is . the rotation matrix hascosθ −sin1  determinant2=?sinθ cosθ? ",determinants
"41. let p = (1,0,−1), q = (1,1,1), and r = (2,2,1). choose s so that pqrs is a parallelogram, and compute its area. choose t,u,v so that opqrstuv is a tilted box, and compute its volume.",determinants
"42. suppose (x,y,z), (1,1,0), and (1,2,1) lie on a plane through the origin. what deter- minant is zero? what equation does this give for the plane?",determinants
"43. suppose (x,y,z) is a linear combination of (2,3,1) and (1,2,3). what determinant is zero? what equation does this give for the plane of all combinations?",determinants
"44. if ax=(1,0,...,0) show how cramer’s rule gives x= first column of a−1.",determinants
"45. (visatoavis)thistakesanoddnumberofexchanges(ivsa,avsi,avis).count the pairs of letters in visa and avis that are reversed from alphabetical order. the difference should be odd. review exercises",determinants
4.1 find the determinants of [ ] [ ] 1 1 1 1 2 −1 0 −1 [ ] [ ] [1 1 1 2] [−1 2 −1 0 ] [ ] and [ ]. [1 1 3 1] [ 0 −1 2 −1] 1 4 1 1 −1 0 −1 2,determinants
"4.2 if b=m−1am, why is detb=deta? show also that deta−1b=1.",determinants
"4.3 starting with a, multiply its first row by 3 to produce b, and subtract the first row of b from the second to producec. how is detc related to deta?",determinants
"4.4 solve 3u+2v=7, 4u+3v=11 by cramer’s rule.",determinants
"4.5 iftheentriesofaanda−1 areallintegers,howdoyouknowthatbothdeterminants are 1 or −1? hint: what is deta times deta−1?",determinants
"4.6 find all the cofactors, and the inverse or the nullspace, of 3 5 cosθ −sinθ a b , , and . 6 9 sinθ cosθ a b",determinants
"4.7 whatisthevolumeoftheparallelepipedwithfourofitsverticesat(0,0,0),(−1,2,2), (2,−1,2), and (2,2,−1)? where are the other four vertices?",determinants
"4.8 how many terms are in the expansion of a 5 by 5 determinant, and how many are sure to be zero if a =0? 21",determinants
"4.9 if p is an even permutation matrix and p is odd, deduce from p +p = p (pt+ 1 2 1 2 1 1 pt)p that det(p +p )=0. 2 2 1 2",determinants
"4.10 if deta > 0, show that a can be connected to i by a continuous chain of matrices a(t) all with positive determinants. (the straight path a(t) = a+t(i−a) does go from a(0)= a to a(1)= i, but in between a(t) might be singular. the problem is not so easy, and solutions are welcomed by the author.)",determinants
"4.11 explain why the point (x,y) is on the line through (2,8) and (4,7) if [ ] x y 1 [ ] det[2 8 1]=0, or x+2y−18=0. 4 7 1",determinants
"4.12 in analogy with the previous exercise, what is the equation for (x,y,z) to be on the plane through (2,0,0), (0,2,0), and (0,0,4)? it involves a 4 by 4 determinant.",determinants
"4.13 if the points (x,y,z), (2,1,0), and (1,1,1) lie on a plane through the origin, what determinant is zero? are the vectors (1,0,−1), (2,1,0), (1,1,1) independent?",determinants
"4.14 if every row of a has either a single +1, or a single −1, or one of each (and is otherwise zero), show that deta=1 or −1 or 0.  ",determinants
"4.15 ifc = a b and d=[u v], thencd=−dc yields 4 equations ax=0: c d w z [ ][ ] [ ] 2a c b 0 u 0 [ ][ ] [ ] [ b a+d 0 b ][v] [0] cd+dc =0 is [ ][ ]=[ ]. [ c 0 a+d c ][w] [0] 0 c b 2d z 0 (a) show that deta=0 if a+d =0. solve for u, v, w, z, the entries of d. (b) show that deta=0 if ad =bc (soc is singular). in all other cases,cd=−dc is only possible with d= zero matrix.",determinants
"4.16 thecircularshiftpermutes(1,2,...,n)into(2,3,...,1). whatisthecorresponding permutation matrix p, and (depending on n) what is its determinant?",determinants
4.17 find the determinant of a = eye(5) + ones(5) and if possible eye(n) + ones(n). 5 chapter eigenvalues and eigenvectors,determinants
"1. find the eigenvalues and eigenvectors of the matrix a= 1 −1 . verify that the trace 2 4 equals the sum of the eigenvalues, and the determinant equals their product.  ",eigenvalues and eigenvectors
"2. with the same matrix a, solve the differential equation du/dt = au, u(0) = 0 . 6 what are the two pure exponential solutions?",eigenvalues and eigenvectors
"3. if we shift to a−7i, what are the eigenvalues and eigenvectors and how are they related to those of a? −6 −1 b=a−7i = . 2 −3",eigenvalues and eigenvectors
"4. solve du/dt =pu, when p is a projection: du 1 1 5 = 2 2 u with u(0)= . dt 1 1 3 2 2 part of u(0) increases exponentially while the nullspace part stays fixed.",eigenvalues and eigenvectors
5. find the eigenvalues and eigenvectors of [ ] [ ] 3 4 2 0 0 2 [ ] [ ] a=[0 1 2] and b=[0 2 0]. 0 0 0 2 0 0 check thatλ +λ +λ equals the trace andλ λ λ equals the determinant. 1 2 3 1 2 3,eigenvalues and eigenvectors
6. giveanexampletoshowthattheeigenvaluescanbechangedwhenamultipleofone row is subtracted from another. why is a zero eigenvalue not changed by the steps of elimination?,eigenvalues and eigenvectors
"7. suppose thatλ is an eigenvalue of a, and x is its eigenvector: ax=λx. (a) show that this same x is an eigenvector of b = a−7i, and find the eigenvalue. this should confirm exercise 3. (b) assumingλ=0, show that x is also an eigenvector of a−1—and find the eigen- value.",eigenvalues and eigenvectors
"8. show that the determinant equals the product of the eigenvalues by imagining that the characteristic polynomial is factored into det(a−λi)=(λ −λ)(λ −λ)···(λ −λ), (16) 1 2 n and making a clever choice ofλ.",eigenvalues and eigenvectors
"9. show that the trace equals the sum of the eigenvalues, in two steps. first, find the coefficient of (−λ)n−1 on the right side of equation (16). next, find all the terms in [ ] a −λ a ··· a 11 12 1n [ ] [ a 21 a 22−λ ··· a 2n ] det(a−λi)=det[ . . . ] [ . . . . . . ] a a ··· a −λ n1 n2 nn that involve (−λ)n−1. they all come from the main diagonal! find that coefficient of (−λ)n−1 and compare.",eigenvalues and eigenvectors
"10. (a) construct 2 by 2 matrices such that the eigenvalues of ab are not the products of the eigenvalues of a and b, and the eigenvalues of a+b are not the sums of the individual eigenvalues. (b) verify, however, that the sum of the eigenvalues of a+b equals the sum of all the individual eigenvalues of a and b, and similarly for products. why is this true?",eigenvalues and eigenvectors
11. the eigenvalues of a equal the eigenvalues of at. this is because det(a−λi) equals det(at−λi). that is true because . show by an example that the eigen- vectors of a and at are not the same.,eigenvalues and eigenvectors
12. find the eigenvalues and eigenvectors of 3 4 a b a= and a= . 4 −3 b a,eigenvalues and eigenvectors
"13. if b has eigenvalues 1, 2, 3,c has eigenvalues 4, 5, 6, and d has eigenvalues 7, 8, 9,   what are the eigenvalues of the 6 by 6 matrix a= b c ? 0 d",eigenvalues and eigenvectors
14. find the rank and all four eigenvalues for both the matrix of ones and the checker board matrix: [ ] [ ] 1 1 1 1 0 1 0 1 [ ] [ ] [1 1 1 1] [1 0 1 0] a=[ ] and c =[ ]. [1 1 1 1] [0 1 0 1] 1 1 1 1 1 0 1 0 which eigenvectors correspond to nonzero eigenvalues?,eigenvalues and eigenvectors
15. whataretherankandeigenvalueswhenaandc inthepreviousexercisearenbyn? remember that the eigenvalueλ=0 is repeated n−r times.,eigenvalues and eigenvectors
"16. if a is the 4 by 4 matrix of ones, find the eigenvalues and the determinant of a−i.",eigenvalues and eigenvectors
17. choose the third row of the “companion matrix” [ ] 0 1 0 [ ] a=[0 0 1] · · · so that its characteristic polynomial |a−λi| is −λ3+4λ2+5λ+6.,eigenvalues and eigenvectors
"18. suppose a has eigenvalues 0, 3, 5 with independent eigenvectors u, v, w. (a) give a basis for the nullspace and a basis for the column space. (b) find a particular solution to ax=v+w. find all solutions. (c) show that ax = u has no solution. (if it had a solution, then would be in the column space.)",eigenvalues and eigenvectors
"19. the powers ak of this matrix a approaches a limit as k →∞: .8 .3 .70 .45 .6 .6 a= , a2 = , and a∞ = . .2 .7 .30 .55 .4 .4 the matrix a2 is halfway between a and a∞. explain why a2 = 1(a+a∞) from the 2 eigenvalues and eigenvectors of these three matrices.",eigenvalues and eigenvectors
20. find the eigenvalues and the eigenvectors of these two matrices: 1 4 2 4 a= and a+i = . 2 3 2 4 a+i has the eigenvectors as a. its eigenvalues are by 1.,eigenvalues and eigenvectors
"21. compute the eigenvalues and eigenvectors of a and a−1: 0 2 −3/4 1/2 a= and a−1 = . 2 3 1/2 0 a−1 has the eigenvectors as a. when a has eigenvalues λ and λ , its inverse 1 2 has eigenvalues .",eigenvalues and eigenvectors
"22. compute the eigenvalues and eigenvectors of a and a2: −1 3 7 −3 a= and a2 = . 2 0 −2 6 a2 has the same as a. when a has eigenvalues λ and λ , a2 has eigenvalues 1 2 .",eigenvalues and eigenvectors
"23. (a) if you know x is an eigenvector, the way to findλ is to . (b) if you knowλ is an eigenvalue, the way to find x is to .",eigenvalues and eigenvectors
"24. what do you do to ax=λx, in order to prove (a), (b), and (c)? (a) λ2 is an eigenvalue of a2, as in problem 22. (b) λ−1 is an eigenvalue of a−1, as in problem 21. (c) λ+1 is an eigenvalue of a+i, as in problem 20. ",eigenvalues and eigenvectors
"25. fromtheunitvectoru= 1,1,3,5 ,constructtherank-1projectionmatrixp=uut. 6 6 6 6 (a) show that pu=u. then u is an eigenvector withλ=1. (b) if v is perpendicular to u show that pv= zero vector. thenλ=0. (c) find three independent eigenvectors of p all with eigenvalueλ=0.",eigenvalues and eigenvectors
"26. solve det(q−λi)=0 by the quadratic formula, to reachλ=cosθ±isinθ: cosθ −sinθ q= rotates the xy-plane by the angleθ. sinθ cosθ find the eigenvectors of q by solving (q−λi)x=0. use i2 =−1.",eigenvalues and eigenvectors
"27. every permutation matrix leaves x=(1,1,...,1) unchanged. thenλ=1. find two moreλ’s for these permutations: [ ] [ ] 0 1 0 0 0 1 [ ] [ ] p=[0 0 1] and p=[0 1 0]. 1 0 0 1 0 0",eigenvalues and eigenvectors
"28. ifahasλ =4andλ =5, thendet(a−λi)=(λ−4)(λ−5)=λ2−9λ+20. find 1 2 three matrices that have trace a+d =9, determinant 20, andλ=4,5.",eigenvalues and eigenvectors
"29. a 3 by 3 matrix b is known to have eigenvalues 0, 1, 2, this information is enough to find three of these: (a) the rank of b, (b) the determinant of btb, (c) the eigenvalues of btb, and (d) the eigenvalues of (b+i)−1.",eigenvalues and eigenvectors
30. choose the second row of a=[0 1] so that a has eigenvalues 4 and 7. ∗ ∗,eigenvalues and eigenvectors
"31. choose a, b, c, so that det(a−λi)=9λ−λ3. then the eigenvalues are −3, 0, 3: [ ] 0 1 0 [ ] a=[0 0 1]. a b c",eigenvalues and eigenvectors
"32. construct any 3 by 3 markov matrix m: positive entries down each column add to 1. if e = (1,1,1), verify that mte = e. by problem 11, λ= 1 is also an eigenvalue of m. challenge: a 3 by 3 singular markov matrix with trace 1 has eigenvalues 2 λ= .",eigenvalues and eigenvectors
33. find three 2 by 2 matrices that haveλ =λ =0. the trace is zero and the determi- 1 2 nant is zero. the matrix a might not be 0 but check that a2 =0.,eigenvalues and eigenvectors
34. this matrix is singular with rank 1. find threeλ’s and three eigenvectors: [ ] [ ] 1 2 1 2   [ ] [ ] a=[2] 2 1 2 =[4 2 4]. 1 2 1 2,eigenvalues and eigenvectors
"35. suppose a and b have the same eigenvalues λ ,...,λ with the same independent 1 n eigenvectors x ,...,x . then a = b. reason: any vector x is a combination c x + 1 n 1 1 ···+c x . what is ax? what is bx? n n",eigenvalues and eigenvectors
"36. find the eigenvalues of a, b, andc: [ ] [ ] [ ] 1 2 3 0 0 1 2 2 2 [ ] [ ] [ ] a=[0 4 5], b=[0 2 0], and c =[2 2 2]. 0 0 6 3 0 0 2 2 2",eigenvalues and eigenvectors
"37. when a+b=c+d, show that (1,1) is an eigenvector and find both eigenvalues: a b a= . c d",eigenvalues and eigenvectors
"38. whenpexchangesrows1and2and columns1and2, theeigenvaluesdon’tchange. find eigenvectors of a and pap forλ=11: [ ] [ ] 1 2 1 6 3 3 [ ] [ ] a=[3 6 3] and pap=[2 1 1]. 4 8 4 8 4 4",eigenvalues and eigenvectors
39. challenge problem: is there a real 2 by 2 matrix (other than i) with a3 = i? its eigenvalues must satisfy λ3 = i. they can be e2πi/3 and e−2πi/3. what trace and determinant would this give? construct a.,eigenvalues and eigenvectors
40. there are six 3 by 3 permutation matrices p. what numbers can be the determinants of p? what numbers can be pivots? what numbers can be the trace of p? what four numbers can be eigenvalues of p?,eigenvalues and eigenvectors
1. factor the following matrices into sλs−1: 1 1 2 1 a= and a= . 1 1 0 0  ,eigenvalues and eigenvectors
"2. find the matrix a whose eigenvalues are 1 and 4, and whose eigenvectors are 3   1 and 2 , respectively. (hint: a=sλs−1.) 1",eigenvalues and eigenvectors
3. find all the eigenvalues and eigenvectors of [ ] 1 1 1 [ ] a=[1 1 1] 1 1 1 and write two different diagonalizing matrices s.,eigenvalues and eigenvectors
"4. if a 3 by 3 upper triangular matrix has diagonal entries 1, 2, 7, how do you know it can be diagonalized? what is λ?",eigenvalues and eigenvectors
5.2 diagonalizationofamatrix 279,eigenvalues and eigenvectors
5. which of these matrices cannot be diagonalized? 2 −2 2 0 2 0 a = a = a = . 1 2 3 2 −2 2 −2 2 2,eigenvalues and eigenvectors
"6. (a) if a2 =i, what are the possible eigenvalues of a? (b) if this a is 2 by 2, and not i or −i, find its trace and determinant. (c) if the first row is (3,−1), what is the second row?  ",eigenvalues and eigenvectors
"7. if a= 4 3 , find a100 by diagonalizing a. 1 2",eigenvalues and eigenvectors
"8. suppose a=uvt is a column times a row (a rank-1 matrix). (a) by multiplying a times u, show that u is an eigenvector. what isλ? (b) what are the other eigenvalues of a (and why)? (c) compute trace(a) from the sum on the diagonal and the sum ofλ’s.",eigenvalues and eigenvectors
9. show by direct calculation that ab and ba have the same trace when a b q r a= and b= . c d s t deduce that ab−ba=i is impossible (except in infinite dimensions).,eigenvalues and eigenvectors
"10. suppose a has eigenvalues 1, 2, 4. what is the trace of a2? what is the determinant of (a−1)t?",eigenvalues and eigenvectors
"11. iftheeigenvaluesofaare1, 1, 2, whichofthefollowingarecertaintobetrue? give a reason if true or a counterexample if false: (a) a is invertible. (b) a is diagonalizable. (c) a is not diagonalizable.",eigenvalues and eigenvectors
"12. suppose the only eigenvectors of a are multiples of x=(1,0,0). true or false: (a) a is not invertible. (b) a has a repeated eigenvalue. (c) a is not diagonalizable.  ",eigenvalues and eigenvectors
13. diagonalizethematrixa= 5 4 andfindoneofitssquareroots—amatrixsuchthat 4 5 r2 =a. how many square roots will there be?,eigenvalues and eigenvectors
14. suppose the eigenvector matrix s has st =s−1. show that a=sλs−1 is symmetric and has orthogonal eigenvectors. problems 15–24 are about the eigenvalue and eigenvector matrices.,eigenvalues and eigenvectors
15. factor these two matrices into a=sλs−1: 1 2 1 1 a= and a= . 0 3 2 2,eigenvalues and eigenvectors
16. if a=sλs−1 then a3 =( )( )( ) and a−1 =( )( )( ).    ,eigenvalues and eigenvectors
"17. if a has λ = 2 with eigenvector x = 1 and λ = 5 with x = 1 , use sλs−1 to 1 1 0 2 2 1 find a. no other matrix has the sameλ’s and x’s.",eigenvalues and eigenvectors
18. suppose a = sλs−1. what is the eigenvalue matrix for a+2i? what is the eigen- vector matrix? check that a+2i =( )( )( )−1.,eigenvalues and eigenvectors
"19. true or false: if the n columns of s (eigenvectors of a) are independent, then (a) a is invertible. (b) a is diagonalizable. (c) s is invertible. (d) s is diagonalizable.",eigenvalues and eigenvectors
"20. if the eigenvectors of a are the columns of i, then a is a matrix. if the eigen- vector matrix s is triangular, then s−1 is triangular and a is triangular.",eigenvalues and eigenvectors
21. describe all matrices s that diagonalize this matrix a: 4 0 a= . 1 2 then describe all matrices that diagonalize a−1.    ,eigenvalues and eigenvectors
22. write the most general matrix that has eigenvectors 1 and 1 . 1 −1,eigenvalues and eigenvectors
"23. find the eigenvalues of a and b and a+b: 1 0 1 1 2 1 a= , b= , a+b= . 1 1 0 1 1 2 eigenvalues of a+b (are equal to)(are not equal to) eigenvalues of a plus eigenval- ues of b.",eigenvalues and eigenvectors
"24. find the eigenvalues of a, b, ab, and ba: 1 0 1 1 1 1 2 1 a= , b= , ab= , and ba= . 1 1 0 1 1 2 1 1 eigenvaluesofab(areequalto)(arenotequalto)eigenvaluesofatimeseigenvalues of b. eigenvalues of ab (are)(are not) equal to eigenvalues of ba.",eigenvalues and eigenvectors
"25. true or false: if the eigenvalues of a are 2, 2, 5, then the matrix is certainly (a) invertible. (b) diagonalizable. (c) not diagonalizable.",eigenvalues and eigenvectors
"26. if the eigenvalues of a are 1 and 0, write everything you know about the matrices a and a2.",eigenvalues and eigenvectors
"27. complete these matrices so that deta=25. then trace=10, andλ=5 is repeated! find an eigcnvector with ax=5x. these matrices will nothe diagonalizabie because there is no second line of eigenvectors. 8 9 4 10 5 a= , a= , and a= . 2 1 −5  ",eigenvalues and eigenvectors
28. the matrix a = 3 1 is not diagonalizable because the rank of a−3i is . 0 3 change one entry to make a diagonalizable. which entries could you change? problems 29–33 are about powers of matrices.,eigenvalues and eigenvectors
29. ak =sλks−1 approachesthezeromatrixask→∞ifandonlyifeveryλhasabsolute value less than . does ak →0 or bk →0? .6 .4 .6 .9 a= and b= . .4 .6 .1 .6,eigenvalues and eigenvectors
30.  find λ and s to diagonalize a in problem 29. what is the limit of λk as k → ∞? what is the limit of sλks−1? in the columns of this limiting matrix you see the .,eigenvalues and eigenvectors
"31. find λ and s to diagonalize b in problem 29. what is b10u for these u ? 0 0 3 3 6 u = , u = , and u = . 0 0 0 1 −1 0",eigenvalues and eigenvectors
32. diagonalize a and compute sλks−1 to prove this formula for ak: 2 1 1 3k+1 3k−1 a= has ak = . 1 2 2 3k−1 3k+1,eigenvalues and eigenvectors
33. diagonalize b and compute sλks−1 to prove this formula for bk: 3 1 3k 3k−2k b= has bk = . 0 2 0 2k problems 34–44 are new applications of a=sλs−1.,eigenvalues and eigenvectors
34. supposethata=sλs−1. takedeterminantstoprovethatdeta=λ λ ···λ =prod- 1 2 n uct ofλ’s. this quick proof only works when a is .,eigenvalues and eigenvectors
"35. the trace of s times λs−1 equals the trace of λs−1 times s. so the trace of a diago- nalizable a equals the trace of λ, which is .  ",eigenvalues and eigenvectors
"36. if a = sλs−1, diagonalize the block matrix b = a 0 . find its eigenvalue and 0 2a eigenvector matrices.",eigenvalues and eigenvectors
37. consider all 4 by 4 matrices a that are diagonalized by the same fixed eigenvector matrix s. show that the a’s form a subspace (ca and a +a have this same s). 1 2 what is this subspace when s=i? what is its dimension?,eigenvalues and eigenvectors
"38. suppose a2 =a. on the left side a multiplies each column of a. which of our four subspaces contains eigenvectors withλ=1? which subspace contains eigenvectors withλ=0? fromthedimensionsofthosesubspaces, ahasafullsetofindependent eigenvectors and can be diagonalized.",eigenvalues and eigenvectors
"39. suppose ax = λx. if λ = 0, then x is in the nullspace. if λ = 0, then x is in the column space. those spaces have dimensions (n−r)+r =n. so why doesn’t every square matrix have n linearly independent eigenvectors?",eigenvalues and eigenvectors
"40. substitute a = sλs−1 into the product (a−λ i)(a−λ i)···(a−λ i) and explain 1 2 n why this produces the zero matrix. we are substituting the matrix a for the number λ in the polynomial p(λ) = det(a−λi). the cayley-hamilton theorem says that this product is always p(a)= zero matrix, even if a is not diagonalizable.  ",eigenvalues and eigenvectors
"41. test the cayley-hamilton theorem on fibonacci’s matrix a = 1 1 . the theorem 1 0 predicts that a2−a−i =0, since det(a−λi) isλ2−λ−1.  ",eigenvalues and eigenvectors
"42. if a= a b , then det(a−λi) is (λ−a)(λ−d). check the cayley-hamilton state- c d ment that (a−ai)(a−di)= zero matrix.    ",eigenvalues and eigenvectors
"43. if a = 1 0 and ab = ba, show that b = a b is also diagonal. b has the same 0 2 c d eigen as a, but different eigen . these diagonal matrices b form a two- dimensional subspace of matrix space. ab−ba = 0 gives four equations for the unknowns a, b, c, d—find the rank of the 4 by 4 matrix.",eigenvalues and eigenvectors
44. if a is 5 by 5. then ab−ba= zero matrix gives 25 equations for the 25 entries in b. show that the 25 by 25 matrix is singular by noticing a simple nonzero solution b.,eigenvalues and eigenvectors
45. find the eigenvalues and eigenvectors for both of these markov matrices a and a∞. explain why a100 is close to a∞: .6 .2 1/3 1/3 a= and a∞ = . .4 .8 2/3 2/3,eigenvalues and eigenvectors
"1. prove that every third fibonacci number in 0,1,1.2,3,... is even.",eigenvalues and eigenvectors
"2. bernadelli studied a beetle “which lives three years only. and propagates in as third year.” they survive the first year with probability 1, and the second with probability 2 1, and then produce six females on the way out: 3 [ ] 0 0 6 [ ] beetle matrix a=[1 0 0]. 2 0 1 0 3 show that a3 =i, and follow the distribution of 3000 beetles for six years.  ",eigenvalues and eigenvectors
"3. for the fibonacci matrix a= 1 1 , compute a2, a3, and a4. then use the text and 1 0 a calculator to find f . 20",eigenvalues and eigenvectors
"4. supposeeach“gibonacci”number g istheaverageofthetwopreviousnumbers k+2 g and g . then g = 1(g +g ): k+1 k k+2 2 k+1 k   g = 1g +1g g g k+2 2 k+1 2 k is k+2 = a k+1 . g =g g g k+1 k+1 k+1 k (a) find the eigenvalues and eigenvectors of a. (b) find the limit as n→∞ of the matrices an =sλns−1. (c) if g =0 and g =1, show that the gibonacci numbers approach 2. 0 1 3",eigenvalues and eigenvectors
5. diagonalize the fibonacci matrix by completing s−1:   1 1 λ λ λ 0 1 2 1 = . 1 0 1 1 0 λ 2   do the multiplication sλks−1 1 to find its second component. this is the kth fi- 0 bonacci number f =(λkłλk)/(λ łλ ). k 1 2 1 2,eigenvalues and eigenvectors
6. the numbersλk andλk satisfy the fibonacci rule f =f +f : 1 2 k+2 k+1 k λk+2 =λk+1+λk and λk+2 =λk+1+λk. 1 1 1 2 2 2 prove this by using the original equation for the λ’s (multiply it by λk). then any combination ofλk andλk satisfies the rule. the combination f =(λk−λk)/(λ − 1 2 k 1 2 1 λ ) gives the right start of f =0 and f =1. 2 0 1,eigenvalues and eigenvectors
"7. lucas started with l = 2 and l = 1. the rule l = l +l is the same, so a 0 1 k+2 k+1 k is still fibonacci’s matrix. add its eigenvectors x +x : 1 2  √   √  λ λ 1(1+ 5) 1(1− 5) 1 l 1 + 2 = 2 + 2 = = 1 . 1 1 1 1 2 l 0 multiplying by ak, the second component is l = λk +λk. compute the lucas k 1 2 number l slowly by l =l +l , and compute approximately byλ10. 10 k+2 k+1 k 1",eigenvalues and eigenvectors
"8. suppose there is an epidemic in which every month half of those who are well be- come sick, and a quarter of those who are sick become dead. find the steady state for the corresponding markov process [ ] [ ][ ] d 1 1 0 d k+1 4 k [ ] [ ][ ] [s ]=[0 3 1 ][s ]. k+1 4 2 k w 0 0 1 w k+1 2 k",eigenvalues and eigenvectors
"9. writethe3by3transitionmatrixforachemistrycoursethatistaughtintwosections, if every week 1 of those in section a and 1 of those in section b drop the course, 4 3 and 1 of each section transfer to the other section. 6",eigenvalues and eigenvectors
10. find the limiting values of y and (k →∞) if k k y =.8y +.3z y =0 k+1 k k 0 z =.2y +.7z z =5. k+1 k k 0 also find formulas for y and z from ak =sλks−1. k k,eigenvalues and eigenvectors
"11. (a) from the fact that column 1 + column 2 = 2(column 3), so the columns are linearly dependent find one eigenvalue and one eigenvector of a: [ ] .2 .4 .3 [ ] a=[.4 .2 .3]. .4 .4 .4 (b) find the other eigenvalues of a (it is markov). (c) if u =(0,10,0), find the limit of aku as k →∞. 0 0",eigenvalues and eigenvectors
"12. supposetherearethreemajorcentersformove-it-yourselftrucks. everymonthhalf of those in boston and in los angeles go to chicago, the other half stay here they are, and the trucks in chicago are split equally between boston and los angeles set up the 3 by 3 transition matrix a, and find the steady state u corresponding to the ∞ eigenvalueλ=1.",eigenvalues and eigenvectors
"13. (a) in what range of a and b is the following equation a markov process? a b 1 u =au = u , u = . k+1 k k 0 1−a 1−b 1 (b) compute u =sλks−1u for any a and b. k 0 (c) under what condition on a and b does u approach a finite limit as k → ∞, and k what is the limit? does a have to be a markov matrix?",eigenvalues and eigenvectors
"14. multinationalcompaniesintheamericas,asia,andeuropehaveassetsof$4trillion. at the start, $2 trillion are in the americas and $2 trillion in europe. each year 1 the 2 american money stays home, and 1 goes to each of asia and europe. for asia and 4 europe, 1 stays home and 1 is sent to the americas. 2 2 (a) find the matrix that gives [ ] [ ] americas americas [ ] [ ] [ asia ] =a[ asia ] europe europe yeark+1 yeark . (b) find the eigenvalues and eigenvectors of a. (c) find the limiting distribution of the $4 trillion as the world ends. (d) find the distribution of the $4 trillion at year k.",eigenvalues and eigenvectors
"15. if a is a markov matrix, show that the sum of the components of ax equals the sum of the components of x. deduce that if ax =λx with λ= 1, the components of the eigenvector add to zero.  ",eigenvalues and eigenvectors
"16. thesolutiontodu/dt =au= 0 −1 u(eigenvaluesiand−i)goesaroundinacircle: 1 0 u=(cost,sint). supposeweapproximatedu/dt byforward,backward,andcentered differences f, b, c: (f) u −u =au or u =(i+a)u (this is euler’s method). n+1 n n n+1 n (b) u −u =au or u =(i−a)−1u (backward euler). n+1 n n+1 n+1 n (c) u −u = 1a(u +u ) or u =(i−1a)−1(i+1a)u . n+1 n 2 n+1 n n+1 2 2 n findtheeigenvaluesofi+a,(iła)−1,and(i−1a)−1(i+1a). forwhichdifference 2 2 equation does the solution u stay on a circle? n",eigenvalues and eigenvectors
"17. what values ofαproduce instability in v =α(v +w ), w =α(v +w )? n+1 n n n+1 n n",eigenvalues and eigenvectors
"18. find the largest a, b, c for which these matrices are stable or neutrally stable: a −.8 b .8 c .8 , , . .8 .2 0 .2 .2 c",eigenvalues and eigenvectors
"19. multiplying term by term, check that (iła)(i+a+a2+···) = i. this series rep- resents (iła)−1. it is nonnegative when a is nonnegative, provided it has a finite sum; the condition for that is λ < 1. add up the infinite series, and confirm that max it equals (iła)−1, for the consumption matrix [ ] 0 1 1 [ ] a=[0 0 1] which hasλ =0. max 0 0 0",eigenvalues and eigenvectors
"20. for a= 0 .2 , find the powers ak (including a0) and show explicitly that their sum 0 .5 agrees with (i−a)−1.",eigenvalues and eigenvectors
21. explain by mathematics or economics why increasing the “consumption matrix” a must increaset =λ (and slow down the expansion). max 1,eigenvalues and eigenvectors
"22. what are the limits as k →∞ (the steady states) of the following?       k 1 k 0 k .4 .2.6 .8 , .4 .2.6 .8 , .4 .2.6 .8 . 0 1 problems 23–29 are about a=sλs−1 and ak =sλks−1",eigenvalues and eigenvectors
23. diagonalize a and compute sλks−1 to prove this formula for ak: 3 2 1 5k+1 5k−1 a= has ak = . 2 3 2 5k−1 5k+1,eigenvalues and eigenvectors
24. diagonalize b and compute sλks−1 to prove this formula for bk: 3 1 3k 3k−2k b= has bk = . 0 2 0 2k,eigenvalues and eigenvectors
"25. the eigenvalues of a are 1 and 9, the eigenvalues of b are ł1 and 9: 5 4 4 5 a= and b= . 4 5 5 4 √ findamatrixsquarerootofafromr=s λs−1,whyistherenorealmatrixsquare root of b?",eigenvalues and eigenvectors
"26. ifaandbhavethesameλ’swiththesamefullsetofindependenteigenvectors,their factorizations into are the same. so a=b.",eigenvalues and eigenvectors
"27. suppose a and b have the same full set of eigenvectors, so that a = sλ s−1 and 1 b=sλ s−1. prove that ab=ba. 2",eigenvalues and eigenvectors
28. (a) when do the eigenvectors forλ=0 span the nullspace n(a)? (b) when do all the eigenvectors forλ=0 span the column space c(a)?,eigenvalues and eigenvectors
"29. the powers ak approach zero if all |λ| < 1, and they blow up if any |λ| > 1. peter i i lax gives four striking examples in his book linear algebra. 3 2 3 2 5 7 5 6.9 a= b= c = d= 1 4 −5 −3 −3 −4 −3 −4 (cid:107)a1024(cid:107)>10700 b1024 =i c1024 =−c (cid:107)d1024(cid:107)<10−78 find the eigenvaluesλ=eiθ of b andc to show that b4 =i andc3 =−i.",eigenvalues and eigenvectors
"1. following the first example in this section, find the eigenvalues and eigenvectors, and the exponential eat, for −1 1 a= . 1 −1",eigenvalues and eigenvectors
"2. for the previous matrix, write the general solution to du/dt = au, and the specific solution that matches u(0) = (3,1). what is the steady state as t → ∞? (this is a continuous markov process;λ=0 in a differential equation corresponds toλ=1 in a difference equation, since e0t =1.)",eigenvalues and eigenvectors
"3. suppose the time direction is reversed to give the matrix −a: du 1 −1 3 = u with u = . 0 dt −1 1 1 find u(t) and show that it blows up instead of decaying as t → ∞. (diffusion is irreversible, and the heat equation cannot run backward.)",eigenvalues and eigenvectors
"4. if p is a projection matrix, show from the infinite series that ep ≈i+1.718p.  ",eigenvalues and eigenvectors
"5. a diagonal matrix like λ = 1 0 satisfies the usual rule eλ(t+t) = eλteλt, because 0 2 the rule holds for each diagonal entry. (a) explain why ea(t+t) =eateat, using the formula eat =seλts−1. (b) show that ea+b =eaeb is not true for matrices, from the example 0 0 0 −1 a= b= (use series for ea and eb). 1 0 0 0",eigenvalues and eigenvectors
"6. the higher order equation y+y=0 can be written as a first-order system by intro- ducing the velocity y as another unknown: d y y y = = . dt y y −y if this is du/dt = au, what is the 2 by 2 matrix a? find its eigenvalues and eigen- vectors, and compute the solution that starts from y(0)=2, y(0)=0.",eigenvalues and eigenvectors
"7. convert y =0 to a first-order system du/dt =au:  d y y 0 1 y = = . dt y 0 0 0 y this 2 by 2 matrix a has only one eigenvector and cannot be diagonalized. compute eat from the series i+at+··· and write the solution eatu(0) starting from y(0)=3, y(0)=4. check that your (y,y) satisfies y =0.",eigenvalues and eigenvectors
"8. suppose the rabbit population r and the wolf population w are governed by dr =4r−2w dt dw =r+w. dt (a) is this system stable, neutrally stable, or unstable? (b) if initially r =300 and w=200, what are the populations at timet? (c) after a long time, what is the proportion of rabbits to wolves?",eigenvalues and eigenvectors
9. decide the stability of u =au for the following matrices: 2 3 1 2 (a) a= . (b) a= . 4 5 3 −1 1 1 −1 −1 (c) a= . (d) a= . 1 −2 −1 −1,eigenvalues and eigenvectors
"10. decide on the stability or instability of dv/dt = w, dw/dt = v. is there a solution that decays?",eigenvalues and eigenvectors
"11. from their trace and determinant, at what time t do the following matrices change betweenstablewithrealeigenvalues,stablewithcomplexeigenvalues,andunstable? 1 −1 0 4−t t −1 a = , a = , a = . 1 2 3 t −1 1 −2 1 t",eigenvalues and eigenvectors
"12. find the eigenvalues and eigenvectors for [ ] 0 3 0 du [ ] =au=[−3 0 4]u. dt 0 −4 0 why do you know, without computing, that eat will be an orthogonal matrix and (cid:107)u(t)(cid:107)2 =u2+u2+u2 will be constant? 1 2 3",eigenvalues and eigenvectors
"13. for the skew-symmetric equation [ ][ ] 0 c −b u 1 du [ ][ ] =au=[−c 0 a ][u ], 2 dt b −a 0 u 3 (a) write out u, u, u and confirm that uu +uu +uu =0. 1 2 3 1 1 2 2 3 3 (b) deduce that the length u2+u2+u2 is a constant. 1 2 3 (c) find the eigenvalues of a. the solution will rotate around the axis w=(a,b,c), because au is the “cross prod- uct” u×w—which is perpendicular to u and w.",eigenvalues and eigenvectors
"14. what are the eigenvalues λ and frequencies ω, and the general solution, of the fol- lowing equation? d2u −5 4 = u. dt2 4 −5",eigenvalues and eigenvectors
15. solve the second-order equation d2u −5 −1 1 0 = u with u(0)= and u(0)= . dt2 −1 −5 0 0,eigenvalues and eigenvectors
"16. inmostapplicationsthesecond-orderequationlookslikemu+ku=0,withamass matrix multiplying the second derivatives. substitute the pure exponential u=eiωtx and find the “generalized eigenvalue problem” that must be solved for the frequency ωand the vector x.",eigenvalues and eigenvectors
"17. with a friction matrix f in the equation u+fu−au = 0, substitute a pure expo- nential u=eλtx and find a quadratic eigenvalue problem forλ. √",eigenvalues and eigenvectors
"18. for equation (16) in the text, withω=1 and 3, find the motion if the first mass is hit att =0; u(0)=(0,0) and u(0)=(1,0).",eigenvalues and eigenvectors
19. every 2 by 2 matrix with trace zero can be written as a b+c a= . b−c −a show that its eigenvalues are real exactly when a2+b2 ≥c2.,eigenvalues and eigenvectors
5.4 differentialequationsandeat 309,eigenvalues and eigenvectors
"20. by back-substitution or by computing eigenvectors, solve [ ] [ ] 1 2 1 1 du [ ] [ ] =[0 3 6]u with u(0)=[0]. dt 0 0 4 1",eigenvalues and eigenvectors
"21. findλ’s and x’s so that u=eλtx solves du 4 3 = u. dt 0 1 what combination u=c eλ 1tx +c eλ 2tx starts from u(0)=(5,−2)? 1 1 2 2",eigenvalues and eigenvectors
"22. solve problem 21 for u(t)=(y(t),z(t)) by back-substitution: dz first solve =z, starting from z(0)=−2. dt dy then solve =4y+3z, starting from y(0)=5. dt the solution for y will be a combination of e4t and et.",eigenvalues and eigenvectors
"23. find a to change y =5y+4y into a vector equation for u(t)=(y(t),y(t)):  du y y = = =au. dt y y what are the eigenvalues of a? find them also by substituting y=eλt into the scalar equation y =5y+4y.",eigenvalues and eigenvectors
"24. a door is opened between rooms that hold v(0)=30 people and w(0)=10 people. the movement between rooms is proportional to the difference v−w: dv dw =w−v and =v−w. dt dt showthatthetotalv+wisconstant(40people). findthematrixindu/dt =au, and its eigenvalues and eigenvectors. what are v and w att =1?",eigenvalues and eigenvectors
25. reverse the diffusion of people in problem 24 to du/dt =−au: dv dw =v−w and =w−v. dt dt thetotalv+wstillremainsconstant. howaretheλ’schangednowthataischanged to −a? but show that v(t) grows to infinity from v(0)=30.,eigenvalues and eigenvectors
"26. the solution to y =0 is a straight line y=c+dt. convert to a matrix equation:  d y 0 1 y y y(0) = has the solution =eat . dt y 0 0 y y y(0) this matrix a cannot be diagonalized. find a2 and compute eat =i+at+ 1a2t2+ 2 ···. multiply your eat times (y(0),y(0)) to check the straight line y(t) = y(0)+ y(0)t.",eigenvalues and eigenvectors
"27. substitute y = eλt into y = 6y−9y to show that λ = 3 is a repeated root. this is trouble; we need a second solution after e3t. the matrix equation is  d y 0 1 y = . dt y −9 6 y show that this matrix has λ = 3,3 and only one line of eigenvectors. trouble here too. show that the second solution is y=te3t.",eigenvalues and eigenvectors
28. figure out how to write my+by+ky=0 as a vector equation mu =au.,eigenvalues and eigenvectors
"29. (a) find two familiar functions that solve the equation d2y/dt2 = −y. which one starts with y(0)=1 and y(0)=0? (b) this second-order equation y =−y produces a vector equation u =au:  y du y 0 1 y u= = = =au. y dt y −1 0 y put y(t) from part (a) into u(t)=(y,y). this solves problem 6 again.",eigenvalues and eigenvectors
"30. aparticularsolutiontodu/dt =au−bisu =a−1b,ifaisinvertible. thesolutions p to du/dt =au give u . find the complete solution u +u to n p n du du 2 0 8 (a) =2u−8. (b) = u− . dt dt 0 3 6",eigenvalues and eigenvectors
"31. if c is not an eigenvalue of a, substitute u = ectv and find v to solve du/dt = au− ectb. this u = ectv is a particular solution. how does it break down when c is an eigenvalue?",eigenvalues and eigenvectors
32. find a matrix a to illustrate each of the unstable regions in figure 5.2: (a) λ <0 andλ >0. 1 2 (b) λ >0 andλ >0. 1 2 (c) complexλ’s with real part a>0. problems 33–41 are about the matrix exponential eat.,eigenvalues and eigenvectors
33. writefivetermsoftheinfiniteseriesforeat. takethet derivativeofeachterm. show that you have four terms of aeat. conclusion: eatu(0) solves u =au.  ,eigenvalues and eigenvectors
34. the matrix b= 0 −1 has b2 =0. find ebt from a (short) infinite series. check that 0 0 the derivative of ebt is bebt.,eigenvalues and eigenvectors
"35. startingfromu(0),thesolutionattimet iseatu(0). goanadditionaltimet toreach eat(eatu(0)). this solution at time t+t can also be written as . conclusion: eat times eat equals .  ",eigenvalues and eigenvectors
36. write a= 1 1 in the form sλs−1. find eat from seλts−1. 0 0  ,eigenvalues and eigenvectors
"37. if a2 =a, show that the infinite series produces eat =i+(et−1)a. for a= 1 1 in 0 0 problem 36, this gives eat =",eigenvalues and eigenvectors
38. generallyeaeb isdifferentfromebea. theyarebothdifferentfromea+b. checkthis using problems 36–37 and 34: 1 1 0 −1 1 0 a= b= a+b= . 0 0 0 0 0 0  ,eigenvalues and eigenvectors
39. write a = 1 1 as sλs−1. multiply seλts−1 to find the matrix exponential eat. 0 3 check eat =i whent =0.  ,eigenvalues and eigenvectors
40. put a= 1 3 into the infinite series to find eat. first compute a2: 0 0 1 0 t 3t 1 et eat = + + +···= . 0 1 0 0 2 0,eigenvalues and eigenvectors
41. give two reasons why the matrix exponential eat is never singular: (a) write its inverse. (b) write its eigenvalues. if ax=λx then eatx= x.,eigenvalues and eigenvectors
"42. find a solution x(t), y(t) of the first system that gets large as t → ∞. to avoid this instability a scientist thought of exchanging the two equations! dx/dt = 0x − 4y dy/dt = −2x + 2y becomes dy/dt = −2x + 2y dx/dt = 0x − 4y.   now the matrix −2 2 is stable. it hasλ<0. comment on this craziness. 0 −4",eigenvalues and eigenvectors
"43. from this general solution to du/dt =au, find the matrix a: 2 1 u(t)=c e2t +c e5t . 1 2 1 1",eigenvalues and eigenvectors
"1. for the complex numbers 3+4i and 1−i, (a) find their positions in the complex plane. (b) find their sum and product. (c) find their conjugates and their absolute values. do the original numbers lie inside or outside the unit circle?",eigenvalues and eigenvectors
2. what can you say about (a) the sum of a complex number and its conjugate? (b) the conjugate of a number on the unit circle? (c) the product of two numbers on the unit circle? (d) the sum of two numbers on the unit circle?,eigenvalues and eigenvectors
"3. ifx=2+iandy=1+3i, findx, xx, 1/x, andx/y. checkthattheabsolutevalue|xy| equals |x| times |y|, and the absolute value |1/x| equals 1 divided by |x|.",eigenvalues and eigenvectors
"4. find a and b for the complex numbers a+ib at the angles θ = 30°,60°,90° on the unit circle. verify by direct multiplication that the square of the first is the second, and the cube of the first is the third.",eigenvalues and eigenvectors
"5. (a) if x = reiθ what are x2, x−1, and x in polar coordinates? where are the complex numbers that have x−1 =x? (b) att =0,thecomplexnumbere(−1+i)t equalsone. sketchitspathinthecomplex plane ast increases from 0 to 2π.",eigenvalues and eigenvectors
6. find the lengths and the inner product of 2−4i 2+4i x= and y= . 4i 4i,eigenvalues and eigenvectors
7. write out the matrix ah and computec =aha if 1 i 0 a= . i 0 1 whatistherelationbetweenc andch? doesitholdwheneverc isconstructedfrom some aha?,eigenvalues and eigenvectors
"8. (a) with the preceding a, use elimination to solve ax=0. (b) show that the nullspace you just computed is orthogonal to c(ah) and not to the usual row space c(at). the four fundamental spaces in the complex case are n(a) and c(a) as before, and then n(ah) and c(ah).",eigenvalues and eigenvectors
9. (a) how is the determinant of ah related to the determinant of a? (b) prove that the determinant of any hermitian matrix is real.,eigenvalues and eigenvectors
"10. (a) how many degrees of freedom are there in a real symmetric matrix, a real diag- onal matrix, and a real orthogonal matrix? (the first answer is the sum of the other two, because a=qλqt.) (b) show that 3 by 3 hermitian matrices a and also unitaryu have 9 real degrees of freedom (columns ofu can be multiplied by any eiθ).",eigenvalues and eigenvectors
"11. write p, q and r in the formλ x xh+λ x xh of the spectral theorem: 1 1 1 2 2 2 1 1 0 1 3 4 p= 2 2 , q= , r= . 1 1 1 0 4 −3 2 2",eigenvalues and eigenvectors
"12. give a reason if true or a counterexample if false: (a) if a is hermitian, then a+ii is invertible. (b) if q is orthogonal. then q+1i is invertible. 2 (c) if a is real, then a+ii is invertible.",eigenvalues and eigenvectors
"13. suppose a is a symmetric 3 by 3 matrix with eigenvalues 0, 1, 2. (a) what properties can be guaranteed for the corresponding unit eigenvectors u, v, w? (b) in terms of u, v, w, describe the nullspace, left nullspace, row space and column space of a. (c) find a vector x that satisfies ax=v+w. is x unique? (d) under what conditions on b does ax=b have a solution? (e) if u, v, w are the columns of s, what are s−1 and s−1as?",eigenvalues and eigenvectors
"14. in the list below, which classes of matrices contain a and which contain b? [ ] [ ] 0 1 0 0 1 1 1 1 [ ] [ ] [0 0 1 0] 1[1 1 1 1] a=[ ] and b= [ ]. [0 0 0 1] 4[1 1 1 1] 1 0 0 0 1 1 1 1 orthogonal, invertible, projection, permutation, hermitian, rank-1, diagonalizable, markov. find the eigenvalues of a and b.",eigenvalues and eigenvectors
"15. what is the dimension of the space s of all n by n real symmetric matrices? the spectral theorem says that every symmetric matrix is a combination of n projection matrices. since the dimension exceeds n, how is this difference explained?",eigenvalues and eigenvectors
16. write one significant fact about the eigenvalues of each of the following. (a) a real symmetric matrix. (b) a stable matrix: all solutions to du/dt =au approach zero. (c) an orthogonal matrix. (d) a markov matrix. (e) a defective matrix (nondiagonalizable). (f) a singular matrix.,eigenvalues and eigenvectors
"17. show that ifu andv are unitary, so isuv. use the criterionuhu =i.",eigenvalues and eigenvectors
"18. show that a unitary matrix has |detu| = 1, but possibly detu is different from detuh. describe all 2 by 2 matrices that are unitary.",eigenvalues and eigenvectors
19. find a third column so thatu is unitary. how much freedom in column 3? [ √ √ ] 1/ 3 i/ 2 √ [ ] u =[1/ 3 0 ]. √ √ i/ 3 1/ 2   √,eigenvalues and eigenvectors
"20. diagonalize the 2 by 2 skew-hermitian matrix k = i i , whose entries are all −1. i i compute ekt = seλts−1, and verify that ekt is unitary. what is the derivative of ekt att =0?",eigenvalues and eigenvectors
"21. describeall3by3matricesthataresimultaneouslyhermitian,unitary,anddiagonal. how many are there?",eigenvalues and eigenvectors
"22. every matrix z can be split into a hermitian and a skew-hermitian part, z =a+k, just as a complex number z is split into a+ib, the real part of z is half of z+z, and the“realpart”ofz ishalfofz+zh. findasimilarformulaforthe“imaginarypart” k, and split these matrices into a+k: 3+i 4+2i i i z = and z = . 0 5 −i i",eigenvalues and eigenvectors
23. show that the columns of the 4 by 4 fourier matrix f in example 5 are eigenvectors of the permutation matrix p in example 6.,eigenvalues and eigenvectors
"24. for the permutation of example 6, write out the circulant matrix c = c i+c p+ 0 1 c p2+c p3. (its eigenvector matrix is again the fourier matrix.) write out also 2 3 the four components of the matrix-vector product cx, which is the convolution of c=(c ,c ,c ,c ) and x=(x ,x ,x ,x ). 0 1 2 3 0 1 2 3",eigenvalues and eigenvectors
"25. for a circulantc =fλf−1, why is it faster to multiply by f−1, then λ, then f (the convolution rule), than to multiply directly byc?",eigenvalues and eigenvectors
"26. find the lengths of u=(1+i,1−i,1+2i) and v=(i,i,i). also find uhv and vhu.",eigenvalues and eigenvectors
"27. prove that aha is always a hermitian matrix, compute aha and aah: i 1 i a= . 1 i i",eigenvalues and eigenvectors
"28. if az = 0, then ahaz = 0. if ahaz = 0, multiply by zh to prove that az = 0. the nullspaces of a and aha are . aha is an invertible hermitian matrix when the nullspace of a contains only z= .",eigenvalues and eigenvectors
"29. when you multiply a hermitian matrix by a real number c, is ca still hermitian? if c=i,showthatiaisskew-hermitian. the3by3hermitianmatricesareasubspace, provided that the “scalars” are real numbers.",eigenvalues and eigenvectors
"30. which classes of matrices does p belong to: orthogonal, invertible, hermitian, uni- tary, factorizable into lu, factorizable into qr? [ ] 0 1 0 [ ] p=[0 0 1]. 1 0 0",eigenvalues and eigenvectors
"31. compute p2, p3, and p100 in problem 30. what are the eigenvalues of p?",eigenvalues and eigenvectors
"32. find the unit eigenvectors of p in problem 30, and put them into the columns of a unitary matrixu. what property of p makes these eigenvectors orthogonal?",eigenvalues and eigenvectors
33. write down the 3 by 3 circulant matrix c = 2i+5p+4p2. it has the same eigen- vectors as p in problem 30. find its eigenvalues.,eigenvalues and eigenvectors
"34. ifu is unitary and q is a real orthogonal matrix, show that u−1 is unitary and also uq is unitary. start fromuhu =i and qtq=i.",eigenvalues and eigenvectors
35. diagonalize a (realλ’s) and k (imaginaryλ’s) to reachuλuh: 0 1−i 0 −1+i a= k = i+1 1 1+i i,eigenvalues and eigenvectors
36. diagonalize this orthogonal matrix to reach q=uλuh. now allλ’s are : cosθ −sinθ q= . sinθ cosθ,eigenvalues and eigenvectors
37. diagonalize this unitary matrixv to reachv =uλuh. again all |λ|=1: 1 1 1−i v = √ . 3 1+i −1,eigenvalues and eigenvectors
"38. if v ,...,v is an orthonormal basis for cn, the matrix with those columns is a 1 n matrix. show that any vector z equals (vhz)v +···+(vhz)v . 1 1 n n",eigenvalues and eigenvectors
39. the functions e−ix and e−ix are orthogonal on the interval 0 ≤ x ≤ 2πbecause their (cid:82) 2π complex inner product is =0. 0,eigenvalues and eigenvectors
"40. the vectors v=(1,i,1), w=(i,1,0) and z= are an orthogonal basis for .",eigenvalues and eigenvectors
"41. if a=r+is is a hermitian matrix, are the real matrices r and s symmetric?",eigenvalues and eigenvectors
42. the (complex) dimension of cn is . find a nonreal basis for cn.,eigenvalues and eigenvectors
43. describe all 1 by 1 matrices that are hermitian and also unitary. do the same for 2 by 2 matrices.,eigenvalues and eigenvectors
44. how are the eigenvalues of ah (square matrix) related to the eigenvalues of a?,eigenvalues and eigenvectors
"45. if uhu=1, show that i−2uuh is hermitian and also unitary. the rank-1 matrix uuh is the projection onto what line in cn?  ",eigenvalues and eigenvectors
"46. ifa+ibisaunitarymatrix(aandbarereal),showthatq= a −b isanorthogonal b a matrix.  ",eigenvalues and eigenvectors
"47. if a+ib is a hermitian matrix (a and b are real), show that a −b is symmetric. b a",eigenvalues and eigenvectors
48. prove that the inverse of a hermitian matrix is again a hermitian matrix.,eigenvalues and eigenvectors
49. diagonalize this matrix by constructing its eigenvalue matrix λ and its eigenvector matrix s: 2 1−i a= =ah. 1+i 3,eigenvalues and eigenvectors
50. a matrix with orthonormal eigenvectors has the form a=uλu−1 =uλuh. prove that aah =aha. these are exactly the normal matrices.,eigenvalues and eigenvectors
"1. ifbissimilartoaandcissimilartob,showthatcissimilartoa. (letb=m−1am andc =n−1bn.) which matrices are similar to i?  ",eigenvalues and eigenvectors
"2. describe in words all matrices that are similar to 1 0 , and find two of them. 0 −1",eigenvalues and eigenvectors
3. explain why a is never similar to a+i.,eigenvalues and eigenvectors
"4. find a diagonal m, made up of 1s and −1s, to show that [ ] [ ] 2 1 2 −1 [ ] [ ] [1 2 1 ] [−1 2 −1 ] a=[ ] is similar to b=[ ]. [ 1 2 1] [ −1 2 −1] 1 2 −1 2",eigenvalues and eigenvectors
5. show (if b is invertible) that ba is similar to ab.,eigenvalues and eigenvectors
"6. (a) ifcd=−dc (and d is invertible), show thatc is similar to −c. (b) deduce that the eigenvalues ofc must come in plus-minus pairs. (c) show directly that ifcx=λx, thenc(dx)=−λ(dx).",eigenvalues and eigenvectors
"7. consider any a and a “givens rotation” m in the 1–2 plane: [ ] [ ] a b c cosθ −sinθ 0 [ ] [ ] a=[d e f], m =[sinθ cosθ 0]. g h i 0 0 1 choose the rotation angleθto produce zero in the (3,1) entry of m−1am. note. this “zeroing” is not so easy to continue, because the rotations that produce zero in place of d and h will spoil the new zero in the corner. we have to leave one diagonalbelowthemainone,andfinishtheeigenvaluecalculationinadifferentway. otherwise,ifwecouldmakeadiagonalandseeitseigenvalues,wewouldbefinding therootsofthepolynomialdet(a−λi)byusingonlythesquarerootsthatdetermine cosθ—and that is impossible.",eigenvalues and eigenvectors
"8. what matrix m changes the basis v = (1,1), v = (1,4) to the basis v = (2,5), 1 2 1 v = (1,4)? the columns of m come from expressing v and v as combinations 2 1 2 ∑m v of the v’s. ij i",eigenvalues and eigenvectors
"9. for the same two bases, express the vector (3,9) as a combination c v +c v and 1 1 2 2 also as d v +d v . check numerically that m connects c to d: mc=d. 1 1 2 2",eigenvalues and eigenvectors
"10. confirm the last exercise: if v = m v +m v and v = m v +m v , and 1 11 1 21 2 2 12 1 22 2 m c +m c =d andm c +m c =d , thevectorsc v +c v andd v +d v 11 1 12 2 1 21 1 22 2 2 1 1 2 2 1 1 2 2 are the same. this is the “change of basis formula” mc=d.",eigenvalues and eigenvectors
"11. if the transformation t is a reflection across the 45° line in the plane, find its matrix with respect to the standard basis v = (1,0), v = (0,1), and also with respect to 1 2 v =(1,1),v =(1,−1). show that those matrices are similar. 1 2",eigenvalues and eigenvectors
"12. the identity transformation takes every vector to itself: tx = x. find the corre- sponding matrix, if the first basis is v = (1,2), v = (3,4) and the second basis is 1 2 w =(1,0), w =(0,1). (it is not the identity matrix!) 1 2",eigenvalues and eigenvectors
13. the derivative of a+bx+cx2 is b+2cx+0x2. (a) write the 3 by 3 matrix d such that [ ] [ ] a b [ ] [ ] d[b]=[2c]. c 0 (b) compute d3 and interpret the results in terms of derivatives. (c) what are the eigenvalues and eigenvectors of d?,eigenvalues and eigenvectors
"14. show that every number is an eigenvalue for t f(x)=df/dx, but the transformation (cid:82) x t f(x)= f(t)dt has no eigenvalues (here −∞<x<∞). 0",eigenvalues and eigenvectors
"15. on the space of 2 by 2 matrices, let t be the transformation that transposes every matrix. find the eigenvalues and “eigenmatrices” for at =λa.",eigenvalues and eigenvectors
"16. (a) find an orthogonal q so that q−1aq=λ if [ ] [ ] 1 1 1 0 0 0 [ ] [ ] a=[1 1 1] and λ=[0 0 0]. 1 1 1 0 0 3, then find a second pair of orthonormal eigenvectors x , x forλ=0. 1 2 (b) verify that p=x xt+x xt is the same for both pairs. 1 1 2 2",eigenvalues and eigenvectors
"17. prove that every unitary matrix a is diagonalizable, in two steps: (i) if a is unitary, andu is too, then so is t =u−1au. (ii) an upper triangular t that is unitary must be diagonal. thus t =λ. any unitary matrix a (distinct eigenvalues or not) has a complete set of orthonormal eigenvectors. all eigenvalues satisfy |λ|=1.",eigenvalues and eigenvectors
"18. find a normal matrix (nnh =nhn) that is not hermitian, skew-hermitian, unitary, or diagonal. show that all permutation matrices are normal.",eigenvalues and eigenvectors
"19. supposet isa3by3uppertriangularmatrix,withentriest . comparetheentriesof ij tth and tht, and show that if they are equal, then t must be diagonal. all normal triangular matrices are diagonal.",eigenvalues and eigenvectors
"20. if n is normal, show that (cid:107)nx(cid:107)=(cid:107)nhx(cid:107) for every vector x. deduce that the ith row of n has the same length as the ith column. note: if n is also upper triangular, this leads again to the conclusion that it must be diagonal.",eigenvalues and eigenvectors
"21. provethatamatrixwithorthonormaleigenvectorsmustbenormal,asclaimedin5t: ifu−1nu =a, or n =uλuh, then nnh =nhn.",eigenvalues and eigenvectors
"22. find a unitaryu and triangular t so thatu−1au =t, for [ ] 0 1 0 5 −3 [ ] a= and a=[0 0 0]. 4 −2 1 0 0",eigenvalues and eigenvectors
"23. if a has eigenvalues 0, 1, 2, what are the eigenvalues of a(a−i)(a−2i)?",eigenvalues and eigenvectors
"24. (a) showby direct multiplication that everytriangular matrix t, say 3 by 3, satisfies its own characteristic equation: (t −λ i)(t −λ i)(t −λ i)=0. 1 2 3 (b) substitutingu−1au fort,deducethefamouscayley-hamiltontheorem: every matrixsatisfiesitsowncharacteristicequation. for3by3thisis (a−λ i)(a− 1 λ i)(a−λ i)=0. 2 3  ",eigenvalues and eigenvectors
"25. the characteristic polynomial of a = a b is λ2−(a+d)λ+(ad−bc). by direct c d substitution, verify cayley-hamilton: a2−(a+d)a+(ad−bc)i =0.",eigenvalues and eigenvectors
"26. if a = 1 above the main diagonal and a = 0 elsewhere, find the jordan form (say ij ij 4 by 4) by finding all the eigenvectors.",eigenvalues and eigenvectors
"27. show, by trying for an m and failing, that no two of the three jordan forms in equa- tion (8) are similar: j =m−1j m, j =m−1j m, and j =m−1j m. 1 2 1 3 2 3",eigenvalues and eigenvectors
"28. solve u =ju by back-substitution, solving first for u (t): 2  du 5 1 u 1 1 =ju= with initial value u(0)= . dt 05 u 2 2 noticete5t in the first component u (t). 1",eigenvalues and eigenvectors
29. compute a10 and ea if a=mjm−1:   14 9 3 −2 2 1 3 2 a= = . −16 −10 −4 3 0 2 4 3,eigenvalues and eigenvectors
30. show that a and b are similar by finding m so that b=m−1am: 1 0 0 1 (a) a= and b= . 1 0 0 1 1 1 1 −1 (b) a= and b= . 1 1 −1 1 1 2 4 3 (c) a= and b= . 3 4 2 1,eigenvalues and eigenvectors
31. which of these matrices a to a are similar? check their eigenvalues. 1 6 1 0 0 1 1 1 0 0 1 0 0 1 . 0 1 1 0 0 0 1 1 1 0 0 1,eigenvalues and eigenvectors
32. there are sixteen 2 by 2 matrices whose entries are 0s and 1s. similar matrices go into the same family. how many families? how many matrices (total 16) in each family?,eigenvalues and eigenvectors
"33. (a) if x is in the nullspace of a, show that m−1x is in the nullspace of m−1am. (b) the nullspaces of a and m−1am have the same (vectors)(basis)(dimension).",eigenvalues and eigenvectors
"34. ifaandbhavetheexactlythesameeigenvaluesandeigenvectors,doesa=b? with nindependenteigenvectors,wedohavea=b. finda=bwhenλ=0,0(repeated), but there is only one line of eigenvectors (x ,0). 1 problems 35–39 are about the jordan form.",eigenvalues and eigenvectors
"35. by direct multiplication, find j2 and j3 when c 1 j = . 0 c guess the form of jk. set k =0 to find j0. set k =−1 to find j−1.",eigenvalues and eigenvectors
"36. if j is the 5 by 5 jordan block with λ = 0, find j2 and count its eigenvectors, and find its jordan form (two blocks).",eigenvalues and eigenvectors
"37. the text solved du/dt = ju for a 3 by 3 jordan block j. add a fourth equation dw/dt =5w+x. follow the pattern of solutions for z, y, x to find w.",eigenvalues and eigenvectors
"38. these jordan matrices have eigenvalues 0, 0, 0, 0. they have two eigenvectors (find them). but the block sizes don’t match and j is not similar to k: [ ] [ ] 0 1 0 0 0 1 0 0 [ ] [ ] [ 0 0 0 0 ] [ 0 0 1 0 ] j =[ ] and k =[ ]. [ 0 0 0 1 ] [ 0 0 0 0 ] 0 0 0 0 0 0 0 0 for any matrix m, compare jm with mk. if they are equal, show that m is not invertible. then m−1jm =k is impossible.",eigenvalues and eigenvectors
"39. prove in three steps that at is always similar to a (we know that the λ’s are the same, the eigenvectors are the problem): (a) for a= one block, find m = permutation so that m−1jm =jt. i i i i i (b) for a= any j, build m from blocks so that m−1jm =jt. 0 0 0 (c) for any a=mjm−1: show that at is similar to jt and so to j and to a.",eigenvalues and eigenvectors
"40. which pairs are similar? choose a, b, c, d to prove that the other pairs aren’t: a b b a c d d c . c d d c a b b a",eigenvalues and eigenvectors
"41. true or false, with a good reason: (a) an invertible matrix can’t be similar to a singular matrix. (b) a symmetric matrix can’t be similar to a nonsymmetric matrix. (c) a can’t be similar to −a unless a=0. (d) a−i can’t be similar to a+i.",eigenvalues and eigenvectors
42. prove that ab has the same eigenvalues as ba.,eigenvalues and eigenvectors
"43. if a is 6 by 4 and b is 4 by 6, ab and ba have different sizes. nevertheless,   i −a ab 0 i a 0 0 = =g. 0 i b 0 0 i b ba (a) what sizes are the blocks of g? they are the same in each matrix. (b) this equation is m−1fm =g, so f and g have the same 10 eigenvalues. f has the eigenvalues of ab plus 4 zeros; g has the eigenvalues of ba plus 6 zeros. ab has the same eigenvalues as ba plus zeros.",eigenvalues and eigenvectors
"44. why is each of these statements true? (a) if a is similar to b, then a2 is similar to b2. (b) a2 and b2 can be similar when a and b are not similar (tryλ=0,0).     (c) 3 0 is similar to 3 1 . 0 4 0 4     (d) 3 0 is not similar to 3 1 . 0 3 0 3 (e) if we exchange rows 1 and 2 of a, and then exchange columns 1 and 2, the eigenvalues stay the same. properties of eigenvalues and eigenvectors how are the properties of a matrix reflected in its eigenvalues and eigenvectors? this question is fundamental throughout chapter 5. a table that organizes the key facts may be helpful. for each class of matrices, here are the special properties of the eigenvalues λ and eigenvectors x . i i symmetric: at =a realλ’s orthogonal xtx =0 i j orthogonal: qt =q−1 all |λ|=1 orthogonal xtx =0 i j skew-symmetric: at =−a imaginaryλ’s orthogonal xtx =0 i j complex hermitian: at =a realλ’s orthogonal xtx =0 i j positive definite: xtax>0 allλ>0 orthogonal similar matrix: b=m−1am λ(b)=λ(a) x(b)=m−1x(a) projection: p=p2 =pt λ=1;0 column space; nullspace reflection: i−2uut λ=−1;1,...,1 u;u⊥ rank-1 matrix: uvt λ=vtu;0,...,0 u;v⊥ inverse: a−1 1/λ(a) eigenvectors of a shift: a+ci λ(a)+c eigenvectors of a stable powers: an →0 all |λ|<1 stable exponential: eat →0 all reλ<0 markov: m >0, ∑n m =1 λ =1 steady state x>0 ij i=1 ij max cyclic permutation: pn =i λ =e2πik/n x =(1,λ ,...,λn−1) k k k k diagonalizable: sλs−1 diagonal of λ columns of s are independent symmetric: qλqt diagonal of λ (real) columns of q are orthonormal jordan: j =m−1am diagonal of j each block gives 1 eigenvector every matrix: a=uσvt rank(a)=rank(σ) eigenvectors of ata, aat inv,u",eigenvalues and eigenvectors
"5.1 find the eigenvalues and eigenvectors, and the diagonalizing matrix s, for 1 0 7 2 a= and b= . 2 3 −15 −4",eigenvalues and eigenvectors
5.2 find the determinants of a and a−1 if λ 2 a=s 1 s−1. 0 λ 2,eigenvalues and eigenvectors
"5.3 if a has eigenvalues 0 and 1, corresponding to the eigenvectors 1 2 and , 2 −1 how can you tell in advance that a is symmetric? what are its trace and determi- nant? what is a?",eigenvalues and eigenvectors
"5.4 inthepreviousproblem,whatwillbetheeigenvaluesandeigenvectorsofa2? what is the relation of a2 to a?",eigenvalues and eigenvectors
5.5 does there exist a matrix a such that the entire family a+ci is invertible for all complex numbers c? find a real matrix with a+ri invertible for all real r.,eigenvalues and eigenvectors
5.6 solve for both initial values and then find eat: du 3 1 1 0 = u if u(0)= and if u(0)= . dt 1 3 0 1,eigenvalues and eigenvectors
"5.7 would you prefer to have interest compounded quarterly at 40% per year, or annu- ally at 50%?",eigenvalues and eigenvectors
"5.8 true or false (with counterexample if false): (a) if b is formed from a by exchanging two rows, then b is similar to a. (b) if a triangular matrix is similar to a diagonal matrix, it is already diagonal. (c) anytwoofthesestatementsimplythethird: aishermitian,aisunitary,a2=i. (d) if a and b are diagonalizable, so is ab.",eigenvalues and eigenvectors
"5.9 whathappenstothefibonaccisequenceifwegobackwardintime,andhowisf −k related to f ? the law f =f +f is still in force, so f =1. k k+2 k+1 k −1",eigenvalues and eigenvectors
5.10 find the general solution to du/dt =au if [ ] 0 −1 0 [ ] a=[1 0 −1]. 0 1 0 can you find a time t at which the solution u(t) is guaranteed to return to the initial value u(0)?,eigenvalues and eigenvectors
"5.11 if p is the matrix that projects rn onto a subspace s, explain why every vector in s is an eigenvector, and so is every vector in s⊥. what are the eigenvai (note the connection to p2 =p, which means thatλ2 =λ.)",eigenvalues and eigenvectors
5.12 show that every matrix of order >1 is the sum of two singular matrices.,eigenvalues and eigenvectors
5.13 (a) show that the matrix differential equation dx/dt = ax +xb has the solution x(t)=eatx(0)ebt. (b) prove that the solutions of dx/dt =ax−xa keep the same eigenvalues for all time.,eigenvalues and eigenvectors
"5.14 if the eigenvalues of a are 1 and 3 with eigenvectors (5,2) and (2,1), find the solutions to du/dt =au and u =au , starting from u=(9,4). k+1 k",eigenvalues and eigenvectors
"5.15 find the eigenvalues and eigenvectors of [ ] 0 −i 0 [ ] a=[i 1 i]. 0 −i 0 what property do you expect for the eigenvectors, and is it true?",eigenvalues and eigenvectors
5.16 by trying to solve  a b a b 0 1 = =a c d c d 0 0 show that a has no square root. change the diagonal entries of a to 4 and find a square root.   0 4,eigenvalues and eigenvectors
"5.17 (a) find the eigenvalues and eigenvectors of a= . 1 0 4 (b) solve du/dt =au starting from u(0)=(100,100). (c) if v(t) = income to stockbrokers and w(t) = income to client, and they help each other by dv/dt = 4w and dw/dt = 1v, what does the ratio v/w approach 4 ast →∞?",eigenvalues and eigenvectors
"5.18 true or false, with reason if true and counterexample if false: (a) for every matrix a, there is a solution to du/dt = au starting from u(0) = (1,...,1). (b) every invertible matrix can be diagonalized. (c) every diagonalizable matrix can be inverted. (d) exchanging the rows of a 2 by 2 matrix reverses the signs of its eigenvalues. (e) if eigenvectors x and y correspond to distinct eigenvalues, then xhy=0.",eigenvalues and eigenvectors
"5.19 if k is a skew-symmetric matrix, show that q=(i−k)(i+k)−1 is an orthogonal   matrix. find q if k = 0 2 . −2 0",eigenvalues and eigenvectors
"5.20 if kh =−k (skew-hermitian), the eigenvalues are imaginary and the eigenvectors are orthogonal. (a) how do you know that k−i is invertible? (b) how do you know that k =uλuh for a unitaryu? (c) why is eλt unitary? (d) why is ekt unitary?",eigenvalues and eigenvectors
"5.21 if m is the diagonal matrix with entries d, d2, d3, what is m−1am? what are its eigenvalues in the following case? [ ] 1 1 1 [ ] a=[1 1 1]. 1 1 1",eigenvalues and eigenvectors
"5.22 if a2 = −i, what are the eigenvalues of a? if a is a real n by n matrix show that n must be even, and give an example.",eigenvalues and eigenvectors
"5.23 if ax=λ x and aty=λ y (all real), show that xty=0. 1 2",eigenvalues and eigenvectors
"5.24 a variation on the fourier matrix is the “sine matrix”: [ ] sinθ sin2θ sin3θ 1 [ ] π s= √ [sin2θ sin4θ sin6θ] with θ= . 2 4 sin3θ sin6θ sin9θ verify that st = s−1. (the columns are the eigenvectors of the tridiagonal −1, 2, −1 matrix.)",eigenvalues and eigenvectors
"5.25 (a) find a nonzero matrix n such that n3 =0. (b) if nx=λx, show thatλ must be zero. (c) prove that n (called a “nilpotent” matrix) cannot be symmetric.",eigenvalues and eigenvectors
"5.26 (a) find the matrix p = aat/ata that projects any vector onto the line through a=(2,1,2). (b) what is the only nonzero eigenvalue of p, and what is the corresponding eigen- vector? (c) solve u =pu , starting from u =(9,9,0). k+1 k 0",eigenvalues and eigenvectors
"5.27 suppose the first row of a is 7, 6 and its eigenvalues are i, −i. find a.",eigenvalues and eigenvectors
5.28 (a) for which numbers c and d does a have real eigenvalues and orthogonal eigen- vectors? [ ] 1 2 0 [ ] a=[2 d c]. 0 5 3 (b) for which c and d can we find three orthonormal vectors that are combinations of the columns (don’t do it!)?,eigenvalues and eigenvectors
"5.29 if the vectors x and x are in the columns of s, what are the eigenvalues and eigen- 1 2 vectors of 2 0 2 3 a=s s−1 and b=s s−1? 0 1 0 1 k .4 .3 a",eigenvalues and eigenvectors
5.30 what is the limit as k →∞ (the markov steady state) of ? .6 .7 b 6 chapter positive definite matrices,eigenvalues and eigenvectors
"1. thequadratic f =x2+4xy+2y2 hasasaddlepointattheorigin,despitethefactthat its coefficients are positive. write f as a difference of two squares.",positive definite matrices
"2. decide for or against the positive definiteness of these matrices, and write out the corresponding f =xtax: 1 3 1 −1 2 3 −1 2 (a) . (b) . (c) . (d) . 3 5 −1 1 3 5 2 −8 the determinant in (b) is zero; along what line is f(x,y)=0?",positive definite matrices
"3. if a 2 by 2 symmetric matrix passes the tests a > 0, ac > b2, solve the quadratic equation det(a−λi)=0 and show that both eigenvalues are positive.",positive definite matrices
"4. decide between a minimum, maximum, or saddle point for the following functions. (a) f =−1+4(ex−x)−5xsiny+6y2 at the point x=y=0. (b) f =(x2−2x)cosy, with stationary point at x=1, y=π.  ",positive definite matrices
5. (a) for which numbers b is the matrix a= 1 b positive definite? b 9 (b) factor a=ldlt when b is in the range for positive definiteness. (c) find the minimum value of 1(x2+2bxy+9y2)−y for b in this range. 2 (d) what is the minimum if b=3?,positive definite matrices
"6. suppose the positive coefficients a and c dominate b in the sense that a+c > 2b. find an example that has ac<b2, so the matrix is not positive definite.",positive definite matrices
"7. (a) what 3 by 3 symmetric matrices a and a correspond to f and f ? 1 2 1 2 f =x2+x2+x2−2x x −2x x +2x x 1 1 2 3 1 2 1 3 2 3 f =x2+2x2+11x2−2x x −2x x −4x x . 2 1 2 3 1 2 1 3 2 3 (b) show that f is a single perfect square and not positive definite. where is f 1 1 equal to 0? (c) factor a into llt, write f =xta x as a sum of three squares. 2 2 2  ",positive definite matrices
"8. if a= a b is positive definite, test a−1 =[p q ] for positive definiteness. b c q r",positive definite matrices
"6.1 minima,maxima,andsaddlepoints 351",positive definite matrices
"9. the quadratic f(x ,x ) = 3(x +2x )2+4x2 is positive. find its matrix a, factor it 1 2 1 2 2 into ldlt, and connect the entries in d and l to 3, 2, 4 in f.",positive definite matrices
"10. if r=[p q ], write out r2 and check that it is positive definite unless r is singular. q r  ",positive definite matrices
"11. (a) if a= a b is hermitian (complex b), find its pivots and determinant. b c (b) complete the square for xhax. now xh =[x x ] can be complex 1 2 a|x |2+2rebx x +c|x |2 =a|x +(b/a)x |2+ |x |2. 1 1 2 2 1 2 2 (c) show that a>0 and ac>|b|2 ensure that a is positive definite.     (d) are the matrices 1 1+i and 3 4+i positive definite? 1−i 2 4−i 6",positive definite matrices
12. decide whether f = x2y2−2x−2y has a minimum at the point x = y = 1 (after showing that the first derivatives are zero at that point).,positive definite matrices
"13. under what conditions on a, b, c is ax2+2bxy+cy2 >x2+y2 for all x, y? problems 14–18 are about tests for positive definiteness.",positive definite matrices
"14. which of a , a , a , a has two positive eigenvalues? test a>0 and ac>b2, don’t 1 2 3 4 compute the eigenvalues. find an x so that xta x<0. 1 5 6 −1 −2 1 10 1 10 a = a = a = a = . 1 2 3 4 6 7 −2 −5 10 100 10 101",positive definite matrices
15. what is the quadratic f =ax2+2bxy+cy2 for each of these matrices? complete the square to write f as a sum of one or two squares d ( )2+d ( )2. 1 2 1 2 1 3 a= and a= . 2 9 3 9,positive definite matrices
"16. show that f(x,y) = x2+4xy+3y2 does not have a minimum at (0,0) even though it has positive coefficients. write f as a difference of squares and find a point (x,y) where f is negative.",positive definite matrices
"17.  if a has independent columns, then ata is square and symmetric and invertible (section 4.2). rewrite xtatax to show why it is positive except when x=0. then ata is positive definite.",positive definite matrices
"18. test to see if ata is positive definite in each case: [ ] 1 1 1 2 [ ] 1 1 2 a= , a=[1 2], and a= . 0 3 1 2 1 2 1",positive definite matrices
"19. find the 3 by 3 matrix a and its pivots, rank, eigenvalues, and determinant: [ ][ ] x   1 [ ][ ] x x x [ a ][x ]=4(x −x +2x )2. 1 2 3 2 1 2 3 x 3",positive definite matrices
"20. for f (x,y) = 1x4+x2y+y2 and f (x,y) = x3+xy−x, find the second derivative 1 4 2 matrices a and a : 1 2 ∂2f/∂x2 ∂2f/∂x∂y a= . ∂2f/∂y∂x ∂2f/∂y2 a is positive definite, so f is concave up (= convex). find the minimum point of 1 1 f and the saddle point of f (look where first derivatives are zero). 1 2",positive definite matrices
"21. the graph of z = x2+y2 is a bowl opening upward. the graph of z = x2−y2 is a saddle. the graph of z = −x2−y2 is a bowl opening downward. what is a test on f(x,y) to have a saddle at (0,0)?",positive definite matrices
22. which values of c give a bowl and which give a saddle point for the graph of z = 4x2+12xy+cy2? describe this graph at the borderline value of c.,positive definite matrices
1. for what range of numbers a and b are the matrices a and b positive definite? [ ] [ ] a 2 2 1 2 4 [ ] [ ] a=[2 a 2] b=[2 b 8]. 2 2 a 4 8 7,positive definite matrices
"2. decide for or against the positive definiteness of [ ] [ ] [ ] 2 2 −1 −1 2 −1 −1 0 1 2 [ ] [ ] [ ] a=[−1 2 −1], b=[−1 2 1 ], c =[1 0 1] . −1 −1 2 −1 1 2 2 1 0",positive definite matrices
3. construct an indefinite matrix with its largest entries on the main diagonal: [ ] 1 b −b [ ] a=[ b 1 b ] with |b|<1 can have deta<0. −b b 1,positive definite matrices
"4. show from the eigenvalues that if a is positive definite, so is a2 and so is a−1.",positive definite matrices
"5. ifaandbarepositivedefinite,thena+bispositivedefinite. pivotsandeigenvalues are not convenient for a+b. much better to prove xt(a+b)x>0.  ",positive definite matrices
"6. from the pivots, eigenvalues, and eigenvectors of a= 5 4 , write a as rtr in three √ √ √ √ √ 4 5√ ways: (l d)( dlt), (q λ)( λqt), and (q λqt)(q λqt). √",positive definite matrices
"7. if a=qλqt is symmetric positive definite, then r=q λqt is its symmetric pos- itive definite square root. why does r have positive eigenvalues? compute r and verify r2 =a for 10 6 10 −6 a= and a= . 6 10 −6 10",positive definite matrices
"8. if a is symmetric positive definite andc is nonsingular, prove that b=ctac is also symmetric positive definite.",positive definite matrices
9. if a=rtr prove the generalized schwarz inequality |xtay|2 ≤(xtax)(ytay).  ,positive definite matrices
"10. the ellipse u2+4v2 =1 corresponds to a= 1 0 . write the eigenvalues and eigen- 0 4 vectors, and sketch the ellipse. √",positive definite matrices
"11. reduce the equation 3u2 −2 2uv+2v2 = 1 to a sum of squares by finding the eigenvalues of the corresponding a, and sketch the ellipse.",positive definite matrices
"12. in three dimensions,λ y2+λ y2+λ y2 =1 represents an ellipsoid when allλ >0. 1 1 2 2 3 3 i describe all the different kinds of surfaces that appear in the positive semidefinite case when one or more of the eigenvalues is zero.",positive definite matrices
13. write down the five conditions for a 3 by 3 matrix to be negative definite (−a is positive definite) with special attention to condition iii: how is det(−a) related to deta?,positive definite matrices
"14. decidewhetherthefollowingmatricesarepositivedefinite,negativedefinite,semidef- inite, or indefinite: [ ] [ ] 1 2 0 0 1 2 3 [ ] [ ] [2 6 −2 0 ] a=[2 5 4], b=[ ], c =−b, d=a−1. [0 −2 5 −2] 3 4 9 0 0 −2 3 is there a real solution to −x2−5y2−9z2−4xy−6xz−8yz=1?",positive definite matrices
15. suppose a is symmetric positive definite and q is an orthogonal matrix. true or false: (a) qtaq is a diagonal matrix. (b) qtaq is symmetric positive definite. (c) qtaq has the same eigenvalues as a. (d) e−a is symmetric positive definite.,positive definite matrices
"16. ifaispositivedefiniteanda isincreased,provefromcofactorsthatthedeterminant 11 is increased. show by example that this can fail if a is indefinite.",positive definite matrices
17. from a = rtr. show for positive definite matrices that deta ≤ a a ···a . (the 11 22 nn length squared of column j of r is a . use determinant = volume.) jj,positive definite matrices
18. (lyapunov test for stability of m) suppose am+mha = −i with positive definite a. if mx=λx show that rea<0. (hint: multiply the first equation by xh and x.),positive definite matrices
19. which 3 by 3 symmetric matrices a produce these functions f = xtax? why is the first matrix positive definite but not the second one? (a) f =2(x2+x2+x2−x x −x x ). 1 2 3 1 2 2 3 (b) f =2(x2+x2+x2−x x −x x −x x ). 1 2 3 1 2 1 3 2 3,positive definite matrices
20. compute the three upper left determinants to establish positive definiteness. verify that their ratios give the second and third pivots. [ ] 2 2 0 [ ] a=[2 5 3]. 0 3 8,positive definite matrices
"21. a positive definite matrix cannot have a zero (or even worse, a negative number) on its diagonal. show that this matrix fails to have xtax>0: [ ][ ] 4 1 1 x   1 [ ][ ] x x x [1 0 2][x ] is not positive when (x ,x ,x )=( , , ). 1 2 3 2 1 2 3 1 2 5 x 3",positive definite matrices
"22. a diagonal entry a of a symmetric matrix cannot be smaller than all λ’s. if it jj were,thena−a i wouldhave eigenvaluesandwouldbepositivedefinite. but jj a−a i has a on the main diagonal. jj",positive definite matrices
23. give a quick reason why each of these statements is true: (a) every positive definite matrix is invertible. (b) the only positive definite projection matrix is p=i. (c) a diagonal matrix with positive diagonal entries is positive definite. (d) a symmetric matrix with a positive determinant might not be positive definite!,positive definite matrices
24. for which s andt do a and b have allλ>0 (and are therefore positive definite)? [ ] [ ] s −4 −4 t 3 0 [ ] [ ] a=[−4 s −4] and b=[3 t 4]. −4 −4 s 0 4 t,positive definite matrices
"25. you may have seen the equation for an ellipse as (x)2+(y )2 = 1. what are a and a b b when the equation is written as λ x2+λ y2 = 1? the ellipse 9x2+16y2 = 1 has 1 2 half-axes with lengths a= , and b= .",positive definite matrices
26. draw the tilted ellipse x2+xy+y2 =1 and find the half-lengths of its axes from the eigenvalues of the corresponding a. √ √,positive definite matrices
"27. withpositivepivotsind, thefactorizationa=ldlt becomesl d dlt. (square √ √ √ roots of the pivots give d = d d.) then c = l d yields the cholesky factor- ization a=cct, which is “symmetrized lu”: 3 0 4 8 from c = find a. from a= findc. 1 2 8 25 √",positive definite matrices
"28. inthecholeskyfactorizationa=cct,withc=l d,thesquarerootsofthepivots are on the diagonal ofc. findc (lower triangular) for [ ] [ ] 9 0 0 1 1 1 [ ] [ ] a=[0 1 2] and a=[1 2 2]. 0 2 8 1 2 7",positive definite matrices
"29. the symmetric factorization a=ldlt means that xtax=xtldltx:         a b x 1 0 a 0 1 b/a x x y = x y . b c y b/a 1 0 (ac−b2)/a 0 1 y the left-hand side is ax2+2bxy+cy2. the right-hand side is a(x+ by)2+ y2. a the second pivot completes the square! test with a=2, b=4, c=10.    ",positive definite matrices
"30. without multiplying a= cosθ −sinθ 2 0 cosθ sinθ , find sinθ cosθ 0 5 −sinθ cosθ (a) the determinant of a. (b) the eigenvalues of a. (c) the eigenvectors of a. (d) a reason why a is symmetric positive definite.",positive definite matrices
"31. for the semidefinite matrices [ ] [ ] 2 −1 −1 1 1 1 [ ] [ ] a=[−1 2 −1] (rank 2) and b=[1 1 1] (rank 1), −1 −1 2 1 1 1 write xtax as a sum of two squares and xtbx as one square.",positive definite matrices
"32. apply any three tests to each of the matrices [ ] [ ] 1 1 1 2 1 2 [ ] [ ] a=[1 1 1] and b=[1 1 1], 1 1 0 2 1 2 to decide whether they are positive definite, positive semidefinite, or indefinite.    ",positive definite matrices
"33. forc = 2 0 and a= 1 1 , confirm thatctac has eigenvalues of the same signs 0 −1 1 1 as a. construct a chain of nonsingular matrices c(t) linking c to an orthogonal q. why is it impossible to construct a nonsingular chain linking c to the identity matrix?",positive definite matrices
"34. if the pivots of a matrix are all greater than 1, are the eigenvalues all greater than 1? test on the tridiagonal −1, 2, −1 matrices.",positive definite matrices
35. use the pivots of a−1i to decide whether a has an eigenvalue smaller than 1: 2 2 [ ],positive definite matrices
"36. an algebraic proof of the law of inertia starts with the orthonormal eigenvectors x ,...,x ofacorrespondingtoeigenvaluesλ >0. andtheorthonormaleigenvectors 1 p i y ,...,y ofctac corresponding to eigenvalues µ <0. 1 q i (a) to prove that the p+q vectors x ,...,x , cy ,...,cy are independent, assume 1 p 1 q that some combination gives zero: a x +···+a x =b cy +···+b cy (=z, say). 1 1 p p 1 1 q q show that ztaz=λ a2+···+λ a2 ≥0 and ztaz=µ b2+···+µ b2 ≤0. 1 1 p p 1 1 q q (b) deduce that the a’s and b’s are zero (proving linear independence). from that deduce p+q≤n. (c) the same argument for the n− p negative λ’s and the n−q positive µ’s gives n− p+n−q ≤ n. (we again assume no zero eigenvalues—which are handled separately). show that p+q = n, so the number p of positive λ’s equals the number n−q of positive µ’s—which is the law of inertia.",positive definite matrices
"37. if c is nonsingular, show that a and ctac have the same rank. thus they have the same number of zero eigenvalues.",positive definite matrices
"38. find by experiment the number of positive, negative, and zero eigenvalues of i b a= bt 0 when the block b (of order 1n) is nonsingular. 2",positive definite matrices
39. do a andctac always satisfy the law of inertia whenc is not square?,positive definite matrices
"40. in equation (9) with m = 1 and m = 2, verify that the normal modes are m- 1 2 orthogonal: xtmx =0. 1 2",positive definite matrices
41. find the eigenvalues and eigenvectors of ax=λmx: 6 −3 λ 4 1 x= x. −3 6 18 1 4,positive definite matrices
"42. if the symmetric matrices a and m are indefinite, ax = λmx might not have real eigenvalues. construct a 2 by 2 example.",positive definite matrices
"43. a group of nonsingular matrices includes ab and a−1 if it includes a and b. “prod- uctsandinversesstayinthegroup.” whichofthesesetsaregroups? positivedefinite symmetric matrices a, orthogonal matrices q, all exponentials eta of a fixed matrix a, matrices p with positive eigenvalues, matrices d with determinant 1. invent a group containing only positive definite matrices.",positive definite matrices
"1. compute ata and its eigenvaluesσ2, 0 and unit eigenvectors v , v : 1 1 2 1 4 a= . 2 8",positive definite matrices
"2. (a) compute aat and its eigenvaluesσ2, 0 and unit eigenvectors u , u . 1 1 2 (b) choose signs so that av =σ u and verify the svd: 1 1 1     1 4 σ t 1 = u u v v . 1 2 1 2 2 8 0 (c) which four vectors give orthonormal bases for c(a), n(a), c(at), n(at)? problems 3–5 ask for the svd of matrices of rank 2.",positive definite matrices
"3. find the svd from the eigenvectors v , v of ata and av =σu : 1 2 i i i 1 1 fibonacci matrix a= . 1 0",positive definite matrices
4. use the svd part of the matlab demo eigshow (or java on the course page web.mit.edu/18.06) to find the same vectors v and v graphically. 1 2,positive definite matrices
"5. compute ata and aat, and their eigenvalues and unit eigenvectors, for 1 1 0 a= . 0 1 1 multiply the three matricesuσvt to recover a. problems 6–13 bring out the underlying ideas of the svd.",positive definite matrices
"6. suppose u ,...,u and v ,...,v are orthonormal bases for rn. construct the matrix 1 n 1 n a that transforms each v into u to give av =u ,...,av =u . j j 1 1 n n",positive definite matrices
"7. construct the matrix with rank 1 that has av = 12u for v = 1(1,1,1,1) and u = 2 1(2,2,1). its only singular value isσ = . 3 1",positive definite matrices
"8. finduσvt if a has orthogonal columns w ,...,w of lengthsσ ,...,σ . 1 n 1 n",positive definite matrices
9. explain howuσvt expresses a as a sum of r rank-1 matrices in equation (3): a=σ u vt+···+σu vt. 1 1 1 r r r,positive definite matrices
"10. suppose a is a 2 by 2 symmetric matrix with unit eigenvectors u and u . if its 1 2 eigenvalues areλ =3 andλ =−2, what areu, σ, andvt? 1 2",positive definite matrices
11. supposeaisinvertible(withσ >σ >0). changeabyassmallamatrixaspossible 1 2 to produce a singular matrix a . hint: u andv do not change: 0     σ t 1 find a from a= u u v v . 0 1 2 1 2 σ 2,positive definite matrices
"12. (a) if a changes to 4a, what is the change in the svd? (b) what is the svd for at and for a−1?",positive definite matrices
13. why doesn’t the svd for a+i just use σ+i?,positive definite matrices
14. find the svd and the pseudoinverse 0+ of the m by n zero matrix.,positive definite matrices
"15. find the svd and the pseudoinversevς+ut of   0 1 0 1 1 a= 1 1 1 1 , b= , and c = . 1 0 0 0 0",positive definite matrices
"16. if an m by n matrix q has orthonormal columns, what is q+?",positive definite matrices
17. diagonalize ata to find its positive definite square root s =vς1/2vt and its polar decomposition a=qs: 1 10 6 a= √ . 10 0 8,positive definite matrices
"18. what is the minimum-length least-squares solution x+ =a+b to the following? [ ][ ] [ ] 1 0 0 c 0 [ ][ ] [ ] ax=[1 0 0][d]=[2]. 1 1 1 e 2 you can compute a+, or find the general solution to atax = atb and choose the solution that is in the row space of a. this problem fits the best planec+dt+ez to b=0 and also b=2 att =z=0 (and b=2 att =z=1). (a) if a has independent columns, its left-inverse (ata)−1at is a+. (b) if a has independent rows, its right-inverse at(aat)−1 is a+. in both cases, verify that x+ =a+b is in the row space. and atax+ =atb.",positive definite matrices
19. split a=uσvt into its reverse polar decomposition qs.,positive definite matrices
20. is (ab)+ =b+a+ always true for pseudoinverses? i believe not.,positive definite matrices
"21. removingzerorowsofu leavesa=lu, wherether columnsorl spanthecolumn space of a and the r rows ofu span the row space. then a+ has the explicit formula ut(u ut)−1(ltl)−1lt. why is a+b in the row space with ut at the front? why does ataa+b = atb, so that x+ =a+b satisfies the normal equation as it should?",positive definite matrices
22. explainwhyaa+ anda+aareprojectionmatrices(andthereforesymmetric). what fundamental subspaces do they project onto?,positive definite matrices
"1. consider the system ax=b given by [ ][ ] [ ] 2 −1 0 x 4 1 [ ][ ] [ ] [−1 2 −1][x ]=[0]. 2 0 −1 2 x 4 3 construct the corresponding quadratic p(x ,x ,x ), compute its partial derivatives 1 2 3 ∂p/∂x , and verify that they vanish exactly at the desired solution. i",positive definite matrices
2. complete the square in p = 1xtax−xtb = 1(x−a−1b)ta(x−a−1b)+constant. 2 2 this constant equals p because the term before it is never negative. (why?) min,positive definite matrices
"3. findtheminimum,ifthereisoneofp = 1x2+xy+y2−3yandp = 1x2−3y. what 1 2 2 2 matrix a is associated with p ? 2",positive definite matrices
"4. another quadratic that certainly has its minimum at ax=b is 1 1 1 q(x)= (cid:107)ax−b(cid:107)2 = xtatax−xtatb+ btb. 2 2 2 comparing q with p, and ignoring the constant 1btb, what system of equations do 2 we get at the minimum of q? what are these equations called in the theory of least squares?",positive definite matrices
"5. for any symmetric matrix a, compute the ratio r(x) for the special choice x = (1,...,1). how is the sum of all entries a related toλ andλ ? ij 1 n  ",positive definite matrices
"6. with a= 2 −1 , find a choice of x that gives a smaller r(x) than the boundλ ≤2 −1 2 1 that comes from the diagonal entries. what is the minimum value of r(x)?",positive definite matrices
"7. ifbispositivedefinite,showfromtherayleighquotientthatthesmallesteigenvalue of a+b is larger than the smallest eigenvalue of a.",positive definite matrices
"8. if λ and µ are the smallest eigenvalues of a and b, show that the smallest eigen- 1 1 value θ of a+b is at least as large as λ +µ . (try the corresponding eigenvector 1 1 1 x in the rayleigh quotients.) note. problems 7 and 8 are perhaps the most typical and most important results that come easily from rayleigh’s principle, but only with great difficulty from the eigenvalue equations themselves.",positive definite matrices
"9. ifbispositivedefinite, showfromtheminimaxprinciple(12)thatthesecond small- est eigenvalue is increased by adding b:λ (a+b)>λ (a). 2 2",positive definite matrices
"10. if you throw away two rows and columns of a, what inequalities do you expect between the smallest eigenvalue µof the new matrix and the originalλ’s?",positive definite matrices
11. find the minimum values of x2−x x +x2 x2−x x +x2 r(x)= 1 1 2 2 and r(x)= 1 1 2 2. x2+x2 2x2+x2 1 2 1 2,positive definite matrices
12. prove from equation (11) that r(x) is never larger than the largest eigenvalueλ . n,positive definite matrices
"13. the minimax principle forλ involves j-dimensional subspaces s : j j (cid:183) (cid:184) equivalent to equation (15) λ =min maxr(x) . j s j xins j (a) ifλ is positive, infer that every s contains a vector x with r(x)>0. j j (b) deduce that s contains a vector y=c−1x with ytctacy/yty>0. j (c) conclude that the jth eigenvalue of ctac, from its minimax principle, is also positive—proving again the law of inertia in section 6.2.",positive definite matrices
14. showthatthesmallesteigenvalueλ ofax=λmxisnotlargerthantheratioa /m 1 11 11 of the corner entries.,positive definite matrices
"15. which particular subspace s in problem 13 gives the minimum value λ ? in other 2 2 words, over which s is the maximum of r(x) equal toλ ? 2 2",positive definite matrices
16.  from the zero submatrix decide the signs of the n eigenvalues: [ ] 0 · 0 1 [ ] [· · 0 2] a=[ ]. [0 0 0 ·] 1 2 · n,positive definite matrices
17. (constrained minimum) suppose the unconstrained minimum x = a−1b happens to satisfy the constraintcx=d. verify that equation (5) correctly gives p =p ; c/min min the correction term is zero.,positive definite matrices
"1. use three hat functions, with h = 1, to solve −u = 2 with u(0) = u(1) = 0. verify 4 that the approximationu matches u=x−x2 at the nodes.",positive definite matrices
2. solve −u = x with u(0) = u(1) = 0. then solve approximately with two hat func- tions and h= 1. where is the largest error? 3,positive definite matrices
"3. suppose −u =2, with the boundary condition u(1)=0 changed to u(1)=0. this “natural” condition on u need not be imposed on the trial functionsv. with h = 1, 3 there is an extra half-hat v , which goes from 0 to 1 between x = 2 and x = 1. (cid:82) 3 (cid:82) 3 compute a = (v)2dx and f = 2v dx. solve ay = f for the finite element 33 3 3 3 solution y v +y v +y v . 1 1 2 2 3 3",positive definite matrices
"4. solve−u =2withasinglehatfunction,butplaceitsnodeatx= 1 insteadofx= 1. 4 2 (sketch this function v .) with boundary conditions u(0) = u(1) = 0, compare the 1 finite element approximation with the true u=x−x2.",positive definite matrices
"5. galerkin’s method starts with the differential equation (say −u = f(x)) instead of the energy p. the trial solution is still u = y v +y v +···+y v , and the y’s are 1 1 2 2 n n chosen to make the difference between −u and f orthogonal to everyv : j galerkin (−y v−y v−···−y v)v dx= f(x)v (x)dx. 1 1 2 2 n n j j integrate the left side by parts to reach ay= f, proving that galerkin gives the same a and f as rayleigh-ritz for symmetric problems.",positive definite matrices
"6. a basic identity for quadratics shows y=a−1b as minimizing: 1 1 1 p(y)= ytay−ytb= (y−a−1b)ta(y−a−1b)− bta−1b. 2 2 2 the minimum over a subspace of trial functions is at the y nearest to a−1b. (that makes the first term on the right as small as possible; it is the key to convergence of u to u.) if a=i and b=(1,0,0), which multiple ofv =(1,1,1) gives the smallest value of p(y)= 1yty−y ? 2 1 (cid:82)",positive definite matrices
"7. for a single hat function v(x) centered at x = 1, compute a = (v)2dx and m = (cid:82) 2 v2dx. inthe1by1eigenvalueproblem, isλ=a/m largerorsmallerthanthetrue eigenvalueλ=π2?",positive definite matrices
"8. for the hat functionsv andv centered at x=h= 1 and x=2h= 2, compute the 2 1 (cid:82) 2 3 3 by 2 mass matrix m = vv dx, and solve the eigenvalue problem ax=λmx. ij i j (cid:82)",positive definite matrices
9. what is the mass matrix m = vv dx for n hat functions with h= 1 ? ij i j n+1 7 chapter computations with matrices,positive definite matrices
"1. for an orthogonal matrix q, show that (cid:107)q(cid:107) = 1 and also c(q) = 1. orthogonal matrices (and their multiplesαq) are the only perfectly conditioned matrices.",computations with matrices
"2. which“famous”inequalitygives(cid:107)(a+b)x(cid:107)≤(cid:107)ax(cid:107)+(cid:107)bx(cid:107),andwhydoesitfollow from equation (5) that (cid:107)a+b(cid:107)≤(cid:107)a(cid:107)+(cid:107)b(cid:107)?",computations with matrices
"3. explainwhy(cid:107)abx(cid:107)≤(cid:107)a(cid:107)(cid:107)b(cid:107)(cid:107)x(cid:107),anddeducefromequation(5)that(cid:107)ab(cid:107)≤(cid:107)a(cid:107)(cid:107)b(cid:107). show that this also implies c(ab)≤c(a)c(b).",computations with matrices
7.2 matrixnormandconditionnumber 397  ,computations with matrices
"4. forthepositivedefinitea= 2 −1 ,compute(cid:107)a−1(cid:107)=1/λ ,(cid:107)a(cid:107)=λ ,andc(a)= −1 2 1 2 λ /λ . find a right-hand side b and a perturbation δb so that the error is the worst 2 1 possible, (cid:107)δx(cid:107)/(cid:107)x(cid:107)=c(cid:107)δb(cid:107)/(cid:107)b(cid:107).",computations with matrices
"5. show that ifλ is any eigenvalue of a, ax=λx, then |λ|≤(cid:107)a(cid:107).",computations with matrices
6. the matrices in equation (4) have norms between 100 and 101. why?,computations with matrices
"7. comparing the eigenvalues of ata and aat, prove that (cid:107)a(cid:107)=(cid:107)at(cid:107).",computations with matrices
"8. for a positive definite a, the cholesky decomposition is a = ldlt = rtr, where √ r = dlt. show directly from equation (12) that the condition number of c(r) is the square root of c(a). elimination without row exchanges cannot hurt a positive definite matrix, since c(a)=c(rt)c(r).",computations with matrices
"9. showthatmax|λ|isnotatruenorm,byfinding2by2counterexamplestoλ (a+ max b)≤λ (a)+λ (b) andλ (ab)≤λ (a)λ (b). max max max max max  ",computations with matrices
"10. show that the eigenvalues of b= 0 a are ±σ, the singular values of a. hint: try at 0 i b2.",computations with matrices
"11. (a) do a and a−1 have the same condition number c? (b) in parallel with the upper bound (8) on the error, prove a lower bound: (cid:107)δx(cid:107) 1(cid:107)δb(cid:107) ≥ . (consider a−1b=x instead of ax=b.) (cid:107)x(cid:107) c (cid:107)b(cid:107)",computations with matrices
12. find the norms λ and condition numbers λ /λ of these positive definite max max min matrices: 100 0 2 1 3 1 . 0 2 1 2 1 1,computations with matrices
13. find the norms and condition numbers from the square roots of λ (ata) and max λ (ata): min −2 0 1 1 1 1 . 0 2 0 0 −1 1,computations with matrices
14. prove that the condition number (cid:107)a(cid:107)(cid:107)a−1(cid:107) is at least 1.,computations with matrices
15. whyisi theonlysymmetricpositivedefinitematrixthathasλ =λ =1? then max min the only matrices with (cid:107)a(cid:107) = 1 and (cid:107)a−1(cid:107) = 1 must have ata = i. they are matrices.,computations with matrices
"16. orthogonal matrices have norm (cid:107)q(cid:107)=1. if a=qr, show that (cid:107)a(cid:107)≤(cid:107)r(cid:107) and also (cid:107)r(cid:107)≤(cid:107)a(cid:107). then (cid:107)a(cid:107)=(cid:107)q(cid:107)(cid:107)r(cid:107). find anexampleof a=lu with(cid:107)a(cid:107)<(cid:107)l(cid:107)(cid:107)u(cid:107).",computations with matrices
"17. (suggested by moler and van loan) compute b−ay and b−az when .217 .780 .563 .341 .999 b= a= y= z= . .254 .913 .659 −.087 −1.0 is y closer than z to solving ax = b? answer in two ways: compare the residual b−ay to b−az. then compare y and z to the true x=(1,−1), sometimes we want a small residual, sometimes a smallδx. √ problems 18–20 are about vector norms other than the usual (cid:107)x(cid:107)= x·x.",computations with matrices
"18. the“1 norm”is(cid:107)x(cid:107) =|x| +···+|x| . the“∞ norm”is(cid:107)x(cid:107) =max|x |. compute 1 1 n ∞ i (cid:107)x(cid:107), (cid:107)x(cid:107) and (cid:107)x(cid:107) for the vectors 1 ∞ x=(1,1,1,1,1) and x=(.1,.7,.3,.4,.5).",computations with matrices
"19. prove that (cid:107)x(cid:107) ≤ (cid:107)x(cid:107) ≤ (cid:107)x(cid:107) . show from the schwarz inequality that the ratios ∞ 1 √ (cid:107)x(cid:107)/(cid:107)x(cid:107) and (cid:107)x(cid:107) /(cid:107)x(cid:107) are never larger than n. which vector (x ,...,x ) gives ∞ √ 1 1 n ratios equal to n?",computations with matrices
20. all vector norms must satisfy the triangle inequality. prove that (cid:107)x+y(cid:107) ≤(cid:107)x(cid:107) +(cid:107)y(cid:107) and (cid:107)x+y(cid:107) ≤(cid:107)x(cid:107) +(cid:107)y(cid:107) . ∞ ∞ ∞ 1 1 1,computations with matrices
21. compute the exact inverse of the hilbert matrix a by elimination. then compute a−1 again by rounding all numbers to three figures: [ ] 1 1 1 2 3 [ ] in matlab : a=hilb(3)=[1 1 1 ]. 2 3 4 1 1 1 3 4 5,computations with matrices
"22. forthesamea,computeb=axforx=(1,1,1)andx=(0,6,−3.6). asmallchange ∆b produces a large change ∆x.",computations with matrices
"23. compute λ and λ for the 8 by 8 hilbert matrix a = 1/(i+ j−1). if ax = b max min ij with (cid:107)b(cid:107) = 1, how large can (cid:107)x(cid:107) be? if b has roundoff error less than 10−16, how large an error can this cause in x?",computations with matrices
"24. if you know l,u, q, and r, is it faster to solve lux=b or qrx=b?",computations with matrices
"25. choosing the largest available pivot in each column (partial pivoting), factor each a into pa=lu: [ ] 1 0 1 1 0 [ ] a= and a=[2 2 0]. 2 2 0 2 0",computations with matrices
"26. find the lu factorization of a = ε 1 . on your computer, solve by elimination 1 1 whenε=10−3,10−6,10−9,10−12,10−15:  ε 1 x 1+ε 1 = . 1 1 x 2 2 the true x is (1,1). make a table to show the error for each ε. exchange the two equations and solve again—the errors should almost disappear.",computations with matrices
"1. for the matrix a = 2 −1 with eigenvalues λ = 1 and λ = 3, apply the power −1 2 1  2 method u = au three times to the initial guess u = 1 . what is the limiting k+1 k 0 0 vector u ? ∞  ",computations with matrices
"2. for the same a and the initial guess u = 3 , compare three inverse power steps to 0 4 one shifted step withα=utau /utu : 0 0 0 0 1 2 1 u =a−1u = u or u=(a−αi)−1u . k+1 k k 0 3 1 2 the limiting vector u is now a multiple of the other eigenvector (1,1). ∞",computations with matrices
3. explain why |λ /λ | controls the convergence of the usual power method. con- n n−1 struct a matrix a for which this method does not converge.  ,computations with matrices
"4. the markov matrix a = .9 .3 has λ = 1 and .6, and the power method u = aku   .1 .7 k 0 converges to .75 . find the eigenvectors of a−1. what does the inverse power .25 method u =a−ku converge to (after you multiply by .6k)? −k 0",computations with matrices
"5. show that for any two different vectors of the same length, (cid:107)x(cid:107) = (cid:107)y(cid:107), the house- holder transformation with v=x−y gives hx=y and hy=x.",computations with matrices
"6. computeσ=(cid:107)x(cid:107), v=x+σz, and h =i−2vvt/vtv, verify hx=−σz: 3 1 x= and z= . 4 0",computations with matrices
"7. using problem 6, find the tridiagonal hah−1 that is similar to [ ] 1 3 4 [ ] a=[3 1 0] 4 0 0",computations with matrices
"8. showthatstartingfroma = 2 −1 ,theunshiftedqralgorithmproducesonlythe 0  −1 2 modest improvement a = 1 14 −3 . 1 5 −3 6",computations with matrices
"9. apply to the following matrix a a single qr step with the shift α= a —which in 22 this case means without shift, since a = 0. show that the off-diagonal entries go 22 from sinθto −sin3θ, which is cubic convergence. cosθ sinθ a= . sinθ 0  ",computations with matrices
10. check that the tridiagonal a= 0 1 is left unchanged by the qr algorithm. it is one 1 0 of the (rare) counterexamples to convergence (so we shift).,computations with matrices
"11. show by induction that, without shifts, (q q ···q )(r ···r r ) is exactly the qr 0 1 k k 1 0 factorization of a . this identity connects qr to the power method and leads to k+1 an explanation of its convergence. if |λ |>|λ |>···>|λ |, these eigenvalues will 1 2 n gradually appear on the main diagonal.",computations with matrices
"12. choose sinθand cosθin the rotation p to triangularize a, and find r:  cosθ −sinθ 1 −1 ∗ ∗ p a= = =r. 21 sinθ cosθ 3 5 0 ∗",computations with matrices
13. choose sinθ and cosθ to make p ap−1 triangular (same a). what are the eigen- 21 21 values?,computations with matrices
"14. when a is multiplied by p (plane rotation), which entries are changed? when p a ij ij is multiplied on the right by p−1, which entries are changed now? ij",computations with matrices
"15. how many multiplications and how many additions are used to compute pa? (a careful organization of all the rotations gives 2n3 multiplications and additions, the 3 same as for qr by reflectors and twice as many as for lu.)",computations with matrices
"16. (turning a robot hand) a robot produces any 3 by 3 rotation a from plane rotations around the x, y, and z axes. if p p p a = i, the three robot turns are in a = 32 31 21 p−1p−1p−1. the three angles are euler angles. choose the firstθso that 21 31 32 [ ] [ ] cosθ −sinθ 0 −1 2 2 [ ]1[ ] p a=[sinθ cosθ 0] [ 2 −1 2 ] is zero in the (2,1) position. 21 2 0 0 1 2 2 −1",computations with matrices
"1. this matrix has eigenvalues 2− 2, 2, and 2+ 2: [ ] 2 −1 0 [ ] a=[−1 2 −1]. 0 −1 2 find the jacobi matrix d−1(−l−u) and the gauss-seidel matrix (d+l)−1(−u) and their eigenvalues, and the numbersω andλ for sor. opt max",computations with matrices
"2. for this n by n matrix, describe the jacobi matrix j =d−1(−l−u): [ ] 2 −1 [ ] [−1 · · ] a=[ ]. [ · · −1] −1 2 show that the vector x = (sinπh,sin2πh,...,sinnπh) is an eigenvector of j with 1 eigenvalueλ =cosπh=cosπ/(n+1). 1",computations with matrices
"3. in problem 2, show that x =(sinkπh,sin2kπh,...,sinnkπh) is an eigenvector of a. k multiply x by a to find the corresponding eigenvalue α . verify that in the 3 by 3 k √ √ k case these eigenvalues are 2− 2, 2, 2+ 2. note. the eigenvalues of the jacobi matrix j = 1(−l−u) = i− 1a are λ = 1− 2 2 k 1α =coskπh. they occur in plus-minus pairs andλ is cosπh. 2 k max problems 4–5 require gershgorin’s “circle theorem”: every eigenvalue of a lies in at least one of the circles c ,...,c , where c has its center at the diagonal entry a . its 1 n i ii radius r =∑ |a | is equal to the absolute sum along the rest of the row. i i=j ij proof. suppose x is the largest component of x. then ax=λx leads to i |x | (λ−a )x = ∑a x , or |λ−a |≤ ∑|a | j ≤ ∑|a |=r . ii i ij j ii ij ij i |x | j=i j=i i j=i",computations with matrices
"4. the matrix [ ] 3 1 1 [ ] a=[0 4 1] 2 2 5 is called diagonally dominant because every |a | > r . show that zero cannot lie in ii i any of the circles, and conclude that a is nonsingular.",computations with matrices
"5. write the jacobi matrix j for the diagonally dominant a of problem 4, and find the three gershgorin circles for j. show that all the radii satisfy r < 1, and that the i jacobi iteration converges.",computations with matrices
"6. the true solution to ax = b is slightly different from the elimination solution to lux =b; a−lu misses zero because of roundoff. one strategyis to do everything 0 indoubleprecision,butabetterandfasterwayisiterativerefinement: computeonly onevectorr=b−ax indoubleprecision,solveluy=r,andaddthecorrectionyto 0 x . problem: multiplyx =x +ybylu,writetheresultasasplittingsx =tx +b, 0 1 0 1 0 and explainwhy t is extremelysmall. this single step brings us almost exactlyto x.",computations with matrices
"7. for a general 2 by 2 matrix a b a= , c d find the jacobi iteration matrix s−1t = −d−1(l+u) and its eigenvalues µ. find i alsothegauss-seidelmatrix−(d+l)−1u anditseigenvaluesλ,anddecidewhether i λ =µ2 . max max",computations with matrices
8. change ax=b to x=(i−a)x+b. what are s and t for this splitting? what matrix s−1t controls the convergence of x =(1−a)x +b? k+1 k,computations with matrices
"9. if λ is an eigenvalue of a, then is an eigenvalue of b = i−a. the real eigen- values of b have absolute value less than 1 if the real eigenvalues of a lie between and .  ",computations with matrices
10. show why the iteration x =(i−a)x +b does not converge for a= 2 −1 . k+1 k −1 2,computations with matrices
"11. why is the norm of bk never larger than (cid:107)b(cid:107)k? then (cid:107)b(cid:107) < 1 guarantees that the powers bk approach zero (convergence). this is no surprise, since |λ| is below max (cid:107)b(cid:107).",computations with matrices
"12. if a is singular, then all splittings a = s−t must fail. from ax = 0, show that s−1tx=x. so this matrix b=s−1t hasλ=1 and fails.",computations with matrices
13. change the 2s to 3s and find the eigenvalues of s−1t for both methods: 3 0 0 1 3 0 0 1 (j) x = x +b (gs) x = x +b. k+1 k k+1 k 0 3 1 0 −1 3 0 0 does |λ| for gauss-seidel equal |λ|2 for jacobi? max max,computations with matrices
"14. write a computer code (matlab or other) for gauss-seidel. you can define s and t from a, or set up the iteration loop directly from the entries a . test it on the −1, ij 2, −1 matrices a of order 10, 20, 50, with b=(1,0,...,0).",computations with matrices
"15. the sor splitting matrix s is the same as for gauss-seidel except that the diagonal isdividedbyω. writeaprogramforsoronannbynmatrix. applyitwithω=1,",computations with matrices
"1.4, 1.8, 2.2 when a is the −1, 2, −1 matrix of order 10.",computations with matrices
"16. when a = at, the arnoldi-lanczos method finds orthonormal q’s so that aq = j b q +a q +b q (withq =0). multiplybyqt tofindaformulafora . the j−1 j−1 j j j j+1 0 j j equation says that aq=qt where t is a matrix.",computations with matrices
"17. what bound on |λ| does gershgorin give for these matrices (see problem 4)? max what are the three gershgorin circles that contain all the eigenvalues? [ ] [ ] .3 .3 .2 2 −1 0 [ ] [ ] a=[.3 .2 .4] a=[−1 2 −1]. .2 .4 .1 0 −1 2 the key point for large matrices is that matrix-vector multiplication is much faster than matrix-matrix multiplication. a crucial construction starts with a vec- tor b and computes ab,a2b,... (but never a2!). the first n vectors span the nth krylov subspace. they are the columns of the krylov matrix k : n   k = b ab a2b ··· an−1b . n the arnoldi-lanczos iteration orthogonalizes the columns of k , and the conjugate n gradient iteration solves ax=b when a is symmetric positive definite. arnoldi iteration conjugate gradient iteration q =b/(cid:107)b(cid:107) x =0, r =b, p =r 1 0 0 0 0 for n=1 to n−1 for n=1 to n v=aq α =(rt r )/(pt ap ) step length x to x n n n−1 n−1 n−1 n−1 n−1 n for j =1 to n x =x +α p approximate solution n n−1 n n−1 h =qtv r =r −α ap new residual b−ax jn j n n−1 n n−1 n v=v−h q β =(rtr )/(rt r ) improvement this step jn j n n n n−1 n−1 h =(cid:107)v(cid:107) p =r +β p next search direction n+1,n n n n n−1 q =v/h note: only 1 matrix vector multiplication aq and ap n+1 n+1,n",computations with matrices
"18. in arnoldi, show that q is orthogonal to q . the arnoldi method is gram-schmidt 2 1 orthogonalization applied to the krylov matrix: k = q r . the eigenvalues of n n n qtaq are often very close to those of a, even for n (cid:191)n. the lanczos iteration is n n arnoldi for symmetric matrices (all coded in arpack).",computations with matrices
"19. in conjugate gradients, show that r is orthogonal to r (orthogonal residuals), and 1 0 ptap = 0 (search directions are a-orthogonal). the iteration solves ax = b by 0 minimizing the error etae in the krylov subspace. it is a fantastic algorithm. 8 chapter linear programming and game theory",computations with matrices
"1. sketch the feasible set with constraints x+2y ≥ 6, 2x+y ≥ 6, x ≥ 0, y ≥ 0. what points lie at the three “corners” of this set?",linear programming and game theory
"2.  on the preceding feasible set, what is the minimum value of the cost function x+y? draw the line x+y= constant that first touches the feasible set. what points minimize the cost functions 3x+y and x−y?",linear programming and game theory
"3. show that the feasible set constrained by 2x+5y≤3, −3x+8y≤−5, x≥0, y≥0, is empty.",linear programming and game theory
"4. show that the following problem is feasible but unbounded, so it has no optimal solution: maximize x+y, subject to x≥0, y≥0, −3x+2y≤−1, x−y≤2.",linear programming and game theory
"5. add a single inequality constraint to x ≥ 0, y ≥ 0 such that the feasible set contains only one point.",linear programming and game theory
"6. what shape is the feasible set x ≥ 0, y ≥ 0, z ≥ 0, x+y+z = 1, and what is the maximum of x+2y+3z?",linear programming and game theory
7. solve the portfolio problem at the end of the preceding section.,linear programming and game theory
"8. inthefeasiblesetforthegeneralmotorsproblem,thenonnegativityx,y,z≥0leaves aneighthofthree-dimensionalspace(thepositiveoctant). howisthiscutbythetwo planes from the constraints, and what shape is the feasible set? how do its corners showthat, withonlythesetwoconstraints, therewillbeonlytwokindsofcarsinthe optimal solution?",linear programming and game theory
"9. (transportationproblem)supposetexas,california,andalaskaeachproduceamil- lionbarrelsofoil;800,000barrelsareneededinchicagoatadistanceof1000,2000, and 3000 miles from the three producers, respectively; and 2,200,000 barrels are needed in new england 1500, 3000, and 3700 miles away. if shipments cost one unit for each barrel-mile, what linear program with five equality constraints must be solved to minimize the shipping cost?",linear programming and game theory
1. compute the row vectorλ=c b−1 and the reduced costs r =c −λn. b n,linear programming and game theory
"2. if r ≥ 0, stop: the current solution is optimal. otherwise, if r is the most i negative component, choose u= column i of n to enter the basis.",linear programming and game theory
"3. compute the ratios of b−1b to b−1u, admitting only positive components of b−1u. (if b−1u < 0, the minimal cost is −∞.) when the smallest ratio occurs at component k, the kth column of the current b will leave.",linear programming and game theory
"4. update b, b−1, or lu, and the solution x =b−1b. return to step 1. b",linear programming and game theory
"1. minimize x +x −x , subject to 1 2 3 2x −4x +x +x =4 1 2 3 4 3x +5x +x +x =2. 1 2 3 5 whichofx ,x ,x shouldenterthebasis,andwhichofx ,x shouldleave? compute 1 2 3 4 5 the new pair of basic variables, and find the cost at the new corner.",linear programming and game theory
"2. after the preceding simplex step, prepare for and decide on the next step.",linear programming and game theory
"3. in example 3, suppose the cost is 3x+y. with rearrangement, the cost vector is c=(0,1,3,0). show that r ≥0 and, therefore, that corner p is optimal.",linear programming and game theory
"4. suppose the cost function in example 3 is x−y, so that after rearrangement c = (0,−1,1,0) at the corner p. compute r and decide which column u should enter the basis. then compute b−1u and show from its sign that you will never meet another corner. we are climbing the y-axis in figure 8.3, and x−y goes to −∞.",linear programming and game theory
"5. again in example 3, change the cost to x+3y. verify that the simplex method takes you from p to q to r, and that the corner r is optimal.",linear programming and game theory
"6. phase i finds a basic feasible solution to ax = b (a corner). after changing signs to make b ≥ 0, consider the auxiliary problem of minimizing w +w +···+w , 1 2 m subject to x ≥ 0, w ≥ 0, ax+w = b. whenever ax = b has a nonnegative solution, the minimum cost in this problem will be zero—with w∗ =0. (a) show that, for this new problem, the corner x = 0, w = b is both basic and fea- sible. therefore its phase i is already set, and the simplex method can proceed to find the optimal pair x∗, w∗. if w∗ = 0, then x∗ is the required corner in the original problem. (b) with a = [1 1] and b = [3], write out the auxiliary problem, its phase i vector x=0,w=b,anditsoptimalvector. findthecornerofthefeasiblesetx −x =3, 1 2 x ≥x ≥0, and draw a picture of this set. 1 2",linear programming and game theory
"7. if we wanted to maximize instead of minimize the cost (with ax = b and x ≥ 0), what would be the stopping test on r, and what rules would choose the column of n to make basic and the column of b to make free?",linear programming and game theory
"8. minimize 2x +x , subject to x +x ≥4, x +3x ≥12, x −x ≥0, x≥0. 1 2 1 2 1 2 1 2",linear programming and game theory
"9. verify the inverse in equation (5), and show that be has bv = u in its kth column. then be is the correct basis matrix for the next stop, e−1b−1 is its inverse, and e−1 updates the basis matrix correctly.",linear programming and game theory
"10. suppose we want to minimize cx=x −x , subject to 1 2 2x −4x +x =6 1 2 3 (all x ,x ,x ,x ≥0). 1 2 3 4 3x +6x +x =12 1 2 4 starting from x =(0,0,6,12), should x or x be increased from its current value of 1 2 zero? how far can it be increased until the equations force x or x down to zero? at 3 4 that point, what is the new x?",linear programming and game theory
"11. for the matrix p = i−at(aat)−1a, show that if x is in the nullspace of a, then px=x. the nullspace stays unchanged under this projection.",linear programming and game theory
"12. (a) minimizethecostctx=5x +4x +8x ontheplanex +x +x =3, bytesting 1 2 3 1 2 3 the vertices p, q, r, where the triangle is cut off by the requirement x≥0. (b) project c = (5,4,8) onto the nullspace of a = [1 1 1], and find the maximum step s that keeps e−spc nonnegative.",linear programming and game theory
"1. what is the dual of the following problem: minimize x +x , subject to x ≥ 0, 1 2 1 x ≥ 0, 2x ≥ 4, x +3x ≥ 11? find the solution to both this problem and its dual, 2 1 1 2 and verify that minimum equals maximum.",linear programming and game theory
"2. what is the dual of the following problem: maximize y subject to y ≥ 0, y ≥ 0, 2 1 2 y +y ≤3? solve both this problem and its dual. 1 2",linear programming and game theory
"3. supposeaistheidentitymatrix(sothatm=n),andthevectorsbandcarenonnega- tive. explainwhyx∗=bisoptimalintheminimumproblem,findy∗ inthemaximum problem, and verify that the two values are the same. if the first component of b is negative, what are x∗ and y∗?",linear programming and game theory
"4. constructa1by1exampleinwhichax≥b,x≥0isunfeasible,andthedualproblem is unbounded.  ",linear programming and game theory
"5. startingwiththe2by2matrixa= 1 0 ,choosebandcsothatbothofthefeasible 0 −1 sets ax≥b, x≥0 and ya≤c, y≥0 are empty.",linear programming and game theory
"6. if all entries of a, b, and c are positive, show that both the primal and the dual are feasible.",linear programming and game theory
"7. show that x=(1,1,1,0) and y=(1,1,0,1) are feasible in the primal and dual, with [ ] [ ] [ ] 0 0 1 0 1 1 [ ] [ ] [ ] [0 1 0 0] [1] [1] a=[ ], b=[ ], c=[ ]. [1 1 1 1] [1] [1] 1 0 0 1 1 3 then, after computing cx and yb, explain how you know they are optimal.",linear programming and game theory
"8. verify that the vectors in the previous exercise satisfy the complementary slackness conditions in equation (2), and find the one slack inequality in both the primal and the dual.      ",linear programming and game theory
"9. suppose that a= 1 0 , b= 1 , and c= 1 . find the optimal x and y, and verify 0 1 −1 1 the complementary slackness conditions (as well as yb=cx).",linear programming and game theory
10. if the primal problem is constrained by equations instead of inequalities—minimize cx subject to ax = b and x ≥ 0—then the requirement y ≥ 0 is left out of the dual: maximize yb subject to ya ≤ c. show that the one-sided inequality yb ≤ cx still holds. whywasy≥0 needed in equation (1) butnot here? this weak duality can be completed to full duality.,linear programming and game theory
"11. (a) without the simplex method, minimize the cost 5x +3x +4x , subject to x + 1 2 3 1 x +x ≥1, x ≥0, x ≥0, x ≥0. 2 3 1 2 3 (b) what is the shape of the feasible set? (c) what is the dual problem, and what is its solution y?",linear programming and game theory
"12. if the primal has a unique optimal solution x∗, and then c is changed a little, explain why x∗ still remains the optimal solution.",linear programming and game theory
"13. writethedualofthefollowingproblem: maximizex +x +x subjectto2x +x ≤ 1 2 3 1 2 4, x ≤6. what are the optimal x∗ and y∗ (if they exist!)? 3  ",linear programming and game theory
"14. ifa= 1 1 , describetheconeofnonnegativecombinationsofthecolumns. ifblies 0 1 inside that cone, say b = (3,2), what is the feasible vector x? if b lies outside, say b=(0,1), what vector y will satisfy the alternative?",linear programming and game theory
"15. in three dimensions, can you find a set of six vectors whose cone of nonnegative combinations fills the whole space? what about four vectors?",linear programming and game theory
"16. use 8h to show that the following equation has no solution, because the alternative holds: 2 2 1 x= . 4 4 1",linear programming and game theory
17. use 8i to show that there is no solution x≥0 (the alternative holds): 1 3 −5 2 x= . 1 −4 −7 3,linear programming and game theory
"18. show that the alternatives in 8j (ax ≥ b, x ≥ 0, ya ≥ 0, yb < 0, y ≤ 0) cannot both hold. hint: yax.",linear programming and game theory
"1. in figure 8.5, add 3 to every capacity. find by inspection the maximal flow and minimal cut.",linear programming and game theory
2. find a maximal flow and minimal cut for the following network:,linear programming and game theory
"3. if you could increase the capacity of any one pipe in the network above, which change would produce the largest increase in the maximal flow?",linear programming and game theory
4. draw a 5-node network with capacity |i− j| between node i and node j. find the largest possible flow from node 1 to node 4.,linear programming and game theory
"5. in a graph, the maximum number of paths from s tot with no common edges equals the minimum number of edges whose removal disconnects s from t. relate this to the max flow-min cut theorem.",linear programming and game theory
"6. find a maximal set of marriages (a complete matching, if possible) for [ ] [ ] 0 0 1 0 0 1 1 0 0 0 [ ] [ ] [1 1 0 1 1] [0 1 0 1 0] [ ] [ ] a=[0 1 1 0 1] and b=[0 0 1 0 1]. [ ] [ ] [0 0 1 1 0] [1 1 1 0 0] 0 0 0 1 0 1 0 0 0 0 sketch the network for b, with heavier lines on the edges in your matching.",linear programming and game theory
"7. for the matrix a in problem 6, which rows violate hall’s condition—by having all their 1s in too few columns? which p by q submatrix of zeros has p+q>n?",linear programming and game theory
"8. how many lines (horizontal and vertical) are needed to cover all the 1s in a in prob- lem6? foranymatrix, explainwhyweak dualityis true: if k marriagesare possible, then it takes at least k lines to cover all the 1s.",linear programming and game theory
"9. (a) suppose every row and every column contains exactly two 1s. prove that a com- plete matching is possible. (show that the 1s cannot be covered by less than n lines) (b) find an example with two or more is in each row and column, for which a com- plete matching is impossible.",linear programming and game theory
"10. if a 7 by 7 matrix has 15 1s, prove that it allows at least 3 marriages.",linear programming and game theory
"11. for infinite sets, a complete matching may be impossible even if hail’s condition is passed. if the first row is all 1s and then every a =1, show that any p rows have ii−1 1s in at least p columns—and yet there is no complete matching.",linear programming and game theory
"12. if figure 8.5 shows lengths instead of capacities, find the shortest path from s to t, and a minimal spanning tree.",linear programming and game theory
13. apply algorithms 1 and 2 to find a shortest spanning tree for the networkof problem,linear programming and game theory
"14. (a) why does the greedy algorithm work for the spanning tree problem? (b) show by example that the greedy algorithm could fail to find the shortest path from s tot, by starting with the shortest edge.",linear programming and game theory
"15. if a is the 5 by 5 matrix with is just above and just below the main diagonal, find (a) a set of rows with 1s in too few columns. (b) a set of columns with is in too few rows. (c) a p by q submatrix of zeros with p+q>5. (d) four lines that cover all the 1s.",linear programming and game theory
16. the maximal flow problem has slack variables w = c −x for the difference be- ij ij ij tween capacities and flows. state the problem of figure 8.5 as a linear program.,linear programming and game theory
1. how will the optimal strategies in the game that opens this section be affected if the $20isincreasedto$70? whatisthevalue(theaveragewinforx)ofthisnewgame?  ,linear programming and game theory
"2. with payoff matrix a= 1 2 , explain the calculation by x of the maximin and byy 3 4 of the minimax. what strategies x∗ and y∗ are optimal?",linear programming and game theory
"3. if a is the largest entry in its row and the smallest in its column, why will x always ij choose column j and y always choose row i (regardless of the rest of the matrix)? show that the preceding problem had such an entry, and then construct an a without one.  ",linear programming and game theory
"4. computey’s best strategy by weighting the rows of a= 3 4 1 with y and 1−y. x 2 0 3 will concentrate on the largest of the components 3y+2(1−y), 4y, and y+3(1−y). find the largest of those three (depending on y) and then find the y∗ between 0 and 1 that makes this largest component as small as possible.",linear programming and game theory
"5. with the same a as in problem 4, find the best strategy for x. show that x uses only the two columns (the first and third) that meet at the minimax point in the graph.",linear programming and game theory
"6. find both optimal strategies, and the value, if 1 0 −1 a= . −2 −1 2  ",linear programming and game theory
"7. suppose a = a b . what weights x and 1−x will give a column of the form c d 1 1 [u u]t, and what weights y and 1−y on the two rows will give a new row [v v]? 1 1 show that u=v.",linear programming and game theory
"8. find x∗, y∗ and the value v for [ ] 1 0 0 [ ] a=[0 2 0]. 0 0 3",linear programming and game theory
9. compute min max (x y +x y ). 1 1 2 2 y≥0 x ≥0 i 1 y +y =1x +x =1 1 2 1 2,linear programming and game theory
"10. explain each of the inequalities in equation (5). then, once the minimax theorem has turned them into equalities, derive (again in words) the saddle point equations (4).",linear programming and game theory
"11. show that x∗ = (1,1,0,0) and y∗ = (1,1) are optimal strategies in our simplified 2 2 2 2 version of poker, by computing yax∗ and y∗ax and verifying the conditions (4) for a saddle point.",linear programming and game theory
"12. has it been proved that no chess strategy always wins for black? this is certainly true when the players are given two moves at a time; if black had a winning strategy, white could move a knight out and back and then follow that strategy, leading to the impossible conclusion that both would win.",linear programming and game theory
"13. ifx choosesaprimenumberandsimultaneouslyy guesseswhetheritisoddoreven (with gain or loss of $1), who has the advantage?",linear programming and game theory
"14. if x is a quarterback, with the choice of run or pass, andy can defend against a run or a pass, suppose the payoff (in yards) is 2 8 defense against run a= 6 −6 defense against pass. run pass what are the optimal strategies and the average gain on each play? ",linear programming and game theory
"1. suppose s and t are subspaces of r13, with dims=7 and dimt=8. (a) what is the largest possible dimension of s∩t? (b) what is the smallest possible dimension of s∩t? (c) what is the smallest possible dimension of s+t? (d) what is the largest possible dimension of s+t?",linear programming and game theory
"2. what are the intersections of the following pairs of subspaces? (a) the x-y plane and the y-z plane in r3, (b) the line through (1,1,1) and the plane through (1,0,0) and (0,1,1). (c) the zero vector and the whole space r3. (d) the plane s perpendicular to (1,1,0) and perpendicular to (0,1,1) in r3. what are the sums of those pairs of subspaces?",linear programming and game theory
"3. withinthe space of all 4 by 4 matrices, let v be the subspace of tridiagonal matrices and w the subspace of upper triangular matrices. describe the subspace v+w, whose members are the upper hessenberg matrices. what is v∩w? verify formula (3). appendixa intersection,sum,andproductofspaces 465",linear programming and game theory
"4. if v∩w contains only the zero vector, then equation (3) becomes dim(v+w) = dimv+dimw. check this when v is the row space of a, w is the nullspace of a, and the matrix a is m by n of rank r. what are the dimensions?",linear programming and game theory
"5. give an example in r3 for which v∩w contains only the zero vector, but v is not orthogonal to w.",linear programming and game theory
"6. if v∩w = {0}, then v+w is called the direct sum of v and w, with the special notation v⊕w. if v is spanned by (1,1,1) and (1,0,1), choose a subspace w so that v⊕w=r3, explain why any vector x in the direct sum v⊕w can be written in one and only one way as x=v+w (with v in v and w in w).",linear programming and game theory
"7. find a basis for the sum v+w of the space v spanned by v = (1,1,0,0), v = 1 2 (1,0,1,0) and the space w spanned by w = (0,1,0,1), w = (0,0,1,1). find also 1 2 the dimension of v∩w and a basis for it.",linear programming and game theory
8. prove from equation (3) that rank(a+b)=rank(a)+rank(b).,linear programming and game theory
"9. theintersectionc(a)∩c(b)matchesthenullspaceof[a b]. eachy=ax =bx 1 2 inthecolumnspacesofbothaandbmatchesx=(x ,−x )inthenullspace,because 1 2 [a b]x=ax −bx =0. checkthaty=(6,3,6)matchesx=(1,1,−2,−3),andfind 1 2 the intersection c(a)∩c(b), for [ ] [ ] 1 5 3 0 [ ] [ ] a=[3 0] b=[0 1]. 2 4 0 2",linear programming and game theory
10. multiply a⊗b times a−1⊗b−1 to get aa−1⊗bb−1 =i⊗i =i . 2d  ,linear programming and game theory
11. what is the 4 by 4 fourier matrix f =f⊗f for f = 1 1 ? 2d 1 −1,linear programming and game theory
"12. suppose ax=λ(a)x and by=λ(b)y. form a long column vector z with n2 compo- nents, x y, then x y, and eventually x y. show that z is an eigenvector for (a⊗i)z= 1 2 n λ(a)z and (a⊗b)z=λ(a)λ(b)z.",linear programming and game theory
13. whatwouldbetheseven-pointlaplacematrixfor−u −u −u =0?,linear programming and game theory
1. find the jordan forms (in three steps!) of [ ] 0 1 2 1 1 [ ] a= and b=[0 0 0]. 1 1 0 0 0,linear programming and game theory
"2. show that the special solution u in equation (17) does satisfy du/dt = au, exactly 2 because of the string ax =8x , ax =8x +x . 1 1 7 7 1",linear programming and game theory
"3. for the matrix b in problem 1, use mejtm−1 to compute the exponential ebt, and compare it with the power series i+bt+(bt)2/2!+···.",linear programming and game theory
"4. show that each jordan block j is similar to its transpose, jt = p−1jp, using the i i i permutation matrix p with 1s along the cross-diagonal (lower left to upper right). deduce that every matrix is similar to its transpose.",linear programming and game theory
5. find “by inspection” the jordan forms of [ ] 1 2 3 [ ] 1 1 a=[0 4 5] and b= . −1 −1 0 0 6,linear programming and game theory
"6. findthejordanformj andthematrixm foraandb(bhaseigenvalues1, 1, 1, −1). what is the solution to du/dt =au, and what is eat? [ ] 0 0 1 0 0 [ ] 1 −1 0 −1 [ ] [0 0 0 1 0] [ ] [ ] [ 0 2 0 1 ] a=[0 0 0 0 1] and b=[ ]. [ ] [−2 1 −1 1 ] [0 0 0 0 0] 2 −1 2 0 0 0 0 0 0",linear programming and game theory
7. suppose that a2 =a. show that its jordan form j =m−1am satisfies j2 =j.,linear programming and game theory
"1. the linienst ersaet( cxty, ) = (31,) t.h en3( colum1n)+ 1( colum2n)= (4,4).",linear programming and game theory
"3. these ""planes"" iinan l tienriesnf e ocutr m-ednisionsapla ce. thep lfaonunero trh­ w malliyn tersects itnah p aoti nlatin.n i en consiesqtueantti loinku e + = 5 leaves nos olut(inooni ntersection).",linear programming and game theory
"5. the tpwooi notnst hep lanaer e( 10,,0 ,0 )a nd( 01,, 0,0 ).",linear programming and game theory
"7. solvabfloe(r 3 ,5,a8n)d ( 1,23,)n;o ts olavblfeo br = (3,57,)o rb = (12,,2). vw,)",linear programming and game theory
"9. colum3n = 2(colu2m)n- colum1n.i fb= (00,, 0 )t,h e(nu , = (c-,2 c, c)",linear programming and game theory
"11. both a = 2 anda = -2 givael inoefs olutiaolln ost.h ea rg ivxe = 0,y = 0.",linear programming and game theory
"13. ther owp ictuhraest wol ines meaett( i42n,)g t. h e column phiacs4t (1u ,1r )e+ 2(-21,) 4(colu1m)n+ 2(colu2m)n right-hsaind(de0 6,) . = =",linear programming and game theory
15. ther owp ictusrheo wfso ulri ntehsec. o lumpni ctuirsie n /our-dsiimoennsapla ce. no solutuinolne tshser ight-hsaindides a c ombinatoifto hnetw o columns.,linear programming and game theory
"17. if x ,y ,z satitshfefiy r sttw oe quatitohnesay,l ssoa titshftheyi redq uatitohnel. i ne v (!!,) , �v! w, l ofs oluticoonnst ain = s( 11,,0)w, = 1, andu = + anda ll cv w combinatio+n ds witch+ d 1. ==",linear programming and game theory
"19. colum3n colum1n; s oluti(oxn,ys, z ) = (1,1,)0o r( 01,,1)a ndy ouc ana dd == anym ultiopfl e (-01,1, )b; (46,,c )n eeds 1c0 f osro lbviality. = =",linear programming and game theory
21. these cond palnadrn oew 2 o ft hmea trainxd a ll coloufmth nesm atriaxr ceh anged. the solutiison no tc hanged. v w,linear programming and game theory
"23. u = 0, = 0; = 1,b ecau1s( ec olum3n) = b. problseem1t a p3a,g 1e5 �o",linear programming and game theory
"1. multipblyyf = = 5,a nds ubtrtaocfi tn d2 x+ 3y = 1 and- 6y = 6.p ivots 2,- 6. -! !",linear programming and game theory
"3. subtractt imes equ1a( toiarod nd timeesq uati1o)tn.h en ews econedq uation is3 y = 3.t heny = 1 andx = 5.i ft her ight-hsadinecd h angseisg ns,od oethse soluti( ox ny, :)= (-5,- 1).",linear programming and game theory
5. 6 x+ 4y is2 time3sx + 2y.t heries n os olutiuonnl etshs er ight-hsainddie s,linear programming and game theory
"2. 1 ° 20.t hena lplo inotnst he l3ixn+ e 2 y 10a rseo lutiionncsl,u d(i0n5,g ) = = and( 4-, 1).",linear programming and game theory
"7. i fa = 2,el iminamtuisoftna itlh.ee quatihoanvsne o s olutiiofa n =. 0 ,e limination stopfso ra rowe xchangteh.e n3 y = -3 givye =s -1a nd4 x + 6y = 6 gives x 3. =",linear programming and game theory
"9. 6 x - 4yi s2 time(s3 x 2y)t.h erefowree n,e ebd2 = 2b1t•h ent herwei lble - lnflnltalrrhlu nu c'al11t1antc'h i3 r-nl111'l'1nfc'� a) �nrfl_ 'j _lt.) <:lri3 an thi3 c'f"".l1""y\o 111'113 sotliuontsos elteecedx recises 429 = 11 2. x -3y = 3 2x -3y 3 x= 3 sutbr2a x c rto 1wf ormr o2w y + 1zg ivye s + 1z n ady 1 sutrbac1t r o1wf ormr o3w = = = x . = 2y- 32: 2 - 5z ° z ° subt2r arco2twf ormr o3w = = x",linear programming and game theory
"13. t hsee cpovinodpt o tsiiwoincl olni tn-a 2 h.i bf -2,w ee xchwainhrtg oew = - 3i.bf = -1( nsgiaurcl ae,s)t hsee ceoqnaudt iio-sny - z =",linear programming and game theory
"0. sloutiiso n a (11,, -1). 15i.r fo 1w ro2wt, h reon2w i zse arfott hefierr ssttee xpc;h atnhzgeee rr ooww i th = ro3wa ntdh eirnseo t hpiirtvid.ocf o ulmn coulm2nt ,h eirnseos ecpoinvodt . 1= 17r. ow2b eco3my-e 4sz = 5th,e rno 3wb eco(mq+e 4 sz) = -5i.qf = - 4, t = thseye smit ssig nual-r not hdip rivtohetni.,f 5t,h teh ierqdu ait°si = o0.n t = chosoizn g thee qua3tyi- o4nz 5 giyv e3sa nd aoetinqu gives = 1, = 1 x= -9. 19t. hsey esmit ssi ngiurflo 3awi r asc mobniatofir oownas n2 df.r otmhe env di ew, 1 thterh epel afnnnoe sta ar nig.tl hehipasp einrfso s w +2 ro3wo n ltethhf-ead n 1 = z= side burti gnhotst-i hdftareohen :eaxd m ep x,l+ ,yz+ = 0, x 2y - 1, - 2x- y = 9.n ot wpol aanrepesra al,bl usetitl lnlos oultoin. �. (n.� l)",linear programming and game theory
21. t hfietf hp iovits t hneth p iviost w = = u + v + 2 u 3,linear programming and game theory
23. t rainguslyeasmrt 2 +v 2 w -2 solutio-n2. v = = = 2w 2 w = 1 w)=,linear programming and game theory
"25. ( ,uv , (/321/,2. -,3.c)hantgole+w oumladkt ehs eye smst niguleaqru al (2 colnus.m)",linear programming and game theory
"27. a °r equari orewex sac gnheb,ut ths eye smit nso innsgual 2am ra:ki estsi ngular = = (opnietv i,on fiyon ifst ouiltnosa)= ;- 2m aksie stni gu(loanre psilovuontt)i,.o no",linear programming and game theory
"29. t hes ecotnedrb mc+ a di s(a b()+c d )-"" ca- bd( onlayd ditional + 1 mulptltiiic.oa n)",linear programming and game theory
"31. e limifniaalftsoia ro n2 ( eqlcu loaunmsa) ,4 (erqwous,aa) l= °( zero = = colnu.)m porblem pgae se1t4 m, 26 5 7 [] 2 1",linear programming and game theory
"1. 4 ,- 2, .w iths idteo,s 1 (a)2n (d0,3 )th,ep alrlaelgoogtero(sa 2m4 ,.) 4 3 17 3 5 1",linear programming and game theory
"3. innperro duacnt0ds,c lo5u4mt ni mesgv iers-o w6 -10 -2. 21 35 7",linear programming and game theory
"5. a x= ( ,00 ,0) s,o x = (21, 1, ) i ass louito;on thseourlt iso nacrx = e( 2,cc c,.) ° ° 3 4 3 1 1 1 4 0",linear programming and game theory
"7. exmapsld:ei agon2a l0 , symm3e 2t r0i ,tc r ian0g u2l 0a ,r 0 ° 4 0 ° 0 7 7 7 0 3 4 .>:��: sk-eswmymercti -30 0. -4 0 ° ontso s elteecedxe rcises a l'l .( )aa ll (b i) li= aiall/l (c n) e waiija si -j- -alj 9 all a21 (ds)e cpoinvad 2o 2-t -- a1'2 all",linear programming and game theory
"11. t hceo efficoifre onswot fbs a r2e,1 ,4fr oma. t hfiert rs oowaf b i[s 63. ] _�}� �. � . [� [j [�j d =n 13a.= b= = = a,e = f= c u o. 15a.b 1= blag ivbe =cs = ab2= b ag isva ed.s oa= al. = 2 17a.( a+ b)+ b(a b),( a+ b)(+b a),a 2+ ab ba b2a lawyesq ual + + + 2 (a+ b). + [p ] [!] :[� ] [: ] �q ] [] !s ] [ :;a �q b!s �",linear programming and game theory
"1. = + = 9 [r cq+ ds. [� � � -�t ] l [ 21a.n = a;b n= ( = = zemraot .r ix c 3 -5,",linear programming and game theory
"23. e3 2e2=1 b( 1, 35 )b u et 2 132eb (1-,5 ,.0t )herno wf eelnsoe ffect - = forirno 1w.",linear programming and game theory
25. c hangifnorgm7 t ao131 w 3li clh nagthee t hipridvfr ootm5 t o9. c hnagian3g3 form to2w iclhla tnhgpeei vfrootm5 t no povi ot. 7 1 0 0,linear programming and game theory
"27. t or esveeer 13,a dd7 t imreos1wt ro owt hmea tirsi3 1 =x 0 1 r 7 0 1 1 01 1 0 1 2 0 1 o.",linear programming and game theory
"29. e 1=3 0 1 ; 00 1 0;e 13 e3 0 1 tesotnt hied enmtait!tr yi x 1 = 0 0 1 1 0 1 1 0 1 -!, �, -%. 31 e. 2 h 1 a .es 2= 1 e3h 2 a.es3= 2 - e4h3a.e s4= 3 otherthweei 'mssae t ic.h a+b+c= 4 a=2",linear programming and game theory
33. a +2 b+ 4c= 8 give b=s 1 . a +3 b+ 9c= 14 c = 1,linear programming and game theory
"35. ( )a ecalocuhmi nes t imaec sou lmno fb. � ][ (b e)b= ] := � ; ; u u :l rowso efb arceo mbinoarfto siwo obfn, ss o atrhmeeu ylp tloief[s 1 2 ],4 2",linear programming and game theory
"37. ( ro 3)w · i i:s a3jaxnj(,ad ) 11= (r1o)w·( cuoml1n) i: = a 1j.a j1 x 315 513 3, 3",linear programming and game theory
"39. ba isb y5,a b = isb y abd = 5di sb y1,a bd :n o,a (b + c): = no. 0 0 1",linear programming and game theory
41. o. o .,linear programming and game theory
"41. ( ab) = (bb) (cb) = 0 1 = 1 0 0 o. (de)v erroyow fb i1s, 0, 3",linear programming and game theory
43. ( a) (eveenryt.y)r (bm)n .p (cn)( thni2ds o ptird suo c)t.s mn 3 3 3 3 1 0 0 0 00 0 [33o j [ j 6 6,linear programming and game theory
45. 2 + 4 1 2 =1 0 + 4 8 4 10 14 4 . 6 6 8 2 1 0 1 2 1 7 1 sotloiuntso s elteecedx ercises 431 b,linear programming and game theory
"47. timbei ss , a a [ l [[ ]]l [ ][ [ l",linear programming and game theory
"49. th(e22 , b) lc oks d c b1i thse bloicnk s schucro mplement: == - a - d (-cjb a).",linear programming and game theory
51. times wiblethl e i dteinmtayt ir ix a == [xlx 2x 3] == [axia x2a x3]' x ] ] [ [,linear programming and game theory
53. a+b a+b agrei. the sa+ c w b+d when and + b+ b == c a= d . c d c+ d a+ c d,linear programming and game theory
"55. 2 x 3 y+ z + 5 t 8 i s wtiht h1eb y,m4a trix 3 1 t5he + = ax= b a == [2 ]. sloutxifio lanl3 s d ""pilfnauo nrde i""nm seni.so x",linear programming and game theory
"57. thdeo ptr odu4c 5t (1b y3 ()b3y 1) i zse frroop oi(xn,t sz o)na [1 ] == y y, z + plan+4e 5z0 i tnh rdeienm senisto.hc eo luomnfsa roen dei-mseinonal x = a y vecrtso.",linear programming and game theory
59. a [34 and' 50; a givaenes r rmoers sage. * v == 5]' v * v = v * 8 3 4 5+u 5 -u +v 5-v,linear programming and game theory
"61. m 1 95 5- u - v5 5+ u + v ; = = 6 2 5+v 5+u- v 5 ·-u 7 m3(1,1, 1) (1,1551),5m; 4( ,11, 1, 1 ) ( 3,34,4343,4) btehcneau bumesres == == 1 t o1 a6d tdo1 6 3w,h iic4sh3( 4.) . l' prbolesme t paeg 56 1.39",linear programming and game theory
"1. u i nso nsairwn'hgneun lo eonntht erm ya idni algi ozsnre ao. 1 00 1 00 1 00 1 00 1 o 0 2 ,1 -02 1 00 01 ;-2 1 02 1 0 ia slo.",linear programming and game theory
3. == = - 1\\ 1- -11 1 0 0 -111 -11 1- 1 1 \ (e-1p i g-1 )g( ep) i e-1p - p1 e = e-1 e = i;a l(sgope ) ( e-1p -1g1)- = i. 1 00 '2 3 2 3 3u 2 3,linear programming and game theory
"5. l u 1 0 5'0 7 ;a fetleiirnm oant0,i 5 v 2 . == 7 o 0 -0 1 0 -01 -1 1 w 1 0 0 0 1 0 0 0 2 01 0 2 1 0 0",linear programming and game theory
7. ; fgh == o 2 1 hgf 0 == 4 2 1 o' 0 0 21 8 4 2 1,linear programming and game theory
9. ( an)o nnsgiuwlhaern i= o.( bs)pu psoe i= 0s:lo ve b going d 1d2d3 d3 lc == o 0 0 d1 -d1 u downwardbg :i evs 0 .t hen0 0 gives lc= c = d2 -d2 v 1 o 0 1 d3 w ijd3 x = ijd•3 ijd3 ontso s eelctdee xreceiss 2 5,linear programming and game theory
"11. l c= bg oidnogw nwgairvd=e s- 2 u; x == cu pawrgdi vxe =s - 2 . c 0 o 0 0 2 1 u permutation 0 0 v - 13 1 - 3 . 2a n3d , 4, • rows 0 1 0 w 0 0 1 1 u permutation 0 0 -1 1 v row1a sn 2d 0, 0 0 1 w o il 0 1 0 1 00 0 0 1 0 1 1",linear programming and game theory
"15. p a l=d ui s1 0 0 1 0 1- 0 1 0 o 1 0 o 1 1 · , 0 01 2 31 0 0 01 2 3 4 -1 o 1 00 1 00 0 01 2 1 1 2 1 1 o . p a l=d ui s0 10 2 42 1 10 o -1 0 o 1 0 1 0 1 1 1 2 01 0 00 0 0 0 1 00 om.a tlab",linear programming and game theory
17. l becom1e 1s and cootdhueespsre a lu. = 2 01 a lao,linear programming and game theory
19. = 4l eatdors o aew x cnhg;a3e b+ 4l0e atdoass n igurl amraitx;0 c= == leatdaors o ewx chea;=n g3l eatdoass i unlgmaartr .i x c,linear programming and game theory
21. £ 13 an£d23= 2( £=3 31 )r:ve erssteet pors e cox+ v 3eyr+ 6z= 11fo rm 1 == ux = 1t im(ex+s y + z5 +) 2 t im(ey+s 2 z 2 +) 1 t im(e=zs 2) g ives c: == == x+3y+6=z11. 1 1 1 1 1,linear programming and game theory
23. 0 1 -2 1 a= 0 2 3- u - . o -2 1 0 0 1 .0 o -6 0 0 1 a= 2 1 0 u = e21 1e 321u = lu. 0 2 1,linear programming and game theory
"25. 2 b y2 :d 0n oatl l;o wed = 1 1 0 1 d g d= 1,e = 1,t ehn=.e 1 e 1 1 2 - f h f 0i s anlolto wed - .e 1 == 1 2 1 m 1 . nop ivionrt o w2 . n 1 2 4 8 2",linear programming and game theory
27. a = 0 3 has= ia ndd = 3 ; al =uh aus a (piovno ts 9 l == 0 0 7 7 2 1 4 thdei agoa n=al dlu)h ;a us d-1a 0 1 3 witihos nt hdei agonal. == = 0 01 a a a a 1 a a a a a�o . a b b b 1 1 b-a b-a b-a b�a,linear programming and game theory
29. .n eed a b c c 1 1 1 c-b c-b c b =1= a b c d 1 1 1 1 d-c d =fc . 1 1 0 a a 0 a 1,linear programming and game theory
31. 1 1 11 = liu; a a+b b = (smael ) b 0 1 1 1 0 b b c + c (smaeu ). 4 1 0 0 4 1 1 1 4 3 . .,linear programming and game theory
"33. 1 1 0 5 givc=e s 0 x = givxe s=0 c = 1 1 1 1 . 1 1 0 0 1 1 1 6 1 1 ,- a = lu.",linear programming and game theory
"35. th2eb y2u ppseurbt mrabi hxa tshfi er tswtpo i v2o,tr se 7a.seo lni:mt iiononan a satrittnsh uep pleetcrf o rnweiret lhi mionnba . t ion 1 11 1 1 4 1 2 3 5",linear programming and game theory
"37. 1 3 6 101 5 4 102 03 5 1 1 5 153 5 70 ,;, 1 1 11 11 pascatilra'gnsl ei nl andu. 4 1 1 1 23 matalb' ls uc oed wwirlelc k 1 2 thpeta t.e rn doneosr ow 1 1 3""6 chal exchafnrosg myemse tric \ 1 31 3 1 4 4 6 4 1 1 matreiswcti ph otsiipviev .o ts 1",linear programming and game theory
39. e achr giht-hsainddce o sotnnsl2 ys tecposmr peatdno 3 /f3o re fluitlmilion na new a\.b,linear programming and game theory
41. 2 e xcnhga;e3 es xcnhga;e5 s0e xcnhgaaensd 5t1h.e n 0 1 0 1 00 0 01,linear programming and game theory
43. p =0 0 pi1 0 o landp 2 0= 1 0 = ; 1 00 0 1 0 01 0 ( pgivceosl ueam xnc h.a nge) 2,linear programming and game theory
45. t here paerremt uimtoaant ricens.e voefn tourapdloelwryoe frpt sm wu sobt e n!,linear programming and game theory
"1. thsea mief:p r = pst hpern- =s ceritnral y- < s n ! 1 o o c � 0 1, nadp6 o 1 0 o",linear programming and game theory
"47. t hseo tliuioxsn= (11,., ,. 1 ..t) h en= pxx. prbolesmte pgae",linear programming and game theory
"16. , 52 ] l [ l l [ sine",linear programming and game theory
1. a i �! a2} = [ -�ail 3= _���: = cos.e,linear programming and game theory
3. a-i = be-i a;- i = u-il1 p- . nst os elteedce xerecsi s 2,linear programming and game theory
5. a a(b)( mopvareen htes=e (sa))b (=) i. [ = [ [ ] -j3 /2 12/]--j3 /2 12/ 0 1] 2,linear programming and game theory
7. alhla vae i. 12/ --j3/2' 12/ -j3/2'1 0 =,linear programming and game theory
"9. i rfo 3 wo f- i wa er(e,ab c,d, )t ,h ae n1 a- iw ouglid2v ea 0 a, 3 =b 0 , + = = + 4a8 b 1t.hs hi ansos l oiuot.n == [�� ][-� _� ][�� [��] [� � ][�",linear programming and game theory
11. (a ) = (b) + l + = �l (c[� ) �] +[ _ b�] = [ � _� l ( - b 1+ a -l )-1 = ba( b)i - + a. l� ��n [ [,linear programming and game theory
13. a tb=8;bta=8a;bt= bat=,linear programming and game theory
15. ( an)n( 12)e /niterosna nadob vdei ag.o (nba()l-n l )2ne n/traobivees + dioangla.,linear programming and game theory
"17. ( at)h ien sveer alo ofw (eurp ptierarn)g mualtarrsii txll loi w(seu rp pternrig)au ­ lram.u ltilpolwy(eiurnp gpt aernrig)ur ml aaitecrsg ivale osw (eurp ptaergnriu)rl a , 1 l mait.xr t(hbme)a idni agoofln ialldsa dn ud1u ;ares mateh etao hss e 2 2 1 l 1 ofd andd resipveeclltilyd. = duu:;,s o hwaevd e d.b y mc­o 2 2 2 2 1 1 1 1== l 1 ,arpintgho eff- idagoonfla illds = duu:;,b ohmt aitcrmeussb ted iaglo.n a 22 1 1 li1 l2d2 == d2, 1ud 1u:;1 == d 1, 1id isn viebrsltole i 1 l 2 == i,u 1 u 2-1 == i. thelnl= l 2,u 1 u 2• == 1 00 1 00 1 35",linear programming and game theory
19. 3 1 0 0 13 ; 0 0 1 5 11 0 0 20 01 [ b��[ a] � ] [� b i a] = l dlt. �2 d /a) _,linear programming and game theory
"21. f robml( a-b=) (i bab) we( i-g beta)- l bi(- ab)l b- -1,a n - = expliincsvpieertr o vbia dnedd a ba rien viebrslteen.cad op ropaicifh b:a i - - inso itn vilebrett,h ebnax =f rxos moen onrzoxe t.h ereafboxar e=a x,o r aby y=a, n d aib c uolndob tei nvelre(t.ni otbtheya t=a xi nso nz,fe orrmo - bax=x). , � -:�.;- :71 -�7. - [][ j[][ ] i [ ]",linear programming and game theory
23. = a-= = so 10,linear programming and game theory
"25. ( ai)na x (010,,,)e quiao1tn equa2t ieouqnai to3ni 0s 1.( b) the = + - = rights-ihdmaeunsssd tai ytsb f b 2 b•3 (cr)o 3wb eocmears o owzf e ros-no 1 + == thipridv ot.",linear programming and game theory
"27. i bfe xcnhgareosw 1as n 2do fa ,th ben-1 e xchacnogle1usa m n2n do s fa -i . i",linear programming and game theory
"29. i af hacsou lmaon zf e srs,ood obea s.s ob a ii ismp ossi.bt lheeirse -no a == 1 1 1 1",linear programming and game theory
"31. 1 1 -1 1 - 1 1 -e', - -1 1 -1 1 1 -110 1 then1 1 ils e- 1 ,a tferre vetrhsoeir ndogetf rh s e3ee elmenmtaaitrycr e == 1 1 1 + ancdh an-gi1tno g1 . soutlinost os elteedce xeirsces 435",linear programming and game theory
"33. a on4e,sg1(vi) et shz ee ro vaec cnatnoboreti, ne vrsbtoli e. * - 31 31 ] 0 ]3 [; �] [� � � 7 -",linear programming and game theory
35. 0 1- 2 [ 1- 12 [i- a -i ]; 7 -+ -+ 31 0 ] � � 8 0 i - [ ia -]. [8 � -+ [1 3 -=i j 1 10 01 01 0 a b a -b 01 01 0 01 ' 001,linear programming and game theory
37. c 0 100 10� 0 0 1 0 -0c 1 10 10 -a ac-b 01 0 0 1 � 0 000 -1c . 1 20 1 ][0 2 -1 -11//2 ] ] [� � [� �,linear programming and game theory
39. 21 0< 2 11o 1 /2[ ia -i ]. -+ -+ = o 0,linear programming and game theory
"41. n oitn vbelfreotc ri 7( euqlac olnu)smc, (eqruwoaslc) , (zecrooln u.)m =: == == 2 1 10 o � 11 0 )",linear programming and game theory
43. a �l 0o 10 t h5eb y5 a-1a l�hs aisso n t hdei alga onnsdau per- = l ' 0010 daigoln.a 0 ']0 [i ] [ a -i [ d i],linear programming and game theory
"45. , ad � 1 · i' -d-c1a- d-1 o - c 1",linear programming and game theory
"47. f oarx b witah on4e,4s )( signurlm aatarnibdx onse(14)a,,\ b == (==1 ,0== ,0,0== ) wlipli cxk andp invb(w aipl)il ct ekhs horstoelsuxtt ion (1,== 1,1,* 1)/4. == ] ] [� [_;�' l [�d ;; 1 1/(a-1)t",linear programming and game theory
49. a t �al (ta) at aan d - - = = = = = 0 [c] thae-ni � (-a1 )t (t a)-l . == c2 c -1= =,linear programming and game theory
51. ( ( ba)-) t ( b-1a -l )t (-a1 t () b -t 1;() u -t 1i )l s w oetrrg iuarln.a l == = [�.],linear programming and game theory
53. ( ax)at y an 5. (bx)t a [45 6.] (c)y a = = = =,linear programming and game theory
55. ( xpt)p(y) xttppy xytbeucsaep tp iu;us lalpyx· y x·pty=j:. == = = = x·p y: 0101 1 011 01 0021 1. 2 . 0 0 1 1 1003 2=1= 31002,linear programming and game theory
57. p a ptr ecotvhseeyr mstrym .e,linear programming and game theory
"59. ( )at htern aspoofrs tae ,ri rs ta tr tt rta r by = = n n. (b()t r r ) (cuomlno fr ). ( cuomlno fr ) lehns gqtueardo fco lumn j j j. == = jj nst os elteedce xrecsies 1 0 1 ybc ybe+ ybs",linear programming and game theory
61. t otaclu rreanra tet y s = -1 1 0 ycs yb+c yes - - 1 - 1 o ybs -ye-sy bs eietwrha y( a x)t y xt (ayt) xby+bxcb yb-sx cy+bx cc yc- sx syc-sx sy.b s := :=,linear programming and game theory
"63. a x. y i s ctohosefit n pwuhtesr,xe. aa ytsi tsh vea olfou uet .p uts",linear programming and game theory
"65. t heasrege r po:sul owtearrni guwliarthd igaoln sad,1i agli onnvaleerdt ,ai nbd permtuitopan.tsw o m or:ee vepne rmiuot,naa slntlo nnsgiurml aaitcr.e s [��] [��] .",linear programming and game theory
"67. r eordtehrreoi wansngd c/oolru omfn s wimlolv e aen,nogtr tiy v ing",linear programming and game theory
69. r andmoamtc reaisra el smsotu revleyrtl ieib.n,linear programming and game theory
"71. t h-e1, 2 -, 1m artiixsn c eti17o.h na as ldtl w iftih- i i= 1 �. := - , l porblsetem pgae 1w7, 63 2 - 1 - 1 2",linear programming and game theory
1. -1 -1 2 -1 - 1 2 1 2 1 1 2 3 2 1 1 1 2 2 3 ldtl 2 4 3 = 1 1 3 -3 5 4 det5 . 3 = 1 1 4 -4 1 - 1 c 0 - 1 2 -1 c 0,linear programming and game theory
"3. a o - 1 2 -1 .e acrhoa wd tdos1 s ,oa o c 0 = -1 2 - 1 c 0 - 1 1 c 0 2 2",linear programming and game theory
"5. ( ulu2,, 3u. = )( n /80,, n /8i)ne sdaot ft htreu vea l(u10e,s-,1 ). - 9 -36 30 1",linear programming and game theory
7. h - = -36 129 - 180 . 30- 108 108,linear programming and game theory
9. t h1e0b y 1 h0i bletmr atxir vsie irlyl -icoon.ne ddit,linear programming and game theory
"11. a larpgiev omtu ilpitlsbi yle edts hsa1 in ne limienaacethni trnbyge lio.tw an 12/ 12/ 1 ��,, extrecmaesw eihm,t u lptliie1ra snp di vots4 i,as - 1/2 01 . = = := -1/2 -1 1 prbolesmte pgae 2�1, 73",linear programming and game theory
"1. ( )at hsee otfa l(l,uv ,) w heura en v da rreai top/s q o fin tre.sg (ebt)h see otf al(l,uv ) wh, e ur e0 o rv = = o. , 2",linear programming and game theory
"3. c a()i tsh xea- xsin;( aits)h lei tnheur go(h1 1)c;(b )i rs; n (bi)ts h lei ne 2 3 thorug(h-, 21 0,;)c (c)i tsh peon it( 00,)i rn ;t hneu lls(pcai)crs e .'iv solutnisto os eelctede xecrsies 437 �.bo'rk erlnue :(s a7)8,( b1)( c1)2, 8,. ,",linear programming and game theory
"7. ( )b(,d, () ea)rs eu bces.spc aan'mtu ltbiy-p l1iyn( ) aa n(dc. c) an'atdi dn( . t) +",linear programming and game theory
9. t hseu omtf wo n onnsgiumlaartc reisy b mesa ing(ual( -ara )t)h.e osfu mt wo singmualtrcariem sa bye n ongsuir.ln a,linear programming and game theory
11. ( )ao npeo bsisylit:ih tmea trciecsaf o ram ssupcbaen octot nai bn .i ng ()by e:ts hseu bascmpeus cto anitan i. b = - (c )t hsebu pscaeo fma rticwehssome ai nd iangaiolas l zlre. o,linear programming and game theory
"13. i (f+ fg (x)i)ths e u usaflg( x(),th )e(n+ gf )ixgsl ((x))w,h hii cdsfi free.in nt ru2lb eoh st idaer1se( ( gh( ) x). r )ul4ie bs r obkeecna tuhsemierg ehb ten ov eirnse fucnito 1n - (1)xs utchhfa ( t1 (-x1)= ) x i.tf h ievn erfsucent ieoxnii swtti sbl,el thvee c-tfo.r +",linear programming and game theory
"15. t he osfu 0m0 ,an) d (0 4,0, i) ns o otnt hpeal n;e ixt ha2sz 8 . (,4 = - y",linear programming and game theory
"17. - ( )at hsebu scpeaosfr 2 ar er 2 i tlsfle,i ntehsr o(u,00g ,)ha ntdh peo i(n00t.,) (bt)h seu sbpaocfre4 sa rre4i tstehelref-,d imle npslniao v nn ea0st, w o­ = . . diemnnsaislou bsp(anc esa nnd v 0,)o en-dismieloln niantehusrgo h 1 v = ° 2 . = (000,,, a n(d,00 ,0 a)0l ,o.n e 0), 3",linear programming and game theory
"19. t hsem aetls�lsupbaccoen tainainndig es i thoerrr . l p p ...,",linear programming and game theory
"21. t hceo lnu mspoafaic tesh xeax- is=a lvle ct(,ox0 r0,s.)t hceo lumno fbs pace itsh xe paln=e alvle ct(,ox r0s.)t hceo lusmpnao cfce tihsle i onvfee ctors - y y, ( (,x2 ,x0 .)",linear programming and game theory
23. cobmitnoiano tfh ceo luomnfcs aislsoa c mobitoninao tfh ceo lsu maon( fsm ae a coulmns pa;c heaasd iffecr0elnustmp ena.) c b,linear programming and game theory
"25. theex atc rolubem nnal regsthe c oulmns paucnesl,bsei asrl aeyd thsaptca e: , in 1 1] (a lregrcl oumsnpc ae) ° [ab ][ __ = 1 (nsoou ltitooan x b). ° ° = (abl reiacnld ouym snp ace) (axbh aass olu.t ion) =",linear programming and game theory
"27. coulmns pacre8 e. vebri ys c moab inaotfti hcoeon l nusm, asxi nbci es = = soblelv.a 1 1 1 12 1 20 °",linear programming and game theory
29. a 1 or1 1; a 2 (colounmn1 is1n .e ) = ° ° ° = 4 ° 0 1 0 0 11 3 60 2 3,linear programming and game theory
31. r contvaeicnwtstoi htwr soc moponetnhtdesoy-'n bte ltoron .g porlbmes te pa8g5e,linear programming and game theory
"2. 2, + + + + +",linear programming and game theory
"1. x = 1x, y z = 0c.h an1 gti oon(,g,x z =) c( -l,1 0,)d (i,-0 1,). y z y,",linear programming and game theory
"3. e chefloorunm = [�� � freve� a arlbl iexls, x 3,x 4;s pelcs ioialounts (10,,,0)0(,,00 1,0, ,)a n(d, 0- 3,01,)c on.is stwehnebtn 2b•c opmlete 2= i slout(i,0o ,n0b 0,p) l usc oabmnityni aoofsn p elsc oiilanoust. i ontso s elteedce xrecises u -3 -3 2 2 1 - v - v v v o nos loution!",linear programming and game theory
"5. + == ; w o 2 2 1,1 ,",linear programming and game theory
"7. 7a olwlsu ot.h celo umsnp aicaspe l na.e c == = v== w == 10 2 2 1 - x2x o xx24,· 02 0-2 1 +",linear programming and game theory
"9. ( ax) 4 ,ofran y row-rerd uced 2 == o == 0 0 . 0 0 -2 1 o a 3b 2 2 - - 1 0 o x4 xx24,· + (bc)om plselotuet xi on foarn y = b o - 2' 1 o o 11x 1 [1] 1 ][[] (11,) -",linear programming and game theory
11. 1x20hanslu lspalcitenh er oughb untos lout.ai noyn == [�] b hamsan yp artsicoiulolutnartoas x b. = p = 1 10 0 11 1 1 1- 11- 1 1. 01 0 (ar) 0 00 0 1 00 0 == 0 . .,linear programming and game theory
"13. r ,r -00 0r0 - . (b) 2 == 00 0 0 - ' - 0 0 r == 10 . . 0 00 0 r (c) 0 = [-�]",linear programming and game theory
15. a n uslplamcaet rix insb yn r-. n =,linear programming and game theory
17. th inki tstur h.ei s i -3 1 0 -2 -4- 5 0,linear programming and game theory
19. t hsep escoilauslat rtiehoc enlo umson f 10 and . n = n = 2 0 1 - 0 1,linear programming and game theory
"21. t hrep ivcooutlm sno fa f roma n byr s ubmaotfrr ainrxks, ot hmaatrt iax * m harsi ndeppeinvdroeostnwg, ti vainrnb gyr i nvleserut bimbaotrfai .(x ht ep ivot roswo fa* a nada rthee s ame, esliinmciein sa tdioonnse a moiern d etrh-ew e jsudtno 'stef ero tah ""*erf ecelo""u mson fz ertohasap tp eara .f)o r 1",linear programming and game theory
23. ( ut)vw(z)t u(tv)wtzh arsa nukn slset o. = v w =,linear programming and game theory
25. w ea rgei vaebn i whihcahrs a nnkt. h ernak n(ba) rakn()af ocres = < rakn()a n. =,linear programming and game theory
"27. r eaa ntdh sea mre e*b ,th ne b( e*e)a-.(1 otg ebt,r e dau crteo if == = == antdh iennv setrset p babc).k ibts o iv anrent miabtlrei xa ,wth ietmnhe seshy ar e thes maer .",linear programming and game theory
"29. s nicres tratwsti hri ndeprewonsdr,t es n ttaw rihtrt is n denpcteo nuldmsen( and [��] thzeensr .)so oits r edueccehdef rlomoi ns whreeii rsb yr. soluitontsos eelctede xrecises 439 12 2 1 1,00 0 0 x3, 31i.cf = r = hax s2, x f4 r ee. 00 0 0 10 2 2 1,0 1 00 x3x,4 icf =1= r = has free. 00 0 0 --1-2 2 --22 10 0 00 1) 1). spceilsa oilouintns = 0 1 (c= and 10 ( c=i n = n 0 00 1 0 1 -2 1,[� ] � 2,[� ] i icf= r = hax sl f r ;ei cef = r = o hax s2 f erer; == if 12,. . c =1= 2 []� 1)[ ] 2)2 0 � speacl.'soonilsnu ti (c or = 1 (c or bye mpty n = = n == n = x matr. l 12/ -3 0 -2 - 3 0 1 0 0 1 . 33x .o omple+tx2e ;xc o,mp1le+ /tx2e 2 0+ x4 - 2 = = 1 0 0 1 0 b22b 31-,b 3b1b4 3 �]3 + x �5 1 [ 2� 35(.a s)lov balief = and ot.h en = = b22 b3-1b3 b1b4 3 (nfoer ev arisa)b(.lb se)lo vaibfl e and + == o. == 2b-31 i b 5 1 b-- 32 b-11 x3 thexn= . + o 1 13",linear programming and game theory
37. bys sytehmaa slt e tawsfoter ev arbielas. a 1.,linear programming and game theory
"39. ( )ath ep raticsuollarux tpii alsow nas my ulptliibeyd (ba)n ys oluctnai on [;;[]�] u] [l� [ n bex.p (c) = theni sshtoe(rrlehn�g)thtan (d) the 0 xn ""homogesnoelouiuttnsih ""noe un l lissp acweh eani isn vleert.i b == 2,[ �] x xn; [o' l",linear programming and game theory
"4.1 m ultxipbpy l sya me is spieasclo luatslioio nncslth uecd ouelm sn p , -i [] i of x apnt dh sep elsc oilauatrineooc nths a nged. � 3 2.",linear programming and game theory
"4.3 f oar, gviersa nke very goitvrheaesnr fk o br, 6 vgerisa n1k, == ==",linear programming and game theory
"2. q 1, q q eveorhtye rg ivreasn k q",linear programming and game theory
"4.5 ( ar)<m ,alwsar<y no( br)= m,r<n.( cr)<m ,r==.n (dr)=m = n. 1 0 0 0 0 0 o . -1 00 1 1 1 01 xn",linear programming and game theory
"4.7 r == 0 and , o 2: nos olubteicoaonuf s e 0 0 0 == 0 0 0 o o 5",linear programming and game theory
3. row ontso s eelctede xrecises 1 1 49a. 0 bc na'etx issitnt cweeoq tuiaoinntsh ruenek nsoc wnan'htvae o ne 2 ; = o 3 souilto.n,linear programming and game theory
"51. a h arsa 4 nk 1 thceo pmlees toultitooan x0 ixs 10),. = 3; == == (23,, - 1 0 0 -2 r 0 1 0 with itnh feer ec omlnu. == -3 -2-,3 0 0 1 o 53(.a f)la se(.bt )ur .e (ct)ur e( onlcyou lmns.) t(urde)(n olyr wos). m n 1 1 1 11 1 0 1 100 1 1 o 0 0 1 11 1 0 0 1 0 1( rd1 o e'scton me o o 55u. anrd = == 000 1 1 1 0 0 0 1 1 for1mt hsui ). o o 0 0 0 0 0 0 0 0 00 0 0 o o",linear programming and game theory
"5.7 i fc loum1n coulmn5 t,h exsni saf erev abrli.iea tssp ecsioallui tsi on == (- 10,,0,0,1.)",linear programming and game theory
"59. c olum5in ss u trhoea nvoep ivsoitni ictase c mobinaotefia orcnllo iuenmrsa ,nx d 5 , ifser ew.it hfuo rp iviotnth oset hcelorun mst,hs ep escloiuatlii (s0o1,n, 0 ,1 ). 1, 5 the nucloltnasipanalmsclu ielp tloef(0s 1, ,1 0 ,,1 )( lai inrne ) . 10 0 -4",linear programming and game theory
6.1 a 0 1 0 == -3 . 0 1 o -2,linear programming and game theory
"6.3 t hsic ornuscttiiispmo sons i:bt lwepoi vcootln ustm,wf oer ev arisoa,bn ltlrheye e colnusm.",linear programming and game theory
6.5 a [��j =,linear programming and game theory
6.7 r miossl ti ktebole iy ;r i mso slti ktebole iyw ihft uorrtohow fz eor.s �,linear programming and game theory
"6.9 a nyze ror ows actfoetmrehe s rewo s: r [ 1 r [� �l == -2 -3], = r i. == prboelms te pgae 2a3, 98 1 1 1ci",linear programming and game theory
1.0 1 1c 2 == °g ivc3e == s c 2 == cl = o. tv bi +u v2- 4v3 + v4 = ° 0 0 1 c3 (depe.n dent),linear programming and game theory
"3. i af 0 thcenou lmn1 0;i d f thbecn(o mlnu1 )- a( cuomln 0;i f == == == ° 2)== f thaelncl o luemnndsz ei(rnao al rplee rpeunldtaio(0rc, 01,)a ,li lnt h xey == ° plamnuesb,ted epetn.)d en soluitontoss elcetdee xecsries 441 1 2 3 1 2 3",linear programming and game theory
"5. ( )a 3 1 2--* o -5- 7 2 31 o -5 -1 1 2 3 ivneret:=:}ii bnldepceonlduemnnts --* 0 -5 -7 (couludsd e hwras.ov) e 0 0 -81/5 1 2 -3 1 2 -3 1 o coluamdntdso0 (b)- 3 1 2 --* 0 7 -7' ,a 1 o , (couuslrewdo s .) 2 -3 1 0 0 0 1 o",linear programming and game theory
"7. t �syu'"" mi- vv 2 + v 3 = 0b eca(u 2w-s ew 3) (-w i- w3) + (w i- w 2) = o.",linear programming and game theory
"9. ( at)h feuo rv ectaortrehs ce o luman3 sb y4o m fa txr wiaih t laetao snfteer e vabrli,sea oa x o.( bd)e penid[�eiv n v]th arsa 0nok r1 . 2 = (co)v+ (c,o0 0,=) 0 h aasn onzseorlou( tktieaao nncy 0=f..) i 3 3 3 3 r . r. r. r . 11(.a l)i nien (bp)ln ae in( cp)l ainne (d) ofa ll 13a.l dli mseinsoar ne2 t.h reo swp caeosfa a nuda rtehs ema e. j �(v �(v�- �(v �(v 15v. w) w)a nwd w) -w).t htew poa isparstn h e = + + + == - v samsep athceeya. r eb aswihse na nda rwien pednedent. a 17i.ef l imipnraotediosuon conemr o rzee rroosw t,hr eo wsa rolefi rnaled yae pe;n dent 1 o 0 1 1 0 0 r 0 1 10 0 -11 0 froe xamippnlr eo b1l6em --* 0 01 1 0 0 1 1 0 10 1 0 0 1 1 1 1 0 0 0 -11 0 . 0 0 1 1 0 0 0 0",linear programming and game theory
"1.9 t hnei ndepveencdtesonprtaass n p aocfde ie mnsnito.hn e ayra eb sasif rot hat spcaei.tf h aerye -c otlhueom fant sh mein ns t ol etshnsa(nm n). > 21c.( ):ua nbya sferor s 2 ; n (u:() ro1aw n rdo 2wo) r( ro1aw n rdo 1w ro2w.) + ""-",linear programming and game theory
23. i nedpendent rcaonnlkc.u om lnussm pnr asnm ram.c olumsn abraes is =} =} nk for rm :=:} rankm n=. =,linear programming and game theory
25. ( at)h oen sloyul tiiosn 0 b ecsatehu ceo lnasuer m piennedd.ne (tba)x b x = 5 = isslo vabbelceat huceso e lsupamnrn. s,linear programming and game theory
"2.7 c oulmn1as n 2da rbea sferos t( hdeeir ffecnoutlm)sn p aocfea san u d;r o w1as n d 2a rbea sfreot sh( ee qruoaswlpc )ae (s,1-; 1,1 i)asb asfroith se( eqnuuallcl)es sp.a",linear programming and game theory
29. r a(nak)2 i f 0a ndd 2r;a nke2 re )x cwehpetn do rc= - d. = c == = = c =,linear programming and game theory
"3.1 l evt= (1,0 0,0, ,). ,. v. (,00 0,1, b) eth ec oordtievn eacr.tsi of itsh e i 4 w = litnheur go(h 213,,4,,)n onoeft hve'a sri en w.",linear programming and game theory
"3.3 ( ai)if wt e rneoa tb assw,ie c uoladd mdo rien depevneodcr,etws nhtiw cohu ld exctehegedvi edni mseinon( bi)if wt e rneoa tb a,sw iecs oudledl ete some k. vectorsl,est s hltaehnage vvi eidnni gms einon k. ns seelcteedxr ecsies to 5 35(.a f)la smei,gb hent os oilount(.b t)ur e7,v ectrorasrd eei pne ndent. 1 0 0 0 0 0 00 0 0 1 0 00 1",linear programming and game theory
"37. ( a)0 0 0, 0 1 0, 0 0 0. (ba)d d1 0 ,0 0 0 0, 0 0 0 0 00 0 0 1 0 0 10 0 0 0 0 0 0 1 0 00 1 0 0 0 0 0 1 (c)- 10 0, 0 0 00 10 araeb asfrio sa ll . , 0 1 0 0 0 0 1 0 0 -100 - a _at. == 39 y.( 0)0r equai+ rb e+ sc oo.n bea sicisos xs c o2sxa n cdo xs c o3s.x == - - ==",linear programming and game theory
"4.1 y l()x, 2 (y)xy, 3( )xc naebx2,,x (3dxi1o)m xr 2 ,,x 2 ( xd2iom) r ,xx2 ,x3 ( d3i.m) .., 1 1 1 1 1",linear programming and game theory
"4.3 1 1 1 1 1 == + + 1 1 1 1 1 1 - 1 1 chket ch(e1 e,n tt1rhy)(e3 ,,n 2 ,)te hn(, 33 ,)t h(en21 t),so h otwht aohtsfi ev pes' arien depefnoducenrond itio.ton ns theeni tnemrisan keer oaswlu lma snc dou lmn sumesql ur:ao swu 1m roswu 2m roswu 3m coulmn 1su cmlo umsnu m == == == == 2 colum3ni assu utmo mbcaeutasisecu omfa l rlos w suomfa lcllo unms.) (== - ==",linear programming and game theory
"4.5 i tfh 5eb ym atr[ixab i]is n vileb,erb it nso tcm oab inaottfih coeon l uomfan .s 5 , [ab i]ss i nagrau,nla dh aisn depceonldue,bmnn i stas c mobinaotftio hosne if clounm.s prbolmes te pgae",linear programming and game theory
2. lt 110,linear programming and game theory
"1. f lasweeo, n klnyo dwi mseoinnasre eq u.la etlnf ullhsapssam caeld liemr == m - r.",linear programming and game theory
"3. c (:a )2(,,01 ,1(),01 0,;) (na:n) 2,( 2-,11 0,,,) - 1(0,0,1,;) r == - r == c(t)a: 2(,,21 0,1,)(,0,1 1 ,0, ;) (na)t: r== 1 (-,1 0,1,;) r == m - c():u( 010,,,)( ,01 0,;) n()u(:,2- 1 ,0) (,11-, ,0,00;), c(tu:)( ,210,1,)(,0,1 1 ,0, ;) (na):(t ,001,.)",linear programming and game theory
"5. a t imeevsec rouylm on fbi zsre ,os oc (ibcs)no taiintneh ndeu llns(paa.c) e",linear programming and game theory
"7. f roamx 0th,er oswp aacnthede n ullmsupsatco ebr etho g.so eenc ahlap3 t. e r == 1 24",linear programming and game theory
9. [ 12 4;]2 4 8 hatshs ema en uslpclae. 3 6 12,linear programming and game theory
"11. a x 0h aasn onzseloruot tihoern<n ,n ancd( ta)i ssm altlhreanrn.s o if == a yt inso stlo vafbrosl eoe m examepa:l [11 a ] n fd (,12 .) == f f. == == 13d. b/e;at hoen pliyv ioast . == n witihn depcelonudmsern:na tn kn ull�parcoesw p aicse ; r ilnervftse e. 'i�5. == {oj; n; 17a. [11 0;]b [00 1]. == == sotliuontsos elcetdee xrecises 443",linear programming and game theory
"1.9 n o-fore xam,pa llielvn ertibbyl em atreishc avee s tamhef ours usbpcaes. n n 1 ° 1 ° 1 1 3=j:.. [11 ]. 21(.a ) . (ibmp)o sbslied:i mseinnos (c) + ° 1 -]3 (d) (ei)m psosbiler:o ws pacec omlnu sapcre equires m [3 -l' 9 == == n. then m r - == n - r . 23i.vn erta ibr: lo ews pcaeb asisc olusmpnca eb asis( 100,,,)( ,01, )0(,,00 1,) ; == == b: (10,0, 1, 0, 0,,) nuslplcaea ndl enfutl lcsebp aasareese mpty.r ows apcbea sis (,0 00;1,, 0 ,) (,00 1,0, 0, 1, ) (,01 0,,)( ,01 0,,)( ,00, 1 ;) and ;c omlnus pcaeb assi ""'1, (-,010,,,01,)0(,,0- ,1,001,0,) , (,0,-0 ,1,0,01;) nullcsebp asais and left nulplasces ibissae mpty. . . // � 25(.)a samreo ws pcaea ndn uslplcaet.h erefroarn(ekd imseinoonfr ows pcaei)s tehs ame(.b s)am ec olusmapnc aen dl enfutsl plca es.a mrea n(kd imseinoonf colnus mpcea.) <",linear programming and game theory
"27. ( )an os loutimeoann s thaatl ways canc'otpm arean d r < m. r m n. n. 0, a (bi)f thneu llsopfa cceo ntaani onnsz evreoc tor. m r > - 1.:",linear programming and game theory
"2.9 r ows apcbea si( s1 23,,4,)(,,01 2,;( ,001,2,;) n ullcsebp asai( s,0 1 , 3), -2,1 ); (,100,,)(,.01, 0,)( 001,,) ; colusmapnc beas is letnf uslplcaeh as ebmapitssy. av° a 0. ° 31i.f andi sar owo f then only isin b ohts paces. == v v . v == v == 33r.o w3 2 (2 r) owr ow1 zerroo ,ws ot hvee cte or(,- s l 21, a) r ient hlefe t == - + nullcse.tp haes maev ecthoarpsp teobn e i nt hneu llspace.",linear programming and game theory
3.5 ( a) andw spnac (a.) (b)v a nds pnac (ta) (. cr) a nk2 i fu a nd ware u z < + v utv t depnedeonrt anda rdee pen.d e(ndtt)h er anokf w is,linear programming and game theory
2. z z,linear programming and game theory
37. ( at)ru e( samera kn).( bf)a l( sea [10) ]. (c f)al s( e a ca nb ei nviebrlte == andas lou nsymmertic)(.dt )r eu.,linear programming and game theory
"39. a 111a ,21 0a ,13 1a ,2 2 0 a ,23 1a ,13 0a ,23 1a ,3 3 0, a2 1 1 == == == == == == == == == (noutn iq.u e) 41r.a nk nm eannsu slaplce zervoe ctaonrxd n 0. r == == == paeg problseem2t . 5n 122 1 -1 ° 1",linear programming and game theory
1. a ° 1 -1 n(a)c ontamiunlst iopfl 1 es n (ta) c ontamiunlst iples == ; ; 1 ° -1 1 1 1 . of -1,linear programming and game theory
"3. th ee nrtieisne acrho wa ddt oz er.to heroerfea,n yc obmintiaowni lhla vthea t smaep ropty:e r1 12 3 0a;ty yly 3 y y2 12, 1 1 == == 1 == 11, 1+ == -2y- y3 3 + 1 + 1 2 3 0. =} + - == 1 1 + + 1 == itm eantsh attht eo tcaulr reenntte rfoirmng =} outsiiszde er. o st os elected exercises",linear programming and game theory
"5. ]hap si vcl ot c3s, + c1c3c 1c2+ c 2c3 + c1+ c 3",linear programming and game theory
"7. c ondiotnb ia orhneis b -4b 5 oh�3- b4 +b 6 0 b ,2 - bs + h6 o. + == == == 3 -1- 1 -1 c1 c2 c5 - c1 - 2c -c5 + + - - -1 3 - 1- 1 1c c1+ c 3 c4 -c3 c4 +",linear programming and game theory
"9. , - - -1- 1 3 -1 c2 c3 c2 c3 + c 6 - c6 - - +- -1- 1- 1 3 cs c4 c6 c+4c 5c 6 + thsoce' s th acto nnteonc otd wei alplep airrn o w j j. 1 0 0- 1 1 0 )'1 0 0 1 0 2 0 0- 1 0 1 y2 0 -4 73 1 5 0 0 2 0 0 1 0 y3 0 3 4 3",linear programming and game theory
"11. 0 00 1 0 0 - 1 y4 0 . ,x - 14 , 10 -1- 1 0 0 00 0 xl 1 - 3. - )'- 3 . 1 14 1 0 1 0 00 0x 2 2 0 1 3 0 1 0 - 1 0 0 0x 3 13 6! 13t.h earr2ee0 c hoiocf3ee dsg oeuostf6 b eca""u6cs hoeos 3e"" 2.0f our = 3!= 3! choigcievasen getlslr,evi ain1g6s pantnrisen.eg",linear programming and game theory
15. t hiinitaks l rebaudii.yln t i,linear programming and game theory
1.7 9 n ode1se2 d g+e4 sl opos 17;n o d-e1se2 d g+e sl6 o op1s. == - =,linear programming and game theory
"1.9 x (=,11 1,1, g) i sv e= 0t;h etna x= a 0 r;a inaskg ian- 1. ax n 0 1 1 1 1 0 1 1 m=",linear programming and game theory
"2.1 and 1 1 0 1 1 1 01 2 3 2 2 2 (m)i=j an a1j+ . ainanj + . · 2 2 3 2 2 anwde g ea ti ka1wk hj eth ne re m = . == 2 2 3 2 ias2 s-etpp tahi t ot o k j. 2 2 23 not3ipc taehf sor ma noidtes etlof . prbolestme pag13e3 2�6, [� [�l �",linear programming and game theory
1. r otati-o� ln,linear programming and game theory
3. i ia x == 1a lwspa ryodaunec leslsei .p 112,linear programming and game theory
"5. t heayrtr ea snofrmteo(d 31),(,2, )6, ( 1--,)3 t.h xea- xsti rnus;v ecrlatl iin shiufpt/ dbouwstnt v aeyr .t ical sotliunost os eelctede xecrsies 445 o 20 0 sceond 0 0 06",linear programming and game theory
"7. d eartieiv v then ullsipssa pcaenb nye( d01 0,,0, )a nd o 0 o·0 matrix o 0 0 0 (010,,0, ,)w higcihvl eisrnp elsa.c eondde rievsoa flt iirnvf enuactiaorzene sr o. thcelo umsnp aicasec cildlteyhns etma aea s� en ulclesb,peu acsasece nodd eriva­ tvieosfc ubairclesi r n.e a",linear programming and game theory
"9. et a dn tea r-aeb asfiotsrhs eo luotfui ""on s == u. jt [c -so i]n[cs os - isn] [ 1 ]0 2",linear programming and game theory
"1.1 e, e e e _ soh -i . s. i n cos sin, cos 0 1 _ h - tj e ' e e",linear programming and game theory
"1.3 ( ay)es .( by)es w.e d o'nnte epda rheens(tebasc) o ra b(cf)o arb c! 1 0 0 0 2 adn a te1;l doturnbaslpeoo fas em agtirviexs o 0 1 0== i; 15 a.== thmea tirtisxe lf. 0 1 0 0 a23 noet 1b eucsatern aspoofmsa et 2r imixas t 3ri.x 0 0 01 == 0 0 0 0 000 o 1 00 1 o 0 1 00 o 01 o . 17 a. ;b== o 0 ;1ab 0 ba 0 1 o. == 0 1 0 == o 0 '1 == 0 o o� 0 o 1 ° i 0 01 o 0 0 1 1 1 t- t- 19(.a )i nivsle werit tihb l3/(ci)is n vbelwreit tih == y- 1.1 (y)y ; (y) � w t(+v t(v) 21w.it h 0l,i rtiiegtaiysv e 0) + t ( .0t )hutso) ow.it eh ( == == == == -1, -t(o).t ( t(o)t.( o) lirniegtaviyet s-( o) certaion)l y thsu = o. - == == s(t(sv(»v v).",linear programming and game theory
"23. == == t(2vt)( v t(vw )t (vt)( w). 25 (. b ) anldi an(r(ec,af )i)as l are 2 ),( df)ia ls+ = + 3 == 27t .( t»( (vv,3v iv2),t; ( v)v ;ti oot(t(v9)(v9 » t(.v ) = == == == t(i,o) t(",linear programming and game theory
"29. ( )a o.( b()00 1,,i)s inntoh ten g.re (ac)o ,1 )o . == ==",linear programming and game theory
3.1 a sosctieialv wag ivaem(si ami+ am d.i isbtrultwai voeev 'egsri ves + m2) 2 == ae(m)e a(m). == [�� ][� �l. 33 n. o m atrai xgv ieas = top reosfrssto:h mea trixh assp ace diemns4i.lo ninae rta rsnofrmoantosint hsapta mcuesc to mfeor m4 b y4 m a­ tric(e61sp r aamres.tt) ehsoem ulptltiiicoabnysa inp robl3ea1mn s3d 2we re speltrc aisnafoiromswna itth4 p oranalmeyer. ts [�� ] [�� ],linear programming and game theory
3.5 t (=i)obtu m = = t(m);tshfieeltlhreaenm.g= itnhkeem.e l [ 37 (. )a m = ; : (lbn) = [� � r(lc=.)h acd. 39r.e orbdsaeibrsyp e rtmauntm iator;ci hxalnegthnesbg ypo siivdteig naolam atr.i x a a2a 4 i 2,linear programming and game theory
"41.1 b b b 5 ;van derdmedoenter=m (i-bnaa()ent-a)e(-b;) the == l e 2 e c 6 poianb,t,sm u sbted ifefreannthtde, n det=f.e0 r( miinntatenirpitoops nlo absls.ei ) e 43 i.t f i s innovtle rett,hi t e( bn )v , .i ., (v nt ) wnioblteal b astihsew.nec uoldn't . cohose (i )v a so uptubta ss.i == wi t 45s . (t( (v -1) 2,) )b us t(v ) ( -21,a) n t d(s v()) ( 1-,2)s .ot ss t. -=i == == == 47t.h hea dmaarmda txri hha osrh togloc noaluomfln esn 2gs.t oth h ien sveeorf h ihst / 4 h/4. ==",linear programming and game theory
"4.9 f las:the en n onzveercowt ooruhsla dvt eob ei ndependent. prblones lte3 � page 1, 148",linear programming and game theory
1. i xii i -/21 ;i yii ==i 3 -j2 ;x t y 0. == ==,linear programming and game theory
"3. ( 2/xx)iy 2(/ iy) == - 1m eatnhsxa yt+ 2xy 20 s,ox yt 0. ii == ==",linear programming and game theory
"5. anda roerh togloa,nsl ao and vi v3 v2 v3.",linear programming and game theory
"7. x ==-(21,0 ,;)y==(-1,- 11;,)t rhoew==z(1,,21 i)os rh tonglao tthnoeu lcles.p a",linear programming and game theory
"9. thoerh togocnmoaplle mietsnh tle i tnhoeru g(h-1 -,1, 1 a)n d0 0,()0., 11 i. af t == 0t,h yetbn yta x (tya )x0 w,h iccnohtd riacytbt s0 . == == == =j:. y 13t.h fieg eus rplainytyi s n irninct loou msnpc aep ar+ tl fent ullpsrapta.c e 15 n. o s umcahti rxb,eu csa(e12 ,,lt) ( 1 -2, ,1 i=)0 . 17t.h meat riwxti ht hbesa ifsov ra s rwiostts.h etnhn eu llisvsp..l a== c we. 3",linear programming and game theory
"1.9 ( )ai v f a nwd a rlei nier ns, v ..la nwd..l a rienr tseecptlniae.ns (gb v). ;] � 21 (. ,1 ,21-)ipserpendtiocapu=.l [ ar� h ans(a=)p ;=b[l 2- 1] has srpoawcp e. == ; ; 23 a. [] ha ssu bcsepsaf uorl in(e11s,);o r tgholon a(t-1o1 ,) (1, 2, ) = = orhtoglo n(ta-o,21 .)a lwaryosswp acneu lcles.p a .1 1 2 -3 2 1 1",linear programming and game theory
"2.5 ( a)2 -3 1 .(b )- 3i nso otrh toglo nta1o. (c )1 cia(n) -3 5 -2 5 1 1 1 [ ] 1 -1 and inn ( at)i si mposbsil:ne optre penadri(.c dua)l has 0 1 -1 o 2 a o.( (e1) ,1w )i bleil n ntuhlel asnprdao cswep ancose u;mc ahit .xr == 1, 27 (. )a i afx bh aass loutaidno tnya 0t,h betyn (a)txy 0. == == == == (bb )i s intnoh ctelo umsnp ascone,o;p t e rpendtiaocl yuil ltn arh l eetn f ulplscae. 29 x.== x r + xn,w hex rir eis tn h reo swp aacnx edn i istn h neu lclest.ph aeanxn == ° x+r anadx a axn ax.ar lvle ctaoxar rsce o bmitnoianosft hceo lumns == == x ofa .ix f (== 10,,)t hern (/121,/ 2.) == 31 (.)a � oar syrmcimm aettxrth,ie c loumsnp aacdnre o swp aarcete h sea me. (bx)i sin t hneu slpclae ainsid nt hcelo umsnp acreo wa csesp:ot hese == z ""eeingvoerchsta""vx e t o . == z 33 x. s pliinx ttr s+ o n ==x (,1- )1+ ( 11, ==) ( 20,.) [_ �] 35 a. x bxm eathnas[t ab ] ot.h rheoem ogeenqeuoauitsnfi uo orn s = = unokwnnasl waysn ohnazvseeor loau. h t eiroen (x,31 a)dn x( ,10,) and == == 3 ax== bx (,565,)ii snb otclhou msnpc ae.ts wop lnaeisnr (thrzoeurgoh) == musitn teecirtnasl ianltee ast! 37 a.ty == 0g iv(exas) t == xyta ty== ot.h eyn axa dnn (atc)a( ). 1- 1- [ : ;",linear programming and game theory
"39. ; ;j subspace s-lst hneullosfp aac e theorres-lefi s a eveisnf niost . = 4 4 41i.vf i aslo'lf r ,t hve..l nc ontoanitlnhyz se e vreoc . tt ho e(nr..l v) ..lr v. == ==",linear programming and game theory
"4.3 ( 11,1,1 , i) as b sasif ro ..l.p a[ 11== 11h] a tsh e palsia tnnsue l lspace. p 1 -",linear programming and game theory
"4.5 c olum1on fa iosrh toglot ntoa hsepc aes panbnyte h2den .d,.,n h.t row.s of a 2 2 -1 47a. -1 2 2 , == 2 - 1 2 ata 91id s iag (o atan )ja i l (c: uo mlino fa ). ( cmonl u == == j).",linear programming and game theory
"49. ()a( 1-,1,0 )i s biohntp lnae.ns orlmv aectaorprees rp enrd,pi lcausnlteaisl l trhee inteecrt(s!)b n eed orhtoglo'nyaetcost pnoa rtswh heoo lrheto gloc noam­ plemienn t( cl)i ncenams e ewti thboeuiotnr gtg hoalon. r5.",linear programming and game theory
"51. w heanb 0t,h ceo lumno fbsi �csao cnet iatnih nneeu dl losfap .tah ceer efore == thdei mseinoocfn (b )< dimseinoonfn (a ).t hsmi enarsa kn(b< )4 r-a(nak.) porlbnes letp aeg",linear programming and game theory
"3.2 , 151 .",linear programming and game theory
1. () a( x+ y )2 >/ fy (raithmmeenat > igce omemteraoinfxc a dny ). 2 2 2 2 (bil)x+ yl<1 (1xli+llyil)1menastha(t+xy)(xt+)y< ixii+121xlilylll+llyil.1 t t t t thleet h-fansdii dsex + x 2 xy + yy a.f tcenarcl elgit hnsii xs y < ilxi yii .ii i,linear programming and game theory
"3. p (1/03,/ ,311 /003;() /5,91 /09,1 /09.) == , 1 . . 1",linear programming and game theory
"5. ce=o1s/,j""nso e==acrcso(/-fo1);p == [/ln li]n alle nets-r. i == n 1",linear programming and game theory
"7. c hoob s== e ( 1.,,.1 .)e ;ql uiaita f1y . .a (n t ha ie pnsra altlb o) e . l == == . ataaata (taa)ta ata",linear programming and game theory
9. p 2 == == p. ataa(taat a)== at(a==a ta) 1 [] [ ] 10 to io-t o [ 1 0],linear programming and game theory
1.1 ( )a p p 1 p-i== (bp)+ p ; == 2== 1 2== l 1 02. 10 ; _2. 10. l 10' 0 1 pip = [�]� th. seu om ft hperj coetioonnttswo po e rpendliicngueilsvta ehrse 2 vecitto.srt e hlpefr jeoctoinotno oannetd h aelp nirpe neen dirlc iugnlieav tehse zevreocr t. o a 1a 1 ana n at a . .,linear programming and game theory
1.3 t arce + 1. == at+ a . at==a a ta== ns steol teecdex reicses,linear programming and game theory
"1.5 i ilal2 x== (ax)t(axx)a taxia,it lx2 l (atx)tx()==a xtaatxi.af t a== == == aat ,t hieainx i ==i 1a\tx i i(.t hmeastcere aisrc ela lneodr mal.) e 17(.)a a tba/ta == 5/ p3 ==; ( /535,/533/,;) == (-21//331,,3/)ha etsa 0. == ()ba t ba/ta - 1;p (13,1,)b nade == (00,0,.) == == == 1 1 1 5 1 3 1 1 1 1",linear programming and game theory
"19. ip 1 1== p12 a np d1b 5 . p 3 9 an3d == - i == 2== - 3 3 1 1 1 1 1 5 1 3 1 1 pb 3 2 == 1 1 -2 -2 4 4 -2 1 1 21p. i == - -2 4 ,4 p2 == 4 4 -2. 9 9 -2 4 4 -2 -2 1",linear programming and game theory
"23. p3 p i p 2+ + 1 -2 -2 4 4- 2 4 -2 4 1 1 1 - -2 4 4 4 4- 2 -2 1 -2 i. - - 9 + -9 + -9 == -2 4 4 -2 -2 1 4 -2 4 25 s. ni cae i isn vlerept, ==i ab(ata )-alt== aa-1(aatt) -i1 p:r jeocotn talol == 2 orf. probsle3et.m3p , ga e 107",linear programming and game theory
"1. x 2e;2 (103 x2 )+ (54 x2 )i msi niizm;e( ,d4- 3t)3(4,) 0. == == - - == 2 1 3 - 1. 3 • x [ l 'p �3 lb ' - p - '- � i pse rpentdboit ochcu ollnausrm. 3 - -- 2 2 3 3",linear programming and game theory
"5. == b4 5,9,a tt - 10,1,th;e b elsitn e (i/5)s2 t p 6 ==; ( /726,1,7/)2. == + ° 1 12/",linear programming and game theory
7. i p==a(ata)-at== 12/ 12/ - 1/2 . ° - 1/2 1,linear programming and game theory
9. () a tp == (ppt) t== p.t hepn ptp == p2 . (b)p rjepoc tst hosenp taoc e == {oj. z ==,linear programming and game theory
"1.1 p + q ip, q== 0t,rn pasostoeq p 0, (p s- oq) (p-q ) p °-- ° + == == == q i. = 13b.es lti 6n/1e53 (-36)/t3;5( 3135/9,355/6,313/,5- 1/315f)r ocm dt. p == +",linear programming and game theory
"15. h(2 i-2 p)2i 4-p 4p2i 4p 4p it.w or eflecgtiiivo.en s == == == + == - + 2l 17p.rj eotcioonnxt o 0 prjeoctoinot(-no1 1,) [� :i� �� + y = = = 2",linear programming and game theory
19. p rojemcarttioixno rtnoo sw p caew oubleadt ( aat)a- ilf rtohwwese ridene pen­ dnet. soltuoinst os elteedce xrecises 449 m 1 2 c o o . x d;b== . 1' == -3 e 2 4 -5,linear programming and game theory
"27. ( aa)ta m,a tb bi+ . . bmt. herex fi otsrh eme e aonft h be' s(.bt )h e == == . + 2 2 vanrciiea ls ie l l== �;t( bi_ x). (c p) (,333,)e, ( -2-,1 3,,)p te o. i == == == 1 11 1 p 1 11 . == - 3 1 11 29(.x x)(-xx )t (at a )-i a t[(-ba x)(-ba x)t]aa()a-i .tf oirn depen­ - == denetro rr,s ssitutbu(tbi- naxg() b . -a'x t ) == (j2 givtehcseo vareim aan�c i tri(xaa t) -alt (j2 a (aat) -i .t hissip mlfiietso(j 2 (aa)t- ni:et fa romuflotarh e corviaamnacrtei. x 9 1 1",linear programming and game theory
"31. __ +. 1b 0l o + 10x9 == 1( 0b l . . + blo), 1 0 0 1 8 ;",linear programming and game theory
33. � [ c' hafinghgt ei e s°t do p x [! slo] va e x s p. ; � ]8. ' 0 == 1 ; = = = 1 4 20· 17 1 0 0 0 c 1 1 1 8,linear programming and game theory
35. clsoepsatbr oala: d . 1 3 8 9 e 1 4 16 20 -- 4 8 26 c 36 -- ata x 8 26 92 d 121 == . -- 269 23 38 400 e -- --,linear programming and game theory
"37. ( )at hbee lsitn e 1i 4s,t wghiotcehhsr otuhcgeeh n ptoei(r,tnb t) (29,.) x == == + (bf)r otmhfi er esqttui ao: en m d � ti == � bi d• i vbiy md tego e ct d-- t == b-- . + + b+ . . wi w�bm 39 x-- w_ i . + - • 2 . w w� i+ . . +",linear programming and game theory
"41. x w (12/,1/4)7;a xw (1/121,3 2/12,/521,) == == axw (axw)wt-wa(xbw ) b (-12/,1/82,-1 4/12), o. - == == problseem3t . p4a,g 1e58",linear programming and game theory
"1. ( a-)4 == c - 2d-,3 == c - d,- 1 == c + d,0 == c + 2d . (bb)es lti ne +t -2 gotehsr ou4gp ho i;aen 2lt lso .( cb)i st hicenou lmn csep.a == iontoss elcetede xreicses b",linear programming and game theory
"3. p rjeoctoinao3: (n 2-/,13 3/-,2 /;t3 h)seu ims i tsneolttfih;caa1 a et t ' 2aa i, 3aa j arperj coeotniso ntthor eheo golodr nitarieo.cnt tshesiurim ps r jeocotnoi ntthoe whoslapeca en sdh obuethl edi dnetity. 1 1 1 1 2 2 2 2 i i 1 1 2 2 2 2 1 1 1 '1 2 2 2 2 1 1 1 \\ 21 2 22 \ \ 7\\. x (ql+ . +.x .qnn) t( lqxl+ .. .x qnn)== x f+ . . x; ib li2 l btb i . == == + + =} xr. .x;,, . + +",linear programming and game theory
"9. t hceo bmitnoianco lstet soq 3 i o sl q o 2q. + 11q.i usp pternrig aucloalru:1hm anq sl 1 == ±1b;yo rhtogaloinctoyml nu2 m usbte . (,0± 10,.,. )b;yo rhtogloyinc taolu3im (sn0 0,±,1 ..,) .a;n sdoo n. 0 1 0 0 1 1 1 1 ,0""",linear programming and game theory
"1. 1 1 1 1 00 0 01 13/ 2/3 -2/3 15q.l== 2/,3q 2 == 13/, q 3== 23/ iistn h leetn f lulcs;ep a -2/3 2/3 13/ ...-[q-.b..!]..[ 1] . x q==ib = 2 9 17r. = xqtb g iv[ es�� ] = [[5 ax nx d6 = ] [ 3] 5]-6 19c.* - (iqcq* 2i) c s- (qq!-lc( )qc qi) 2b ceasuqe iq l== o. 8- x oxo . byo rhtoglointtayhc,eol essftnu cotniasr0 es i2n = 0a n0d == + 2j3 a. ==o 12/a,l == 0b ,i == 2/.n 2",linear programming and game theory
"25. (,xx ) thcel eoslsti ne== 1i3/s( horiszionnctea== l 0 .) y",linear programming and game theory
"27. ( /1-j2,- 1-j2/,0 0,,) ( /1-0),1-0)/,/2-0) ,)0, (-12/v'3,- 1/2v'3,1 2/v'3,- 1/v'3). 29 a. == a== (1-,10 ,, ; b== 0 b )- p== (,��, - 10) , ;c == - - == (��,,, - 1�) c a b p p /.4 nottihcpeeta teirntn h soeor htoglov neactao,br ,s n ex(t11,,1, 1 , ) c. 2 31(.)a t reu.( bt)re uq . x== xlql+ x 2q 2.iqixi =ix +rx ibceuasq e tq 2== 0. proebmsl te pgae 305, 196 4 0 0 0 16 0 0 0",linear programming and game theory
"1. f2 == 0 0 0 4 , f4 == 0 16 0 ==0 42[. 0 04 0 0 0 16 0 0 4 0 0 0 0 0 16",linear programming and game theory
3. t hseu btmraiif sx3•,linear programming and game theory
"5. ex ==i - 1f ox ==r (2kl ),en i==io f ro e == 2k+ nn / 2k, i inset re.g + sotloiuntso s elceteedx reicses 45",linear programming and game theory
"7. c (10,1,0 ,.) ==",linear programming and game theory
"9. ( a) f t im(e10s0,, ,)0 cloumn ozff e r(o,11 1,1, .) == y == == (bc) (11,1,1 ,) /4 . == 1 1 2 2 0 ceven1 y ' 0 0 11 c. == == == == 1 -+ c od==d 0 -+ yii 0== -+ y 2 . 'r 0 0 0 0 ..'..",linear programming and game theory
"1.3 c o == (fa+ f l+ f 2 f3)/4, c l( fa- == if f2+ if3)/4,c2 == (fa- fl+ f 2 - 1)/4 + 1- 3 , c3 == (fa+ if f2- if3)/ 4;f o dmde,afna ==s 0 f,2 == 0f,3 == -fl,t hecno == 0, 1- i c2 == 0c,3 == -c,is oci assl oo d.!d 1 1 1 1 1 '2 1 1 1 1 1 1 1 1 15f. - l - f. == - - == h 1 2 1 1 2 1 - 1 4 -2 1 1 . . l -l l 1 1 1 1 e2tci/6 e2/tc3i e4tc/3i 17 d. == anfd3== 1 . e4tc /3i e 2i/tc3 1 0 1 0 2 3 3 19 a. == daig(i,li ,,i );p, == 0 o l adnp t l aedt a o -1 == o. 1 00 21 e. i egnalvuee so 2 -1 - 10 , 2 --ii 3 2,e 2 2 -(-1)( -)1 4, == == == == == == el - e 3 2 i-3- i9 chetcrka0 +c 2e+ 4+ 2 8. == == 2. == 23t.h feo cuorm poanre( enc + t o s2)c (+c +i 3)ct;h ( enc - o c2) i(ic -3)ct;h en + (oc c)2 (-c+i 3 )ct;h ( eo nc c)2 i-c(i- c3)t'h essteea prtsehf ef t! - + porlbesme tp aeg 4�2u 206 4 1 2 �",linear programming and game theory
1. d e(ta2 ) 8a dnedt- a)( (1)deat andde(ta) and(da-e1 t) 2 . == == == == == - 3'.t hreo owp eorsnal teviaed eat u nchanrgueld5e .t bh yem nu ltiapr loywi ng by 1( ru3l)gie v etshr eo ewx cghera un:ld eebt == -deat. -,linear programming and game theory
"5. fotrhfi er msatt ,tr owir xoewx cgheawsni plrlo dtuhiceede n tmiartti.ytx hseeo cnd matnreiextd hsrr eoeewx cnhgaterose ai.c h",linear programming and game theory
7. d eat o( isgnul;ad reu)t 1;6d eutt 1;6d eut-11 /1;6d emt 16 == == == == == (e2 xcnhgsae.),linear programming and game theory
9. t hnewe d eterim(si n1ma fn()tda bc.) - - n n n,linear programming and game theory
"1.1 qi q q) q iifd eti s tnhodeten t 1 == (detw oubllwdou p aoporpca rhz e.rb out remaainon rsot oghnmaalit .xrs od eq tm usbte1o r- 1 . 3 13(.a r)u l3e( ftaocr-in1fgr oema crho wg)i vdee(sktt ) (- 1)dek t.t h en == -det dektt dektg ivdeekst o. == == - k iontsos elteecedx recises o 0 0 1 o 0 1 0 (b) hadse == t1 . - 10 0 o - 1 0 0 0 15a.d dienvgec royln u motfto h afie r cslotu mmna ksie at zceorlous mdone a,t o. == iefv erroyow fa a dtdos1th ,e env erroyow fa ia dtd0osa ndde (ta i) o. - - == [!!l - butd eatn eendob te1 a: hadse( ta 0b,ud tea t 0 1 . = j) 1= == == 17 d. e (ta )1 ,0d e(t-a1 ) / 0' d e(ta- a a2 - a7 100 f roa 5a dn j) == == == + == ==",linear programming and game theory
2. a == n,linear programming and game theory
"1.9 tkaindge termgiinv(aednset (tsd et (-i)(dde)(t d etf ore vetnh e c) d) c). n == n reasfoianl(isbn egsc ( ea i- u) 1a)n tdh ceo ncliuwssr niogo.n == + d -b ad-b c 1 ad-b ca d-b c 21 d. e (t-a)1 det == -c a (da- cb2) ad- cb' ad-b c -bacd 23d.e etrmiannt3 6a ndde tenrmti 5na. == == -,� 25 d. e (lt) 1d,e( tu) -,6d e(ta)- 6d,e (tu-1 l-1) and == == == == de(tu 1 l-- 1 a ) 1. ==",linear programming and game theory
"27. r ow3- ro2w ro2w ro1ws oai ssni gular. == - 29a. i rse ncgtuarsl oda e(ta t=j:. (a d)ae tt() d ae):tt hseear neod te fined. 31 t. h hei lbdeerttne arnmatirs1e8 , 1-02 ,.461 -04 ,1 6. 1-0,7.371 -012 , x x x x",linear programming and game theory
"54. 1-01 84,.81- 025 ,2 .71- 03 39,. 71- 04 ,3 .221 -053 .p ivotrsao tsai re x x x x x ofd etiennrtam,ss oth tee nptihvi ost r 1n -e053 /a01-4 13-01 ° :v esrmyl a.l == 33 t. h eg eldsaetrt ermoif0n -am1natt crseif sron 12,., ,. a r. 1e1 , 2, 3, 5, 9 ,3, 2 , == 5,61 443,20o,nt hwee ba tw w.wmhawrto.lwdoralfm.icahdomamardsimmauxm dteemrintrpaonbelmhtm.la nalds oin t h""eo ni-nleen cyclopiendtieag ero f sqeueen""swc:w .rwseechaa.trtc.owmi.t- hisa nid,st hlega re4sb ty4 d eterminant (sehe admaaridin n die1sx. 6) 35 d. e t m) 1 a b c ds.bu trac4tf o rmrr ooww1 s2, a,n 3d. then (j + + == + + + surbatac( tr o1w)b r(o2w) c r(o3wf) or mr o4wt. h sli evsae an gturrml iaaratix + + wit1h1,1 , a,n 1d a b c do n diitasg. o nal + + + + porblsetem pgae21 5",linear programming and game theory
"1. ( aa) 12a 21aa3 434 1e;v seodn ea,t 1. == == (bb) 13b 2b 3211 4b 18od;sd od, eb t -1.8 == ==",linear programming and game theory
3. (at)ur e( prordueu.l)c t f(absl)e( ail)sl. (c f)as le( e d[ t11 0 0; 111; 10] 2 .) ==,linear programming and game theory
"5. t h1e, c1foca tiofsrn .t h1e2 ,c oaftcohraa s1 i cno mlnu1 w,ih ct focat�o-'lr2' -1 multbiy(p_ l11 y+ )2 a nadl -so1t o f finn df n f n sot hdee tienramnts 1 2 == - + -• f arfei bonnaubcmrecsei,x cenip tsth ues luf na- 1• sotloiuntso s elteecedx recises 453",linear programming and game theory
7. coafcteoxnrpi sao:dn et4 3()-4 (+14 )-( 4) 4-(1)1 2. == == - ( 1 ' 1,linear programming and game theory
"9. ( a()n- ln)(!ae cthe rn -m 1 ).() b 1 + - ·+ ··+ ) n l 2! (n - i!) 1 3 (c )( n+ n2- 3.) 3 a ] j � 11 [_ � ] [� �] [ : � d�e [t � det [ �_ ] 1 . 1 = = � = .]� ![ � ;}[� _� 5 de de(tab)t.es at [12 ,]b [det ] = = = = = [n [�� ] de(tab);a b [12 ,]d e_t 0 de(tba).s nigular: = = = = rakn(ba)< rakn(a< )n < m. 13 d. e at 1+ 1 +81 2-9, 4- 6 12s,ro o wasri edn peenddeebnt t 0;s, ro o sw == - == == + ardee pen(dreonwrt o 12w ro3w;) d ect -1c, h aisn deenpdrewonst. == == 15e.a cohft hseit xe rimdnse a ti zsre ;ot hrea inaksmt o s2tc; lo um2hn a nsop ivot.",linear programming and game theory
"17. a la2a33424ah a-s,a 41a2a33421ah a+ss, od e at 0; 1 == debt",linear programming and game theory
"4. 1 4.8 == - . . == 19 (. a i)a fl l a22a 330 t hfeuonrt eramrsseu re. zeros == == == ()bf itfeteenr �mrzsee .r o 21 s. mo et eralm2a (3a.. .a nwi n btgifh roem luians o zte !rm oovreo sw1 2,., ,.ni .n to as' rosw f3,, ())t.eh snteoh nezne wriobl eol n t hmea i dina glo.n a a, . . . 1",linear programming and game theory
"27. i bn lia n-lianii (n+ 1)n 1 . == - == - == 29 w. e m uscths oeio fsor mc olu2ma nn1sdc, o lu4ma nn3sda, n sdoo nt.h eorrenef � n/2 musbtee vtehona vdeea tn=j:. o .t hneu mboefer x cnhgaiessn s oen == (-1) . 31s.l 3s,2 8s,3, 21t.hr eu lloeo lkkiese v esreyc nounmdb ienfri o bna'csc i == == == sqeuen.c3 e5 ,, ,8 1,, 32 13,45 ,5 . ,s oth geu sei sss4 5.5t hfiev neo nrzoe . . . . == terimtnsh ef oburilmgfar os 4a r(eiw t3hws h eprreoe bm3l 9h a2ss8 )1 1 - 9 - + 9 - 9� 5. ==",linear programming and game theory
"33. chan3gt i2oni gtne h c omreerd utchdeees t erfm2ni+nb2ay 1nt ti mtehcseo factor sn-l oft hcaotm eenrt trhyic.fso ca tiotsrh dee teromfi na(notns eis zmea rl,)l e whicfh2n t.i hse recfhoarne3g itncoghn ag2et shde e mtieanrnttof 2n 2+ f-2n, whiicfsh2n +. 1 35 (. a ) deevlte r1yd; eu tk deatk 26,-, 6 f or 1k2, 3, . == == == == ��,. (bp)i v5o,ts",linear programming and game theory
"37. t hsei txe rmasrc eo rrreocw1t 2.r o2w row 03, t hsmeoar tiixss i nrg.u la - == +",linear programming and game theory
"39. t hfiev neo nzteerriomnd se a t 5a re == (2()2()2()2)(- 1()-1()-1()-1)- (-1()- 1)2(()2-)( 2()2()-1()-1) + - ((2-1)( )-1 ()2.) ontso s eletcede xrecises 41 w. i ta 1h 11 t,h -e1 2,-, 1m atrhiaxds e· t==1a nidvn ers1e n 1 - (a-)ji == == + i ma(, x j). 43 s. bu ttria1ncf gor mt hn en, e ntsruyb tirtcasoca fttcosc rn fn or mt hdee tenrtm.i na cnn thactoa fctiosr 1( msalplaesrmc aatl)rs .ibu xtrnag1cf torim1 l eaov.e s == 5 problesmet4 .vp4 age 20- 10 - 12 20- 10- 12 . 1 1",linear programming and game theory
5. abc (at)h aer eath atpo rafa lglreailmso det sot htear gnile haarse a � ab'c' ' 4 2. t(htbear) gni le hatshs ea maeera i;itj s su mto vtetodeh o irgin. ==,linear programming and game theory
"7. t hpei voofa ta sr2 e, 63f o,rmd etiennratm2s, 3 66t;,hp ei voofb tar se2 3,o,. 2 p",linear programming and game theory
"9. ( a)t ask( e,123,4,5,t) o (, 325,,14),. 1 p- ()b task e,(,2314,5,)to (, 345,2,,1. ) 11t.h peo weorfp s a raelp lem rutoanmt aiitcrsesoe,v e unatlolnoyeft o hesm atcreis musbter eeptaedi.p f r its h sema ea sps , te hnp rs- == i. 13 (. a )d ea t 3,d eb t1 -6,d eb t2 3 s,ox l -6/3- 2a nx d2 33/ 1. == == == == == == == ()bi aii 11b 3,i 2 1b -2,ib 31 1s.x 1 � a nxd2 - a� d 3x � . == 4, == == == o == == n = 15 (. )a x l� a nx d2 02 : n os oultionx l g a nx d2 � u: nd etde.r mine == == (b) == == 17i.thf e fi rcsotl uimna ni assl oteh r ighntds- ihb dta eh deena t deb t1. b othb 2 == b xl i1b/i a i adn 3ar es inrgs,ui lnaacc oel uimrsne ptee.tad heroerfe 1 1a nd == == x x 2 3 o. == ==",linear programming and game theory
"19. a iafl cloa fcto0r( se vienns ian rgolowerc olnu,)thm edne t 0( noe ris.ne v) == == u� ] a hsan oz ecrooaft cobrusit t iisne vrnlt.oei tb = 1 1",linear programming and game theory
"21. a ct a - a - idfe t 1a nwde k notwehc oaftco,rt shen anadsl od et 1. == == == snica ei tsh ievn erosfa e 1 a m usbtet hceoa fctmoartf roic rx. -, 23k.n owc ipn, rg o b2l2ge imvd eea st (dc e) t1 w in t h4 so cwaecn o rnusctt == 1-11 == . i c/t a a. a- detu sitnhkgen owna cct.ioo nfrvsteofir ntd == 25 (. )a c ofatcoc r21s c 31c 32 o. == == == l c c ,c c ,c c s- (b)12 21 31 1332 23m ake symmteir.c == == == 27 (. aar)e 3 1a 2 1.0 t(irbna)g alree 5a. ( tcir a)n agrale e5. == == == 4 2 1 1 1 1 � � 2 29 (. a )a rea o3 54 1 1 == 5 � (b) + nw5e tnrgiarlaeea - 0 1 51 == 5 + 7 == 1.2 1 0",linear programming and game theory
3.1 t heed goefts hh ey perhcvauelb egehnt .j 1+ 1 1 1 2t.h veo ludmehet == + + is 1.6( h h/a2os rh toonrmcaoll nustm.h edne( th/ 2)1 l eaadgsat ion 24== == deht 1.6) == sotliuontoss elteecedx ercises 455 y 2 -r - - - - - x1y 2-x 2y1== recntgalae+sb +(dn c ot) . i i a i i b arefaorsm rencgtala e2 t(rn igaal)e ==,linear programming and game theory
"33. y1 smaeb saesr encgtbla e2 t(rnigabl)e == -*- - - i i c d smaeh eigrhetcgstl dae n 2t(irnagdl)e. d == -- - ---1 -- - - - i - x 2 xl sot rnigalae s++ d b 21( 1xy 2- x 2y1). == ,, - cheacnek x apmlwei t,hb )( (a,32,)e ,d ) (14,,)a narde a 10t.hl ei ne == ( == == form( ,0e i)sn t 3eh pas sl oe /pa ae n edq tuiayo ne+ ex/ a s.et p3w okrsb ecause == . (,bd )io sn t hlanit!ed e e+/b a i tsre us,i nadc- eb ea eraa ea stt 2e p. == == n n 35 t. h ned- imseinlo nacubec ohrannse2 -r1 2es gd,e asn,2d nf aecso fd imseinon n n 1t.h ceu bwehs oeed gaertseh r eo wosf2 1h avso lu2m.e - �--",linear programming and game theory
"31. j rt.h ceou lmsna roer thoagnotdnh aellie rn gart1eha snr d . == i. b/rbx br/cboyes sien 39 • bex/ bb/eby (s-ien)r/ (oces)r/ r 41s. (21,-, 1) g ivperasa loalg erlwahmso,ea r etah leie shn gotfs psar ocdruoct: == ip iq ps i ii (-i2 -,2 -,1) i i3 t.h sic omaeslsof or ma d teerm!it nhaen t x == == othfueorrc omecrosub le(d 00 ,0, ,) ( ,002,,)( 12,2,,)( 110,,.)t hve<\llumteheo f tilbtoiexisd e dlt 1. == x y z 43 d. e t3 2 1 ° 7x y- 5placnnoet sa itntwheo v.e ctors == == + z; 1 23 45 v. i shaa tsh fiev ree rvslevasi v,s, v a, slaaa,.n adv ish atsh e rterwvsloeas via nvds. s nic5e -io2sd d, avnaidvsi ash ave iotppeap riotsy. prloebsme t 2",linear programming and game theory
"9. t hceo fiecfioef(n _t )- ain( 1 a- a.). ( a. n- ai)as + l. . +.a .ni nde (ta- a i), at ermi ntchlaautnod ff-edsi alag ioejnx aclbuohdat eus a a nadj - jas.cu ha - n1 1 terdmo e'sitnn vo_lav-)e.s ot hceoffi ecieonf(_t a n)-ind e(ta- a)im ust ( comfoerm t hper odduocwttnh e dmiaaniga.ntol hcatof ecfiieia nslt +l . . . + nan == a 1+ . +a n. . . t t",linear programming and game theory
"11. t rsapnoas ea - ide: (t a- a)i de(ta- a)i de(ta -a)i. == == 13t.h eeie gvnaelsou f aar1e 2, 3, 7, , 9 .8 , 15r. a(n)ak 1, a0., .. , ,0 (nt rna)cr;ea( nc )k 2,a 0.,. ,n. 2/ -,n /2 == == == == (tra0c.)e ontso s elteecedx recises 5,",linear programming and game theory
"1.9 a a nad2 a nad00 a lhla vtehs ema ee igvceetn.ot rhseeie gnvuaealsr1 ea n0d. f5o r a,a n0 d.5 2 f oar2 , 1 a n° df oar00 . t herea2 f i ohsra wela fbye twaea enand 1",linear programming and game theory
"1. [-�[� ��[ -�r� \ u n 1 ] ] [�[ _[;�� _r;, [ �]� � _ 1 a 0,0, 3; t hteh icrlodu monfs i s a mu1l taintpdhl oeet hcoeofmrl n uasr e",linear programming and game theory
3. == 1 ont hpel aonrheto glot nioa t. 1 a anad3c anbnedo ita go.nt alhiehzyae voden olnyle i onfee ie gnvesc.t or,linear programming and game theory
7. a == [ 1 -1] [ ° 1] [ - 11] g ivae s == [1 - ] 1[ ° 1] [ -11] - 1= 5100 0501 [3. 1 3· 3] � 510+0 5 10-0 4 - + . 1 3 sotloiuntso s elteecedx ercises 457,linear programming and game theory
"9. t r(aacbe) tr(acbea==)a +qb +src d+.tt hetnr (aacbe b-a)== 0 ( lawyas.) == soa b- ba== i i sp oissmibfrolm ea itcsrse,i nidc oene osht a vter azce.er o 1 1 1 1 0 0 11 1 .",linear programming and game theory
1.1 ( )at ured;ea t == 2 o#-.( bf)la se0; . (cf)as le; 0 is 0 0 2 0 0 2 0 diag!o nal l 1 13 a. [ � n [ � � [ �]-l r 1 '.[2 ] 1;£b u sru q arroe.o ts = - 2 11 1 1 111 [-�� l []�[ [�0][- i} [ � [ 0 ]0,linear programming and game theory
"1.7 a -_ ] 1] ] 0 1 05 0 = 0 5' 19 (. )a f laesd:o 'nktn oa' ws . (b )t ur.e (ct)ur e.( df)la esn:e eedi gveenctors ofs ! 1) 1) 21 t. h ceo luomfsna srm eu llteoisf(p 2,a n(d,0 ieni tohred.sre ma ref roa -i . 1 1. 1, 23 a. a nbd haav==le a nad==2 a+ b h aas==l a==3 e.i egnlvuaoefas+ b 2 aern oetq tueoa ilngv leauoefsp ale uisegn lvuaoefbs .",linear programming and game theory
"25. ()at ure.( bf)la es. (cf)la s(eam ighhatv 2eo r3 i ndepeenidgeennt- vec)t.o rs 27a. = [�_� ] (oo rth re,)a = [ _1! ]. a == [ � �� lo n leyie gvnectaorre s (c-,c ) . 1; k 29 s. aski -a pproazcehreaosn odin fil efyv eiari<y b 0f orma .==a9 n d --+ a .==3 . . . [� . [i o[i [ibi[_o i[il",linear programming and game theory
"31. �. ] -i}i ] 10 j ] 10- a s b ().9 (.3) = = = = b1[0� ] �umof tshetow .o = � �r k [ [ [ 3 2k 33b.k = � -n � � -n = [ � k). 2 35t.r aacb==e ( qa b+s+ )( c drt ==)+( qar) c+ +( b s td+==) t rabcape.r oforof diagloibnzlcaaesa et:ht er aocfse as i -i s ttrehao ecf( sai -)== sa w,h icthhe i s suofmt hase'.",linear programming and game theory
"37. th ea 'fsro ma s ubacs,esp inccaae n adl+ a 2h avtehse ma es w.h e sn== i t, he as'g itvhese u bsopfda icaegl mo antrcaiedsi.m sein4o.n",linear programming and game theory
"39. t wo plrmeos:tb hneu slplaacncedo lusmpcnaec n ao veprs,lox ac oubled b iont h. themraeny o bte i ndepeeinegdnevneitctn th oer sc soplaucmen. r � [� 41 a. [ � ha] as2 n an ad2 - a- i== z emraort icxo nficraymelsy ­ = = hamil.t on ontso s elteedce xercises",linear programming and game theory
"43. b ys fb,h a tshs ema ee iegnve(c1t0o,)ardn (s ,0 1 a)as ,so bi assl odi aglo.n athe : [;c ]-[ [�] � ;]:= eqtuiaoanbs- ba= ; aer-b == aondc ==:o d ra2nk. a2 xl x2 al 45a.h a}sq 1a nd . w4 i th (12,)a nd (1-,1) a.oo h as 1a nd a2 == == == ala == 2 == 100 100 0( asmeei egnvres.ca) t ohas 1a nd= (). 4,w hich zies.r onear == = 100 00 soa ivse nreyra a • porblsetem pa2g6e2 5�u3",linear programming and game theory
"1. th ef ibcocnniua mbearrestv eosndt,od d,td he.no ddo dd thnee txwto + == even. aroed (drf oomd + de ven a+ no dd .dthe )evner nee ptao d+do dd == even. 2 i ; ; f20 [ 3 [ 4 [",linear programming and game theory
"3. a = a a = n a = ;} = 6 67.5 2 �] [��i]i -��] 5 .a = sas1 -= [i = a�al 2 [ �[j_ (no st -i) i.c e � � 2 sak s-1 = a�al [ �i 2 �] [ � 1 ��][ _� -��][� = ][ ( � aa )/at� i=(a 2)] l, o",linear programming and game theory
"7. d iercatd dil til o+ o1ng i ve. s.. l di, a 2s1, 3 ,4 , 7, ,1 11,,82 ,94 776,, 1 23. k ak i + 0 myc alatcougrli sv e (.611. 8) . 1. 122.991. ,.w .h ircohu nfd st oo f == lid",linear programming and game theory
"132. == 1 0 7 6 112 ? 2' 4 ,",linear programming and game theory
9. t hmea rktornvast iimoant irsi x �o f.r aocntsi 1d o'nmto v.e 6 1 1 1 3 a a4,linear programming and game theory
"1.1 (a0),( , 11 -,2) .( b) 1a n-d0 .2(.cl )i m( i34 t,4, ) == eigenvector a== == fro 1. == o<a< 1",linear programming and game theory
"13. ( a) 0< b< 1. [/ -a) 1 lk o b(/-i a) b (1 ] (bu)- [ k _ 1 -1] 0 (a-b)[ k 1 l - ab- 2b --- - (a- b)k b -a +l b - al+ 2(1a ) 1 a b- --- - - - (a- b)k b -a + lb -a +l 2b a 1/3 == b -a + l ilfa- b l< 1 ; b == -/13 a) 2(1 - nomtar kov. b -a + l xxlx2 3 15th.e c omnpeonotfas xa dd +t o+ (ecahc omlnua ddtos1a dnn oboidsy ax xx2)3a ',i-x x lix23 lo.st thc)emo ponoenft asdt da o x (1+ + if + musbte + zre.o : [:] 12/,",linear programming and game theory
17. iusn stfabrol e andb elsf rto a 12/n.e utfroarl ± 1/2. lla> lla = < a sotloiuntso s elteecedx recises 459 0 0 2 1 12 2 3 2,linear programming and game theory
"25. = s./a s-i= [i has 2� = a ].,jb w ouhladva =e .j§ anad= j=i so r r , ittsre ai ncso rte naolt tehl a� t _- � ]cnah avj=ie = ia n-di a,nr de saqlue ar . roo[- �t � j",linear programming and game theory
"27. a sas-la nbd sa2s-l.d igaolnm aatcreiaswl aygsi ave a2 a2a. == l == i == l theanb ba,f orm s as-lsas2-l=- sa1a2s-l saa2 s-l== == l = l sas2-lsas-l ba. l == 29 b. h aas= ia n-di, s ob4 h aa4 s == 1 1ca;h n adas (==±1 ,j3"" i/)2 == e x(p±in/ 3,) 3 3 024 so ==a - 1a dn- 1t.h ecn -ja n cdl == -c. == prbolestme page 5a,4 215 '*p8 +",linear programming and game theory
"1. a l -2a nad 2== 0 x;= (1-,1)a dn x 2= (,11 ); == l 1 -2t - 2t + 1e - + t ] a 1 e . [2t 2t e == 2 _ e- + 1 -e+ 1",linear programming and game theory
"3. [:! _;� }: 2t u(t=) ast 00,e +00. -+ -+",linear programming and game theory
"5. ( )ae �(+ tt) == sea(t+t)8 -1= seatt s e -la== sea st -l s ea st- l et ae t a. == - [- (b)a= i+ a= un eb=i + b= [�� la + b= �� ] gies � v + .[ co1s - isn1 . . · · ea b . ]fr o mx mape 3 inth tee xatt=, t1. t hmiast insx = si1n cos1 e i dieffrefonrmte a.e b t [4t+ 3",linear programming and game theory
"7. e at = i a+t= [� ,. a et u( 0 )_ ] ] 4 1 - ·",linear programming and game theory
"9. ( aa) l 7+f!? ,a 2 7f!? - ,r eal> 0 u, n bsltae(.ba )l ,j7,a 2 -,j7, == = = = 0, real > 0 u,n tbsale( ca)l -l i,jti ,a 2 -lvti-;,r ea l> unbsltea = == (da) 0a, = -2,n uetrsatblalley. l 2 == 11a. i1 us n tbsalfeot r 1n,e rualtslytb alfeot r 1a.i usn asbtflroe t4, n eutrally 2 < > < 4, 4 < satlbaett ==s tbalwei trhea alf ro t 5a,dn s tbalwei tcho mpalf erox < t 5a.3 i usn bsltfeaoa rl l0 b,te ucsateht er aic2s.et > > 13 (. )a u �== c u 2- b 3u,u ;== -cu i+ a 3u,u �== bul au2 givueu�s+l u ; 2+u u u�3== - o.( bb)ec auesaiet as n o rhtoglom natairxlu,it( i)2 i== leiaut 0(1)2 i iu(io1)i1 2 s == ontso s elteedce xrecises 0 + b cotnas.n (tc a) == an±d(j a 2 2+ c )2is.ek wy-msmecmt artirhiacpveuesr e imgairnya as'. ! [n_! j6t",linear programming and game theory
"15. u ( )t = co2st + cos g l a x afx+ a 2 xo,r( a a f- a2 i)xo .",linear programming and game theory
17. == == - > 0a- b+c2 >2 )0 2 2 eigelnuvaerasre ae lw he(nt ra-c4de e)t 4(,linear programming and game theory
19. a +22bc2>. =} - - =} [�j [�]+,linear programming and game theory
"21. u l = e4t u 2= et[ -� iufj o() = (, -5 2 )t, huet(n) = 3e4t 2et[ - n [�]:[� ,� ] !(5 [j�",linear programming and game theory
23. = the an = ±j4t ). 0 0,linear programming and game theory
"25. a l == adn 2==a 2.n owvt e) == 2 + i oet2 -+ 00 ast -+ 00. "" �] � a [_",linear programming and game theory
"27. = hatsr a6dc,ee 9t a, = 3a n3dw, ith oinnldye poeenineegdn evnetct or (1, 3 cte,",linear programming and game theory
"3. )t hat yg == iveys ==' 3 e3.at lsot3e ts olyv"" ==e 6s'y 9-y. (01) ( 0)",linear programming and game theory
"29. y ( t ==) c otss at ratyts == and ' == o.th ev ecetqoturia hoanus == (,yy ' == ) y t). (cto-,ss in c b, (a",linear programming and game theory
"31. s but csitu b ut ==i necgvt g ivceetscv == c aec vt e-tc,bo r ci)v = orv == - (a i -)1 pratiacsruo tlliuiofni .s e ingvleau,te ha en inso itn vbelret:i == - an - i thvif sias l. 33 d. e a/dtt == a + a2 t+ � a3 t2 + � a4t3+ . == . a (+ ia +t � a2 t2+ �a �3 +t . ). == . . aea.t",linear programming and game theory
35. t hselo utaittoi nt m e +i asstl o ea( t+t) uo()t.h uesatt i meeatse queaa(tl+ ts) . +,linear programming and game theory
41. ( )at hien voefrea sitees - a. t(bi)af x == axt heeanxt == eataxn eda =j:.to . m · 1 [=�;l m a sas-,linear programming and game theory
43. a = 2a n5dw ietiheg nevctorasn d then = = prloebsme tp aeg,linear programming and game theory
1. ( bs)u m == 4+ 3 ;ip rod ==u 7c+ ti . (c3) 4i == 3 - 4i;1 i = i ; 13+ 4 ii == 5;1 1- ii == j2.b othn umbleioreusst i dtehu en ciictrl e. -1 +,linear programming and game theory
"3. x == 2- ix,x == 5, yx == 7i,ix / == 12/5- (i5/i)x,/ y== 1/2- (i2/i) ; chetchkixa ty l== j50 == ilxylla nid/ix l == /0 == i/ilx. 1:",linear programming and game theory
"5. ( ax)2 == r e2i2b,x -i == (ir /)e-i,xb == re-i;xb -i == x givie1x2 s == ont hunei t cir.c le sotliunost os elteedce xreicses 461",linear programming and game theory
"17. ( uvh)(vu) vhuhuv vhi v is.o u vi usn itary. == == == ib 19t. hteih rcdlou mnof u c nab e( 1,i /)-0) , m ulltiibepyad n nyu bmree • -2,",linear programming and game theory
21. a h a+s1 o r- 1i ena cdhi agleo nnt;ear iygphostsii blsi.t ie,linear programming and game theory
"23. c oluomffno su rmiatierxur a reei genvoefpcb teocrasu se 2 3 pu== d i(al,gw ,w , )uw( nadw i==. )",linear programming and game theory
25. n2 s tsef podri retcitmx eo;sn nlly on gs tsef rpof a nfd-i b yf f(tna dnf ro a). c 0 l+ i 2 3 1 . . o 2 +1 ia dna ah == [] ar nere mltmlan• atce ns . 1 3 u l-'i t -i 2 (haah)== haahh== haaa agi.n,linear programming and game theory
29. c a sithsie lrltm niif aror ea(lai )h iha -==ai i ssk ehwe-rm.it ian - c; == 0 01 2 3 l,linear programming and game theory
"31. p == 10 0 ,p i,p oo p99 p p;a cubreo ootfs 11 , == == == == 0 1 0 2 54 2 + 54+ 2 2n3i /4irr3/",linear programming and game theory
33. c 4 2 5 2+ 5 p+ 4 phaasc( )== 2 5 +e +4 e == == 4/3 8in3/ 5 4 2 2+ 5 e ni +4 e - r 1 1 - 1+ i [ 2 0 1 1 1 i- ] ] ],linear programming and game theory
35. a = [ t -j3 1+ i 1 0 - 1-j3 - 1- i l ' r 1 1 - 1- i [2i 0 1 1 1+ i . t ] = (az )= [ ] ]. k -j3 1 i- 1 0 -i-j3 r1-+ i 1 nst os eelctdee xrecises � [ 1+ -j3 -1+i ][1 0][1 -j3 1 i]. v_ � + - ith 2 6+ 2-j3 .,linear programming and game theory
"37. l 1 + i 1 -j3 0 -1 - 1- i 1+ -j3 w l == l + - v vhgi s-v--aerale,u ntiagriyv ieia s1 t,h ternea z ceogr isva e 1-, 1. == == == 39d. on'mtu lteiix-pt lisyme x e;i c ojngutateh fier tshtj e,o2 1i n e2 ix d x [ e2ix/ 2]�1i i o. == ==",linear programming and game theory
41. r + (r+i s )hr t- is ;t isrsy mmrcieb tus ti ssek wsy-mmei.tc r is== ==,linear programming and game theory
43. [ 1an][d 1- ;][ a b +i c] wi. t a 2 h + b2 + c 2 ==,linear programming and game theory
1. b ' - ic -a 2,linear programming and game theory
45. ( i -2uuhh==)i- u2u(h-i;2 uu=h1 )-4uu+h4(uuuh)hu=i t;hm eatxr i uhup rjocetso t ohlneit ne uth.r ough wea rgei vae+ ni b( a+i )bha t-i tb.t hean atan bd _bt.,linear programming and game theory
47. == == == == 1_,linear programming and game theory
"49. a [/1 i][� �] �[2: i� -] ; s as-i .r eeagile arilvsu1 ae n4d. = 2 1 = prboelms te pgae sag, 302 1 i",linear programming and game theory
1. n-bn n-im-amn (mn-)iam(n;)o nmly-ii m ii ssi milar c == == == == toi.,linear programming and game theory
"3. i af i, . ., .aa rne ei gleunoevafas t, h ael+n 1 .,., .+ a 1n eairgele unoevafsa+ i . soa a nad+ i nheavvtheeers amee iegnvusaeal,n cdna 'btes iamri.l",linear programming and game theory
"5. i bfi isn viebrthletenb, a ba(b)bi i ssi mri altabo . == - the( ,31e)n torfmy - aim i gsc oes+ h s ienw,h iczheo ir itfsae n -gh/.",linear programming and game theory
"9. t hceeo fifciearn ciet == s 1 c, 2 == 2 d, l == 1d,2 == 1c;h emcck == d. [�l 11t. hree flieomcnat triwxi bthsa iv isa nvd 2i as � th besa iv isa nd 2( smvae = [�� l - rfleeico!tng)i vbe s imf u- nth aen m bm-i . = = = 0 1 0 0 0 0 3 (ad) 0 02 .( bd) 0 0 t0h idredr ivmaaitt.xrit vhteeh ird",linear programming and game theory
"13. == == == 0 0 0 0 0 0 2 3 dertiivvoaef1s x, a, n xd arzee rsood, o.( ca) 0( tprl;ieo )nloyn e == = idnepenediegnetn( v,10e 0,c.)t or [� [��j n ��l[ j � � 15t. heei gleunaevr1sae, 1 1,,- 1e.i geatinrcmes .[ - 1 i (ar)rh u auhuha(u )h i. ( bi)tf sitrainguralnaudntiathreyin,ts",linear programming and game theory
"17. == - - == diagle onneta(srth ieei gvleaunemuss)ht a avsbelo uvtael1 ut.eh eanlo lff- diagonal enetsarr izee breouc satehc eou lmnasrt ebo e u nvietc .t ors 2 2 2 2 19t. h1e1, e niteorsfr hr rrhg iivt 11e 1iult +i 121t +i 131tsot 12 t 13o. == == == == copmranigt h2e2, e niteorsfr hr tthg ivt 23e so .s or m usbted igaoln.a == == i 1 i 21i. nf ua u-,t hnennh ua u(-u)-huahihe sq lu auta oah uh. == == 1 i thiitssh sema ea sua huah (auu -)(hauu -) nhn .s o insro m.a l n == == sotloiuntso s elteedce xercises 463 23 t. h eeie gnveasolf u i()-a are0 0,0, . a(a 21) - ! � !",linear programming and game theory
"25. a lwsa[ y : ���; ! ��! ]�( a d )[ �] + ( da b-)c,[] = [�� ] � - + 27m.- 13 mj0 , mj jm stoh leatsw qti nelqiiuetaasre ea tsyryi.fn ogr 1 2 froctehse == == m m j m-1j m. fircslotu monf tboe z ersoo, c anbneio ntv leerct.ani onbhta v1e 2 == 9 29a 10 210[ 61 - 4 595] . e a '2[ e 1 3 ] == · -80 == -61 - 1'1 :f 3 .[1 � l�[� n u �l [ n� ar s ei mi [� la� r b;y] i etls afn [ d� � ] by itself. 1",linear programming and game theory
"3.3 ( )a( m1a- m()mx-)m -(1xa)m -10o . ( bt)h neu llsopfa aa cneds == == == ofm -1a m h vaet hsema ed iemnnsd.iffi oerevn]etc todriesffr eabnnsatde. s 35 · j 2 == [c 02 2 2c ] 'cj ==3 [ c 3 3 cc2 ] 3'j ==k [ ckk 0 kc- 1 k . j 'o -_ c i, j- -_ 1 [ c - 01- cc-2 ] 0",linear programming and game theory
1. - 2 e wte) ((wo+t) xo ()� ty(0+�) 3z t()0s)•t,linear programming and game theory
"37. == + 1 39 (. )a c hoosm ei rveerdsiean gamola trgiexm t i- tji oi m m( i ne acbhl ock == == mo mi mo1j m o j.t (b)h atsh obsleo ckosni tdsi angatolgo e t == (ca )t == (m- )t1™ jt i s( m-l )tmol jmotm (mommt-)i(amommt,) == at and issi mrti ol a a. i a 0, 41(.) at ur:es>.n� h as the dooteh'set (bnr. ) f lasdeig.ao lniaazn eos nym­ == - _� � [� ][ � ] metmraitcar niadi ssmy mect.r( icf)la se: and� arsei mri.l a (dt)ur :ea lelie gnveasol fu arien cerdbe ya1 st,h udisff eenfrtor mt eh a + 1 eiegvnaloufe s a. 43d.i gaon6ab ly6s' a4nb dy haaslt lhs ema ee iegnlvuaaess pl6us 4�a b b a 4 - zesr.o proebmsl te6 1.up a3g16e a-c 2b 2 -2 0;x2 4xy2 y2( x2 y2-) 22 y",linear programming and game theory
1. (ideffreen cof == - 4 == < + + == + sqrueas.),linear programming and game theory
3. d e( ta a i-)a2 - ( +a c ).).a -cb 2 0 g iva el s( a( + c) == == == + + j(a- c2 ) + b2 )2j a na d2 (a( + c-)j (-ac 2) 4bj22;a)) l0 i ass um of a 0 == c2) (ac+ 2 )- 42 b > acb 2 . poistniuvbmee ;r2 s beca(uas e recdeutso + > > > 2+ a12a a-c b. better:p rwoadyuct == b,linear programming and game theory
"5. ( )ap ositdievfienw ihte-en3 < < 3. b (b) �] [ ! [ 9�� �2 ] ] [ ]�·( c )to h e. . mii. n si2 m9u1 ( m [! b 1 - - 2 b) = � [ ]] [ l [�] � [.- � whe !n � [�which i9s 2 ](dn)o m inimluemt, = b = y x x- y -3yth,e n apoparches -+ 00 , == - 00 . ontso s elteecedx recises - 11 j3 +1 1+ 1+ij3 i1 - 6 v=[ j[ o 1[ ] =+v32o l+i1 j3+ 0- 1 -1- i1+ j3 wl. thl 2 r:;",linear programming and game theory
"39. d ont'm ulteiix-pt liyme ix e;cs o jngutateh fier stj, e2 tixdh exn[ e2 ix o. ri ==(s++ ir ) ==s rh -it trs ; s ==",linear programming and game theory
41. issmy mectb ruiti s ws-mksmeyect.r i +i 1]; [ bb +b2 c2 1 +.==,linear programming and game theory
43. [ 1 a]n [- d a . cjwih t 2 a - - lc ;-a (2iuu h2)uuhh2 uu2hi 4) u+u-4h hu )hu(u u,linear programming and game theory
4.5 - it;hm ea trix (/ uhu == i - u ==. == prjocetosn tthole i tnheur goh ab (+ ia b)=i ah -t bti .a a t==b _ tb.,linear programming and game theory
"4.7 w ea rgei ven + then and 1- i == 1 1 1 2+ -2 -i2i - . == ' a[= j o [[ ]== as1s 14. . 6 - 12 0 l+4 i2 49 .r eeaileg vnaluaensd . j prboelms te5 .,p6 age 302 nbn-n 1m==--a 11nm( ==m-am nl()n);mi -==m1 i",linear programming and game theory
"1. c only issi mri la == toi . , a,+ 1 ,+ 1 a +",linear programming and game theory
"3. i afi . .,a .na reeie gnlvuaoefs t haeln .,. n . aa reeie gnlvuaoefs i. a a + i so and nevhearv es atmeheiee gn vuaealsdn,c na'btes iamri.l b bab (b)=ba- 1 abo",linear programming and game theory
"5. i fi isn vieb,rtl theen issi mrti ol a (,3 ma-m1 + =/ the 1e)n try ofig sc oes hs ienw, h iczhe iritfosa n -gh . . e 7 1=,2 ==,1 ,1 ;m =c",linear programming and game theory
"9. t hceo fecfiieanrtes d1 d 2 check d. c == c2 == 1 a[ �j=� 11th. er eflemcattxiwr otiinbh asviia snv d2i s thbesa ivisa nvd( smae 2 b=[ �� mj [ -�n a bm=,- im - refle!cg)ti ivoens if then = 0 1 0 0 0 0 == 20. ==0 0 0 0 3 13(.a d) (bd) thidredr ivmaattxrit.ivh teeh ird 0 0 0 0 0 0 = 1, == == 0 2 3 deriivvoeafst xa,n xd arzee ,rs ood o. ( ca) (tperl;)io nloyn e (001.,), indepeeinegvdneecntto r 1,- 11., [ �[1 ��,j � j [� �j - 15t. heeie gnlvuaarees eiegnmtcareis �j [� thtu 1-uah(uau==ih- .1 )h",linear programming and game theory
"17. ( a) (bi)tf sitarnigrualnuadnriytt,ahietns",linear programming and game theory
"1. = diagl oeinnea(tsthre eie gnlvuaemsu)sh ta vasbelo uvtael uteh eanlo lff- idagonal enitearsrz ee breoc aucsoel utarmhetne bos eu nvietoc r.ts 11, tt==ht ht == === 2 2 2 2 19t. he eniteorsf giiv1e1t1 i111t i121t+i 131tsot12 t13 o. 22, ttht ht == + companrgti he enretiso f givt 2e3so . s ot m ustd ibaeg onal. n==u a - ,ui nhn a== -=(u1 )uhuh-u aihua h ahu. 21i. f then iesq utaol auh=h(aa u1uu a)-- uh=in( h)nn . thitsh seima es ua s so inso r.m al sotloiunst os elteedce xercises 463 23t.h eei genovfa a a l( u i e-( s}a- 2 1 a) r0 e , o.0 ,",linear programming and game theory
"25. a lwyas[ !:�� �; !���-( ]+a d )[� ] +( da!-bc[� )� ] = [ � �] ! 1 27m. -j 3m == 0 s, to h leat swtio n eqieuasar eleai sttryiy.nf gom rj 1 == j 2m f roctehse 1 m m j m-jm. fircslotu monf tboe z er�oo, cn anboeit n vle.erc tainbhnaovt1e == 2 61 13 9 29 a10 == 210[ 45] .e a== 2[ e ] · -08 -59 ' - 1'1 - 16 � 31[ .� �j. [� � [ �l�j [ � ] ars e� i am;ri [ l �]� byi tsaenl[ d� f � by] it.s elf 1 1 1 33 ..( a( )m 1a- m)(m-x ==) m -(xa ==) m -0 == 0. (. bt) h neu slplaocfa e a sdn 1 ofm -am h avteh e dsia]mmee. dn iseffrievone]ntc taondrdis ff ebraes.ne ts 35j.2 == [c2 2c 2] j3 _ [c3 3 32 c jk _ [ckk c-1 k. jo _ i, j 1 - _ [c - 1 - c2 1- ] 0 c' 0- 'c - 0 ck' - - 0 c-' 2 3 37w.t e) == ((wo+t) xo (+)� yt0(+)� zt(0e5t) . ) 1 39 (. a )o ocsm hei == rveeerd sialgm oantatrogi exm t i- jimi == m! i n ebalkco hc l mo mi mojm o j.t ()b hatsh obsleo ckosni tdsi alg ogtneoat == l (ca )t == (m-1 )ttjmt i s( m)t-moijmomt == (mommt)1 a-m(momt), at '.a andi ssi amtrio l a 0,",linear programming and game theory
"41. ( at)ur eo:n 'eah s == thoet hdeore' .st (n)bf lasdeai.g onaan loisnzyem - - _ [�� ][ �] metmraitxcar niadi s syi.mc m(ecftr)la s:e and �arsemi ialr. (dt)ur ea:l elie gnlvuaoefsa i+ a rien ecardsb ey1 t,hs ud ieffrefonrmtt he a. eingvleauoefs 4 4a;b ba -4",linear programming and game theory
43. d iago6b nya alnsd b y haaslt lhs ema ee igveanlause sp l6u s 6 zeor.s,linear programming and game theory
"61. , 316 problesmet page",linear programming and game theory
1. a -c 2 b == 2- 4 == -2 < 0;x2 + 4 yx 2+2 y == (+x2 y)2- 22 y ( deierffncoef squeas.r) 2,linear programming and game theory
3. d e( ta- a i)== -(ac )++aa -cb == 0 g ival e== s ( a( + c) + 2 a j (-ac 2 )+ b 2 )j2 a (a( c+-) j (-ac 2)+ 4 2 b)2j;)a 0 and2== 1 iass uomf > 2 2 2 2 a 0 (+ac )(-ac )+4 b acb . poistniuvmeb ;e 2r sbeucsae recdeutso > > > -2. a1aacb bettwear:yp rodu2c== t,linear programming and game theory
"5. ( ap)o isvidete finwihte-en3 < b < 3. (b[ )! � ][ !� ] [ � 9� b2] [ n� ( ct)h mei nimius-m 2 9 (� b2 ) = � n � [! [][ [] � � -.� when ] = whiicsh = [](dn)o m inimluemt, 9 b2 y � x == -y3t,h x e-n y a pparcohes 00, -00. ontso s elteedce xrecises 1 - 1- 1 1 - 1- 1",linear programming and game theory
7. ( aa )l - 1 1 1 anad 2= - 1 2 -2. = - 1 1 1 1 -2-1 1 2 ()b1 (x-lx 2- 3x) == 0w hexln- x 2- x 3 o. 1= = o 0 1 2 2 (c1 2)= x(i x- 2- 3x)+x( 2- x3 3)+ x�l;='- 1 1 o. - 1- 31 [� [�;[�] �,linear programming and game theory
"9. a ] = �[]n� t hc eo ecffiieonftt hsse uq araerse the = 1 piviondt ,ws h ertehcaeeos fif icenitnsst ihsdeuqe a raercseo luomfln .s 2 2 2 11(. ap)vi oatrsae a ncd i-b/alan dde at = ac -i b,l (bm)u ltii 21pxblyy 2 (-cl bf)al.(c n)o wx ahx ia ss uomf rseqsu(.)ad d et- 1(i nndietfie) and == det+ (p1oi stdievfieen ,)i t = 2 13a . 1a n(d-a 1 ()c 1 )-b ,t hsi meana -s i it psho iasttdi evfien ite. > >",linear programming and game theory
"15. i( ,xy )x 2 + 4 yx 9+2y= ( +x2 y2 +) 5 2 y;i (xy,)x 2 + 6 yx 9+2 y == == == 2 (x +3y). 17x .t a ta x == ( a)t(x xa ==) l negstuhqa r ==e 0do nilafy x == os.ni caeh aisn depen­ decnotl nustm,ho inshl payp ewnhsex n o=. 4 -4 8",linear programming and game theory
"19. a == -4 4 -8h aosn olnype i v == 4o,rt a ==n 1ke ,ie gnvuae2ls,40 0,, 8 -8 16 deat o. = 2 2 21a . x+2 by+x c yhaass adpdoliaent( t, 0 i0af) c < b 2.t hmea tiriisxn fidinete (a< 0a nad 0.) > porbmls ete page 6a2, 326 ;",linear programming and game theory
1. a i psso itdienvfiieft roea 2b.i nse vpeoirst dievfie:n n iotte[i! cl e >,linear programming and game theory
3. d eat -=b2 -33 2 b+ 1 i nse agtiavt(e a ndr b)n e�. a ==,linear programming and game theory
"5. i xfta x 0a nxdte x 0f ro axn-# y0 , xtt( h ea+nb )x0 c;no dit()ii.o n > > > - [i [n- in",linear programming and game theory
"7. p oistai'vbsee carui sssey mmeatn,.,fardi co r. = r = >",linear programming and game theory
"9. i tax yl2 ixtrt r yl2 ir(x)tr yl2 < (btyh oer driysn cawharizn equality) == == irixl2 1lr1yll2 (txrt rx ()tyrt r y)( txx (a )yt a y.) = == -];: []� 11a . [= _� ha as= a1n4d a, x 1es an�d[� aj loe nieggn ve.c tors 13n . geatdifievneim tater: i (ciext )as x < ""f0ro a lln onrzove ectxo.(r isai l)l theeie gnavluoefa s s tasiyfa i o.( iidiea)tl< 0, ade0t, ad3 <e 0t. < 2> sotloiuntso s elteedce xecrsies 465 (iva)lt lhp ei v(owtistr hooewux tcn hegass)tai yds if< o .( vt)h eirasme a trix r wiintdhe pceonlduemnntts h aas tu_= c rthr .",linear programming and game theory
15. flasem usctno taeiing envoefac )tt;or ru(essm aee igeuneavssaa l);t reu (q i (qta q q-aqi ssi mrti oal )at;ur e( eivagleuonef-sa a r-e).. > 0 .) = e e,linear programming and game theory
"1.7 s arttf orm = (roowfr t )c(oluomfrn ) = lensgqtruheao dfc omlnu of j j j ajj rt.h ednea t= (dre)t2 ( volouftmeh e p ra rlalelpdei2)<p eprodutchte of == . . . ,l engstqrhuesaod fa ltlhc eo mlnuosfr .th pirso diuaslc a2t 2 an.n l ; 2 - 1 0 2 - 1- 1 ..,/ - �j,; 19 a. ==- 1 2 - 1 h apsi v2o,t sa = - 1 2 - 1i ssi ngular; 0 - 1 2 -1- 1 2 1 0 a 1 0 1 0 21x.t a xi sn optoi stiwvhee( nlx x,,x 3 ) (,010,)be caoufts heze e roont he 2 == daigo.n al,",linear programming and game theory
23. ( ap)osi tdievfien rietqeup ioirstedisev tee rm(ilansaaonl:tl> 0a) (.ba )l l pro­ jcetimoarnti ceexpscte a rsei unalgr.( ct)h dei ageonnetasrl di ao rfie te si gen­ 1 val.u (edst)h nee gadteifitvnmeeia txr ihadse t 1w henni e sv en. -1 = + 2 2 2,linear programming and game theory
"25. a l= iajanad= l/2bs,ao 1/� anbd ifz/. theell9ixp+s16ey 2 = = = � �. 1 haaxswe ishht alf-al e=na gntbd=h s 8",linear programming and game theory
27. a =[�� ][i=� []n�=c �[�] hacsct= [: 2 5] - ' - 29 a. x2 + 2bxy+ c2 = a -'!(yx2 ) ac y:2 ;2b x22 + 8 xy iyo2 2x(+2y2 )+ 22 y. � + + + = 2 31 x. at x 2x(i- � 2x- �3x) �x( 2- x3)2 ;x tb x= ( lx+ 2x+ x )32 .b h aosn e =' + pivot.,linear programming and game theory
"33. a ancdta ch ava 1e> 0a, 2 = .oc t()= t q (-lt q)r,q = [�� l + - r =[ �l� ch as pooinsveiae tno dn nee tgiaeviegv leaunbeu,ith atsw poo isviet eigveaulne. s",linear programming and game theory
"35. t hpei voofat s�- ar2e. 55.9,-, 0 .8s1o, e oingveea lnoufae �- inse agvt.ei 1 1 �. theanh aasne ingveaslmuaelt lhaenr i",linear programming and game theory
"37. r a(nctka c<)r aank,b u atl rsao(n ckatc >)r akn((c-tc)ta c-ci ) r=a ank.",linear programming and game theory
"39. no.i cfi nso stqa urceta, c i nso tths ema es imzaetx ar sai . 6 4-a/81 3 a/18 . 54 41d. e [t - - ]= 0g ivaels5 4a, s· - 3 _ a/1 68 4a/18 = 2= _ - ni. eigveenctors [ []",linear programming and game theory
"43. g orpu:so rhtongaomla itcr;et as froa ltal;l m la tcrewisi dteh=t 1 i.afi pso istive e k deifit,nte hger ooufap lp lo wear csot naionnspl oyis videte fitnmeia tcre.is ontso s elteedce xreicses v2 ] [!��l",linear programming and game theory
"3. a ta 1 haes. i ngveaels0' u1 + -j5 and2 - -j5 . == == == 2 2 a2 a at a 2-a ta a2.a � snice ,t ehe ingveectoofr sa rteh sema ea sf or snice (1-j5 ) == 0 = - negvaet,i l a is but theu nietie gnvcetoarrste h sem aea si n 0'1 == = a seu c2 t)2 i' o:6 nf o.a r 2 e, x cefportt h eeff ecotft hsim inussi g(nb aeucsew en eea dv 2 == [lal/a+ji] [2/al+ja�] 2 1j1/ + a j1+ a� u== v == andu 1 ==1 2 == - v 0 r 1/ ar 1 3",linear programming and game theory
"5. a at [;i] uig �� a u �_��.� has with andt with 2 = = = = = 1 1 1vre;/ 1v'2/ 1 01 1 3 ata 21 haa s 1l w ihtv i== 210) 0)/ / ,a i witv 2h -/0 v'21 , = = == = o 1v'3/ -10/ . anndu llvev 3ct or1 0/ == [ � �] [ �� � ] then � [u i u 2] [v i v 2 v3t ,] = u a",linear programming and game theory
"11. t om ake singrut,lh saem allcehsantsg e+eit 1tsss m la.lsets i ngrvu allauet oz er.o a / aj 13t.h es ingullaureo sfv a+ arneo t thecyo mfeor me iegnvuaelsof (a/ t)(a + i.) + 1 !4 , [� 15a.+ b = = 1 4 a a; +i st h1rei gh]vter-s1i oenf1 b+ 1 i s ltethif-en 1vsee1or fb . ] [06 [ ] 10[-]l41 ' at a - [ takseq urae roofo4 at nsd 1 6 == 6 10 2 06 17 1 1 1 11] · == 1 1 3 1[ 11 ][ 0[-] 11 ] [ 1 too btasi == n - 2 0 4 3 an dq = a -s 1 == 2 3 1 == [1 ] 1 - 3 ' v10 (taa )+ab a tb",linear programming and game theory
"19. () aw itihn depecnolduenmns,t trhoews paicasel olfr n;c heck 1 = . at( at) - ba at (b) isi nt hreo ws pcaeb ecuase times ainnyt hvasetpcc ae;t or is l (a)taa+ba ta(aaatt)b- a==bt . ata +x abt'. now bothc saegsi ve == == sotloiuntso s elcetdee xrecises 467 21t. ak e a[ � =� an] bd= [�n th eanb= [�l� fr ocm+i pnr ob1l5em l [jt [t wehavae+= � b+= � =(ab +),nad(ab)�+b+a+.",linear programming and game theory
"23. a qi :eqi a+ = 2 :e+q12f aa+= qi :e+:eqi.s uqargiinvge s == =} => 2 2 (aa +) == q i:e:e+:e:e+ qf == qi :e:e+ qi.s ow eh avperjc oeitosn(:a +a)= aa+ (a+at ) a nsdi mrilflyoaa r+ a .a a+ a nad+ a p rjeocotn tthoce lo umn == spaacnredo swp aocfae . porblsete6m 8, 4p age '- 344 -- -)i,x{\.rt -;5-tn)-� 1 px()a) - 4xl 4- 3xh absp/ bix= 2xi- x 2- 4, • . bp/ bx 2x-x/3nadbp f<j1 3 - x+2 x-4. 2� -- xi + 2 == 2 3",linear programming and game theory
3. b pi/xa x+ y= 0a nadp /iyb =+ 2xy- 3 0g ixv e- 3a nyd 3=p. 2 == == == [�]�. hansom inim(ulme t yi itas ss otceiwdai thtehs emidmeafitnriitxe (0). -+,linear programming and game theory
"5. p uxt (1...,1 i),rn ay lh e'iqsgu ot(itedhneetn otmobiren caonm)seni.sc r ex( ) == iasl wsba eytwaeia e nnad ,n w eg entia < xt a x sumof a la ijl< n a.n ==",linear programming and game theory
"7. s nicxeb� x 0f roa lnloz nevreoc txox,r t(sa b)wxi blell a rthgaexntra x. + > sot hrea ylqeuiogthiil sea nrftgro e ra +f cabat ln e(l ii ngv eaenlasur ien acsr.ee d)",linear programming and game theory
"9. s nicxet b x 0t,hr ea ylqeuiogthfi oaer+ n bti lsa rtghetarhnq e u otfioaer.n t ;. , 11t. hsem laleeiesgvnta liunaex s= a xan adx = 'amx a� ar ne(d 3 v'3 )-/4. 13(. aa)j mijn s[msarjxx (x)i]n0 m eatnhsea vte s rjcy o nitnaasv ecxt or = > ytctacy taxx wihtr x() o.( by) c=- 1 x . vgeiqsu . ot_ r iy(e)n t > = yt y rx()o . > 15t. heex trseumbes s piasscp ea nbnyte hedei e gnevctxolarn sxd 2 2 ' i i 17i. fc x ca(-b)e uqaldst hecna- b di sze riont hcerro ecttieormn in == - equa5t)io.n ( porlbdes lte6 .,5p a3g50e 2 - 1 0 31/6 12/",linear programming and game theory
"1. a y bi 4s - 1 2 -1 4/61 -b-- 12/ .t hlei rnfi enaietlee ment == - o 1 2 31/6 12/ - 3 �1,�, . = 16vi + 1�v 2+ tv63e uqlatsh eex a uc= t t ' 6� , 136a t ntohdxee = s u 2 - 1 0 2 1 �.",linear programming and game theory
"3. a 33= 3,b� thean 3 - 1 2 - 1, b 2, ya = b gives == 3 o -1 1 1 5 1 8 y == - . 9 9 ontso s elteedce xrecises j�v v/,j j��v' ; ['�jv;j j��� v' ;",linear programming and game theory
"5. i ntebgyrp aa:tr et s dx dx dx = = = - - au. smae � . a 4,",linear programming and game theory
"7. m their1 2(rr ayatlieqoiu goht itehnsetu p baoscnem uiolpftlo efs = = 2 v ( )xi)ls g aertrh tahnte re eu ingveaa l u]'( e. == h6/ 1,41,",linear programming and game theory
"9. t hmea smsa txr im itsi mthees tridimaagtxor.ni al porbelms te7 .,p2 gae35 7",linear programming and game theory
"1. i fq iosrh togointnasol r,im i si q i i m ai xq i xi/ ili x i i1 b ecaqup sree serves = l = ii q- lengqtxlhl:i lxlif oerv exr.ayl so iosrh togoannahdla n so rmo nseo, == c() 1. q ==",linear programming and game theory
"3. i aibxi i < iaiii/b ix1/, b yt hdee finoitfth ineoo nro mfa, a ntdh i eb nixi < i 1b /i ix iil· i iaibiii aiiiibiii . divibdyiilx nlagln mda xiinzm,gi < thsea mietsu r ef otrh e inrvsei eb ,i1 -a - ii <i i b-il ll a lli-i ;ci( ab) < (ac)(b) by m ulltyiitpnhsgee c ineqiue.as lit iaii i 1/a xi/i i ii, i",linear programming and game theory
"5. i nt hdee finitimoanx x choxot soe t bhpeea i rctrue liagenvector == iqnu eosni ta ;ii xi iiai li i,x si o rtaohti eii sa i a nmdxa imruamtai lot e tiia i sas . == at aat aatx ax aa(tax)",linear programming and game theory
"7. and havtehs ema ee igveaunle,ss icne gives a == = a ta( )ax a( xa.) /ai"" i aiit. i equaltihltergya eteos iefgn lvuameesa ns == == [��l [ �l�",linear programming and game theory
"9. a b aam(x+a b )a amcxa +) aa mcxb (i) sn 1 ce 0 + 0,) = = > > aa(axb)a m(a)axmaa(xb.) ma(xa ) and > so is nnoortm .a m a 11(. ay)es c ,a ()i aiiiiai-111ca (l -),s in( cai e)- i- i a sa ga.i (nba )i - b x = == == 181 lb l 11 l8 xi i 181ixi 181 lb l leatdos iaiiiii1 la . t hiiss > � . ibll-< l - ilxli ilxllc i l b ll -",linear programming and game theory
"13. ia iii ancd 1i;aii i .j2 ancdi isn fi(nsigiturnel aiaii i .j2 and 1. = 2 = = f); = c = i 15i. a fa mxa inm 1, t haelna l i 1 a na d ssi- it.h oen mlaytc reish wit = == = == = iaii i ia -iii i1 ar eo trhgoonmaalt rbieccaaeuast sh , eat sbo ei . = = b- a (-1,70)0 b a-z (.0031,.0 01).6",linear programming and game theory
"17. t hree sli dua imsu cshm altanlh er y = == buti msu cchls oetrto h seo lutthiaonn z y. . 19x. t +x ;i sn ot llstemhraam na( xtx) ( liix)i2 a nndo lta rtghearn + .. . 2 == 00 (iixi + in 1x )2 ,w hiics( h1x 11 1 )1. c ertxali+n ...l+ y x ;< nm axt)(sx,o + . . ilxi <i -fo lx1 l 0001 c hoose (six l, gs n i xg 2,n , s ixgn)nt goe x t· i1i 1. b1x y y == o • • y == i·i (11.,,,. 1) . shcwrzat,hiiassmtosllxtllyilll-fo ilx chooxs e fomra ximum == == raito-fose 9 -36 30 i 3 3 a- -36 129 -810 . 21t. heex aicntsv eeorf t heb yh ilmbaetrxrt i is == 30 -108 108 1",linear programming and game theory
23. t hleag reisxlltl ii a-bl ilis/ ain tm; hl era geesrtri o1 sr- 01 /6ainm. = [10 [0 1,linear programming and game theory
"25. e xchang] et o[2 ] [ 22 ]u 2 i.wtph ]an d 1 0 -+ 0 -1 == = 1 0 22 sotloiuns tsoe lteedce xrecises 469 2 2 2 2 2 2 0 0 0 2 l = a 1 0 1 0 - 11 0 0 � � � u �l 2 2 -+ 0 0 0 0 0 -11 2 2 0 0 1 0 1 0 0 2 0 0 u.th epna= luwipt h=0 0 1 anld= 0 1 0 = . 0 0 1 1 0 0 .5 -.51 '"" ' po�lbesmte7 m3p,a 3g6e5 norlmiazteod unvietc .t or k k",linear programming and game theory
"3. u k/1a= c ix i+ c 2x 2( 2a/ la)+_.' c+nxn(na/la) c ix ii fr aailtoials/i iai 1. � < �] thleag rersattc inooto rl,ws he kni lsa .r g=ea [ � haias12 la=d a nndo covnergee.n c 2x(-y)x t hx x - (x x - (x hh(x) hy",linear programming and game theory
5. -y) = -y) yt.h en = is = (-xy )(-txy ) == hy. x = -' 1 0 0 1 -5 0 �] 3 4 1 12,linear programming and game theory
"7. u = [�0 -,- 5 5 u-antdh ue-nal u -5 29 5 25 = == . 4 3 12 16 0 0 5 5 25 25 e e e e ee [?cs si]n [c?s- isn] [1 coss i]n .. e e e. r= q 9 2 sine o sin cos 0 -sin . c(l+ t) _3.�] thernq =[ -s c -s k 11a . ssthuam(teoq '. q. k i )r(k1• r•)o• th ieqs rf catoroinoz faa ti - - (cretnaltiyre iu kf= 1.)b yc ornusictotank+,= rkqk, sk=o a k+rq1i = l (qi· ·q ja·q o'""q k)qi·p omsutlltnyiigbpy( kr ··r·o, t) haesu smptgiiovne s -l rk··r·o= iq· ·qj·a 1 •ka +ftmeorv itnhqge' tstho e l etf-hsaintddhes ii ,ts h e k+ i requriersfeuodalr t .",linear programming and game theory
13. a haesie gnveas4la un2 dp. u otn e euiengnivte icnrt oo1wro fp: ieti tihse r -]: -]i �[ -�]a�n pda p-l= [� or� u anpda p-l= [ �- l�,linear programming and game theory
"15. pi aju s4esnm u lltaiiotcpin( sf2 ro eeanrctyih nr owasn jdi ). b yf atconrgoi ut e, cost heeni ter1sa n±dt aenn eeodn l2ynm ultaiotpinlwsih,ci cht ol�n e3 ads foprr . porbmls ete7 a,4p a3g7e2 o i o 2 1",linear programming and game theory
"1. d -(-l- u)= � 0 � e,iegnlvuae=s 0 ±, 1,j'2/;( d+ l)-(l-u)= jl i o i o 1 o 2 2 o i i,e igveaulne0s0, 1, 2 /; =4 -,j'22,r eduacamitxn3 o g 2- ,j'2 0.2. 4 wopt � o i i 4 8 ontso s elteedce xersceis",linear programming and game theory
"3. a xk( -2 2 co kshrc ) kxj;xk ! (i s2 n khrcs, i3 nkh +rc s iknhrc . ,). . = = = 1 rc rc (ckohrcs) kxf.oh r -, haaes i gveaunle2 s- 2 ocs 2- vi2,2 - cos 2, = 4 -4= -2= 3rc . 2- co- s 2+ m2 . 4 = v 1 1 ° 3 3 2 1 1",linear programming and game theory
"5. j d-(l+ u ) -° ° ;t hteh rcreicelh easvr ea dsri lu ,r 2 �, = = = = 4 3 4 2 2 0 5 5 4 . r 3 .t hirec netersz earrseooa ,l aia lt 1i 4 15 1. = 5 < < 1/2 ° -bal (b)c 1 1",linear programming and game theory
"7. - d-(+l u ) [ ]hajl s ;- d( l+)- u = -/ed 0 = ± ad == [�b� e�l �a = 0b,e d/a; ma adxo eeqslu jl �a . ..",linear programming and game theory
"9. i af x ax, th-ean) x( -1a ).rx elae iegnalvuoefsb i- ah ave (1 == = = 1-1a i1 p,r ovtihdaaei tbds te we° ea nn2 d. < 11a. lwa/aylsbi iia iiibiii.ci hoosae bt ofi nidb2 i1!ib ii2 i.t hecnoh ose < = < 3 3 a b2 t ofi nidb ii ii b2iii bi""i ib1i • 1c onti(nouuresi en diuoc.ns tn)ice = < < ibii i m aixa( bii),it ns o s uirsptreh iabtii i 1g ivcenosvg enerce. > < [1�] 13j. achosabs i-l t � � �] wi tilhma ax � g. auss -esidheasls-l t [ = = = � witialhm ax (ali m froj caboi.) 2 = = ax",linear programming and game theory
"15. s cucevses rioravexelat(oisroi)nnm atlab. 17t. hmea ximruosmwu mgsi avlelli a .a9dn ia i 4t.h e cairuroncddli easgl ona < < enrtiest iggibhvoteue. nfr di tsra :st hcei riac- l.1e2 cotnasit nhoet hceirr cles .7 < ia- .13 .a5dn i - a. 11 .a6dn a ll etihgrleeune.evs scaeona:dt hceri cle < < ia- 21 < 2 c notsat ihcnei riac - l 2e 1< 1a nadllt heree ingveal2+u ,jie ,s2 a,dn 2 - ,ji.",linear programming and game theory
"19. r b l-aab b (-bbtltab b)iasob rot ghnolat or o b:t hrseedi ausl l = = = r b a-xae ro trohgoanetaa slcet .hpt os hotwhp aiits th oogroltn aoa po ab, = = simypp liti ocf ip:p i iaibl2 bl- (batba )badn btb(lbatb2 . c) ertainly c = = (apb i) 0tb, ea ucsaet a(.t ths maipilfiatcipounatl i nptio b- a lab+ = = = (t bb- 2 iabta b+a iralbi2 )llbtbbfo.ar g odod sicuosnss eeni umeirclanile ar agleabb yrt reetfhaenbnd ua .) porbmls ete page",linear programming and game theory
"1. == == vaarlbiseh obuexl3 ,dt or edtuhcceeo sthtle.ev aivnragib alseh obuexl ,sds ince 21/ 2, 2, ilsse tsh 4a1/nw .it xh3 a dnx 4i tnh besas it,h ceo nasitrgntixsv3 e x4 == = antdh ceo isnstwo x l+ x2- x3 -2. == th""er edcusocteasdr"" e [11 ] sco, h nagies gnoooatdndt hcero nieosrp ti.m al",linear programming and game theory
3. r == [� j,linear programming and game theory
"5. a tp, [-53; ] t ehna tq, � iosp tibmeacla u>0 s e0 r = r == - ; r r",linear programming and game theory
"7. f or a mparxoibtmlhuseemtm o ptpeibsnetgc omeos. t ihfsal isa,nt dh ieht if r < cmoponietsnh ltera getshtte,hnc a otl nuo mfn e ntteehbr sasi; th ser ulef otrh e 8c vecltevoairtn hgbe a siitshs se ma e.",linear programming and game theory
"9. . ..0 .] [. . ... . ] s , i nv c eu s.o is ctrroheecm tat xr.i be b [· b e == v = u = t t l 11i.a fx 0 t, h penx x- a(a)a- ax x. � = == porblsetem page 8�ju3 99",linear programming and game theory
"1. m axim' iy4 z+l e l 1 2y,w ihy t >l0 y, 2> 0 2,y +l y 2 < 13,y ::s2 1 t;hp er ihmaasl � , � xi 2, xi �t,hd eu haaly s i yi ,ocst5 . = == == == =",linear programming and game theory
"3. t hdeu maalx iizmyebsw, ih y t > ct.h erex fobar ney d car ef aesbil,ae nd = == gitvhesem a ev alcuf eob r c sotithtn eh p er imadlu abalyn ;td h meuysb teo pt.i mal sf "" fb ot,htehnoeptixm*iacslhantg(oe,obd 2 ,. . b)nady* (,0 c2 , '""c n). l l < 0, n ==",linear programming and game theory
5. b [0l t] a ncd [1- 0.] == ==,linear programming and game theory
"7. s niccex 3 ybx,a nda roep timal by sf. = = y t",linear programming and game theory
"9. x * [1o ], * y[10 ,] w ihy t *b 1 c*xt.h sece onidn equiabnlto iht ies == :=' := == a x*> b anyd*a ca rseit crstot,eh s eccoonmdp oonfy e* na tnxsd* a rzee .r o < 11(.) a i x 0 x,i 1�,: 0c,t x 3. (bi)it ts eh fi rqsautd rwaintth the = = == == tetrahientd hrceomo ne cru otff .( cm)a xiizmes ujebcttoy l> 0 y, l5 , < ' yl, y l 3,y l4 y; l* 3 . < < = � � 13h. erce [1 1 w ita1 h ][ �] .n o c ornasixt>n 0 ts ot hdeu waill l = = t haveeq uyaly ia t (oa ry c)tt.h agti v2y esl 1 a ny dl 1 a ny d2 2 == c == = == == anndfo e assioboll.nuse ott hiper immuashlta vea sm axi:mx ulm -na nd 00 = x2 2na nxd3 0 g icvoes xtl + x 2+ x 3 n b(irartarliarlg.ye ) = = . = == 1 0 0- 1 0 0 1 0 -10 15t. hceo luomfn0 s 1 0 0- 1 0 or0 1 -10. o 0 1 0 0- 1 0 0 - 11 17t. kaey [1- ]1 t; h y ena> 0 y, bo . == < nst os elteecde erxcises porblmes te page 8�4, 406",linear programming and game theory
"1. t hmea xiflmoaiwl s1 3w,i tthhm ei nicmuastle r panatgni od6fe or mt hoet her nod.e s",linear programming and game theory
3. i ncsrietnahgce pa cayio tfp ipfeormsn od4te on od6oe r n od4te o n od5e will prodtuhlcergae e isntc rietnah mseae x iflmoawtl.h m ea xiflmoaiwln cerfseor ams 8t o9 .,linear programming and game theory
5. a ssicganp aci1tt oi eeagsdle .tls h mea ximnuumm boefdr ij isonptta hfsor m s = ttot hen tehmqeau xailmsflu wom. the nmuimnebori fmeu dmgw ehsso ree moval disccotnfsonr met i tsh mei nimcuutmth .em na flxo wm in cut. s =,linear programming and game theory
"7. r osw ,,14 a5vn idto hela a'lcslo nido;itn theb y3s3 u btmxrac iomifrnogm r1o,w s 4,5 a,n d co1l2,,u5 h m an3ss 3 + 5. >",linear programming and game theory
"9. ( )at hmea thrai2sxn i wsh iccnahon tb ec ovebryles edst h anln i nbeeuscs ae ealcihcn oev eexrastc wtiolsiy t.t a sk leni ;nt ehsemruesb teac ompmlaehttice.n g 1 1 1 11 1 00 01 (b) 0 0 1 0 . ctahbneec o1rv eewsdi ftohu r; lfimivanerea rsgiaerse 1 1 00 01 1 11 1 1 noptos sible. 11i.e fa ch+ m1a rersti he aocnpclteyam balne t ehriens o foon#re1t om arry m m, (veetnoh ugahla elra cpcteatbol e #1). 13a. logri1tg himv 1e3-s3, - ,22 -,52 -,44 -,6a nd al2gg oirv2ie-t,s45h -m26 -4, , 3-123,-t. he saere eq ualh-s lheonrsgtpnteansittner ge. s",linear programming and game theory
"1.5 ( )ar osw1 3,5, o nlyi hiscan vo uelm n2sa n4d. ( bc)ol um1n3,s5, (i2n ,r ows",linear programming and game theory
"4. ) (cz)e rsou btmrafiorxmr ow13s,5 , a ncdo lsu1 m3,n5, . r(odw2),s a 4n d colu2m4,nc sor va elil.s porlbmes ebt.,p5 age 431",linear programming and game theory
"1. - 10xi+ 70(-ixi) 1x0i- i0i(-xi)o,xrl � , x � -10y+i 10i(-i)y 2 = = = = ; 7y0i- 101-(y)io,ry i � y � avegrepayaoyffax 6. 2 = , = = ;",linear programming and game theory
"3. i fc ohossc eolumnw illo sicetsh smo alelnetasjrit( y i n ir)ow.wi nlolt j, x y x mov,se intciheists h lera egsetn tirnty h raotwi .np romb 2l,ae l 2w aasn 2 = equiloiftb hrkiiisun mid fw.e e xacnhgteh2 ea n4db leoiwtn ,oe ntrhyath si s preorpatny,d msitxrgeaidate resre e qu.i red",linear programming and game theory
"5. t hbee ssrtta tferog cyo mbtihnteew lsoi ntpoer so dahu ocreni tz1aoilgnuear,a n­ x � � teetihnhiges i og7fh/ t3t .hc eo mbinias( t3+iy2 o (n-1 y)+) (y+ 3 (-1y)) = �, � 7/ 3s,o cohostehcseou lmnwsi fterhq nuceie0s, x .",linear programming and game theory
"7. f ocro lnuswm,e wxaain+ t ( -1 xi)bx el (-1 xi)d so u, = + = x(la- b e+- d) d- bf.o rrwo syi, +a ( -1y )iey bl+ ( -1y )id = = = v excnhgabeasn d.c omeparwei th u v: (a- b)d(- b) ad -be u_ - · xi(a-b) +b-- +b -- smaea ftbe re � = v. a -b- e+ d --a -b - e+ d --- sotloiuntso s elteedce xercsi se 473",linear programming and game theory
"9. t hien nmearx imiutsmh l ea rogfyel a rn yd 2;x c oennctraotnet sho antse u.jeb ct toy i y 2 == 1t ,h mei ntiimm ofth �a egr eyir �s n .o tai ==c je . + [��t j",linear programming and game theory
"1.1 a x* anyd ax*�y i �y 2 �f ro satrlalt oefg iy*es a == == == == + y; -jl �; [�� - 1 anyda* x �xl� x 2- x 3- x,4w hich canniont exceed == + betwieysea *nx* �. == (,� 13 v. al ue( frga maie.) choos2e so yrc hsoeoosd odre v:ex n*y * �.) 0 x 3, = == porblsetem p age a, 420",linear programming and game theory
1. ya e)l aersgdti ( m s n t ) == 7 wh se en t. (bs)ma ellsdti ( m s n t ) == 2 . smaelsdlti m t)8 w hen (e) (s+ == s e t. 3 ()dl aersgdti m t) (aolfrl 1) . (s - 13 + %t� au a 12a 13 a 14 all a12 0 0 a 21 a 22a 2 3 a 24 0 a 22a 2 3 0,linear programming and game theory
"3. v w anv d n w cnotain and + 0 a 32 a 33a 3 4 0 0 a 23 a 34 .. 0 0 a 34 a 44 0 0 0 a 44 di( mv w) and d( ivnm w ) 7a;d dg etto div m diwm. + == 13 == 20== + thlei ntrehosu (g,11h1 ,a)n (d,1 1 2,)hv aev nw {o.}",linear programming and game theory
"7.o nbesa ifsov r + w ivs, iv 2 , di( mvn w ) == 1w ihbt as (i ,01s -, 1 0,). wi;",linear programming and game theory
"9. t hietn rescetiocno lousfmp nai cse sth teh rloiunge(,6h 3 ,6): == y 1 1 1 5 3 0 1 53 0 [1] 1] 1 2 y 1 [ macthe[sa b x] o. == 3 0 == 0 == 3 0 0 == 3 -2 2 4 0 2 2 4 0 2 -3 thcelo umsnp ahcaedvsie m seinotnh esiuram n idn teecrtgsiiovne1",linear programming and game theory
"2. 3 == 2+2 . + 1 1 1 1 - 1 - 1 1- -11 . - 1- 11 13a. 3d ( iad a id a id)' == 0 j® j)+ 1( 0 ® 1)+ (1® 1® porblsetem p age 8, 421 0 1 0 [��]",linear programming and game theory
"1. (aids i agloiblnze;aa) o (egievncetors and = = 0 0 (,10 ,0) j j 0 0 0 (,2 1,-0)). 1 t 2t 2",linear programming and game theory
3. i b+ sinbceo a.sl o i +t .j ebt == 0 1 0 == t == elt == o 0 0 1 00 [�� ] ��,linear programming and game theory
"5. j � (disetieignnlvcusat;e)j (hba as but = = = 0,0 1.) rank",linear programming and game theory
"for an orthogonal matrix \( q \), show that \( \|q\| = 1 \) and also \( c(q) = 1 \). orthogonal matrices (and their multiples \( \alpha q \)) are the only perfectly conditioned matrices.",computations with matrices
"which “famous” inequality gives \( \| (a+b)x \| \leq \|a\| \|x\| + \|b\| \|x\| \), and why does it follow from equation (5) that \( \|a+b\| \leq \|a\| + \|b\| \)?",computations with matrices
"explain why \( \|abx\| \leq \|a\| \|b\| \|x\| \), and deduce from equation (5) that \( \|ab\| \leq \|a\| \|b\| \). show that this also implies \( c(ab) \leq c(a) c(b) \).",computations with matrices
"for the positive definite matrix \( a = \begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix} \), compute \( \|a^{-1}\| = \frac{1}{\lambda_1} \), \( \|a\| = \lambda_2 \), and \( c(a) = \frac{\lambda_2}{\lambda_1} \). find a right-hand side \( b \) and a perturbation \( \delta b \) so that the error is the worst possible, \( \frac{\|\delta x\|}{\|x\|} = c \frac{\|\delta b\|}{\|b\|} \).",computations with matrices
"show that if \( \lambda \) is any eigenvalue of \( a \), \( ax = \lambda x \), then \( |\lambda| \leq \|a\| \).",computations with matrices
the matrices in equation (4) have norms between 100 and 101. why?,computations with matrices
"comparing the eigenvalues of \( a^t a \) and \( a a^t \), prove that \( \|a\| = \|a^t\| \).",computations with matrices
"for a positive definite matrix \( a \), the cholesky decomposition is \( a = ldl^t = r^t r \), where \( r = \sqrt{d} l^t \). show directly from equation (12) that the condition number of \( r \) is the square root of \( c(a) \). elimination without row exchanges cannot hurt a positive definite matrix, since \( c(a) = c(r^t) c(r) \).",computations with matrices
"show that \( \max |\lambda| \) is not a true norm, by finding 2 by 2 counterexamples to \( \lambda_{\max}(a+b) \leq \lambda_{\max}(a) + \lambda_{\max}(b) \) and \( \lambda_{\max}(ab) \leq \lambda_{\max}(a) \lambda_{\max}(b) \).",computations with matrices
"show that the eigenvalues of \( b = \begin{pmatrix} 0 & a \\ a^t & 0 \end{pmatrix} \) are \( \pm \sigma_i \), the singular values of \( a \). hint: try \( b^2 \).",computations with matrices
"(a) do \( a \) and \( a^{-1} \) have the same condition number \( c \)? \\
    (b) in parallel with the upper bound (8) on the error, prove a lower bound: \( \frac{\|\delta x\|}{\|x\|} \geq \frac{1}{c} \frac{\|\delta b\|}{\|b\|} \). (consider \( a^{-1} b = x \) instead of \( ax = b \).)",computations with matrices
"find the norms \( \lambda_{\max} \) and condition numbers \( \frac{\lambda_{\max}}{\lambda_{\min}} \) of these positive definite matrices:
    \[
    \begin{pmatrix} 100 & 0 \\ 0 & 2 \end{pmatrix}, \quad
    \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \quad
    \begin{pmatrix} 3 & 1 \\ 1 & 1 \end{pmatrix}.
    \]",computations with matrices
"orthogonal matrices have norm \( \|q\| = 1 \). if \( a = qr \), show that \( \|a\| \leq \|r\| \) and also \( \|r\| \leq \|a\| \). then \( \|a\| = \|q\| \|r\| \). find an example of \( a = lu \) with \( \|a\| < \|l\| \|u\| \).",computations with matrices
"(suggested by moler and van loan) compute \( b - ay \) and \( b - az \) when
    \[
    b = \begin{pmatrix} 0.217 \\ 0.254 \end{pmatrix}, \quad
    a = \begin{pmatrix} 0.780 & 0.563 \\ 0.913 & 0.659 \end{pmatrix}, \quad
    y = \begin{pmatrix} 0.341 \\ -0.087 \end{pmatrix}, \quad
    z = \begin{pmatrix} 0.999 \\ -1.0 \end{pmatrix}.
    \]
    is \( y \) closer than \( z \) to solving \( ax = b \)? answer in two ways: compare the residuals \( b - ay \) and \( b - az \). then compare \( y \) and \( z \) to the true \( x = (1, -1) \). sometimes we want a small residual, sometimes a small \( \delta x \).",computations with matrices
"the “\( \ell_1 \) norm” is \( \|x\|_1 = |x_1| + \cdots + |x_n| \). the “\( \ell_\infty \) norm” is \( \|x\|_\infty = \max |x_i| \). compute \( \|x\|_2 \), \( \|x\|_1 \), and \( \|x\|_\infty \) for the vectors
    \[
    x = (1, 1, 1, 1, 1) \quad \text{and} \quad x = (0.1, 0.7, 0.3, 0.4, 0.5).
    \]",computations with matrices
"prove that \( \|x\|_\infty \leq \|x\|_2 \leq \|x\|_1 \). show from the cauchy-schwarz inequality that the ratios \( \|x\|_2 / \|x\|_\infty \) and \( \|x\|_1 / \|x\|_2 \) are never larger than \( \sqrt{n} \). which vector \( (x_1, \dots, x_n) \) gives ratios equal to \( \sqrt{n} \)?",computations with matrices
"all vector norms must satisfy the triangle inequality. prove that
    \[
    \|x + y\|_\infty \leq \|x\|_\infty + \|y\|_\infty \quad \text{and} \quad \|x + y\|_1 \leq \|x\|_1 + \|y\|_1.
    \]",computations with matrices
"compute the exact inverse of the hilbert matrix \( a \) by elimination. then compute \( a^{-1} \) again by rounding all numbers to three significant figures:
    \[
    a = \begin{pmatrix}
    1 & 1/2 & 1/3 \\
    1/2 & 1/3 & 1/4 \\
    1/3 & 1/4 & 1/5
    \end{pmatrix}.
    \]",computations with matrices
"for the same \( a \), compute \( b = ax \) for \( x = (1, 1, 1) \) and \( x = (0.6, -3.6, 0) \). a small change \( \delta b \) produces a large change \( \delta x \).",computations with matrices
"compute \( \lambda_{\max} \) and \( \lambda_{\min} \) for the 8 by 8 hilbert matrix \( a_{ij} = \frac{1}{i + j - 1} \). if \( ax = b \) with \( \|b\| = 1 \), how large can \( \|x\| \) be? if \( b \) has roundoff error less than \( 10^{-16} \), how large an error can this cause in \( x \)?",computations with matrices
"if you know \( l \), \( u \), \( q \), and \( r \), is it faster to solve \( lux = b \) or \( qrx = b \)?",computations with matrices
"choosing the largest available pivot in each column (partial pivoting), factor each \( a \) into \( pa = lu \):
    \[
    a = \begin{pmatrix} 1 & 0 \\ 2 & 2 \end{pmatrix} \quad \text{and} \quad
    a = \begin{pmatrix} 1 & 0 & 1 \\ 2 & 2 & 0 \\ 0 & 2 & 0 \end{pmatrix}.
    \]",computations with matrices
"find the lu factorization of \( a = \begin{pmatrix} \epsilon & 1 \\ 1 & 1 \end{pmatrix} \). on your computer, solve by elimination when \( \epsilon = 10^{-3}, 10^{-6}, 10^{-9}, 10^{-12}, 10^{-15} \):
    \[
    \begin{pmatrix} \epsilon & 1 \\ 1 & 1 \end{pmatrix}
    \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}
    =
    \begin{pmatrix} 1 + \epsilon \\ 2 \end{pmatrix}.
    \]
    the true solution is \( (1, 1) \). make a table to show the error for each \( \epsilon \). exchange the two equations and solve again—the errors should almost disappear.",computations with matrices
"show that starting from \( a_0 = \begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix} \), the unshifted qr algorithm produces only the modest improvement
    \[
    a_1 = \frac{1}{5} \begin{pmatrix} 14 & -3 \\ -3 & 6 \end{pmatrix}.
    \]",computations with matrices
"apply to the following matrix \( a \) a single qr step with the shift \( \alpha = a_{22} \)—which in this case means without shift, since \( a_{22} = 0 \). show that the off-diagonal entries go from \( \sin \theta \) to \( -\sin 3\theta \), which is cubic convergence.
    \[
    a = \begin{pmatrix} \cos \theta & \sin \theta \\ \sin \theta & 0 \end{pmatrix}.
    \]",computations with matrices
check that the tridiagonal matrix \( a = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \) is left unchanged by the qr algorithm. it is one of the (rare) counterexamples to convergence (so we shift).,computations with matrices
"show by induction that, without shifts, \( (q_0 q_1 \cdots q_k)(r_k \cdots r_1 r_0) \) is exactly the qr factorization of \( a_{k+1} \). this identity connects qr to the power method and leads to an explanation of its convergence. if \( |\lambda_1| > |\lambda_2| > \cdots > |\lambda_n| \), these eigenvalues will gradually appear on the main diagonal.",computations with matrices
"choose \( \sin \theta \) and \( \cos \theta \) in the rotation \( p_{21} \) to triangularize \( a \), and find \( r \):
    \[
    p_{21} a = \begin{pmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{pmatrix} \begin{pmatrix} 1 & -1 \\ 3 & 5 \end{pmatrix} = \begin{pmatrix} * & * \\ 0 & * \end{pmatrix} = r.
    \]",computations with matrices
choose \( \sin \theta \) and \( \cos \theta \) to make \( p_{21} a p_{21}^{-1} \) triangular (same \( a \)). what are the eigenvalues?,computations with matrices
"when \( a \) is multiplied by \( p_{ij} \) (plane rotation), which entries are changed? when \( p_{ij} a \) is multiplied on the right by \( p_{ij}^{-1} \), which entries are changed now?",computations with matrices
"how many multiplications and how many additions are used to compute \( pa \)? (a careful organization of all the rotations gives \( 2^3 n^3 \) multiplications and additions, the same as for qr by reflectors and twice as many as for lu.)",computations with matrices
"(turning a robot hand) a robot produces any 3 by 3 rotation \( a \) from plane rotations around the \( x \), \( y \), and \( z \) axes. if \( p_{32} p_{31} p_{21} a = i \), the three robot turns are in \( a = p_{21}^{-1} p_{31}^{-1} p_{32}^{-1} \). the three angles are euler angles. choose the first \( \theta \) so that
    \[
    p_{21} a = \begin{pmatrix} \cos \theta & -\sin \theta & 0 \\ \sin \theta & \cos \theta & 0 \\ 0 & 0 & 1 \end{pmatrix} \frac{1}{2} \begin{pmatrix} -1 & 2 & 2 \\ 2 & -1 & 2 \\ 2 & 2 & -1 \end{pmatrix}
    \]
    is zero in the \( (2,1) \) position.",computations with matrices
"this matrix has eigenvalues \( 2 - \sqrt{2} \), \( 2 \), and \( 2 + \sqrt{2} \):
    \[
    a = \begin{pmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{pmatrix}.
    \]
    find the jacobi matrix \( d^{-1}(-l - u) \) and the gauss-seidel matrix \( (d + l)^{-1}(-u) \), and their eigenvalues, and the numbers \( \omega_{\text{opt}} \) and \( \lambda_{\text{max}} \) for sor.",computations with matrices
"for this \( n \times n \) matrix, describe the jacobi matrix \( j = d^{-1}(-l - u) \):
    \[
    a = \begin{pmatrix} 2 & -1 & \cdots & 0 \\ -1 & \ddots & \ddots & \vdots \\ \vdots & \ddots & \ddots & -1 \\ 0 & \cdots & -1 & 2 \end{pmatrix}.
    \]
    show that the vector 
    \[
    x_1 = (\sin \pi h, \sin 2 \pi h, \dots, \sin n \pi h)
    \]
    is an eigenvector of \( j \) with eigenvalue \( \lambda_1 = \cos \pi h = \cos \frac{\pi}{n+1} \).",computations with matrices
"in problem 2, show that 
    \[
    x_k = (\sin k \pi h, \sin 2k \pi h, \dots, \sin nk \pi h)
    \]
    is an eigenvector of \( a \). multiply \( x_k \) by \( a \) to find the corresponding eigenvalue \( \alpha_k \). verify that in the \( 3 \times 3 \) case these eigenvalues are \( 2 - \sqrt{2} \), \( 2 \), and \( 2 + \sqrt{2} \).",computations with matrices
"note: the eigenvalues of the jacobi matrix 
    \[
    j = \frac{1}{2}(-l - u) = i - \frac{1}{2}a
    \]
    are 
    \[
    \lambda_k = 1 - \frac{1}{2} \alpha_k = \cos k \pi h.
    \]
    they occur in plus-minus pairs, and \( \lambda_{\text{max}} \) is \( \cos \pi h \).",computations with matrices
"problems 4–5 require gershgorin's ""circle theorem"": every eigenvalue of \( a \) lies in at least one of the circles \( c_1, \dots, c_n \), where \( c_i \) has its center at the diagonal entry \( a_{ii} \). its radius \( r_i = \sum_{j \neq i} |a_{ij}| \) is equal to the absolute sum along the rest of the row. proof: suppose \( x_i \) is the largest component of \( x \). then \( ax = \lambda x \) leads to
    \[
    (\lambda - a_{ii}) x_i = \sum_{j \neq i} a_{ij} x_j,
    \]
    or
    \[
    |\lambda - a_{ii}| \leq \sum_{j \neq i} |a_{ij}| \frac{|x_j|}{|x_i|} \leq \sum_{j \neq i} |a_{ij}| = r_i.
    \]",computations with matrices
"the matrix 
    \[
    a = \begin{pmatrix}
    3 & 1 & 1 \\
    0 & 4 & 1 \\
    2 & 2 & 5
    \end{pmatrix}
    \]
    is called \emph{diagonally dominant} because every \( |a_{ii}| > r_i \) (where \( r_i = \sum_{j \neq i} |a_{ij}| \)). show that zero cannot lie in any of the gershgorin circles and conclude that \( a \) is nonsingular.",computations with matrices
"write the jacobi matrix \( j \) for the diagonally dominant \( a \) of problem 4, and find the three gershgorin circles for \( j \). show that all the radii satisfy \( r_i < 1 \), and that the jacobi iteration converges.",computations with matrices
"the true solution to \( ax = b \) is slightly different from the elimination solution to \( lux_0 = b \); \( a - lu \) misses zero because of roundoff. one strategy is to do everything in double precision, but a better and faster way is iterative refinement: compute only one vector
    \[
    r = b - ax_0
    \]
    in double precision, solve \( luy = r \), and add the correction \( y \) to \( x_0 \). \\
    \textbf{problem:} multiply \( x_1 = x_0 + y \) by \( lu \), write the result as a splitting
    \[
    sx_1 = tx_0 + b,
    \]
    and explain why \( t \) is extremely small. this single step brings us almost exactly to \( x \).",computations with matrices
"for a general \( 2 \times 2 \) matrix
    \[
    a = \begin{pmatrix} a & b \\ c & d \end{pmatrix},
    \]
    find the jacobi iteration matrix
    \[
    s^{-1}t = -d^{-1}(l + u)
    \]
    and its eigenvalues \( \mu_i \). find also the gauss-seidel matrix
    \[
    -(d + l)^{-1}u
    \]
    and its eigenvalues \( \lambda_i \), and decide whether \( \lambda_{\max} = \mu_{\max}^2 \).",computations with matrices
"change \( ax = b \) to
    \[
    x = (i-a)x + b.
    \]
    what are \( s \) and \( t \) for this splitting? what matrix \( s^{-1}t \) controls the convergence of the iteration
    \[
    x_{k+1} = (i-a)x_k + b \, ?
    \]",computations with matrices
"if \( \lambda \) is an eigenvalue of \( a \), then \( \mu = 1 - \lambda \) is an eigenvalue of \( b = i - a \). the real eigenvalues of \( b \) have absolute value less than 1 if the real eigenvalues of \( a \) lie between two numbers. (assume in this context that the real eigenvalues of \( a \) lie between 0 and 2.)",computations with matrices
"show why the iteration
    \[
    x_{k+1} = (i-a)x_k + b
    \]
    does not converge for 
    \[
    a = \begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix}.
    \]",computations with matrices
"why is the norm of \(b^k\) never larger than \(\|b\|^k\)? then \(\|b\| < 1\) guarantees that the powers \(b^k\) approach zero (convergence). this is no surprise, since \(|\lambda|_{\max}\) is below \(\|b\|\).",computations with matrices
"if \(a\) is singular, then all splittings \(a = s - t\) must fail. from \(ax = 0\), show that
    \[
    s^{-1}t x = x.
    \]
    thus, the matrix \(b = s^{-1}t\) has an eigenvalue \(\lambda = 1\) and fails.",computations with matrices
"change the 2s to 3s and find the eigenvalues of \(s^{-1}t\) for both methods:
    \begin{itemize}",computations with matrices
"\textbf{jacobi (j):} 
        \[
        \begin{pmatrix} 3 & 0 \\ 0 & 3 \end{pmatrix} x_{k+1} = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} x_k + b.
        \]",computations with matrices
"\textbf{gauss-seidel (gs):} 
        \[
        \begin{pmatrix} 3 & 0 \\ -1 & 3 \end{pmatrix} x_{k+1} = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} x_k + b.
        \]
    \end{itemize}
    does \(|\lambda|_{\max}\) for gauss-seidel equal \(|\lambda|_{\max}^2\) for jacobi?",computations with matrices
"write a computer code (matlab or other) for gauss-seidel. you can define \(s\) and \(t\) from \(a\), or set up the iteration loop directly from the entries \(a_{ij}\). test it on the \(-1,\,2,\,-1\) matrices \(a\) of order 10, 20, and 50, with \(b = (1,0,\dots,0)\).",computations with matrices
"the sor splitting matrix \(s\) is the same as for gauss-seidel except that the diagonal is divided by \(\omega\). write a program for sor on an \(n \times n\) matrix. apply it with \(\omega = 1,\;1.4,\;1.8,\;2.2\) when \(a\) is the \(-1,\,2,\,-1\) matrix of order 10.",computations with matrices
"when \(a = a^t\), the arnoldi-lanczos method finds orthonormal vectors \(q_j\) such that
    \[
    aq_j = b_{j-1}q_{j-1} + a_jq_j + b_jq_{j+1} \quad (q_0 = 0).
    \]
    multiply by \(q_j^t\) to derive a formula for \(a_j\). this equation indicates that \(aq = q^t t\), where \(t\) is a tridiagonal matrix.",computations with matrices
"what bound on \(|\lambda|_{\max}\) does gershgorin's circle theorem give for the following matrices? what are the three gershgorin circles that contain all the eigenvalues?
    \[
    a_1 = \begin{pmatrix} 0.3 & 0.3 & 0.2 \\ 0.3 & 0.2 & 0.4 \\ 0.2 & 0.4 & 0.1 \end{pmatrix}, \quad
    a_2 = \begin{pmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{pmatrix}.
    \]
    the key point for large matrices is that matrix-vector multiplication is much faster than matrix-matrix multiplication. a crucial construction starts with a vector \(b\) and computes \(ab,\, a^2b,\, \dots\) (but never \(a^2\)!); the first \(n\) vectors span the \(n\)th krylov subspace. they are the columns of the krylov matrix
    \[
    k_n = \begin{bmatrix} b & ab & a^2b & \cdots & a^{n-1}b \end{bmatrix}.
    \]
    the arnoldi-lanczos iteration orthogonalizes the columns of \(k_n\), and the conjugate gradient iteration solves \(ax = b\) when \(a\) is symmetric positive definite.",computations with matrices
"\textbf{arnoldi iteration \quad conjugate gradient iteration}

    \medskip
    \textbf{arnoldi iteration:} \\
    set 
    \[
    q_1 = \frac{b}{\|b\|}.
    \]
    for \( n = 1 \) to \( n-1 \):
    \begin{itemize}",computations with matrices
compute \( v = aq_n \).,computations with matrices
"for \( j = 1 \) to \( n \), compute
        \[
        h_{j,n} = q_j^t v.
        \]",computations with matrices
"update
        \[
        v \leftarrow v - \sum_{j=1}^{n} h_{j,n} q_j.
        \]",computations with matrices
"set
        \[
        q_{n+1} = \frac{v}{\|v\|}.
        \]
    \end{itemize}
    
    \medskip
    \textbf{conjugate gradient iteration:} \\
    initialize 
    \[
    x_0 = 0,\quad r_0 = b,\quad p_0 = r_0.
    \]
    for \( n = 1 \) to \( n \):
    \begin{itemize}",computations with matrices
"compute the step length
        \[
        \alpha_n = \frac{r_{n-1}^t r_{n-1}}{p_{n-1}^t a p_{n-1}}.
        \]",computations with matrices
"update the approximate solution:
        \[
        x_n = x_{n-1} + \alpha_n\, p_{n-1}.
        \]",computations with matrices
"compute the new residual:
        \[
        r_n = r_{n-1} - \alpha_n\, a p_{n-1}.
        \]",computations with matrices
"compute the improvement factor:
        \[
        \beta_n = \frac{r_n^t r_n}{r_{n-1}^t r_{n-1}}.
        \]",computations with matrices
"update the search direction:
        \[
        p_n = r_n + \beta_n\, p_{n-1}.
        \]
    \end{itemize}
    
    \noindent
    \textbf{note:} only one matrix-vector multiplication with \( a \) (applied to \( q_n \) or \( p_{n-1} \)) is required in each iteration.",computations with matrices
"in arnoldi, show that \( q_2 \) is orthogonal to \( q_1 \). (hint: the arnoldi method is simply gram--schmidt orthogonalization applied to the krylov matrix 
    \[
    k_n = \begin{bmatrix} b & ab & a^2b & \cdots & a^{n-1}b \end{bmatrix}
    \]
    with \( k_n = q_n r_n \). moreover, the eigenvalues of \( q_n^t a q_n \) are often very close to those of \( a \), even for \( n > n \). note that the lanczos iteration is just arnoldi specialized for symmetric matrices, and all of these methods are implemented in arpack.)",computations with matrices
"in conjugate gradients, show that the residual \( r_1 \) is orthogonal to \( r_0 \) (i.e. the residuals are orthogonal), and that the search directions satisfy
    \[
    p_0^t a p_0 = 0.
    \]
    explain why the iteration solves \( ax = b \) by minimizing the error \( e^t a e \) in the krylov subspace. (this is one of the remarkable properties of the conjugate gradient method.)",computations with matrices
"consider a $10000 \times 10000$ dense matrix stored in row-major order. assuming each entry is a double-precision floating-point number (8 bytes), determine the memory required to store the matrix. if the matrix is instead stored in a sparse format with only $0.01\%$ nonzero entries, estimate the memory usage for the compressed sparse row (csr) format.",computations with matrices
"explain the trade-offs between dense and sparse matrix storage formats in terms of memory efficiency, computational complexity, and ease of implementation. provide examples where one format significantly outperforms the other.",computations with matrices
"given a block storage representation of a matrix where each block is stored separately, analyze the benefits and drawbacks of such a structure in terms of memory fragmentation, cache efficiency, and parallel computation.",computations with matrices
suppose you have a large-scale linear system $ax = b$ where $a$ is sparse with a banded structure. discuss the optimal memory storage technique for $a$ and its impact on the efficiency of solving the system.,computations with matrices
discuss the advantages of using hierarchical matrices ($h$-matrices) for storing large dense matrices in scientific computing. how do these structures reduce memory usage while preserving computational efficiency?,computations with matrices
derive the memory complexity of storing a symmetric positive definite matrix using the cholesky decomposition in dense and sparse formats. compare the savings achieved when using sparse representations.,computations with matrices
consider a large sparse matrix arising from the discretization of a partial differential equation (pde). explain how domain decomposition techniques can be leveraged for memory-efficient storage and parallel computation.,computations with matrices
"in the context of large matrix operations, explain how out-of-core memory management techniques, such as paging and buffering, help handle matrices that exceed available ram. illustrate with an example.",computations with matrices
"suppose a high-performance computing (hpc) application requires efficient memory allocation for storing and manipulating large adjacency matrices of graphs. compare the use of adjacency lists, csr, and adjacency matrices in terms of memory footprint and retrieval efficiency.",computations with matrices
analyze the impact of cache locality on the performance of matrix-vector multiplications for large matrices stored in row-major and column-major formats. provide empirical evidence or theoretical justification for your claims.,computations with matrices
explain the structure and memory benefits of using the block compressed row storage (bcrs) format for large sparse matrices with a block-diagonal structure. when is bcrs preferable to csr?,computations with matrices
consider a $n \times n$ matrix with only $o(n)$ nonzero entries per row. derive the memory complexity of storing such a matrix in csr format and compare it with naive dense storage.,computations with matrices
explain the significance of floating-point precision in matrix storage. how does using single-precision versus double-precision arithmetic affect memory consumption and computational stability?,computations with matrices
"suppose a machine learning model requires storing an extremely large covariance matrix. how can low-rank approximations (e.g., singular value decomposition, cur decomposition) help in reducing memory consumption?",computations with matrices
discuss the impact of memory bandwidth on large-scale matrix computations and suggest techniques to minimize memory bottlenecks in modern computing architectures.,computations with matrices
"explain how tensor decomposition techniques (e.g., tucker decomposition, cp decomposition) can be used to store and manipulate large-scale data efficiently. provide a concrete example where such a method is beneficial.",computations with matrices
"given a large sparse matrix with $n^2$ elements but only $o(n)$ nonzero values, discuss efficient memory allocation strategies to avoid overallocation.",computations with matrices
how does handling overflow and underflow in matrix computations impact numerical stability? provide real-world examples where such issues arise and discuss mitigation strategies.,computations with matrices
explain the role of compressed storage formats such as run-length encoding (rle) in reducing the memory footprint of structured matrices. what are the limitations of rle for general sparse matrices?,computations with matrices
"suppose an iterative solver like conjugate gradient (cg) is used for solving large sparse linear systems. explain how memory-efficient preconditioning techniques, such as incomplete lu (ilu) and jacobi preconditioning, can optimize storage requirements.",computations with matrices
compare the memory efficiency of storing a matrix using coordinate (coo) format versus csr format. derive conditions under which one format is preferable to the other.,computations with matrices
discuss the trade-offs between explicit and implicit storage of large permutation matrices in numerical linear algebra applications.,computations with matrices
"consider a markov transition matrix that is both large and sparse. what memory-efficient storage schemes would you recommend, and why?",computations with matrices
"in large-scale simulations involving finite element methods (fem), how can sparsity patterns be exploited to optimize memory usage?",computations with matrices
explain the impact of matrix sparsity on the performance of iterative solvers versus direct solvers. how does memory access pattern influence computational efficiency?,computations with matrices
"how do modern parallel computing architectures (e.g., gpus) handle large matrix storage and memory bandwidth constraints? discuss techniques such as shared memory, tiling, and memory coalescing.",computations with matrices
consider a toeplitz matrix stored in memory. how can its special structure be used to optimize storage and computation for matrix-vector multiplication?,computations with matrices
suppose you need to store and manipulate a sequence of time-dependent sparse matrices. what strategies would you use to minimize memory overhead while ensuring efficient access to historical data?,computations with matrices
discuss the role of low-memory algorithms such as stochastic gradient descent (sgd) in handling extremely large data matrices in machine learning applications.,computations with matrices
"consider a quantum computing simulation that involves exponentially large matrices. how can tensor networks (e.g., matrix product states) be used to efficiently store and manipulate such matrices?",computations with matrices
consider a large $n \times n$ matrix $a$ that needs to be multiplied with another matrix $b$ of the same dimension. describe in detail how the strassen algorithm can be parallelized for multi-core processors. discuss the advantages and limitations of this approach in terms of computational complexity and memory usage.,computations with matrices
"in a distributed computing environment, the matrix-vector multiplication $ax = b$ is performed using row-wise partitioning across $p$ processors. derive the communication cost and computational complexity of this approach and discuss the trade-offs between communication and computation.",computations with matrices
"describe how matrix inversion can be efficiently implemented using parallel computing techniques. compare the performance of lu decomposition, gauss-jordan elimination, and block matrix inversion in a multi-core system.",computations with matrices
"explain the concept of load balancing in parallel matrix computations. given a heterogeneous computing environment with processors of varying speeds, formulate an optimization problem to achieve optimal load balancing when computing $c = ab$.",computations with matrices
consider a sparse matrix $a$ distributed across multiple gpus. how can the sparse matrix-vector product (spmv) be optimized to achieve high performance? discuss trade-offs between memory access patterns and computational throughput.,computations with matrices
"define and analyze the impact of amdahl’s law in parallel matrix operations. for a given matrix multiplication problem where 80\% of the execution time can be parallelized, determine the theoretical speedup achievable with 16 processors.",computations with matrices
"in a distributed cloud computing environment, discuss how matrix factorization techniques (such as qr or svd) can be efficiently implemented. explain the challenges of data communication and how they can be mitigated.",computations with matrices
explain the concept of gpu-accelerated matrix computations. compare the advantages and disadvantages of using a gpu versus a cpu for computing eigenvalues and eigenvectors of a large symmetric matrix.,computations with matrices
consider a block matrix $a$ that is partitioned into submatrices for parallel processing. explain how an optimal block size can be chosen to balance computation and communication overhead in a distributed memory system.,computations with matrices
describe the synchronization challenges faced in parallel cholesky decomposition. what strategies can be employed to minimize synchronization overhead?,computations with matrices
discuss how pipelining techniques can be applied to improve the performance of matrix-matrix multiplication in a distributed computing system.,computations with matrices
"in the context of large-scale machine learning, explain how distributed matrix computations are used for training deep neural networks. discuss how gpus and tpus are leveraged for optimizing matrix operations.",computations with matrices
"given an $n \times n$ dense matrix $a$, describe how a mapreduce framework can be used to compute its matrix-vector product efficiently. explain the role of mappers and reducers in this computation.",computations with matrices
derive an efficient parallel algorithm for computing the determinant of a large sparse matrix. analyze its computational complexity and discuss how it can be implemented on a shared-memory system.,computations with matrices
"in the context of matrix operations on a cloud computing platform, explain the trade-offs between latency, bandwidth, and computational efficiency when processing a large matrix dataset.",computations with matrices
consider a system of linear equations $ax = b$ where $a$ is a large distributed matrix. discuss iterative methods such as jacobi and gauss-seidel that can be parallelized. compare their convergence properties.,computations with matrices
explain the concept of matrix tiling and discuss its effectiveness in improving cache efficiency in multi-threaded matrix computations.,computations with matrices
"given a highly parallelized matrix multiplication operation, explain the impact of memory latency and bandwidth on the overall performance. propose strategies to mitigate these bottlenecks.",computations with matrices
how can matrix transpose be performed efficiently in a parallel computing environment? discuss various algorithms and their trade-offs in terms of performance and memory access.,computations with matrices
analyze the role of cuda and opencl in parallel matrix computations. compare their programming models and performance characteristics for matrix multiplication.,computations with matrices
discuss fault tolerance mechanisms in distributed matrix computations. what strategies can be employed to ensure reliability in the event of node failures?,computations with matrices
"compare different load balancing strategies (e.g., static vs dynamic) for parallelizing large-scale matrix operations. provide examples where each strategy would be most beneficial.",computations with matrices
explain the concept of communication-avoiding algorithms in matrix computations. how do they improve performance in distributed systems?,computations with matrices
"given a $10^6 \times 10^6$ matrix stored across a distributed system, analyze the impact of network bandwidth and memory constraints on performing lu decomposition.",computations with matrices
discuss the parallelization of the power iteration method for computing the dominant eigenvalue of a large sparse matrix.,computations with matrices
explain how the concept of divide-and-conquer can be applied to matrix operations such as inversion and decomposition in a parallel computing setup.,computations with matrices
"given a distributed computing cluster with $p$ nodes, formulate an optimal scheduling strategy to minimize the total execution time for computing $c = a^tb$.",computations with matrices
discuss how hybrid parallelism (combining shared and distributed memory models) can be used to optimize large-scale matrix computations.,computations with matrices
compare the efficiency of matrix operations in single-node multi-threading environments versus distributed multi-node environments. what are the key factors affecting performance?,computations with matrices
consider a high-performance computing system executing parallel matrix computations. discuss strategies for optimizing energy efficiency while maintaining computational throughput.,computations with matrices
consider a large sparse matrix $a$ of size $n \times n$ with sparsity $s$. describe the computational complexity of performing matrix-vector multiplication $ax = b$ using a naive dense representation versus a compressed sparse row (csr) format. discuss under what conditions the csr format provides a computational advantage.,computations with matrices
"explain the key differences between compressed sparse row (csr), compressed sparse column (csc), and coordinate (coo) storage formats for sparse matrices. derive the memory requirements for storing an $n \times n$ sparse matrix with $k$ nonzero elements in each of these formats.",computations with matrices
"discuss how the structure of a sparse matrix affects the performance of sparse matrix-matrix multiplication (spmm). provide examples where different sparsity patterns (e.g., banded, block diagonal, unstructured) lead to different computational complexities.",computations with matrices
"the efficiency of sparse cholesky factorization heavily depends on the matrix's sparsity pattern. given a sparse symmetric positive definite matrix, describe how reordering strategies such as approximate minimum degree (amd) and nested dissection can improve performance.",computations with matrices
"derive an optimal algorithm for computing the product of two sparse matrices $c = ab$, where $a$ and $b$ are stored in compressed sparse row (csr) format. discuss how to minimize computational overhead and memory usage.",computations with matrices
"explain the role of fill-in during lu factorization of a sparse matrix. given a specific sparsity pattern, describe how fill-in can be minimized through reordering techniques such as cuthill-mckee.",computations with matrices
"given an $n \times n$ sparse matrix $a$ with only $o(n)$ nonzero elements, discuss efficient iterative methods (e.g., jacobi, gauss-seidel, conjugate gradient) for solving $ax = b$. compare their convergence rates and computational complexities.",computations with matrices
describe the memory access patterns involved in sparse matrix-vector multiplication (spmv) when using the csr format. how can cache blocking and prefetching improve computational performance?,computations with matrices
"explain how graph theory can be used to analyze sparsity patterns in matrices. given a matrix $a$, define its adjacency graph and describe how graph partitioning techniques help in optimizing parallel sparse computations.",computations with matrices
"discuss the trade-offs between direct solvers (e.g., sparse lu, cholesky) and iterative solvers (e.g., gmres, bicgstab) for solving large sparse linear systems. when is each method preferable?",computations with matrices
"consider the sparse eigenvalue problem $ax = \lambda x$, where $a$ is a large sparse symmetric matrix. describe how krylov subspace methods such as the lanczos algorithm efficiently compute a few dominant eigenvalues.",computations with matrices
how does the sparsity pattern of a matrix affect the numerical stability of sparse direct solvers? provide examples where poor conditioning arises due to sparsity structure.,computations with matrices
"discuss the role of preconditioning in solving sparse linear systems. compare different preconditioners (e.g., jacobi, incomplete lu, multigrid) in terms of effectiveness and computational cost.",computations with matrices
"given a sparse matrix $a$ stored in csr format, analyze the impact of parallelizing the sparse matrix-vector multiplication operation. discuss load balancing and data locality challenges.",computations with matrices
describe an efficient method for transposing a sparse matrix stored in csr format. how does the complexity compare to transposing a dense matrix?,computations with matrices
analyze the impact of different sparse matrix formats on performance when performing spmv on gpu architectures. which format is generally preferable for cuda-based implementations?,computations with matrices
consider a scientific computing application where a large sparse jacobian matrix must be computed and stored. discuss strategies for efficiently handling sparsity when computing derivatives using automatic differentiation.,computations with matrices
derive a method to efficiently compute the inverse of a large sparse triangular matrix stored in csr format. discuss when explicit inversion is necessary and when it should be avoided.,computations with matrices
explain the concept of blocking in sparse matrix computations. how does it improve performance in modern cpu and gpu architectures?,computations with matrices
the conjugate gradient method is often preferred for solving large sparse positive-definite systems. prove its convergence properties and derive its complexity for an $n \times n$ sparse matrix with $o(n)$ nonzeros.,computations with matrices
"describe the impact of sparsity patterns on parallel sparse matrix algorithms. given a block-diagonal matrix structure, how can thread synchronization be minimized?",computations with matrices
discuss the role of matrix reordering in reducing computational complexity of sparse triangular solves. compare the reverse cuthill-mckee and minimum degree algorithms.,computations with matrices
explain the computational challenges of performing lu decomposition on highly sparse matrices. how does the supernodal approach improve performance in sparse lu factorizations?,computations with matrices
"given a real-world application such as finite element analysis (fea), describe how sparsity in the system matrix is leveraged to optimize computations.",computations with matrices
explain the impact of matrix sparsity on iterative solvers such as gmres and bicgstab. how does the condition number of a sparse matrix influence convergence?,computations with matrices
discuss hybrid approaches that combine direct and iterative methods for solving sparse linear systems. provide examples where hybridization leads to significant performance gains.,computations with matrices
"given a symmetric sparse matrix with bandwidth $b$, analyze the computational complexity of performing cholesky factorization. compare the complexity with that of a full dense matrix.",computations with matrices
describe the impact of matrix compression techniques such as hierarchical matrices (h-matrices) and tensor train decomposition on optimizing sparse computations.,computations with matrices
"how do adaptive sparse matrix storage formats (e.g., adaptive csr, adaptive blocking) dynamically switch between different storage formats based on sparsity patterns? discuss their advantages in high-performance computing.",computations with matrices
"in deep learning, sparse weight matrices are often used to reduce computational cost. describe how pruning techniques affect sparse matrix computations and propose an efficient algorithm for performing sparse-dense matrix multiplications in neural network training.",computations with matrices
consider an $n \times n$ invertible matrix $a$. discuss the computational complexity of computing its inverse using gaussian elimination versus lu decomposition. under what conditions is lu decomposition more efficient?,computations with matrices
"describe the role of pivoting in gaussian elimination for matrix inversion. explain how partial pivoting and complete pivoting improve numerical stability, and analyze their impact on the computational complexity.",computations with matrices
"compare the efficiency of lu decomposition, qr decomposition, and singular value decomposition (svd) for solving linear systems $ax = b$. under what circumstances is each method preferable?",computations with matrices
"given a symmetric positive definite matrix $a$, explain why cholesky decomposition $a = ll^t$ is preferred over lu decomposition. provide a proof of existence and discuss computational advantages.",computations with matrices
discuss the concept of matrix condition number $\kappa(a) = \|a\| \|a^{-1}\|$ in the context of numerical stability in matrix inversion. how does $\kappa(a)$ affect the accuracy of computed solutions?,computations with matrices
"prove that for any invertible matrix $a$, the inverse can be computed using the cayley-hamilton theorem and the characteristic equation of $a$. under what conditions is this method practical?",computations with matrices
the moore-penrose pseudoinverse $a^+$ is used for inverting singular or non-square matrices. derive $a^+$ using singular value decomposition (svd) and explain its applications in least-squares problems.,computations with matrices
discuss strassen’s algorithm for matrix inversion and analyze its computational complexity. how does it compare to the standard gaussian elimination method?,computations with matrices
define the concept of a stable algorithm in matrix decomposition. provide examples of numerically stable and unstable algorithms for computing matrix inverses.,computations with matrices
"suppose $a$ is an $n \times n$ matrix that is nearly singular. describe techniques for handling its inversion numerically, including regularization methods such as tikhonov regularization.",computations with matrices
"given a large sparse matrix $a$ stored in compressed sparse row (csr) format, analyze the computational trade-offs between direct inversion and iterative solvers for solving $ax = b$.",computations with matrices
"explain the difference between full qr decomposition and reduced qr decomposition. when solving $ax = b$ for an overdetermined system, why is the reduced form of qr preferred?",computations with matrices
describe how the sherman-morrison formula can be used to efficiently compute the inverse of a matrix when a rank-one update is applied. derive the formula and discuss its applications.,computations with matrices
the woodbury matrix identity provides an efficient way to compute the inverse of a perturbed matrix. derive the identity and explain its use in large-scale systems.,computations with matrices
"prove that if $a$ is an orthogonal matrix, then $a^{-1} = a^t$. discuss the implications of this property for numerical computation and stability in solving linear systems.",computations with matrices
describe the role of householder reflections in qr decomposition. compare them to the gram-schmidt process in terms of numerical stability and computational efficiency.,computations with matrices
discuss the computational cost of performing an lu decomposition on a dense $n \times n$ matrix. compare this cost with the computational cost of direct matrix inversion.,computations with matrices
consider a diagonalizable matrix $a = pdp^{-1}$. explain how this decomposition can be used to efficiently compute $a^{-1}$. under what conditions does this method become unstable?,computations with matrices
explain the role of the condition number $\kappa(a)$ in cholesky decomposition. how does ill-conditioning affect the computed factors in $a = ll^t$?,computations with matrices
the jacobi and gauss-seidel iterative methods are often used as alternatives to direct inversion. analyze the convergence criteria for these methods and discuss when they outperform direct methods.,computations with matrices
"suppose $a$ is a symmetric matrix with eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_n$. show that the inverse $a^{-1}$ exists if and only if all $\lambda_i \neq 0$, and express $a^{-1}$ in terms of its eigenvalues and eigenvectors.",computations with matrices
"describe the impact of parallelization on matrix decomposition algorithms. compare cpu-based and gpu-based implementations of lu, qr, and svd.",computations with matrices
"given a block matrix
          \[
          a =
          \begin{bmatrix}
              b & c \\
              d & e
          \end{bmatrix}
          \]
          where $b, c, d, e$ are submatrices, derive the formula for computing $a^{-1}$ using the schur complement.",computations with matrices
"explain the difference between the ldl decomposition and cholesky decomposition for symmetric matrices. when is ldl preferred, and how does it affect numerical stability?",computations with matrices
"given a low-rank matrix $a$, explain how rank-revealing qr decomposition can be used to efficiently approximate $a^{-1}$. discuss its applications in data compression.",computations with matrices
suppose we need to solve $ax = b$ multiple times for different vectors $b$. explain why lu decomposition with forward and backward substitution is more efficient than computing $a^{-1}$ explicitly.,computations with matrices
"given a rectangular matrix $a$ of rank $r$, explain how svd provides an optimal low-rank approximation of $a$. how is this property used in practical applications such as image compression?",computations with matrices
discuss how iterative refinement can be used to improve the accuracy of a computed inverse $a^{-1}$. derive an expression for the error reduction per iteration.,computations with matrices
"suppose a large, dense matrix $a$ is ill-conditioned. compare the effectiveness of qr decomposition versus svd in computing solutions to the linear system $ax = b$.",computations with matrices
the bunch-kaufman decomposition is an alternative to lu decomposition for symmetric indefinite matrices. explain how it works and analyze its computational complexity.,computations with matrices
discuss the fundamental trade-offs between computational efficiency and numerical accuracy in solving large-scale linear systems $ax = b$. compare direct methods such as gaussian elimination with iterative solvers like the conjugate gradient method.,computations with matrices
consider a matrix $a$ with condition number $\kappa(a)$. explain how $\kappa(a)$ influences numerical errors in computed solutions when using floating-point arithmetic. provide examples illustrating how small perturbations in input data affect the solution.,computations with matrices
"in practical applications, matrix factorizations such as lu and qr are used instead of directly computing $a^{-1}$. explain why this approach improves both accuracy and efficiency. discuss scenarios where explicit inversion might still be useful.",computations with matrices
"given an ill-conditioned matrix $a$, describe strategies to minimize numerical errors when solving $ax = b$. compare the effects of using qr decomposition, regularization methods, and iterative refinement.",computations with matrices
floating-point precision can introduce rounding errors in matrix computations. derive an upper bound on the rounding error when performing matrix-matrix multiplication $c = ab$ using standard ieee 754 floating-point arithmetic.,computations with matrices
the trade-off between storage and accuracy is particularly important for large matrices. compare dense matrix representations with compressed sparse formats in terms of numerical stability and memory efficiency.,computations with matrices
"discuss the role of machine precision $\epsilon$ in numerical linear algebra. how does it impact the accuracy of matrix computations involving inversion, eigenvalue computation, and least-squares solutions?",computations with matrices
consider a system $ax = b$ where $a$ is nearly singular. discuss why iterative solvers like gmres or bicgstab are preferred over direct solvers. analyze their convergence behavior and numerical accuracy.,computations with matrices
"approximate matrix computations, such as low-rank approximations, trade accuracy for efficiency. discuss how the singular value decomposition (svd) can be used to approximate large matrices efficiently while minimizing reconstruction error.",computations with matrices
discuss the impact of finite precision arithmetic on matrix factorizations such as lu and cholesky decomposition. how do numerical stability and rounding errors propagate in these methods?,computations with matrices
"in parallel computing environments, matrix computations must balance load efficiency with numerical stability. discuss the trade-offs in distributing computations across multiple processors and the impact on rounding error accumulation.",computations with matrices
"when solving least-squares problems, normal equations ($a^tax = a^tb$) and qr decomposition ($ax = b$ using $a = qr$) are two approaches. compare their numerical accuracy and computational efficiency, particularly in cases where $a$ is ill-conditioned.",computations with matrices
iterative refinement is a technique used to improve the accuracy of solutions to linear systems. derive the iterative refinement formula and analyze how it compensates for floating-point errors.,computations with matrices
"many matrix computations use iterative methods (e.g., jacobi, gauss-seidel) instead of direct solvers. analyze their convergence properties and discuss the trade-offs in terms of speed, memory usage, and accuracy.",computations with matrices
"consider the problem of computing eigenvalues of a large matrix. compare the efficiency and numerical stability of the power method, qr algorithm, and krylov subspace methods (e.g., arnoldi iteration).",computations with matrices
explain how the choice of basis in a vector space can affect numerical accuracy in matrix computations. provide examples where changing the basis improves the numerical conditioning of a problem.,computations with matrices
"many real-world problems involve computing the inverse of a nearly singular matrix. compare tikhonov regularization and truncated svd as methods to handle this situation, discussing accuracy versus efficiency.",computations with matrices
floating-point underflow and overflow pose challenges in large-scale matrix computations. analyze how these issues arise in recursive matrix computations and suggest strategies to mitigate their effects.,computations with matrices
sparse matrix solvers often use preconditioners to improve convergence rates in iterative methods. explain the role of preconditioning in the conjugate gradient method and discuss the trade-offs between computational cost and numerical accuracy.,computations with matrices
"compare the efficiency and accuracy of explicit matrix exponentiation versus krylov subspace methods (e.g., lanczos algorithm) for computing $e^a$ in large-scale applications.",computations with matrices
discuss the effects of round-off errors in gram-schmidt orthogonalization and explain why the modified gram-schmidt (mgs) method is numerically more stable than the classical gram-schmidt (cgs) algorithm.,computations with matrices
a low-rank approximation of a matrix is often used in machine learning and data science. compare the computational complexity and accuracy trade-offs of using the truncated svd versus randomized algorithms for approximating a matrix.,computations with matrices
the woodbury matrix identity allows efficient computation of the inverse of a rank-$k$ update to a matrix. derive this identity and discuss its practical applications in reducing computational cost.,computations with matrices
"consider a block matrix 
          \[
          a =
          \begin{bmatrix}
              b & c \\
              d & e
          \end{bmatrix}
          \]
          where $b$ and $e$ are square matrices. discuss the accuracy and efficiency trade-offs in computing $a^{-1}$ using the schur complement.",computations with matrices
many iterative algorithms for matrix computations rely on stopping criteria. compare the impact of absolute versus relative error tolerances in determining algorithm termination.,computations with matrices
"the chebyshev polynomial approximation is used in matrix function computations. explain how this method balances computational efficiency with accuracy, and discuss its application in computing $f(a) = a^{-1}$.",computations with matrices
many real-world problems require solving large systems of linear equations repeatedly with different right-hand sides. explain why using lu factorization is more efficient than computing the inverse directly.,computations with matrices
consider the computation of eigenvalues of a symmetric matrix using the qr algorithm. explain how wilkinson’s shift improves numerical accuracy while maintaining computational efficiency.,computations with matrices
"when performing high-dimensional matrix computations, distributed computing frameworks like apache spark and mpi-based libraries are often used. discuss the trade-offs between accuracy and parallel efficiency in distributed matrix computations.",computations with matrices
approximate solutions to linear systems are often preferred over exact solutions when computational resources are limited. compare the trade-offs between krylov subspace methods and direct factorizations in solving large-scale sparse systems.,computations with matrices
"consider a linear transformation $t: \mathbb{r}^2 \to \mathbb{r}^2$ represented by a matrix $a$. discuss how the action of $a$ on any vector $\mathbf{v}$ can be understood geometrically in terms of basis transformations. provide examples where $a$ represents a reflection, rotation, or shear.",computations with matrices
"given the transformation matrix 
    \[
    a = \begin{bmatrix} 3 & 4 \\ 0 & 2 \end{bmatrix},
    \]
    describe its geometric effect on the unit square in $\mathbb{r}^2$. compute the transformed coordinates of the square's vertices and determine whether the transformation preserves orientation.",computations with matrices
a reflection transformation in $\mathbb{r}^2$ is given by a matrix $r$. show that $r^2 = i$ and explain geometrically why applying the transformation twice results in the identity transformation.,computations with matrices
"the matrix 
    \[
    r_{\theta} = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}
    \]
    represents a rotation in $\mathbb{r}^2$. derive its eigenvalues and eigenvectors and interpret them geometrically.",computations with matrices
consider a transformation matrix $a$ such that $\det(a) = -1$. explain why such a transformation must involve a reflection or a combination of transformations that reverse orientation.,computations with matrices
"in 3d space, a reflection about a plane passing through the origin can be represented by a matrix. derive the general form of such a matrix when reflecting about the plane $ax + by + cz = 0$.",computations with matrices
"consider the transformation matrix
    \[
    s = \begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix}.
    \]
    show that $s$ represents a shear transformation and analyze its effect on an arbitrary vector in $\mathbb{r}^2$. determine whether the transformation preserves angles and distances.",computations with matrices
discuss the concept of eigenvectors as invariant directions under a linear transformation. provide an example of a transformation in $\mathbb{r}^3$ with exactly one eigenvector and interpret its geometric meaning.,computations with matrices
"the standard projection onto the plane $z = 0$ in $\mathbb{r}^3$ is given by the matrix
    \[
    p = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}.
    \]
    find its eigenvalues and eigenvectors and explain their geometric significance.",computations with matrices
"in $\mathbb{r}^3$, the matrix
    \[
    a = \begin{bmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}
    \]
    represents a rotation about the $z$-axis. compute the angle of rotation and find the eigenvectors of $a$.",computations with matrices
explain the geometric significance of the singular value decomposition (svd) of a matrix $a$. interpret the left and right singular vectors as directions of transformation and scaling.,computations with matrices
the composition of two linear transformations $t_1$ and $t_2$ corresponds to matrix multiplication. show how the order of multiplication affects the geometric interpretation of transformations.,computations with matrices
prove that every 2d linear transformation with an orthogonal matrix either preserves or reverses orientation. provide examples illustrating each case.,computations with matrices
consider a 3d transformation matrix that represents a rotation followed by a reflection. discuss how the determinant of the resulting matrix helps determine whether the transformation preserves or reverses orientation.,computations with matrices
"given a linear transformation $t$ that scales all vectors by a factor $\lambda$, show that $t$ has $\lambda$ as its only eigenvalue. provide examples where this occurs.",computations with matrices
discuss why a matrix with linearly dependent columns collapses the dimensionality of the space. provide examples where a $3 \times 3$ matrix maps $\mathbb{r}^3$ onto a plane.,computations with matrices
explain how the rank of a transformation matrix affects its geometric interpretation. give examples of rank-deficient matrices and their effects on space.,computations with matrices
the determinant of a matrix provides information about volume scaling under a transformation. derive the geometric interpretation of the determinant for transformations in $\mathbb{r}^2$ and $\mathbb{r}^3$.,computations with matrices
"given a transformation matrix $a$ with eigenvalues $\lambda_1, \lambda_2, \lambda_3$, explain how the determinant and trace of $a$ relate to its geometric properties.",computations with matrices
"find the eigenvectors and eigenvalues of the following matrix and interpret their geometric significance:
    \[
    a = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}.
    \]",computations with matrices
"in computer graphics, homogeneous coordinates are used to represent transformations. explain how translation, scaling, and rotation are implemented using $3 \times 3$ transformation matrices in $\mathbb{r}^2$.",computations with matrices
consider the linear transformation $t: \mathbb{r}^3 \to \mathbb{r}^3$ given by a rotation about an arbitrary axis. show that the eigenvector corresponding to eigenvalue 1 is the direction of the axis of rotation.,computations with matrices
explain how a non-diagonalizable matrix corresponds to a transformation that does not have a complete set of linearly independent eigenvectors. provide a geometric interpretation.,computations with matrices
prove that any symmetric matrix represents a transformation that preserves orthogonality of eigenvectors and discuss its implications in geometry.,computations with matrices
the jordan form of a matrix provides insight into its geometric properties. explain the geometric meaning of jordan blocks and discuss their significance in repeated transformations.,computations with matrices
show that any transformation in $\mathbb{r}^2$ that preserves the dot product must be a rotation or reflection. prove that such transformations correspond to orthogonal matrices.,computations with matrices
"given two transformations represented by matrices $a$ and $b$, discuss under what conditions $ab = ba$ holds and provide a geometric interpretation of commuting matrices.",computations with matrices
the polar decomposition of a matrix expresses it as a product of a unitary matrix and a symmetric positive semi-definite matrix. explain its geometric significance and applications.,computations with matrices
consider an affine transformation in $\mathbb{r}^2$ represented by a matrix and a translation vector. explain how this differs from a purely linear transformation and discuss the role of augmented matrices.,computations with matrices
"define the concept of isotropic scaling, anisotropic scaling, and shear transformations in $\mathbb{r}^3$. discuss the geometric interpretation of each using suitable transformation matrices.",computations with matrices
"in computer graphics, a transformation matrix is used to rotate a 3d object about an arbitrary axis. derive the general rotation matrix for a rotation by angle $\theta$ about an axis defined by the unit vector $\mathbf{u} = (u_x, u_y, u_z)$. discuss its properties and geometric interpretation.",computations with matrices
homogeneous coordinates are commonly used in computer graphics to represent affine transformations. explain why an extra dimension is introduced and derive the homogeneous transformation matrix for translation in $\mathbb{r}^3$.,computations with matrices
"given a 3d transformation matrix that combines scaling, rotation, and translation, explain how these operations can be decomposed into individual transformation matrices. discuss how the order of transformations affects the final result.",computations with matrices
"in image processing, convolution operations are performed using matrices. given a 3x3 kernel matrix $k$, show how applying convolution to an image matrix $i$ can be formulated as matrix multiplication.",computations with matrices
the shear transformation in 2d graphics is used to distort an image along the x or y axis. derive the matrix representation of a shear transformation and analyze its effect on a square grid.,computations with matrices
"explain how the singular value decomposition (svd) is used in image compression. given an image matrix, describe the process of rank-k approximation and its effect on storage efficiency and quality.",computations with matrices
"in computer graphics, perspective projection is used to create realistic 3d renderings. derive the perspective projection transformation matrix and explain how it maps 3d points to a 2d plane.",computations with matrices
"given an image transformation matrix that rotates an image by 45 degrees, determine how the bounding box of the image changes. discuss the implications for image cropping and resizing.",computations with matrices
discuss the role of eigenvalues and eigenvectors in principal component analysis (pca) for dimensionality reduction in image processing. explain how pca can be applied to face recognition.,computations with matrices
"in 3d graphics, reflection across an arbitrary plane can be achieved using matrix transformations. derive the matrix representation of a reflection about the plane $ax + by + cz = d$.",computations with matrices
"a common problem in graphics is aliasing, which occurs when a high-resolution image is downsampled. explain how matrix-based filtering techniques (e.g., gaussian blur) can be used to reduce aliasing effects.",computations with matrices
"the affine transformation matrix is a general form that includes rotation, scaling, translation, and shear. show how the determinant of an affine transformation matrix affects the area or volume of transformed objects.",computations with matrices
the discrete cosine transform (dct) is widely used in jpeg image compression. explain the role of dct as a matrix operation and how it relates to energy compaction in image representation.,computations with matrices
"a 3d scene can be transformed using a sequence of matrices for rotation, translation, and scaling. explain how these matrices are combined in the rendering pipeline and discuss their computational complexity.",computations with matrices
"given a transformation matrix that scales an image non-uniformly, analyze its effect on image aspect ratio and explain how to compensate for distortions using inverse transformations.",computations with matrices
"in texture mapping, an image (texture) is mapped onto a 3d object using uv coordinates. describe how this process is implemented using transformation matrices and discuss interpolation techniques for smooth rendering.",computations with matrices
the fast fourier transform (fft) is often used in image processing for filtering operations. explain how fft can be expressed in terms of matrix-vector multiplication and discuss its computational advantages.,computations with matrices
the harris corner detection algorithm relies on the eigenvalues of a structure tensor matrix. explain how eigenvalues are used to identify corners and edges in an image.,computations with matrices
"given an image transformation pipeline consisting of rotation, scaling, and shearing, explain how the combined transformation matrix is obtained and how it affects the image properties.",computations with matrices
"in augmented reality applications, homography matrices are used to warp images onto surfaces. derive the homography matrix for mapping a quadrilateral to another quadrilateral and discuss its applications.",computations with matrices
"show that the composition of two orthogonal transformation matrices (e.g., rotations and reflections) results in another orthogonal matrix. explain the geometric interpretation in the context of 3d transformations.",computations with matrices
"in image enhancement, edge detection techniques such as sobel filtering use convolution matrices. explain the mathematical principles behind edge detection and the role of matrix derivatives.",computations with matrices
the 3d viewing transformation in graphics involves converting world coordinates to camera coordinates. explain the role of the view matrix in this process and derive its general form.,computations with matrices
"non-rigid transformations, such as morphing, involve continuous deformations of an image. explain how matrix interpolation techniques can be used to generate smooth transitions between two images.",computations with matrices
the rank of a transformation matrix determines the dimensionality of the transformed space. provide examples of matrices used in graphics that reduce dimension and discuss their significance.,computations with matrices
"given a 2d image transformation matrix that rotates by $\theta$ and scales by a factor of $s$, derive the inverse transformation and discuss its use in correcting distortions.",computations with matrices
"in animation, linear transformations are used to manipulate character models. explain how skeletal animation uses transformation matrices and how blending multiple transformations creates smooth motion.",computations with matrices
the eigenfaces method for face recognition relies on matrix decomposition. explain how eigenvectors of a covariance matrix are used to represent facial features and discuss the advantages of this approach.,computations with matrices
discuss the numerical stability of transformation matrices in graphics. explain how floating-point errors can accumulate during successive transformations and propose techniques to mitigate these issues.,computations with matrices
the laplacian matrix is used in image sharpening and edge detection. explain how the second derivative of an image can be approximated using matrix operations and provide examples of its applications.,computations with matrices
let $p$ be an $n \times n$ stochastic matrix representing a discrete-time markov chain. show that the sum of each row in $p$ is 1 and explain the significance of this property in modeling probabilistic transitions.,computations with matrices
"given a markov chain with transition matrix $p$, define a stationary distribution $\pi$ and derive the equation $\pi p = \pi$. discuss the conditions under which a stationary distribution is unique.",computations with matrices
"prove that for any finite, irreducible, and aperiodic markov chain, the transition matrix $p$ has a unique stationary distribution. provide a numerical example to illustrate the concept.",computations with matrices
"the fundamental matrix $z$ for an absorbing markov chain is defined as $z = (i - q)^{-1}$, where $q$ is the submatrix of transient states. derive the expected number of steps before absorption using $z$.",computations with matrices
consider a markov chain with a transition matrix $p$. show that the eigenvalue 1 is always present in $p$ and explain how the corresponding eigenvector relates to the steady-state distribution.,computations with matrices
"a markov chain is said to be ergodic if it is irreducible and aperiodic. prove that for an ergodic markov chain, the power of the transition matrix $p^k$ converges to a rank-one matrix as $k \to \infty$.",computations with matrices
"in a birth-death process represented by a tridiagonal stochastic matrix, derive the conditions under which the markov chain is irreducible and has a unique limiting distribution.",computations with matrices
"consider a markov chain modeling web page ranking, where the transition matrix represents hyperlink transitions. describe how google's pagerank algorithm uses eigenvectors of a modified stochastic matrix to rank web pages.",computations with matrices
"given a finite-state markov chain with transition matrix $p$, define the mean recurrence time of a state and show how it is related to the stationary distribution.",computations with matrices
"define the concept of detailed balance in a reversible markov chain. prove that if a stationary distribution $\pi$ satisfies the detailed balance equation $\pi_i p_{ij} = \pi_j p_{ji}$, then the chain is time-reversible.",computations with matrices
"show that for any finite-state markov chain with transition matrix $p$, all eigenvalues satisfy $|\lambda| \leq 1$. discuss the conditions under which $\lambda = 1$ is the unique dominant eigenvalue.",computations with matrices
"suppose a markov chain models a queueing system, where the state represents the number of customers in a queue. derive the steady-state probabilities for an m/m/1 queue using the balance equations.",computations with matrices
"let $p$ be the transition matrix of a finite irreducible markov chain. show that as $k \to \infty$, the probability distribution vector $\mathbf{v}p^k$ converges to the stationary distribution, regardless of the initial distribution $\mathbf{v}$.",computations with matrices
"in a markov decision process (mdp), transition probabilities depend on chosen actions. describe how stochastic matrices are extended in mdps and explain the role of the bellman equation in computing optimal policies.",computations with matrices
"show that for a doubly stochastic transition matrix, the uniform distribution is always a stationary distribution. provide an example of a markov chain with a doubly stochastic transition matrix and analyze its steady-state behavior.",computations with matrices
"given a reducible markov chain with a block diagonal transition matrix, explain how the system decomposes into smaller independent markov chains. discuss implications for long-term behavior.",computations with matrices
"define the notion of mixing time for a markov chain, which measures how fast a chain converges to its stationary distribution. derive an upper bound on the mixing time in terms of the spectral gap.",computations with matrices
"consider a markov chain with transition matrix $p$. prove that if $p$ is symmetric, then all its eigenvalues are real. discuss the implications of this property in terms of convergence to equilibrium.",computations with matrices
define the first passage time $t_{ij}$ as the expected number of steps for a markov chain to move from state $i$ to state $j$. derive the system of equations satisfied by $t_{ij}$ and solve for a simple example.,computations with matrices
"describe an application of markov chains in financial modeling, such as credit rating transitions or stock price movements. formulate a markov model for the chosen application and analyze its long-term behavior.",computations with matrices
"discuss the relationship between an undirected graph’s adjacency matrix and its eigenvalues. provide a detailed proof of the spectral theorem for an undirected graph, showing how the eigenvalues of the adjacency matrix are related to the graph’s connectivity and its properties like the number of components, cliques, and paths. illustrate with examples involving simple connected graphs.",computations with matrices
"given a graph $g$ with adjacency matrix $a$, prove the perron-frobenius theorem for the largest eigenvalue of the adjacency matrix in the case where $g$ is strongly connected and irreducible. show the implications of this theorem for the degree centrality of nodes in the graph and provide an algorithmic approach to calculate this centrality using eigenvectors of $a$.",computations with matrices
"define the laplacian matrix of a graph and derive the relationships between the spectrum of the laplacian matrix and various graph properties such as connectivity, number of spanning trees, and the number of connected components. provide a detailed example with a graph and calculate the eigenvalues of the laplacian matrix.",computations with matrices
discuss the notion of graph connectivity in terms of the laplacian matrix. prove that the multiplicity of the eigenvalue zero in the laplacian matrix corresponds to the number of connected components in the graph. illustrate this with a graph and demonstrate how to compute the number of connected components using the laplacian matrix.,computations with matrices
"explain the concept of spectral clustering using the laplacian matrix of a graph. discuss the steps involved in performing spectral clustering, including how the eigenvectors of the laplacian matrix are used to partition the graph into clusters. provide a concrete example of spectral clustering on a small graph, and explain the role of the second smallest eigenvalue (the fiedler value).",computations with matrices
"in a network represented by a directed graph, the adjacency matrix is often non-symmetric. investigate the relationship between the eigenvalues of the adjacency matrix and the graph’s structure in the case of a directed graph. discuss how the in-degree and out-degree centralities of nodes are related to the eigenvectors of the adjacency matrix, and provide an example calculation.",computations with matrices
"consider a graph with $n$ nodes and an associated adjacency matrix $a$. prove that the diagonal entries of the laplacian matrix of the graph are equal to the degree of the corresponding vertex, and show how this fact influences the properties of the laplacian eigenvectors. provide examples with several types of graphs, such as complete graphs, bipartite graphs, and trees.",computations with matrices
"investigate the relationship between the graph's adjacency matrix and its centrality measures. define and compute the degree centrality, closeness centrality, and betweenness centrality of a node in terms of the eigenvalues and eigenvectors of the adjacency matrix and laplacian matrix. provide an example where these centralities are computed for a real-world network.",computations with matrices
"prove that the adjacency matrix of a bipartite graph is a block matrix, where the blocks correspond to the partitions of the bipartite graph. discuss how this block structure can simplify the computation of the spectral properties of the graph, and provide a detailed example of a bipartite graph where the eigenvalues are calculated.",computations with matrices
"define the concept of the graph’s spectral radius in terms of the adjacency matrix. discuss its implications for the network's stability, and prove a relationship between the spectral radius and the largest eigenvalue of the graph’s laplacian matrix. use an example to illustrate the concept of the spectral radius in the context of a strongly connected graph.",computations with matrices
"explain the use of the adjacency matrix in modeling the spread of a virus in a network. derive the relationship between the adjacency matrix and the rate of spread, using eigenvalue analysis to discuss the long-term behavior of the epidemic in terms of graph structure. provide a case study of a small network and analyze the epidemic's behavior using the spectral properties of the graph.",computations with matrices
"describe the power method for finding the dominant eigenvalue of a graph’s adjacency matrix. discuss the convergence criteria and the implications of this method for large-scale graph analysis, particularly in network analysis. provide a numerical example of applying the power method to find the largest eigenvalue of an adjacency matrix.",computations with matrices
investigate the use of the laplacian matrix in network flow analysis. derive the conditions under which the flow in a network can be modeled by the eigenvectors of the laplacian matrix. discuss the applications of this result in optimizing network flows and computing minimum cuts. provide an example of a flow network and demonstrate the use of the laplacian eigenvectors.,computations with matrices
examine the concept of random walks on graphs in terms of the graph’s laplacian and adjacency matrices. derive the relationship between the transition matrix of a random walk and the eigenvalues of the adjacency matrix. discuss how the eigenvectors of the laplacian matrix can be used to study the mixing time of a random walk and provide a concrete example with a simple graph.,computations with matrices
"consider a graph with the adjacency matrix $a$ and the laplacian matrix $l$. prove that the graph is connected if and only if the algebraic multiplicity of the eigenvalue zero of $l$ is 1. discuss the practical implications of this result in network design, particularly in the context of ensuring network connectivity. illustrate this with an example of a disconnected graph and compute its laplacian eigenvalues.",computations with matrices
"given a linear time-invariant (lti) system with the state-space representation 
    \[
    \dot{x}(t) = ax(t) + bu(t), \quad y(t) = cx(t) + du(t),
    \]
    where $a$, $b$, $c$, and $d$ are matrices of appropriate dimensions, discuss the conditions under which this system is controllable. provide a detailed derivation of the controllability matrix and explain how to use the matrix rank condition to determine whether the system is controllable. include an example of an lti system and perform a controllability analysis using the controllability matrix.",computations with matrices
"consider a state-space model of a system where the system matrix $a$ is diagonalizable. show that the solution to the differential equation $\dot{x}(t) = ax(t) + bu(t)$ can be expressed as a matrix exponential. provide the explicit form of the solution, including the influence of the control input $u(t)$ on the state evolution. illustrate this with an example involving specific values for $a$ and $b$.",computations with matrices
"derive the necessary and sufficient conditions for the observability of a system with the state-space representation
    \[
    \dot{x}(t) = ax(t) + bu(t), \quad y(t) = cx(t) + du(t).
    \]
    define the observability matrix and show that the rank of this matrix determines the observability of the system. use a practical example of an lti system and compute the observability matrix to verify if the system is observable.",computations with matrices
"using the lyapunov stability criterion, prove that a linear system is asymptotically stable if and only if the real parts of all the eigenvalues of the system matrix $a$ are negative. discuss the implications of this result for the stability analysis of systems in control theory and provide an example of a system where the stability of the system is determined using the eigenvalues of the matrix $a$.",computations with matrices
"define the controllability and observability grammian matrices for a linear system and derive the conditions under which these matrices are positive definite. explain the role of these grammian matrices in the design of optimal controllers and observers, and provide examples where these grammian matrices are used to determine the controllability and observability of specific systems.",computations with matrices
explain the concept of pole placement in the context of state-space systems. derive the necessary conditions for pole placement in terms of the system’s controllability matrix. discuss how to use the controllability matrix to design state feedback controllers that place the poles of the closed-loop system in desired locations. provide a detailed example of pole placement in a simple second-order system.,computations with matrices
"consider a system with state-space representation
    \[
    \dot{x}(t) = ax(t) + bu(t), \quad y(t) = cx(t) + du(t).
    \]
    discuss the use of matrix methods to solve the system’s state equations, particularly focusing on solving for the state vector $x(t)$ given initial conditions. derive the general solution using the matrix exponential and discuss the role of the system matrix $a$ and input matrix $b$ in shaping the solution. illustrate with a numerical example of solving the state equations for a given system.",computations with matrices
"given the state-space system 
    \[
    \dot{x}(t) = ax(t) + bu(t), \quad y(t) = cx(t) + du(t),
    \]
    explain the process of converting this system into its transfer function representation using matrix algebra. discuss how the transfer function captures the behavior of the system in the frequency domain and derive the transfer function for a second-order system as an example.",computations with matrices
discuss the use of the kalman controllability and observability conditions in the context of real-time control systems. derive the kalman controllability and observability matrices and explain how they are used to determine whether a system is controllable or observable from the perspective of both state feedback and measurement feedback. provide a case study where these conditions are applied to assess the controllability and observability of a physical system.,computations with matrices
"prove that the controllability matrix $w_c = [b, ab, a^2b, \dots, a^{n-1}b]$ for a state-space representation is full rank if and only if the system is controllable. discuss how the rank condition can be used to design controllers and identify the controllability properties of a system. include an example of a system and demonstrate the calculation of the controllability matrix and its rank.",computations with matrices
"consider a state-space representation of a system 
    \[
    \dot{x}(t) = ax(t) + bu(t), \quad y(t) = cx(t) + du(t),
    \]
    and derive the conditions under which the system is asymptotically stable. using matrix methods, discuss the relation between the system matrix $a$ and the eigenvalues of the system. discuss the design of controllers to ensure stability, focusing on the feedback matrix and its impact on the eigenvalues.",computations with matrices
"discuss the design of an observer for an lti system in state-space form. explain how the observer matrix $l$ is chosen to ensure that the estimation error converges to zero. provide a detailed derivation of the observer’s dynamics and the observer gain matrix $l$, including conditions under which the observer converges. provide an example of designing an observer for a simple system and verify the stability of the observer.",computations with matrices
"if a 4 by 4 matrix has $\det(a) = \frac{1}{2}$, find $\det(2a)$, $\det(-a)$, $\det(a^2)$, and $\det(a^{-1})$.",determinants
"if a 3 by 3 matrix has $\det(a) = -1$, find $\det\left(\frac{1}{2}a\right)$, $\det(-a)$, $\det(a^2)$, and $\det(a^{-1})$.",determinants
"row exchange: add row 1 of $a$ to row 2, then subtract row 2 from row 1. then add row 1 to row 2 and multiply row 1 by $-1$ to reach $b$. which rules show the following?
    \[
    \det(b) = \begin{vmatrix} c & d \\ a & b \end{vmatrix} = -\det(a) = -\begin{vmatrix} a & b \\ c & d \end{vmatrix}.
    \]
    those rules could replace rule 2 in the definition of the determinant.",determinants
"by applying row operations to produce an upper triangular $u$, compute:
    \[
    \det \begin{bmatrix}
    1 & 2 & -2 & 0 \\
    2 & 3 & -4 & 1 \\
    -1 & -2 & 0 & 2 \\
    0 & 2 & 5 & 3
    \end{bmatrix}
    \]
    and
    \[
    \det \begin{bmatrix}
    2 & -1 & 0 & 0 \\
    -1 & 2 & -1 & 0 \\
    0 & -1 & 2 & -1 \\
    0 & 0 & -1 & -2
    \end{bmatrix}.
    \]
    exchange rows 3 and 4 of the second matrix and recompute the pivots and determinant.",determinants
"count row exchanges to find these determinants:
    \[
    \det \begin{bmatrix}
    0 & 0 & 0 & 1 \\
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0
    \end{bmatrix} = \pm 1
    \]
    and
    \[
    \det \begin{bmatrix}
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
    1 & 0 & 0 & 0
    \end{bmatrix} = -1.
    \]",determinants
"for each $n$, how many exchanges will put (row $n$, row $n - 1$, ..., row 1) into the normal order (row 1, ..., row $n$)? find $\det p$ for the $n \times n$ permutation matrix with 1s on the reverse diagonal. problem 5 had $n = 4$.",determinants
"find the determinants of:
    \begin{enumerate}",determinants
"a rank one matrix
        \[
        a = \begin{bmatrix}
        1 \\
        4 \\
        2
        \end{bmatrix}
        \begin{bmatrix}
        2 & -1 & 2
        \end{bmatrix};
        \]",determinants
"the upper triangular matrix
        \[
        u = \begin{bmatrix}
        4 & 4 & 8 & 8 \\
        0 & 1 & 2 & 2 \\
        0 & 0 & 2 & 6 \\
        0 & 0 & 0 & 2
        \end{bmatrix};
        \]",determinants
the lower triangular matrix $u^t$;,determinants
the inverse matrix $u^{-1}$;,determinants
"the “reverse-triangular” matrix that results from row exchanges,
        \[
        m = \begin{bmatrix}
        0 & 0 & 0 & 2 \\
        0 & 0 & 2 & 6 \\
        0 & 1 & 2 & 2 \\
        4 & 4 & 8 & 8
        \end{bmatrix}.
        \]",determinants
show how rule 6 (det = 0 if a row is zero) comes directly from rules 2 and 3.,determinants
"suppose you do two row operations at once, going from
    \[
    \begin{bmatrix}
    a & b \\
    c & d
    \end{bmatrix}
    \]
    to
    \[
    \begin{bmatrix}
    a - mc & b - md \\
    c - \ell a & d - \ell b
    \end{bmatrix}.
    \]
    find the determinant of the new matrix, by rule 3 or by direct calculation.",determinants
"if \( q \) is an orthogonal matrix, so that \( q^t q = i \), prove that \( \det q \) equals \( +1 \) or \( -1 \). what kind of box is formed from the rows (or columns) of \( q \)?",determinants
prove again that \( \det q = 1 \) or \( -1 \) using only the product rule. if \( |\det q| > 1 \) then \( \det q^n \) blows up. how do you know this can’t happen to \( q^n \)?,determinants
"use row operations to verify that the 3 by 3 “vandermonde determinant” is
    \[
    \det \begin{bmatrix}
    1 & a & a^2 \\
    1 & b & b^2 \\
    1 & c & c^2
    \end{bmatrix} = (b - a)(c - a)(c - b).
    \]",determinants
"(a) a skew-symmetric matrix satisfies \( k^t = -k \), as in
    \[
    k = \begin{bmatrix}
    0 & a & b \\
    -a & 0 & c \\
    -b & -c & 0
    \end{bmatrix}.
    \]
    in the 3 by 3 case, why is \( \det(-k) = (-1)^3 \det k \)? on the other hand \( \det k^t = \det k \) (always). deduce that the determinant must be zero.

    (b) write down a 4 by 4 skew-symmetric matrix with \( \det k \) not zero.",determinants
"true or false, with reason if true and counterexample if false:
    \begin{enumerate}",determinants
"if \( a \) and \( b \) are identical except that \( b_{11} = 2a_{11} \), then \( \det b = 2 \det a \).",determinants
the determinant is the product of the pivots.,determinants
"if \( a \) is invertible and \( b \) is singular, then \( a + b \) is invertible.",determinants
"if \( a \) is invertible and \( b \) is singular, then \( ab \) is singular.",determinants
the determinant of \( ab - ba \) is zero.,determinants
"if every row of \( a \) adds to zero, prove that \( \det a = 0 \). if every row adds to 1, prove that \( \det(a - i) = 0 \). show by example that this does not imply \( \det a = 1 \).",determinants
"find these 4 by 4 determinants by gaussian elimination:
    \[
    \det
    \begin{bmatrix}
    11 & 12 & 13 & 14 \\
    21 & 22 & 23 & 24 \\
    31 & 32 & 33 & 34 \\
    41 & 42 & 43 & 44
    \end{bmatrix}
    \]
    and
    \[
    \det
    \begin{bmatrix}
    1 & t & t^2 & t^3 \\
    t & 1 & t & t^2 \\
    t^2 & t & 1 & t \\
    t^3 & t^2 & t & 1
    \end{bmatrix}
    \]",determinants
"find the determinants of
    \[
    a =
    \begin{bmatrix}
    4 & 2 \\
    1 & 3
    \end{bmatrix}
    \]
    \[
    a^{-1} =
    \frac{1}{10}
    \begin{bmatrix}
    3 & -2 \\
    -1 & 4
    \end{bmatrix}
    \]
    \[
    a - \lambda i =
    \begin{bmatrix}
    4 - \lambda & 2 \\
    1 & 3 - \lambda
    \end{bmatrix}
    \]
    for which values of \( \lambda \) is \( a - \lambda i \) a singular matrix?",determinants
"evaluate \( \det a \) by reducing the matrix to triangular form (rules 5 and 7).
    \[
    a =
    \begin{bmatrix}
    1 & 1 & 3 \\
    0 & 4 & 6 \\
    1 & 5 & 8
    \end{bmatrix}
    \]
    \[
    b =
    \begin{bmatrix}
    1 & 1 & 3 \\
    0 & 4 & 6 \\
    0 & 0 & 1
    \end{bmatrix}
    \]
    \[
    c =
    \begin{bmatrix}
    1 & 1 & 3 \\
    0 & 4 & 6 \\
    1 & 5 & 9
    \end{bmatrix}
    \]
    what are the determinants of \( b \), \( c \), \( ab \), \( a^t a \), and \( c^t \)?",determinants
"suppose that \( cd = -dc \), and find the flaw in the following argument: taking determinants gives \( (\det c)(\det d) = -(\det d)(\det c) \), so either \( \det c = 0 \) or \( \det d = 0 \). thus \( cd = -dc \) is only possible if \( c \) or \( d \) is singular.",determinants
"do these matrices have determinant 0, 1, 2, or 3?
    \[
    a =
    \begin{bmatrix}
    0 & 0 & 1 \\
    1 & 0 & 0 \\
    0 & 1 & 0
    \end{bmatrix}
    \]
    \[
    b =
    \begin{bmatrix}
    0 & 1 & 1 \\
    1 & 0 & 1 \\
    1 & 1 & 0
    \end{bmatrix}
    \]
    \[
    c =
    \begin{bmatrix}
    1 & 1 & 1 \\
    1 & 1 & 1 \\
    1 & 1 & 1
    \end{bmatrix}
    \]",determinants
"the inverse of a 2 by 2 matrix seems to have determinant = 1:
    \[
    \det a^{-1} = \det \left( \frac{1}{ad - bc}
    \begin{bmatrix}
    d & -b \\
    -c & a
    \end{bmatrix} \right)
    =
    \frac{1}{ad - bc} \cdot (ad - bc)
    = 1
    \]
    what is wrong with this calculation? what is the correct \( \det a^{-1} \)?",determinants
"reduce \( a \) to \( u \) and find \( \det a = \) product of the pivots:
    \[
    a =
    \begin{bmatrix}
    1 & 1 & 1 \\
    1 & 2 & 2 \\
    1 & 2 & 3
    \end{bmatrix}
    \]
    and
    \[
    a =
    \begin{bmatrix}
    1 & 2 & 3 \\
    2 & 2 & 3 \\
    3 & 3 & 3
    \end{bmatrix}
    \]
    \setcounter{enumi}{22} % start enumeration at 23",determinants
"by applying row operations to produce an upper triangular \( u \), compute:
    \[
    \det
    \begin{bmatrix}
    1 & 2 & 3 & 0 \\
    2 & 6 & 6 & 1 \\
    -1 & 0 & 0 & 3 \\
    0 & 2 & 0 & 7
    \end{bmatrix}
    \]
    and
    \[
    \det
    \begin{bmatrix}
    2 & 1 & 1 & 1 \\
    1 & 2 & 1 & 1 \\
    1 & 1 & 2 & 1 \\
    1 & 1 & 1 & 2
    \end{bmatrix}
    \]",determinants
"use row operations to simplify and compute these determinants:
    \[
    \det
    \begin{bmatrix}
    101 & 201 & 301 \\
    102 & 202 & 302 \\
    103 & 203 & 303
    \end{bmatrix}
    \]
    and
    \[
    \det
    \begin{bmatrix}
    1 & t & t^2 \\
    t & 1 & t \\
    t^2 & t & 1
    \end{bmatrix}
    \]",determinants
"elimination reduces \( a \) to \( u \). then \( a = lu \):
    \[
    a =
    \begin{bmatrix}
    3 & 3 & 4 \\
    6 & 8 & 7 \\
    -3 & 5 & -9
    \end{bmatrix}
    =
    \begin{bmatrix}
    1 & 0 & 0 \\
    2 & 1 & 0 \\
    -1 & 4 & 1
    \end{bmatrix}
    \begin{bmatrix}
    3 & 3 & 4 \\
    0 & 2 & -1 \\
    0 & 0 & -1
    \end{bmatrix}
    = lu.
    \]
    find the determinants of \( l \), \( u \), \( a \), \( u^{-1}l^{-1} \), and \( u^{-1}l^{-1}a \).",determinants
"if \( a_{ij} \) is \( i \) times \( j \), show that \( \det a = 0 \). (exception when \( a = [1] \).)",determinants
"if \( a_{ij} \) is \( i + j \), show that \( \det a = 0 \). (exception when \( n = 1 \) or \( 2 \).)",determinants
"compute the determinants of these matrices by row operations:
    \[
    a =
    \begin{bmatrix}
    0 & a & 0 \\
    0 & 0 & b \\
    c & 0 & 0
    \end{bmatrix},
    \quad
    b =
    \begin{bmatrix}
    0 & a & 0 & 0 \\
    0 & 0 & b & 0 \\
    0 & 0 & 0 & c \\
    d & 0 & 0 & 0
    \end{bmatrix},
    \quad
    c =
    \begin{bmatrix}
    a & a & a \\
    a & b & b \\
    a & b & c
    \end{bmatrix}.
    \]",determinants
"what is wrong with this proof that projection matrices have \( \det p = 1 \)?
    \[
    p = a(a^t a)^{-1}a^t \quad \text{so} \quad |p| = |a| \frac{1}{|a^t| |a|} |a^t| = 1.
    \]",determinants
"(calculus question) show that the partial derivatives of \( \ln(\det a) \) give \( a^{-1} \):
    \[
    f(a, b, c, d) = \ln(ad - bc) \quad \text{leads to} \quad
    \begin{bmatrix}
    \partial f / \partial a & \partial f / \partial c \\
    \partial f / \partial b & \partial f / \partial d
    \end{bmatrix}
    = a^{-1}.
    \]",determinants
"(matlab) the hilbert matrix \texttt{hilb(n)} has \( i, j \) entry equal to \( 1/(i + j - 1) \). print the determinants of \texttt{hilb(1)}, \texttt{hilb(2)}, ..., \texttt{hilb(10)}. hilbert matrices are hard to work with! what are the pivots?",determinants
"(matlab) what is a typical determinant (experimentally) of \texttt{rand(n)} and \texttt{randn(n)} for \( n = 50, 100, 200, 400 \)? (and what does ""inf"" mean in matlab?)
    \setcounter{enumi}{32} % start enumeration at 33",determinants
"using matlab, find the largest determinant of a 4 by 4 matrix of 0s and 1s.",determinants
"if you know that \( \det a = 6 \), what is the determinant of \( b \)?
    \[
    \det a =
    \begin{vmatrix}
    \text{row 1} \\
    \text{row 2} \\
    \text{row 3}
    \end{vmatrix}
    = 6
    \]
    \[
    \det b =
    \begin{vmatrix}
    \text{row 1} + \text{row 2} \\
    \text{row 2} + \text{row 3} \\
    \text{row 3} + \text{row 1}
    \end{vmatrix}
    = ?
    \]",determinants
"suppose the 4 by 4 matrix \( m \) has four equal rows all containing \( a, b, c, d \). we know that \( \det(m) = 0 \). the problem is to find \( \det(i + m) \) by any method:
    \[
    \det(i + m) =
    \begin{vmatrix}
    1 + a & b & c & d \\
    a & 1 + b & c & d \\
    a & b & 1 + c & d \\
    a & b & c & 1 + d
    \end{vmatrix}
    \]
    partial credit if you find this determinant when \( a = b = c = d = 1 \). sudden death if you say that \( \det(i + m) = \det i + \det m \).",determinants
"for these matrices, find the only nonzero term in the big formula (6):
    \[
    a =
    \begin{bmatrix}
    0 & 1 & 0 & 0 \\
    1 & 0 & 1 & 0 \\
    0 & 1 & 0 & 1 \\
    0 & 0 & 1 & 0
    \end{bmatrix}
    \quad \text{and} \quad
    b =
    \begin{bmatrix}
    0 & 0 & 1 & 2 \\
    0 & 3 & 4 & 5 \\
    6 & 7 & 8 & 9 \\
    0 & 0 & 0 & 1
    \end{bmatrix}.
    \]
    there is only one way of choosing four nonzero entries from different rows and different columns. by deciding even or odd, compute \( \det a \) and \( \det b \).",determinants
expand those determinants in cofactors of the first row. find the cofactors (they include the signs \( (-1)^{i+j} \)) and the determinants of \( a \) and \( b \).,determinants
"true or false?
    \begin{enumerate}",determinants
the determinant of \( s^{-1} a s \) equals the determinant of \( a \).,determinants
"if \( \det a = 0 \), then at least one of the cofactors must be zero.",determinants
"a matrix whose entries are 0s and 1s has determinant 1, 0, or \( -1 \).",determinants
\begin{enumerate},determinants
"find the lu factorization, the pivots, and the determinant of the 4 by 4 matrix whose entries are \( a_{ij} = \min(i, j) \). (write out the matrix.)",determinants
"find the determinant if \( a_{ij} = \min(n_i, n_j) \), where \( n_1 = 2 \), \( n_2 = 6 \), \( n_3 = 8 \), \( n_4 = 10 \). can you give a general rule for any \( n_1 \leq n_2 \leq n_3 \leq n_4 \)?",determinants
"let \( f_n \) be the determinant of the \( 1, 1, -1 \) tridiagonal matrix (n by n):
    \[
    f_n = \det
    \begin{bmatrix}
    1 & -1 & 0 & \cdots & 0 \\
    1 & 1 & -1 & \cdots & 0 \\
    0 & 1 & 1 & \ddots & 0 \\
    \vdots & \ddots & \ddots & \ddots & -1 \\
    0 & \cdots & 0 & 1 & 1
    \end{bmatrix}.
    \]
    by expanding in cofactors along row 1, show that \( f_n = f_{n-1} + f_{n-2} \). this yields the fibonacci sequence \( 1, 2, 3, 5, 8, 13, \ldots \) for the determinants.",determinants
"suppose \( a_n \) is the \( n \times n \) tridiagonal matrix with 1s on the three diagonals:
    \[
    a_1 = [1], \quad
    a_2 =
    \begin{bmatrix}
    1 & 1 \\
    1 & 1
    \end{bmatrix}, \quad
    a_3 =
    \begin{bmatrix}
    1 & 1 & 0 \\
    1 & 1 & 1 \\
    0 & 1 & 1
    \end{bmatrix}, \ldots
    \]
    let \( d_n \) be the determinant of \( a_n \); we want to find it.
    \begin{enumerate}",determinants
expand in cofactors along the first row to show that \( d_n = d_{n-1} - d_{n-2} \).,determinants
"starting from \( d_1 = 1 \) and \( d_2 = 0 \), find \( d_3, d_4, \ldots, d_8 \). by noticing how these numbers cycle around (with what period?) find \( d_{1000} \).",determinants
"evaluate this determinant by cofactors of row 1:
        \[
        \begin{vmatrix}
        4 & 4 & 4 & 4 \\
        1 & 2 & 0 & 1 \\
        2 & 0 & 1 & 2 \\
        1 & 1 & 0 & 2
        \end{vmatrix}.
        \]",determinants
check by subtracting column 1 from the other columns and recomputing.,determinants
"compute the determinants of \( a_2, a_3, a_4 \). can you predict \( a_n \)?
    \[
    a_2 =
    \begin{bmatrix}
    0 & 1 \\
    1 & 0
    \end{bmatrix}, \quad
    a_3 =
    \begin{bmatrix}
    0 & 1 & 1 \\
    1 & 0 & 1 \\
    1 & 1 & 0
    \end{bmatrix}, \quad
    a_4 =
    \begin{bmatrix}
    0 & 1 & 1 & 1 \\
    1 & 0 & 1 & 1 \\
    1 & 1 & 0 & 1 \\
    1 & 1 & 1 & 0
    \end{bmatrix}.
    \]
    use row operations to produce zeros, or use cofactors of row 1.",determinants
"how many multiplications are required to find an \( n \times n \) determinant from:
    \begin{enumerate}",determinants
the big formula (6)?,determinants
"the cofactor formula (10), building from the count for \( n-1 \)?",determinants
the product of pivots formula (including the elimination steps)?,determinants
"in a \( 5 \times 5 \) matrix, does a \( + \) sign or \( - \) sign go with \( a_{15}a_{24}a_{33}a_{42}a_{51} \) down the reverse diagonal? in other words, is \( p = (5,4,3,2,1) \) even or odd? the checkerboard pattern of \( \pm \) signs for cofactors does not give \( \det p \).",determinants
"if \( a \) is \( m \times n \) and \( b \) is \( n \times m \), explain why
    \[
    \det
    \begin{bmatrix}
    0 & a \\
    -b & i
    \end{bmatrix}
    = \det(ab).
    \]
    \textit{hint:} postmultiply by
    \[
    \begin{bmatrix}
    i & 0 \\
    b & i
    \end{bmatrix}.
    \]
    do an example with \( m < n \) and an example with \( m > n \). why does your second example automatically have \( \det(ab) = 0 \)?",determinants
"suppose the matrix \( a \) is fixed, except that \( a_{11} \) varies from \( -\infty \) to \( +\infty \). give examples in which \( \det a \) is always zero or never zero. then show from the cofactor expansion (8) that otherwise \( \det a = 0 \) for exactly one value of \( a_{11} \).",determinants
"compute the determinants of \( a \), \( b \), and \( c \) from six terms. are the rows independent?
    \[
    a =
    \begin{bmatrix}
    1 & 2 & 3 \\
    3 & 1 & 2 \\
    3 & 2 & 1
    \end{bmatrix},
    \quad
    b =
    \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 4 & 4 \\
    5 & 6 & 7
    \end{bmatrix},
    \quad
    c =
    \begin{bmatrix}
    1 & 1 & 1 \\
    1 & 1 & 0 \\
    1 & 0 & 0
    \end{bmatrix}.
    \]",determinants
"compute the determinants of \( a \), \( b \), and \( c \). are their columns independent?
    \[
    a =
    \begin{bmatrix}
    1 & 1 & 0 \\
    1 & 0 & 1 \\
    0 & 1 & 1
    \end{bmatrix},
    \quad
    b =
    \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
    \end{bmatrix},
    \quad
    c =
    \begin{bmatrix}
    a & 0 \\
    0 & b
    \end{bmatrix}.
    \]",determinants
"show that \( \det a = 0 \), regardless of the five nonzeros marked by \( x \)'s:
    \[
    a =
    \begin{bmatrix}
    x & x & x \\
    0 & 0 & x \\
    0 & 0 & x
    \end{bmatrix}.
    \]
    (what is the rank of \( a \)?)",determinants
"this problem shows in two ways that $\det a = 0$ (the $x$'s are any numbers):
    \[
    a =
    \begin{bmatrix}
    x & x & x & x & x \\
    x & x & x & x & x \\
    0 & 0 & 0 & x & x \\
    0 & 0 & 0 & x & x \\
    0 & 0 & 0 & x & x
    \end{bmatrix}.
    \]
    \begin{enumerate}",determinants
how do you know that the rows are linearly dependent?,determinants
explain why all 120 terms are zero in the big formula for $\det a$.,determinants
"find two ways to choose nonzeros from four different rows and columns:
    \[
    a =
    \begin{bmatrix}
    1 & 0 & 0 & 1 \\
    0 & 1 & 1 & 1 \\
    1 & 1 & 0 & 1 \\
    1 & 0 & 0 & 1
    \end{bmatrix},
    \quad
    b =
    \begin{bmatrix}
    1 & 0 & 0 & 2 \\
    0 & 3 & 4 & 5 \\
    5 & 4 & 0 & 3 \\
    2 & 0 & 0 & 1
    \end{bmatrix}.
    \]
    (b has the same zeros as a.) is $\det a$ equal to $1+1$, $1-1$, or $-1-1$? what is $\det b$?",determinants
place the smallest number of zeros in a $4 \times 4$ matrix that will guarantee $\det a = 0$. place as many zeros as possible while still allowing $\det a \neq 0$.,determinants
"if $a_{11} = a_{22} = a_{33} = 0$, how many of the six terms in $\det a$ will be zero?",determinants
"if $a_{11} = a_{22} = a_{33} = a_{44} = 0$, how many of the 24 products $a_{1j}a_{2k}a_{3\ell}a_{4m}$ are sure to be zero?",determinants
how many $5 \times 5$ permutation matrices have $\det p = +1$? those are even permutations. find one that needs four exchanges to reach the identity matrix.,determinants
"if $\det a \neq 0$, at least one of the $n!$ terms in the big formula (6) is not zero. deduce that some ordering of the rows of $a$ leaves no zeros on the diagonal. (don’t use $p$ from elimination; that $pa$ can have zeros on the diagonal.)",determinants
prove that 4 is the largest determinant for a $3 \times 3$ matrix of 1s and $-1$s.,determinants
"how many permutations of $(1,2,3,4)$ are even and what are they? extra credit: what are all the possible $4 \times 4$ determinants of $i + p_{\text{even}}$?",determinants
"problems 24–33 use cofactors $c_{ij} = (-1)^{i+j} \det m_{ij}$. delete row $i$, column $j$.
    \begin{enumerate}",determinants
"find cofactors and then transpose. multiply $c^t a$ and $c^t b$ by $a$ and $b$!
        \[
        a =
        \begin{bmatrix}
        2 & 1 \\
        3 & 6
        \end{bmatrix},
        \quad
        b =
        \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6 \\
        7 & 0 & 0
        \end{bmatrix}.
        \]",determinants
"find the cofactor matrix $c$ and compare $ac^t$ with $a^{-1}$:
    \[
    a =
    \begin{bmatrix}
    2 & -1 & 0 \\
    -1 & 2 & -1 \\
    0 & -1 & 2
    \end{bmatrix},
    \quad
    a^{-1} =
    \frac{1}{4}
    \begin{bmatrix}
    3 & 2 & 1 \\
    2 & 4 & 2 \\
    1 & 2 & 3
    \end{bmatrix}.
    \]",determinants
"this problem shows in two ways that $\det a = 0$ (the $x$'s are any numbers):
\[
a =
\begin{bmatrix}
x & x & x & x & x \\
x & x & x & x & x \\
0 & 0 & 0 & x & x \\
0 & 0 & 0 & x & x \\
0 & 0 & 0 & x & x
\end{bmatrix}
\]
\begin{itemize}",determinants
"find two ways to choose nonzeros from four different rows and columns:
\[
a =
\begin{bmatrix}
1 & 0 & 0 & 1 \\
0 & 1 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 0 & 0 & 1
\end{bmatrix}
\quad
b =
\begin{bmatrix}
1 & 0 & 0 & 2 \\
0 & 3 & 4 & 5 \\
5 & 4 & 0 & 3 \\
2 & 0 & 0 & 1
\end{bmatrix}
\]
is $\det a$ equal to $1+1$ or $1-1$ or $-1-1$? what is $\det b$?",determinants
\begin{itemize},determinants
"if $\det a \neq 0$, at least one of the $n!$ terms in the big formula is not zero. deduce that some ordering of the rows of $a$ leaves no zeros on the diagonal. (don't use $p$ from elimination; that $pa$ can have zeros on the diagonal.)",determinants
"problems 24–33 use cofactors $c_{ij} = (-1)^{i+j} \det m_{ij}$. delete row $i$, column $j$.
\begin{itemize}",determinants
"find the cofactor matrix $c$ and compare $ac^t$ with $a^{-1}$:
\[
a =
\begin{bmatrix}
2 & -1 & 0 \\
-1 & 2 & -1 \\
0 & -1 & 2
\end{bmatrix}
\]
\[
a^{-1} =
\frac{1}{4}
\begin{bmatrix}
3 & 2 & 1 \\
2 & 4 & 2 \\
1 & 2 & 3
\end{bmatrix}
\]",determinants
"for these matrices, find the only nonzero term in the big formula (6):
    \begin{itemize}",determinants
"\( a = \begin{pmatrix}
            0 & 1 & 0 & 0 \\
            1 & 0 & 1 & 0 \\
            0 & 1 & 0 & 1 \\
            0 & 0 & 1 & 0
        \end{pmatrix} \)",determinants
"\( b = \begin{pmatrix}
            0 & 0 & 1 & 2 \\
            0 & 3 & 4 & 5 \\
            6 & 7 & 8 & 9 \\
            0 & 0 & 0 & 1
        \end{pmatrix} \)
    \end{itemize}
    there is only one way of choosing four nonzero entries from different rows and different columns. by deciding even or odd, compute \( \det a \) and \( \det b \).",determinants
"true or false?
    \begin{itemize}",determinants
"let \( f_n \) be the determinant of the 1, 1, -1 tridiagonal matrix (n by n):
    \[
    f_n = \det \begin{pmatrix}
        1 & -1 & 0 & \cdots & 0 \\
        1 & 1 & -1 & \cdots & 0 \\
        0 & 1 & 1 & \ddots & \vdots \\
        \vdots & \ddots & \ddots & \ddots & -1 \\
        0 & \cdots & 0 & 1 & 1
    \end{pmatrix}.
    \]
    by expanding in cofactors along row 1, show that \( f_n = f_{n-1} + f_{n-2} \). this yields the fibonacci sequence 1, 2, 3, 5, 8, 13, ... for the determinants.",determinants
"suppose \( a_n \) is the n by n tridiagonal matrix with 1s on the three diagonals:
    \[
    a_1 = \begin{pmatrix} 1 \end{pmatrix}, \quad
    a_2 = \begin{pmatrix}
        1 & 1 \\
        1 & 1
    \end{pmatrix}, \quad
    a_3 = \begin{pmatrix}
        1 & 1 & 0 \\
        1 & 1 & 1 \\
        0 & 1 & 1
    \end{pmatrix}, \ldots
    \]
    let \( d_n \) be the determinant of \( a_n \); we want to find it.
    \begin{itemize}",determinants
"compute the determinants of \( a_2, a_3, a_4 \). can you predict \( a_n \)?
    \[
    a_2 = \begin{pmatrix}
        0 & 1 \\
        1 & 0
    \end{pmatrix}, \quad
    a_3 = \begin{pmatrix}
        0 & 1 & 1 \\
        1 & 0 & 1 \\
        1 & 1 & 0
    \end{pmatrix}, \quad
    a_4 = \begin{pmatrix}
        0 & 1 & 1 & 1 \\
        1 & 0 & 1 & 1 \\
        1 & 1 & 0 & 1 \\
        1 & 1 & 1 & 0
    \end{pmatrix}.
    \]
    use row operations to produce zeros, or use cofactors of row 1.",determinants
"how many multiplications to find an n by n determinant from
    \begin{itemize}",determinants
"in a 5 by 5 matrix, does a + sign or - sign go with \( a_{15} a_{24} a_{33} a_{42} a_{51} \) down the reverse diagonal? in other words,
::contentreference[oaicite:0]{index=0}",determinants
"for these matrices, find the only nonzero term in the big formula (6):
    \[
    a =
    \begin{bmatrix}
    0 & 1 & 0 & 0 \\
    1 & 0 & 1 & 0 \\
    0 & 1 & 0 & 1 \\
    0 & 0 & 1 & 0
    \end{bmatrix}
    \quad \text{and} \quad
    b =
    \begin{bmatrix}
    0 & 0 & 1 & 2 \\
    0 & 3 & 4 & 5 \\
    6 & 7 & 8 & 9 \\
    0 & 0 & 0 & 1
    \end{bmatrix}.
    \]
    there is only one way of choosing four nonzero entries from different rows and different columns. by deciding even or odd, compute $\det a$ and $\det b$.",determinants
expand those determinants in cofactors of the first row. find the cofactors (they include the signs $(-1)^{i+j}$) and the determinants of $a$ and $b$.,determinants
"let $f_n$ be the determinant of the $1, 1, -1$ tridiagonal matrix (n by n):
    \[
    f_n = \det
    \begin{bmatrix}
    1 & -1 \\
    1 & 1 & -1 \\
    & 1 & 1 & -1 \\
    & & \ddots & \ddots & \ddots \\
    & & & 1 & 1
    \end{bmatrix}.
    \]
    by expanding in cofactors along row 1, show that $f_n = f_{n-1} + f_{n-2}$. this yields the fibonacci sequence $1, 2, 3, 5, 8, 13, \ldots$ for the determinants.",determinants
"suppose $a_n$ is the $n$ by $n$ tridiagonal matrix with 1s on the three diagonals:
    \[
    a_1 =
    \begin{bmatrix}
    1
    \end{bmatrix},
    \quad
    a_2 =
    \begin{bmatrix}
    1 & 1 \\
    1 & 1
    \end{bmatrix},
    \quad
    a_3 =
    \begin{bmatrix}
    1 & 1 & 0 \\
    1 & 1 & 1 \\
    0 & 1 & 1
    \end{bmatrix},
    \quad \ldots
    \]
    let $d_n$ be the determinant of $a_n$; we want to find it.
    \begin{itemize}",determinants
"compute the determinants of $a_2$, $a_3$, $a_4$. can you predict $a_n$?
    \[
    a_2 =
    \begin{bmatrix}
    0 & 1 \\
    1 & 0
    \end{bmatrix},
    \quad
    a_3 =
    \begin{bmatrix}
    0 & 1 & 1 \\
    1 & 0 & 1 \\
    1 & 1 & 0
    \end{bmatrix},
    \quad
    a_4 =
    \begin{bmatrix}
    0 & 1 & 1 & 1 \\
    1 & 0 & 1 & 1 \\
    1 & 1 & 0 & 1 \\
    1 & 1 & 1 & 0
    \end{bmatrix}.
    \]
    use row operations to produce zeros, or use cofactors of row 1.",determinants
"using matlab, find the largest determinant of a $4 \times 4$ matrix of 0s and 1s.",determinants
"if you know that $\det a = 6$, what is the determinant of $b$?
    \[
    a =
    \begin{vmatrix}
    & & & \\
    & & & \\
    & & & \\
    & & & \\
    \end{vmatrix}
    = 6
    \]
    \[
    b =
    \begin{vmatrix}
    \text{row 1} + \text{row 2} \\
    \text{row 2} + \text{row 3} \\
    \text{row 3} + \text{row 1} \\
    \end{vmatrix}
    \]",determinants
"suppose the $4 \times 4$ matrix $m$ has four equal rows all containing $a, b, c, d$. we know that $\det(m) = 0$. the problem is to find $\det(i + m)$ by any method:
    \[
    i + m =
    \begin{vmatrix}
    1 + a & b & c & d \\
    a & 1 + b & c & d \\
    a & b & 1 + c & d \\
    a & b & c & 1 + d \\
    \end{vmatrix}
    \]
    partial credit if you find this determinant when $a = b = c = d = 1$. sudden death if you say that $\det(i + m) = \det i + \det m$.",determinants
"find the determinant and all nine cofactors $c_{ij}$ of this triangular matrix:
    \[
    a = \begin{bmatrix}
    1 & 2 & 3 \\
    0 & 4 & 0 \\
    0 & 0 & 5
    \end{bmatrix}
    \]
    form $c^t$ and verify that $a c^t = (\det a) i$. what is $a^{-1}$?",determinants
"use the cofactor matrix $c$ to invert these symmetric matrices:
    \[
    a = \begin{bmatrix}
    2 & -1 & 0 \\
    -1 & 2 & -1 \\
    0 & -1 & 2
    \end{bmatrix}
    \quad \text{and} \quad
    b = \begin{bmatrix}
    1 & 1 & 1 \\
    1 & 2 & 2 \\
    1 & 2 & 3
    \end{bmatrix}
    \]",determinants
"find $x$, $y$, and $z$ by cramer's rule in equation (4):
    \[
    ax + by = 1 \quad \text{and} \quad cx + dy = 0
    \]
    \[
    x + 4y - z = 1 \quad \text{and} \quad x + y + z = 0 \quad \text{and} \quad 2x + 3z = 0.
    \]",determinants
"find the determinant when a vector $x$ replaces column $j$ of the identity (consider $x_j = 0$ as a separate case):
        \[
        m = \begin{bmatrix}
        1 & x_1 \\
        1 & \cdot \\
        x_j & \cdot \\
        1 & x_n \\
        1
        \end{bmatrix}
        \]
        then $\det m = \ldots$.",determinants
"if $ax = b$, show that $am$ is the matrix $b_j$ in equation (4), with $b$ in column $j$.",determinants
derive cramer's rule by taking determinants in $am = b_j$.,determinants
"draw the triangle with vertices $a = (2,2)$, $b = (-1,3)$, and $c = (0,0)$. by regarding it as half of a parallelogram, explain why its area equals
        \[
        \text{area}(abc) = \frac{1}{2} \det \begin{bmatrix}
        2 & 2 \\
        -1 & 3
        \end{bmatrix}.
        \]",determinants
"move the third vertex to $c = (1,-4)$ and justify the formula
        \[
        \text{area}(abc) = \frac{1}{2} \det \begin{bmatrix}
        x_1 & y_1 & 1 \\
        x_2 & y_2 & 1 \\
        x_3 & y_3 & 1
        \end{bmatrix} = \frac{1}{2} \det \begin{bmatrix}
        2 & 2 & 1 \\
        -1 & 3 & 1 \\
        1 & -4 & 1
        \end{bmatrix}.
        \]
        hint: subtracting the last row from each of the others leaves
        \[
        \det \begin{bmatrix}
        2 & 2 & 1 \\
        -1 & 3 & 1 \\
        1 & -4 & 1
        \end{bmatrix} = \det \begin{bmatrix}
        1 & 6 & 0 \\
        -2 & 7 & 0 \\
        1 & -4 & 1
        \end{bmatrix} = \det \begin{bmatrix}
        1 & 6 \\
        -2 & 7
        \end{bmatrix}.
        \]
        sketch $a_0 = (1,6)$, $b_0 = (-2,7)$, $c_0 = (0,0)$ and their relation to $a$, $b$, $c$.",determinants
explain in terms of volumes why $\det(3a) = 3^n \det(a)$ for an $n \times n$ matrix $a$.,determinants
"predict in advance, and confirm by elimination, the pivot entries of
    \[
    a = \begin{bmatrix}
    2 & 1 & 2 \\
    4 & 5 & 0 \\
    2 & 7 & 0
    \end{bmatrix}
    \quad \text{and} \quad
    b = \begin{bmatrix}
    2 & 1 & 2 \\
    4 & 5 & 3 \\
    2 & 7 & 0
    \end{bmatrix}.
    \]",determinants
"find all the odd permutations of the numbers $\{1,2,3,4\}$. they come from an odd number of exchanges and lead to $\det(p) = -1$.",determinants
"suppose the permutation $p$ takes $(1,2,3,4,5)$ to $(5,4,1,2,3)$.
    \begin{enumerate}",determinants
"what does $p^2$ do to $(1,2,3,4,5)$?",determinants
"what does $p^{-1}$ do to $(1,2,3,4,5)$?",determinants
"if $p$ is an odd permutation, explain why $p^2$ is even but $p^{-1}$ is odd.",determinants
"prove that if you keep multiplying $a$ by the same permutation matrix $p$, the first row eventually comes back to its original place.",determinants
"if $a$ is a $5 \times 5$ matrix with all $|a_{ij}| \leq 1$, then $\det(a) \leq \ldots$. volumes or the big formula or pivots should give some upper bound on the determinant.",determinants
"solve these linear equations by cramer's rule $x_j = \frac{\det(b_j)}{\det(a)}$:
    \begin{enumerate}",determinants
$2x_1 + 5x_2 = 1$ \quad and \quad $x_1 + 4x_2 = 2$.,determinants
$2x_1 + x_2 = 1$ \quad and \quad $x_1 + 2x_2 + x_3 = 70$ \quad and \quad $x_2 + 2x_3 = 0$.,determinants
"use cramer's rule to solve for $y$ (only). call the $3 \times 3$ determinant $d$:
    \begin{enumerate}",determinants
$ax + by = 1$ \quad and \quad $cx + dy = 0$.,determinants
$ax + by + cz = 1$ \quad and \quad $dx + ey - fz = 0$ \quad and \quad $gx + hy + iz = 0$.,determinants
"cramer's rule breaks down when $\det(a) = 0$. example (a) has no solution, whereas (b) has infinitely many. what are the ratios $x_j = \frac{\det(b_j)}{\det(a)}$?
    \begin{enumerate}",determinants
$2x_1 + 3x_2 = 1$ \quad and \quad $4x_1 + 6x_2 = 1$. (parallel lines),determinants
$2x_1 + 3x_2 = 1$ \quad and \quad $4x_1 + 6x_2 = 2$. (same line),determinants
"quick proof of cramer's rule. the determinant is a linear function of column 1. it is zero if two columns are equal. when $b = a x = x_1 a_1 + x_2 a_2 + x_3 a_3$ goes into column 1 to produce $b_1$, the determinant is
    \[
    \left| \begin{array}{ccc} 
    b & a_2 & a_3
    \end{array} \right| = \left| \begin{array}{ccc} 
    x_1 a_1 + x_2 a_2 + x_3 a_3 & a_2 & a_3
    \end{array} \right| = x_1 \left| \begin{array}{ccc} 
    a_1 & a_2 & a_3
    \end{array} \right| = x_1 \det(a).
    \]
    \begin{enumerate}",determinants
what formula for $x_1$ comes from left side = right side?,determinants
what steps lead to the middle equation?,determinants
"if the right side $b$ is the last column of $a$, solve the $3 \times 3$ system $a x = b$. explain how each determinant in cramer's rule leads to your solution $x$.",determinants
"find $a^{-1}$ from the cofactor formula $c^t/\det(a)$. use symmetry in part (b):
    \begin{enumerate}",determinants
"$a = \begin{bmatrix} 
        1 & 2 & 0 \\
        0 & 3 & 0 \\
        0 & 4 & 1
        \end{bmatrix}$",determinants
"$a = \begin{bmatrix} 
        2 & -1 & 0 \\
        -1 & 2 & -1 \\
        0 & -1 & 2
        \end{bmatrix}$",determinants
"if all the cofactors are zero, how do you know that $a$ has no inverse? if none of the cofactors are zero, is $a$ sure to be invertible?",determinants
"find the cofactors of $a$ and multiply $a c^t$ to find $\det(a)$:
    \[
    a = \begin{bmatrix} 
    1 & 1 & 4 \\
    1 & 2 & 2 \\
    1 & 2 & 5
    \end{bmatrix}, \quad c = \begin{bmatrix} 
    6 & -3 & 0 \\
    \cdots & \cdots & \cdots
    \end{bmatrix}, \quad a c^t = \dots
    \]
    if you change that corner entry from 4 to 100, why is $\det(a)$ unchanged?",determinants
suppose $\det(a) = 1$ and you know all the cofactors. how can you find $a$?,determinants
"from the formula $a c^t = (\det(a)) i$, show that $\det(c) = (\det(a))^{n-1}$.",determinants
"(for professors only) if you know all 16 cofactors of a $4 \times 4$ invertible matrix $a$, how would you find $a$?",determinants
"if all entries of $a$ are integers, and $\det(a) = 1$ or $-1$, prove that all entries of $a^{-1}$ are integers. give a $2 \times 2$ example.",determinants
"l is lower triangular and s is symmetric. assume they are invertible:
    \[
    l = \begin{bmatrix} 
    a & 0 & 0 \\
    b & c & 0 \\
    d & e & f
    \end{bmatrix}, \quad s = \begin{bmatrix} 
    a & b & d \\
    b & c & e \\
    d & e & f
    \end{bmatrix}.
    \]
    \begin{enumerate}",determinants
which three cofactors of $l$ are zero? then $l^{-1}$ is lower triangular.,determinants
which three pairs of cofactors of $s$ are equal? then $s^{-1}$ is symmetric.,determinants
"for $n = 5$, the matrix $c$ contains cofactors, and each $4 \times 4$ cofactor contains terms, and each term needs multiplications. compare with $5^3 = 125$ for the gauss-jordan computation of $a^{-1}$.",determinants
"problems 27–36 are about area and volume by determinants:
    \begin{enumerate}",determinants
"find the area of the parallelogram with edges $\mathbf{v} = (3,2)$ and $\mathbf{w} = (1,4)$.",determinants
"find the area of the triangle with sides $\mathbf{v}$, $\mathbf{w}$, and $\mathbf{v} + \mathbf{w}$. draw it.",determinants
"find the area of the triangle with sides $\mathbf{v}$, $\mathbf{w}$, and $\mathbf{w} - \mathbf{v}$. draw it.",determinants
"a box has edges from $(0,0,0)$ to $(3,1,1)$, $(1,3,1)$, and $(1,1,3)$. find its volume and also find the area of each parallelogram face.",determinants
"the corners of a triangle are $(2,1)$, $(3,4)$, and $(0,5)$. what is the area?",determinants
"a new corner at $(-1,0)$ makes it lopsided (four sides). find the area.",determinants
"the parallelogram with sides $(2,1)$ and $(2,3)$ has the same area as the parallelogram with sides $(2,2)$ and $(1,3)$. find those areas from $2 \times 2$ determinants and say why they must be equal. (i can't see why from a picture. please write to me if you do.)",determinants
"the hadamard matrix $h$ has orthogonal rows. the box is a hypercube! what is
    \[
    \det(h) = \left| \begin{array}{cccc} 
    1 & 1 & 1 & 1 \\
    1 & 1 & -1 & -1 \\
    1 & -1 & -1 & 1 \\
    1 & -1 & 1 & -1
    \end{array} \right|
    \]
    (volume of a hypercube in $\mathbb{r}^4$)?",determinants
"if the columns of a $4 \times 4$ matrix have lengths $l_1$, $l_2$, $l_3$, $l_4$, what is the largest possible value for the determinant (based on volume)? if all entries are 1 or -1, what are those lengths and the maximum determinant?",determinants
show by a picture how a rectangle with area $x_1 y_2$ minus a rectangle with area $x_2 y_1$ produces the area $x_1 y_2 - x_2 y_1$ of a parallelogram.,determinants
"when the edge vectors $\mathbf{a}$, $\mathbf{b}$, $\mathbf{c}$ are perpendicular, the volume of the box is $|\mathbf{a}| \times |\mathbf{b}| \times |\mathbf{c}|$. the matrix $a^t a$ is \dots. find $\det(a^t a)$ and $\det(a)$.",determinants
an $n$-dimensional cube has how many corners? how many edges? how many $(n-1)$-dimensional faces? the $n$-cube whose edges are the rows of $2i$ has volume \dots. a hypercube computer has parallel processors at the corners with connections along the edges.,determinants
"the triangle with corners $(0,0)$, $(1,0)$, $(0,1)$ has area $\frac{1}{2}$. the pyramid with four corners $(0,0,0)$, $(1,0,0)$, $(0,1,0)$, $(0,0,1)$ has volume \dots. the pyramid in $\mathbb{r}^4$ with five corners at $(0,0,0,0)$ and the rows of $i$ has what volume?",determinants
"problems 37–40 are about areas $da$ and volumes $dv$ in calculus:
    \begin{enumerate}",determinants
"polar coordinates satisfy $x = r \cos \theta$ and $y = r \sin \theta$. polar area $j dr d\theta$ includes $j$:
        \[
        j = \left| \begin{matrix} 
        \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\
        \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta}
        \end{matrix} \right|
        = \left| \begin{matrix} 
        \cos \theta & -r \sin \theta \\
        \sin \theta & r \cos \theta
        \end{matrix} \right|
        = r.
        \]
        the two columns are orthogonal. their lengths are \dots. thus $j = r$.",determinants
"spherical coordinates $\rho$, $\phi$, $\theta$ give $x = \rho \sin \phi \cos \theta$, $y = \rho \sin \phi \sin \theta$, $z = \rho \cos \phi$. find the jacobian matrix of 9 partial derivatives: $\frac{\partial x}{\partial \rho}$, $\frac{\partial x}{\partial \phi}$, $\frac{\partial x}{\partial \theta}$ are in row 1. simplify its determinant to $j = \rho^2 \sin \phi$. then $dv = \rho^2 \sin \phi \, d\rho \, d\phi \, d\theta$.",determinants
"the matrix that connects $r$, $\theta$ to $x$, $y$ is in problem 37. invert that matrix:
        \[
        j^{-1} = \left| \begin{matrix}
        \frac{\partial r}{\partial x} & \frac{\partial r}{\partial y} \\
        \frac{\partial \theta}{\partial x} & \frac{\partial \theta}{\partial y}
        \end{matrix} \right| = \left| \begin{matrix}
        \cos \theta & ? \\
        ? & ?
        \end{matrix} \right| = ?
        \]
        it is surprising that $\frac{\partial r}{\partial x} = \frac{\partial x}{\partial r}$. the product $jj^{-1} = i$ gives the chain rule:
        \[
        \frac{\partial x}{\partial x} = \frac{\partial x}{\partial r} \frac{\partial r}{\partial x} + \frac{\partial x}{\partial \theta} \frac{\partial \theta}{\partial x} = 1.
        \]",determinants
"the triangle with corners $(0,0)$, $(6,0)$, and $(1,4)$ has area \dots. when you rotate it by $\theta = 60^\circ$ the area is \dots. the rotation matrix has determinant:
        \[
        \left| \begin{matrix} 
        \cos \theta & -\sin \theta \\
        \sin \theta & \cos \theta
        \end{matrix} \right|
        = \left| \begin{matrix} 
        \frac{1}{2} & ? \\
        ? & ?
        \end{matrix} \right| = ?
        \]",determinants
"let $p = (1,0,-1)$, $q = (1,1,1)$, and $r = (2,2,1)$. choose $s$ so that $pqrs$ is a parallelogram, and compute its area. choose $t$, $u$, $v$ so that $opqrstuv$ is a tilted box, and compute its volume.",determinants
"suppose $(x,y,z)$, $(1,1,0)$, and $(1,2,1)$ lie on a plane through the origin. what determinant is zero? what equation does this give for the plane?",determinants
"suppose $(x,y,z)$ is a linear combination of $(2,3,1)$ and $(1,2,3)$. what determinant is zero? what equation does this give for the plane of all combinations?",determinants
"if $ax = (1, 0, \dots, 0)$, show how cramer's rule gives $x = \text{first column of } a^{-1}$.",determinants
"(visa to avis) this takes an odd number of exchanges (ivsa, avsi, avis). count the pairs of letters in visa and avis that are reversed from alphabetical order. the difference should be odd.",determinants
"find the determinants of:
    \[
    \left[ \begin{array}{cccc}
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 2 \\
    1 & 1 & 3 & 1 \\
    1 & 4 & 1 & 1 \\
    \end{array} \right]
    \quad \text{and} \quad
    \left[ \begin{array}{cccc}
    2 & -1 & 0 & -1 \\
    -1 & 2 & -1 & 0 \\
    0 & -1 & 2 & -1 \\
    -1 & 0 & -1 & 2 \\
    \end{array} \right]
    \]",determinants
"if $b = m^{-1} a m$, why is $\det b = \det a$? show also that $\det a^{-1} b = 1$.",determinants
"starting with $a$, multiply its first row by 3 to produce $b$, and subtract the first row of $b$ from the second to produce $c$. how is $\det c$ related to $\det a$?",determinants
"solve $3u + 2v = 7$, $4u + 3v = 11$ by cramer's rule.",determinants
"if the entries of $a$ and $a^{-1}$ are all integers, how do you know that both determinants are 1 or -1? hint: what is $\det a$ times $\det a^{-1}$?",determinants
"find all the cofactors, and the inverse or the nullspace, of:
    \[
    \left[ \begin{array}{cc}
    3 & 5 \\
    6 & 9 \\
    \end{array} \right],
    \quad
    \left[ \begin{array}{cc}
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta \\
    \end{array} \right],
    \quad
    \left[ \begin{array}{cc}
    a & b \\
    a & b \\
    \end{array} \right].
    \]",determinants
"what is the volume of the parallelepiped with four of its vertices at $(0, 0, 0)$, $(-1, 2, 2)$, $(2, -1, 2)$, and $(2, 2, -1)$? where are the other four vertices?",determinants
"how many terms are in the expansion of a $5 \times 5$ determinant, and how many are sure to be zero if $a_{21} = 0$?",determinants
"if $p_1$ is an even permutation matrix and $p_2$ is odd, deduce from $p_1 + p_2 = p_1 (p_1^t + p_2^t) p_2$ that $\det(p_1 + p_2) = 0$.",determinants
"if $\det a > 0$, show that $a$ can be connected to $i$ by a continuous chain of matrices $a(t)$ all with positive determinants. (the straight path $a(t) = a + t(i - a)$ does go from $a(0) = a$ to $a(1) = i$, but in between $a(t)$ might be singular. the problem is not so easy, and solutions are welcomed by the author.)",determinants
"explain why the point $(x,y)$ is on the line through $(2,8)$ and $(4,7)$ if
    \[
    \det \left[ \begin{array}{ccc}
    x & y & 1 \\
    2 & 8 & 1 \\
    4 & 7 & 1 \\
    \end{array} \right] = 0, \quad \text{or} \quad x + 2y - 18 = 0.
    \]",determinants
"in analogy with the previous exercise, what is the equation for $(x,y,z)$ to be on the plane through $(2,0,0)$, $(0,2,0)$, and $(0,0,4)$? it involves a 4 by 4 determinant.",determinants
"if the points $(x,y,z)$, $(2,1,0)$, and $(1,1,1)$ lie on a plane through the origin, what determinant is zero? are the vectors $(1,0,-1)$, $(2,1,0)$, $(1,1,1)$ independent?",determinants
"if every row of $a$ has either a single $+1$, or a single $-1$, or one of each (and is otherwise zero), show that $\det a = 1$ or $-1$ or $0$.",determinants
"if $c = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$ and $d = \begin{bmatrix} u & v \\ w & z \end{bmatrix}$, then $cd = -dc$ yields 4 equations $ax = 0$:
    \[
    cd + dc = 0 \quad \rightarrow \quad
    \left[ \begin{array}{cccc}
    2a & c & b & 0 \\
    b & a+d & 0 & b \\
    c & 0 & a+d & c \\
    0 & c & b & 2d \\
    \end{array} \right]
    \left[ \begin{array}{c}
    u \\
    v \\
    w \\
    z \\
    \end{array} \right]
    = 
    \left[ \begin{array}{c}
    0 \\
    0 \\
    0 \\
    0 \\
    \end{array} \right].
    \]
    \begin{enumerate}",determinants
"show that $\det a = 0$ if $a+d = 0$. solve for $u$, $v$, $w$, $z$, the entries of $d$.",determinants
show that $\det a = 0$ if $ad = bc$ (so $c$ is singular).,determinants
"in all other cases, $cd = -dc$ is only possible with $d = \text{zero matrix}$.",determinants
"the circular shift permutes $(1, 2, \dots, n)$ into $(2, 3, \dots, 1)$. what is the corresponding permutation matrix $p$, and (depending on $n$) what is its determinant?",determinants
"find the determinant of $a = \text{eye}(5) + \text{ones}(5)$ and, if possible, $\text{eye}(n) + \text{ones}(n)$.",determinants
"prove that if \( a \) is an \( n \times n \) matrix and \( b \) is a matrix obtained from \( a \) by swapping two rows, then \( \det(b) = -\det(a) \). additionally, explain how this property extends to higher dimensions, and deduce the implications for the determinant of a matrix when two rows are identical. use this result to prove that the determinant of a singular matrix is zero.",determinants
"let \( a \) be a square matrix. prove that \( \det(a^t) = \det(a) \). further, provide a geometric interpretation of this property in terms of volumes of parallelepipeds in euclidean space. investigate the relationship between the determinant of a matrix and the determinant of its inverse, and deduce a formula for the determinant of the inverse matrix when \( \det(a) \neq 0 \).",determinants
"consider the matrix \( a \in \mathbb{r}^{n \times n} \) and suppose that its eigenvalues are \( \lambda_1, \lambda_2, \dots, \lambda_n \). prove that the determinant of \( a \) is equal to the product of its eigenvalues, i.e., \( \det(a) = \prod_{i=1}^{n} \lambda_i \). use this result to analyze the effect of matrix diagonalization on the determinant, and explain how this formula applies when \( a \) is a diagonalizable matrix.",determinants
"prove that for any square matrix \( a \), the determinant satisfies the multilinearity property, i.e., for vectors \( \mathbf{v}_1, \dots, \mathbf{v}_n \in \mathbb{r}^n \), the determinant is linear in each row of the matrix. specifically, show that if one row of a matrix is a linear combination of two other rows, then the determinant of the matrix is zero. discuss the implications of this property for solving systems of linear equations.",determinants
"let \( a \) be a square matrix, and consider the following statement: “if \( a \) is a triangular matrix (either upper or lower), then \( \det(a) \) is the product of the diagonal elements.” prove this statement by induction on \( n \), the size of the matrix. extend the result to block triangular matrices, and provide a geometric interpretation of the determinant in the context of scaling and shearing transformations in euclidean space.",determinants
"prove the following generalization of the cofactor expansion formula for the determinant: for any \( n \times n \) matrix \( a \), the determinant can be expanded along any row or column as follows:
    \[
    \det(a) = \sum_{i=1}^{n} (-1)^{i+j} a_{ij} \det(a_{ij})
    \]
    where \( a_{ij} \) is the matrix obtained by removing the \( i \)-th row and \( j \)-th column from \( a \). demonstrate how this formula can be applied to calculate the determinant of a 4x4 matrix, and explore the computational complexity of this method for larger matrices.",determinants
"let \( a \) and \( b \) be \( n \times n \) matrices. prove that \( \det(ab) = \det(a) \det(b) \). additionally, investigate whether this property holds for matrices of different dimensions and provide an example of a situation where the product of two non-square matrices is not defined. discuss the implications of this result in the context of matrix factorization methods such as lu and qr decomposition.",determinants
"consider a square matrix \( a \in \mathbb{r}^{n \times n} \) and a scalar \( \lambda \). prove that \( \det(\lambda a) = \lambda^n \det(a) \). use this result to deduce the behavior of the determinant under scalar multiplication, and explain how it can be used to determine the volume scaling factor when applying a linear transformation represented by \( a \) to a geometric object in \( \mathbb{r}^n \).",determinants
"let \( a \) be a square matrix, and suppose that \( a \) is symmetric. prove that the eigenvalues of \( a \) are real, and that \( a \) is diagonalizable. use this result to show that the determinant of a symmetric matrix is always the product of its eigenvalues, and discuss how this property is useful in analyzing the stability of dynamical systems modeled by symmetric matrices.",determinants
"let \( a \) be a square matrix, and let \( b \) be the matrix obtained by adding a scalar multiple of one row of \( a \) to another row. prove that \( \det(b) = \det(a) \). discuss the implications of this result for row reduction in gaussian elimination, and explain how it helps in proving the invertibility criterion for matrices (i.e., that a matrix is invertible if and only if its determinant is non-zero).",determinants
"given an \( n \times n \) matrix \( a \), prove that the determinant is invariant under row swapping, i.e., if two rows of a matrix are swapped, the determinant changes its sign. provide a detailed proof and explain how this property follows from the cofactor expansion formula. furthermore, demonstrate how this property can be used in conjunction with gaussian elimination to determine the determinant of a matrix by transforming it into an upper triangular form.",determinants
"let \( a \) be an \( n \times n \) matrix, and let \( r_1 \), \( r_2 \), and \( r_3 \) represent rows of \( a \). suppose that we perform a row operation that adds a scalar multiple of \( r_1 \) to \( r_2 \) (i.e., \( r_2 \leftarrow r_2 + \alpha r_1 \)) and adds a scalar multiple of \( r_3 \) to \( r_1 \) (i.e., \( r_1 \leftarrow r_1 + \beta r_3 \)). prove that the determinant of the matrix after these operations is equal to the determinant of the original matrix, i.e., \( \det(a') = \det(a) \), and discuss the geometric interpretation of this result in the context of volume scaling. extend this result to a sequence of multiple such row operations.",determinants
"prove that if a matrix \( a \) is transformed into row echelon form (ref) by a sequence of elementary row operations, then the determinant of the matrix is the product of the diagonal entries of the resulting upper triangular matrix, up to a sign correction determined by the number of row swaps. specifically, show that if \( a \) is transformed to an upper triangular matrix \( u \) by a sequence of row operations that involve a certain number of row swaps, the determinant of \( a \) is given by
    \[
    \det(a) = (-1)^s \prod_{i=1}^{n} u_{ii}
    \]
    where \( s \) is the number of row swaps, and \( u_{ii} \) are the diagonal entries of \( u \). use this result to compute the determinant of a given matrix by row reduction.",determinants
"let \( a \) be an \( n \times n \) matrix, and let \( \{e_1, e_2, \dots, e_k\} \) be a sequence of elementary row operations that transform \( a \) into an upper triangular matrix. prove that the determinant of \( a \) is the product of the determinants of the elementary matrices corresponding to the row operations. specifically, show that
    \[
    \det(a) = \prod_{i=1}^{k} \det(e_i) \cdot \det(\text{upper triangular matrix after row operations})
    \]
    and discuss the properties of elementary matrices, including their effect on the determinant (i.e., \( \det(e) = 1 \) for a row swap, \( \det(e) = \alpha \) for scaling a row by a scalar \( \alpha \), and \( \det(e) = 1 \) for adding a multiple of one row to another).",determinants
"consider the matrix \( a \in \mathbb{r}^{n \times n} \), and suppose that \( a \) is transformed into its row echelon form by a sequence of elementary row operations. let \( l \) be the lower triangular matrix representing the matrix of multipliers used during gaussian elimination. prove that \( \det(a) = \det(l) \cdot \det(u) \), where \( u \) is the upper triangular matrix obtained after row reduction, and discuss how row operations affect the determinant of a matrix. specifically, prove that the determinant of \( l \) is 1, and use this result to compute the determinant of \( a \) using only the upper triangular matrix \( u \).",determinants
"given the matrix 
    \[
    a = \begin{pmatrix} 2 & 3 & 1 \\ 4 & 7 & 2 \\ 6 & 10 & 3 \end{pmatrix},
    \]
    apply gaussian elimination to find its determinant. provide a detailed step-by-step solution, including all row operations performed to reduce the matrix to row echelon form, and show how the determinant is calculated by the product of the diagonal entries, adjusted for row swaps. discuss the computational efficiency of this method for larger matrices and how it relates to the concept of matrix rank.",determinants
"consider the matrix 
    \[
    a = \begin{pmatrix} 1 & 3 & 2 & 4 \\ 2 & 7 & 3 & 8 \\ 3 & 12 & 4 & 13 \\ 4 & 16 & 5 & 17 \end{pmatrix}.
    \]
    using gaussian elimination, reduce \( a \) to upper triangular form and calculate its determinant. include all the intermediate steps in your solution and explain the effect of each row operation on the matrix. afterward, investigate the geometric meaning of the determinant of a 4x4 matrix in terms of volume and its relationship with linear transformations.",determinants
"let \( a \in \mathbb{r}^{5 \times 5} \) be a matrix with entries 
    \[
    a = \begin{pmatrix} 
    1 & 2 & 3 & 4 & 5 \\ 
    6 & 7 & 8 & 9 & 10 \\ 
    11 & 12 & 13 & 14 & 15 \\ 
    16 & 17 & 18 & 19 & 20 \\ 
    21 & 22 & 23 & 24 & 25
    \end{pmatrix}.
    \]
    perform gaussian elimination on \( a \) to reduce it to row echelon form and compute the determinant of \( a \). discuss how the determinant changes after each row operation and the computational challenges faced when dealing with larger matrices.",determinants
"let \( a = \begin{pmatrix} 3 & 4 & 5 \\ 2 & 5 & 7 \\ 6 & 2 & 3 \end{pmatrix} \). use row operations to reduce \( a \) to upper triangular form and compute its determinant. show all steps of your solution and analyze the impact of elementary row operations on the matrix. in addition, prove that gaussian elimination is equivalent to performing a sequence of elementary row operations on a matrix.",determinants
"consider the following matrix \( a \):
    \[
    a = \begin{pmatrix} 
    1 & 2 & 1 & 3 \\ 
    0 & 1 & 4 & 5 \\ 
    2 & 3 & 5 & 6 \\ 
    1 & 2 & 2 & 4
    \end{pmatrix}.
    \]
    using gaussian elimination, reduce \( a \) to row echelon form, and then compute its determinant. explain the significance of each row operation performed, and discuss how the result can be interpreted geometrically. afterward, compare the determinant of this matrix with that of a similar matrix with a small perturbation in the entries.",determinants
"given the matrix 
    \[
    a = \begin{pmatrix} 2 & 4 & 1 & 3 \\ 1 & 3 & 5 & 7 \\ 3 & 6 & 4 & 8 \\ 2 & 5 & 6 & 9 \end{pmatrix},
    \]
    apply gaussian elimination to reduce \( a \) to row echelon form, and calculate the determinant. discuss the mathematical principles behind row operations that preserve the determinant and those that change its value. analyze how gaussian elimination reveals important properties of the matrix, such as its rank and invertibility.",determinants
"let 
    \[
    a = \begin{pmatrix} 1 & 0 & 2 \\ 3 & 1 & 4 \\ 5 & 2 & 6 \end{pmatrix}.
    \]
    perform gaussian elimination on \( a \) and calculate its determinant. provide a detailed solution, including every row operation performed, and discuss how the determinant is affected by the operations. extend this method to a general \( 3 \times 3 \) matrix and derive a general formula for the determinant using gaussian elimination.",determinants
"consider the matrix 
    \[
    a = \begin{pmatrix} 2 & 1 & 3 & 4 \\ 5 & 2 & 1 & 3 \\ 7 & 3 & 2 & 4 \\ 1 & 5 & 6 & 7 \end{pmatrix}.
    \]
    use gaussian elimination to reduce this matrix to row echelon form, and then compute its determinant. afterward, investigate the effect of performing gaussian elimination with partial pivoting on the accuracy and stability of the determinant computation.",determinants
"let \( a \) be a matrix in \( \mathbb{r}^{3 \times 3} \), and suppose that it is in row echelon form:
    \[
    a = \begin{pmatrix} 1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & 0 & 6 \end{pmatrix}.
    \]
    compute its determinant directly from the row echelon form and explain why this method is more efficient than using cofactor expansion. discuss how row echelon form simplifies the determinant computation and why it is crucial for matrix inversion algorithms.",determinants
"consider the matrix 
    \[
    a = \begin{pmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & 1 \end{pmatrix}.
    \]
    perform gaussian elimination on \( a \) and find its determinant. discuss the implications of row echelon form for systems of linear equations and how gaussian elimination can be used to solve such systems while computing the determinant. additionally, demonstrate how gaussian elimination can be used to compute the determinant of a singular matrix.",determinants
"let \( a \) be a diagonal matrix with entries \( a_1, a_2, \dots, a_n \) on the diagonal. prove that the determinant of \( a \) is the product of the diagonal entries, i.e., 
    \[
    \det(a) = \prod_{i=1}^{n} a_i.
    \]
    subsequently, consider a diagonal matrix \( d = \text{diag}(2, 3, 5) \) and compute its determinant. further, demonstrate that if the matrix \( a \) is scaled by a scalar \( \alpha \), then the determinant of \( \alpha a \) is given by \( \alpha^n \det(a) \), where \( n \) is the size of the matrix. apply this property to an arbitrary \( 3 \times 3 \) diagonal matrix.",determinants
"let \( a \) be a symmetric matrix, and suppose that \( a \) is diagonalizable, i.e., \( a = p \lambda p^t \), where \( p \) is an orthogonal matrix and \( \lambda \) is a diagonal matrix of eigenvalues. show that the determinant of \( a \) is the product of its eigenvalues, i.e., 
    \[
    \det(a) = \prod_{i=1}^{n} \lambda_i.
    \]
    compute the determinant of a symmetric matrix \( a = \begin{pmatrix} 4 & 2 \\ 2 & 3 \end{pmatrix} \) by finding its eigenvalues and use the above property to verify the result. discuss the geometric interpretation of this property in terms of the scaling effect of linear transformations represented by symmetric matrices.",determinants
"prove that the determinant of a triangular matrix, whether upper or lower triangular, is the product of the diagonal entries. consider the matrix 
    \[
    a = \begin{pmatrix} 
    3 & 5 & 1 \\ 
    0 & 2 & 4 \\ 
    0 & 0 & 6
    \end{pmatrix}.
    \]
    compute its determinant by applying the formula for triangular matrices and discuss how this property simplifies the computation of determinants. furthermore, explain why triangular matrices are crucial in solving systems of linear equations and in lu decomposition.",determinants
"let \( a \) be a block matrix of the form 
    \[
    a = \begin{pmatrix} 
    b & c \\ 
    d & e
    \end{pmatrix},
    \]
    where \( b \), \( c \), \( d \), and \( e \) are square matrices. suppose that \( b \) and \( e \) are invertible. prove that the determinant of \( a \) is given by 
    \[
    \det(a) = \det(b) \det(e - d b^{-1} c).
    \]
    as an example, calculate the determinant of the following block matrix, 
    \[
    a = \begin{pmatrix} 
    2 & 0 & 1 \\ 
    0 & 3 & 4 \\ 
    0 & 0 & 5
    \end{pmatrix}.
    \]
    provide a detailed step-by-step solution and interpret the result in the context of matrix factorization techniques.",determinants
"consider the matrix 
    \[
    a = \begin{pmatrix} 
    1 & 0 & 0 \\ 
    0 & 1 & 0 \\ 
    0 & 0 & 0
    \end{pmatrix}.
    \]
    discuss the properties of singular matrices and show that the determinant of \( a \) is zero. further, prove that if a matrix has a row or column of zeros, its determinant is zero. explain how this property relates to the concept of the rank of a matrix and provide an example of a matrix with full rank where this property does not hold.",determinants
"let \( a \) be a circulant matrix of the form 
    \[
    a = \begin{pmatrix} 
    a_0 & a_1 & \cdots & a_{n-1} \\ 
    a_{n-1} & a_0 & \cdots & a_{n-2} \\ 
    \vdots & \vdots & \ddots & \vdots \\ 
    a_1 & a_2 & \cdots & a_0
    \end{pmatrix}.
    \]
    prove that the determinant of a circulant matrix is the product of the eigenvalues, which can be computed using the discrete fourier transform (dft) of the first row of the matrix. apply this result to compute the determinant of the circulant matrix 
    \[
    a = \begin{pmatrix} 
    1 & 2 & 3 \\ 
    3 & 1 & 2 \\ 
    2 & 3 & 1 
    \end{pmatrix}.
    \]
    discuss how this result extends to higher-dimensional circulant matrices and its connection to the concept of circulant graphs.",determinants
"let \( a \) be a \( 2 \times 2 \) matrix given by 
    \[
    a = \begin{pmatrix} 
    a & b \\ 
    c & d
    \end{pmatrix}.
    \]
    show that the determinant of \( a \) is given by 
    \[
    \det(a) = ad - bc.
    \]
    additionally, prove that the determinant of the inverse of \( a \), if it exists, is given by 
    \[
    \det(a^{-1}) = \frac{1}{\det(a)}.
    \]
    using this result, calculate the determinant of the inverse of the matrix 
    \[
    a = \begin{pmatrix} 
    3 & 2 \\ 
    5 & 7
    \end{pmatrix}.
    \]
    discuss how this property extends to larger square matrices and its implications in matrix inversion techniques.",determinants
"let \( a \) be an orthogonal matrix, i.e., \( a^t a = i \). prove that the determinant of an orthogonal matrix is either \( 1 \) or \( -1 \), and provide a detailed explanation of why this property holds. as an example, compute the determinant of the matrix 
    \[
    a = \begin{pmatrix} 
    0 & 1 \\ 
    -1 & 0
    \end{pmatrix}.
    \]
    discuss the significance of this result in the context of rotations in euclidean space and the preservation of volume by orthogonal transformations.",determinants
"let \( a \) be a matrix of the form 
    \[
    a = \begin{pmatrix} 
    0 & 0 & 1 \\ 
    1 & 0 & 0 \\ 
    0 & 1 & 0
    \end{pmatrix}.
    \]
    compute the determinant of \( a \) and explain how this matrix relates to the concept of a permutation matrix. show that the determinant of a permutation matrix is \( 1 \) if the number of row swaps is even, and \( -1 \) if the number of row swaps is odd. apply this result to compute the determinant of the permutation matrix corresponding to the 3-cycle \( (1 \ 2 \ 3) \).",determinants
"let \( a \) be an upper hessenberg matrix, i.e., a matrix where all entries below the first subdiagonal are zero. prove that the determinant of an upper hessenberg matrix can be computed by a recursive formula involving the diagonal and subdiagonal entries. compute the determinant of the following \( 4 \times 4 \) upper hessenberg matrix:
    \[
    a = \begin{pmatrix} 
    1 & 2 & 3 & 4 \\ 
    5 & 6 & 7 & 8 \\ 
    0 & 10 & 11 & 12 \\ 
    0 & 0 & 14 & 15
    \end{pmatrix}.
    \]
    discuss the role of hessenberg matrices in numerical methods, particularly in eigenvalue algorithms and lu decomposition.",determinants
"let \( a = \begin{pmatrix} 
    1 & 2 & 3 \\ 
    0 & 1 & 4 \\ 
    5 & 6 & 0
    \end{pmatrix} \). compute the determinant of \( a \) using cofactor expansion along the first row. subsequently, use the formula for minors and cofactors to compute the determinant and verify the result through row reduction. explain how these methods relate to the underlying properties of matrix determinants.",determinants
"given the matrix \( a = \begin{pmatrix} 
    4 & 2 & 1 \\ 
    3 & 1 & 2 \\ 
    1 & 3 & 4
    \end{pmatrix} \), compute the determinant of \( a \) by applying the cofactor expansion method along the second row. then, using this result, compute the minor and cofactor for the entry in the second row, first column. discuss the geometric interpretation of the determinant in terms of the volume of a parallelepiped.",determinants
"prove the cofactor expansion formula for the determinant of an \( n \times n \) matrix. specifically, for a given \( n \times n \) matrix \( a \), show that 
    \[
    \det(a) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \det(a_{ij}),
    \]
    where \( a_{ij} \) is the matrix obtained by deleting the \( i \)-th row and \( j \)-th column of \( a \). apply this formula to compute the determinant of the matrix 
    \[
    a = \begin{pmatrix} 
    1 & 0 & 2 \\ 
    3 & 4 & 1 \\ 
    5 & 6 & 0
    \end{pmatrix}.
    \]
    illustrate the importance of this expansion in computing the determinant of large matrices.",determinants
"let \( a \) be a \( 4 \times 4 \) matrix given by 
    \[
    a = \begin{pmatrix} 
    1 & 2 & 3 & 4 \\ 
    5 & 6 & 7 & 8 \\ 
    9 & 10 & 11 & 12 \\ 
    13 & 14 & 15 & 16
    \end{pmatrix}.
    \]
    compute the determinant of \( a \) using the cofactor expansion along the first row and confirm the result by applying row reduction. discuss the computational advantages and limitations of each method in the context of this matrix.",determinants
"using the formula for minors and cofactors, compute the determinant of the matrix 
    \[
    a = \begin{pmatrix} 
    3 & 1 & 4 & 1 \\ 
    5 & 9 & 2 & 6 \\ 
    5 & 3 & 5 & 7 \\ 
    2 & 8 & 5 & 3
    \end{pmatrix}.
    \]
    then, verify the result by reducing the matrix to upper triangular form and computing the determinant from the product of diagonal entries. discuss how these two methods provide insights into the structure of the matrix.",determinants
"prove the formula for the determinant of a product of two matrices, i.e., 
    \[
    \det(ab) = \det(a) \det(b),
    \]
    where \( a \) and \( b \) are square matrices of the same size. use this property to compute the determinant of the matrix 
    \[
    a = \begin{pmatrix} 
    1 & 0 \\ 
    2 & 3
    \end{pmatrix} \quad \text{and} \quad b = \begin{pmatrix} 
    4 & 5 \\ 
    6 & 7
    \end{pmatrix}.
    \]
    discuss how this result is used in the context of lu decomposition and matrix factorizations.",determinants
"let \( a \) be an upper triangular matrix of the form 
    \[
    a = \begin{pmatrix} 
    2 & 4 & 3 \\ 
    0 & 5 & 1 \\ 
    0 & 0 & 6
    \end{pmatrix}.
    \]
    compute the determinant of \( a \) and compare this result with that obtained from cofactor expansion. explain why the determinant of a triangular matrix is simply the product of its diagonal entries, and discuss the implications for matrix inversion and linear systems.",determinants
"given the matrix \( a = \begin{pmatrix} 
    1 & 1 & 1 \\ 
    2 & 4 & 8 \\ 
    3 & 9 & 27
    \end{pmatrix} \), compute its determinant by applying row operations to transform the matrix into upper triangular form. discuss how the choice of row operations affects the computation and how the determinant is affected by elementary row operations.",determinants
"use the method of lu decomposition to compute the determinant of the matrix 
    \[
    a = \begin{pmatrix} 
    4 & 3 & 2 \\ 
    2 & 1 & 3 \\ 
    3 & 2 & 1
    \end{pmatrix}.
    \]
    compute the lu factorization and use the property \( \det(a) = \det(l) \det(u) \) to find the determinant. compare the result with the cofactor expansion method and discuss the computational efficiency of lu decomposition for larger matrices.",determinants
"let \( a \) be a matrix of the form 
    \[
    a = \begin{pmatrix} 
    2 & 4 & 1 \\ 
    0 & 3 & 5 \\ 
    6 & 7 & 8
    \end{pmatrix}.
    \]
    using the cofactor expansion method, compute the determinant of \( a \). then, use row operations to simplify \( a \) and verify your result. discuss how row operations simplify the calculation and the role of the determinant in understanding the invertibility of the matrix.",determinants
"show that the determinant of a matrix is a multilinear function of its rows (or columns). specifically, prove that if a matrix is formed by linear combinations of rows (or columns), the determinant is linear in each row (or column). use this property to compute the determinant of the matrix 
    \[
    a = \begin{pmatrix} 
    x_1 & x_2 \\ 
    y_1 & y_2
    \end{pmatrix}.
    \]
    discuss the significance of this property in the context of solving linear systems and analyzing matrix transformations.",determinants
"given the matrix 
    \[
    a = \begin{pmatrix} 
    1 & 3 & 2 \\ 
    4 & 2 & 1 \\ 
    5 & 6 & 3
    \end{pmatrix},
    \]
    compute the determinant of \( a \) using cofactor expansion. next, use the lu decomposition method to compute the determinant of the same matrix and compare the results. discuss the efficiency of each method in solving real-world problems such as numerical optimization.",determinants
"let \( a \) be a \( 3 \times 3 \) matrix with entries \( a_{ij} \), and suppose that the matrix is invertible. prove that if a matrix is invertible, then its determinant is nonzero. furthermore, show that the inverse of a matrix can be computed using the adjugate matrix, where 
    \[
    a^{-1} = \frac{1}{\det(a)} \text{adj}(a).
    \]
    compute the inverse of the matrix \( a = \begin{pmatrix} 
    2 & 1 & 3 \\ 
    1 & 2 & 1 \\ 
    3 & 1 & 2
    \end{pmatrix} \) and verify the result by multiplying \( a \) and \( a^{-1} \).",determinants
"using the formula for minors and cofactors, compute the determinant of the following matrix:
    \[
    a = \begin{pmatrix} 
    7 & 4 & 2 \\ 
    5 & 3 & 1 \\ 
    9 & 6 & 3
    \end{pmatrix}.
    \]
    additionally, show how the determinant can be used to verify the linear independence of the rows or columns of a matrix. discuss how this property can be useful in applications such as determining the rank of a matrix and solving systems of linear equations.",determinants
"let \( a \) be a \( 4 \times 4 \) block matrix defined as
    \[
    a = \begin{pmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22}
    \end{pmatrix},
    \]
    where \( a_{11} \), \( a_{12} \), \( a_{21} \), and \( a_{22} \) are \( 2 \times 2 \) matrices. using the block determinant formula, express the determinant of \( a \) in terms of the determinants of the blocks. assume that \( a_{11} \) and \( a_{22} \) are invertible. prove that the determinant of \( a \) is given by 
    \[
    \det(a) = \det(a_{11}) \det(a_{22} - a_{21} a_{11}^{-1} a_{12}).
    \]
    apply this formula to compute the determinant of the block matrix
    \[
    a = \begin{pmatrix}
    2 & 1 & 0 & 0 \\
    1 & 2 & 0 & 0 \\
    0 & 0 & 3 & 4 \\
    0 & 0 & 4 & 3
    \end{pmatrix}.
    \]",determinants
"let \( a \) be a block matrix of the form
    \[
    a = \begin{pmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22}
    \end{pmatrix},
    \]
    where \( a_{11} \) and \( a_{22} \) are square matrices of size \( n \times n \). if \( a_{12} = a_{21}^t \), prove that the determinant of \( a \) can be expressed as
    \[
    \det(a) = \det(a_{11}) \det(a_{22} - a_{21} a_{11}^{-1} a_{12}).
    \]
    using this result, compute the determinant of the matrix
    \[
    a = \begin{pmatrix}
    3 & 2 & 1 & 0 \\
    2 & 3 & 0 & 1 \\
    1 & 0 & 4 & 3 \\
    0 & 1 & 3 & 4
    \end{pmatrix}.
    \]",determinants
"let \( a = \begin{pmatrix} 
    a_{11} & a_{12} \\ 
    a_{21} & a_{22} 
    \end{pmatrix} \) be a block matrix where \( a_{11} \), \( a_{12} \), \( a_{21} \), and \( a_{22} \) are square matrices of order \( n \). show that if \( a_{11} \) is invertible, then the determinant of \( a \) is given by
    \[
    \det(a) = \det(a_{11}) \det(a_{22} - a_{21} a_{11}^{-1} a_{12}).
    \]
    prove this result and apply it to compute the determinant of the matrix
    \[
    a = \begin{pmatrix} 
    4 & 2 & 1 & 0 \\ 
    1 & 3 & 2 & 0 \\ 
    0 & 1 & 5 & 4 \\ 
    0 & 0 & 3 & 6 
    \end{pmatrix}.
    \]",determinants
"consider the block matrix
    \[
    a = \begin{pmatrix} 
    1 & 0 & 0 \\ 
    0 & a_{22} & a_{23} \\ 
    0 & a_{32} & a_{33} 
    \end{pmatrix},
    \]
    where \( a_{22} \), \( a_{23} \), \( a_{32} \), and \( a_{33} \) are square matrices of size \( n \times n \). show that the determinant of \( a \) can be computed as the product
    \[
    \det(a) = \det(a_{22}) \det(a_{33} - a_{32} a_{22}^{-1} a_{23}).
    \]
    apply this result to compute the determinant of the block matrix
    \[
    a = \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 2 & 1 \\
    0 & 1 & 3
    \end{pmatrix}.
    \]",determinants
"let \( a \) be a block matrix of the form
    \[
    a = \begin{pmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22}
    \end{pmatrix},
    \]
    where \( a_{11} \) and \( a_{22} \) are square matrices of order \( n \). if \( a_{21} = a_{12}^t \), show that the determinant of \( a \) satisfies the relation
    \[
    \det(a) = \det(a_{11}) \det(a_{22} - a_{21} a_{11}^{-1} a_{12}).
    \]
    use this formula to compute the determinant of the matrix
    \[
    a = \begin{pmatrix}
    1 & 2 & 0 & 0 \\
    2 & 1 & 0 & 0 \\
    0 & 0 & 3 & 1 \\
    0 & 0 & 1 & 3
    \end{pmatrix}.
    \]",determinants
"consider a block matrix \( a \) given by
    \[
    a = \begin{pmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22}
    \end{pmatrix}.
    \]
    if \( a_{11} \) is invertible, prove that
    \[
    \det(a) = \det(a_{11}) \det(a_{22} - a_{21} a_{11}^{-1} a_{12}).
    \]
    compute the determinant of the matrix
    \[
    a = \begin{pmatrix}
    3 & 1 & 0 & 0 \\
    1 & 2 & 0 & 0 \\
    0 & 0 & 5 & 6 \\
    0 & 0 & 6 & 5
    \end{pmatrix}.
    \]",determinants
"let \( a = \begin{pmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22}
    \end{pmatrix} \) be a block matrix. show that if \( a_{11} \) is invertible, then the determinant of \( a \) is
    \[
    \det(a) = \det(a_{11}) \det(a_{22} - a_{21} a_{11}^{-1} a_{12}).
    \]
    apply this result to compute the determinant of the block matrix
    \[
    a = \begin{pmatrix}
    2 & 1 & 0 & 0 \\
    1 & 3 & 0 & 0 \\
    0 & 0 & 4 & 2 \\
    0 & 0 & 2 & 4
    \end{pmatrix}.
    \]",determinants
"show that for a block matrix \( a = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \), if \( a_{11} \) is invertible, then the determinant of \( a \) is
    \[
    \det(a) = \det(a_{11}) \det(a_{22} - a_{21} a_{11}^{-1} a_{12}).
    \]
    use this formula to compute the determinant of the matrix
    \[
    a = \begin{pmatrix}
    5 & 3 & 1 & 0 \\
    3 & 5 & 0 & 1 \\
    1 & 0 & 6 & 2 \\
    0 & 1 & 2 & 6
    \end{pmatrix}.
    \]",determinants
"given a block matrix \( a = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \), where \( a_{11} \) and \( a_{22} \) are square matrices, and assuming that \( a_{11} \) is invertible, show that the determinant of \( a \) can be written as
    \[
    \det(a) = \det(a_{11}) \det(a_{22} - a_{21} a_{11}^{-1} a_{12}).
    \]
    compute the determinant of the matrix
    \[
    a = \begin{pmatrix}
    7 & 4 & 1 & 0 \\
    4 & 7 & 0 & 1 \\
    1 & 0 & 8 & 3 \\
    0 & 1 & 3 & 8
    \end{pmatrix}.
    \]",determinants
"let \( a = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \) be a block matrix where \( a_{11} \) and \( a_{22} \) are square matrices. prove that if \( a_{11} \) is invertible, then the determinant of \( a \) is
    \[
    \det(a) = \det(a_{11}) \det(a_{22} - a_{21} a_{11}^{-1} a_{12}).
    \]
    apply this formula to compute the determinant of the matrix
    \[
    a = \begin{pmatrix}
    8 & 2 & 1 & 0 \\
    2 & 8 & 0 & 1 \\
    1 & 0 & 9 & 4 \\
    0 & 1 & 4 & 9
    \end{pmatrix}.
    \]",determinants
"let \( a \) be a block matrix of the form
    \[
    a = \begin{pmatrix} 
    a_{11} & a_{12} \\ 
    a_{21} & a_{22} 
    \end{pmatrix}.
    \]
    if \( a_{11} \) is invertible, show that
    \[
    \det(a) = \det(a_{11}) \det(a_{22} - a_{21} a_{11}^{-1} a_{12}).
    \]
    use this to compute the determinant of the matrix
    \[
    a = \begin{pmatrix}
    6 & 2 & 1 & 0 \\
    2 & 6 & 0 & 1 \\
    1 & 0 & 7 & 3 \\
    0 & 1 & 3 & 7
    \end{pmatrix}.
    \]",determinants
"consider the following system of linear equations:
    \[
    \begin{aligned}
    3x + 4y - 2z &= 1, \\
    2x - y + 3z &= 4, \\
    5x - 2y + z &= 3.
    \end{aligned}
    \]
    solve this system using cramer's rule. first, compute the determinants of the coefficient matrix and the matrices formed by replacing each column of the coefficient matrix with the constant vector. verify the solution by substituting the values back into the original system.",determinants
"solve the system of linear equations:
    \[
    \begin{aligned}
    x + 2y + 3z &= 9, \\
    2x + 3y + 4z &= 10, \\
    3x + 4y + 5z &= 11.
    \end{aligned}
    \]
    use cramer's rule to find the values of \(x\), \(y\), and \(z\). compute each of the relevant determinants and interpret the results geometrically.",determinants
"given the system of equations
    \[
    \begin{aligned}
    x + y - 2z &= 1, \\
    2x - y + 3z &= 4, \\
    3x + 2y + z &= 5,
    \end{aligned}
    \]
    apply cramer's rule to find the values of \(x\), \(y\), and \(z\). discuss the relationship between the determinant of the coefficient matrix and the existence of a unique solution.",determinants
"solve the following system of equations using cramer's rule:
    \[
    \begin{aligned}
    2x - y + 4z &= 3, \\
    x + 3y - 2z &= 7, \\
    4x - 2y + 3z &= 5.
    \end{aligned}
    \]
    calculate the determinants of the matrix of coefficients and the matrices obtained by replacing each column with the right-hand side vector. verify that the solution satisfies the original system.",determinants
"solve the system of equations
    \[
    \begin{aligned}
    4x + 3y - 2z &= 5, \\
    2x + y + z &= 3, \\
    x - y + 2z &= 1,
    \end{aligned}
    \]
    using cramer's rule. explicitly compute the determinants of the coefficient matrix and the matrices obtained by replacing columns with the constant vector.",determinants
"given the system of linear equations
    \[
    \begin{aligned}
    3x + y + 2z &= 7, \\
    x - 2y + z &= 1, \\
    4x + 3y - z &= 2,
    \end{aligned}
    \]
    solve for \(x\), \(y\), and \(z\) using cramer's rule. compute the required determinants, and explain how cramer's rule applies in this context.",determinants
"solve the following system of linear equations using cramer's rule:
    \[
    \begin{aligned}
    2x - 3y + 4z &= 5, \\
    x + 2y - z &= 4, \\
    3x - y + 2z &= 6.
    \end{aligned}
    \]
    discuss the geometric interpretation of the solution and compute the determinants for each variable.",determinants
"consider the system of equations
    \[
    \begin{aligned}
    5x + 2y + 3z &= 12, \\
    x + 4y - z &= 2, \\
    3x - y + 2z &= 5.
    \end{aligned}
    \]
    solve this system using cramer's rule. calculate each determinant step-by-step, and determine if the system has a unique solution, infinitely many solutions, or no solution.",determinants
"solve the system of equations
    \[
    \begin{aligned}
    4x + 5y - z &= 8, \\
    2x - 3y + 3z &= 5, \\
    x + 2y + 4z &= 7,
    \end{aligned}
    \]
    using cramer's rule. compute the determinants involved and check that the solution satisfies all the equations.",determinants
"solve the system of equations
    \[
    \begin{aligned}
    2x + 3y + z &= 10, \\
    x - y + 2z &= 5, \\
    3x + y - z &= 4.
    \end{aligned}
    \]
    use cramer's rule to find the values of \(x\), \(y\), and \(z\). discuss how the determinants of the modified matrices relate to the coefficients of the equations and the constants.",determinants
"given the matrix \( a = \begin{pmatrix} 2 & 1 & 3 \\ 4 & 2 & 6 \\ 7 & 5 & 8 \end{pmatrix} \), compute the cofactor matrix \( c \) of \( a \) and use it to find the inverse of \( a \). show all steps involved in the computation of the minors, cofactors, and the adjugate matrix.",determinants
let \( a = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 0 & 5 & 6 & 7 \\ 8 & 9 & 10 & 11 \\ 12 & 13 & 14 & 15 \end{pmatrix} \). calculate the cofactor matrix of \( a \) and use it to find the inverse of \( a \). discuss the implications of the determinant of \( a \) in determining the existence of the inverse.,determinants
"compute the determinant of the matrix \( a = \begin{pmatrix} 3 & 2 \\ 5 & 4 \end{pmatrix} \) using the cofactor expansion along the first row. then, use the cofactor matrix to calculate the inverse of \( a \).",determinants
"given the matrix \( b = \begin{pmatrix} 1 & 3 & 2 \\ 4 & 1 & 3 \\ 2 & 5 & 7 \end{pmatrix} \), compute the cofactor matrix and the inverse of \( b \) by following the process of minors, cofactors, and adjugation.",determinants
compute the determinant of the matrix \( c = \begin{pmatrix} 2 & 1 \\ 1 & 3 \end{pmatrix} \) and then find its inverse using the cofactor matrix. verify the result by multiplying \( c \) with its inverse to check if the product is the identity matrix.,determinants
"consider the matrix \( a = \begin{pmatrix} 4 & 3 \\ 6 & 5 \end{pmatrix} \). use the cofactor method to find the determinant and inverse of \( a \), and interpret the determinant geometrically.",determinants
"let \( a = \begin{pmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & 1 \end{pmatrix} \). compute the cofactor matrix, adjugate matrix, and the inverse of \( a \). discuss how the structure of the matrix affects the cofactor calculations.",determinants
"given a 3x3 matrix \( m = \begin{pmatrix} 1 & 0 & 3 \\ 2 & 1 & 1 \\ 4 & 5 & 2 \end{pmatrix} \), compute the area of the triangle formed by the vectors corresponding to the rows of \( m \) using the determinant. explain how the geometry of the vectors relates to the value of the determinant.",determinants
consider the matrix \( a = \begin{pmatrix} 1 & 2 & 0 \\ 0 & 3 & 1 \\ 4 & 1 & 2 \end{pmatrix} \). use the determinant to calculate the volume of the parallelepiped formed by the vectors corresponding to the rows of \( a \). show all steps and provide a geometric interpretation of the result.,determinants
"let \( a = \begin{pmatrix} 5 & 3 & 1 \\ 6 & 4 & 2 \\ 7 & 5 & 3 \end{pmatrix} \). use cramer's rule, cofactor matrices, and determinants to solve the system of linear equations formed by \( a \) and a constant vector \( \mathbf{b} \). find the solution and verify your calculations.",determinants
"given the 4x4 matrix \( a = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 0 & 5 & 6 & 7 \\ 0 & 0 & 8 & 9 \\ 0 & 0 & 0 & 10 \end{pmatrix} \), calculate the determinant and the inverse of \( a \) using the cofactor expansion method. explain the computational challenges and benefits of using this method for large matrices.",determinants
"consider the following matrix:
    \[
    a = \begin{pmatrix} 1 & 1 & 1 \\ 2 & 2 & 2 \\ 3 & 3 & 3 \end{pmatrix}
    \]
    compute its determinant and inverse, and discuss the implications of the determinant being zero. what does this imply about the rank and the invertibility of \( a \)?",determinants
"given the matrix \( a = \begin{pmatrix} 2 & 4 & 1 \\ 6 & 5 & 3 \\ 7 & 8 & 9 \end{pmatrix} \), calculate its determinant and use the cofactor method to find the inverse of \( a \). discuss the relationship between the determinant and the volume of the parallelepiped formed by the rows of \( a \).",determinants
let \( a = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \). compute the inverse of \( a \) using its cofactor matrix. what does the result suggest about the identity matrix and its properties?,determinants
consider the matrix \( m = \begin{pmatrix} 2 & 1 & 1 \\ 1 & 2 & 1 \\ 1 & 1 & 2 \end{pmatrix} \). compute the determinant of \( m \) and use it to find the area of the triangle formed by the rows of \( m \). discuss the geometric interpretation of the result.,determinants
"use the cofactor method to compute the inverse of the matrix
    \[
    a = \begin{pmatrix} 3 & 1 & 4 \\ 2 & 2 & 1 \\ 5 & 1 & 3 \end{pmatrix}.
    \]
    after calculating the inverse, verify it by multiplying \( a \) and its inverse and checking if the result is the identity matrix.",determinants
"given the matrix
    \[
    a = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 0 & 5 & 6 & 7 \\ 0 & 0 & 8 & 9 \\ 0 & 0 & 0 & 10 \end{pmatrix},
    \]
    compute the determinant using cofactor expansion along the first row. discuss how the structure of the matrix simplifies the computation.",determinants
"given the matrix
    \[
    a = \begin{pmatrix} 3 & 2 \\ 1 & 4 \end{pmatrix},
    \]
    compute the determinant and the inverse of \( a \) using the cofactor matrix. interpret the determinant in terms of area.",determinants
"given the matrix
    \[
    a = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{pmatrix},
    \]
    compute the determinant and inverse of \( a \) using the cofactor method. discuss the geometric interpretation of the determinant and inverse of a diagonal matrix.",determinants
"compute the determinant of the matrix
    \[
    a = \begin{pmatrix} 3 & 4 \\ 5 & 6 \end{pmatrix}
    \]
    and use it to determine the volume of the parallelogram formed by the rows of \( a \).",determinants
"given the matrix
    \[
    a = \begin{pmatrix} 2 & 4 \\ 1 & 2 \end{pmatrix},
    \]
    compute its determinant using cofactor expansion and explain how the determinant relates to the area of a rectangle in the plane.",determinants
"find the determinant and inverse of the matrix
    \[
    a = \begin{pmatrix} 1 & 3 \\ 2 & 5 \end{pmatrix},
    \]
    and explain the meaning of the determinant in terms of scaling factors for area.",determinants
"calculate the cofactor matrix of the matrix
    \[
    a = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix}
    \]
    and use it to find the inverse, if possible. discuss why the inverse does not exist for this matrix.",determinants
"using the cofactor matrix, find the inverse of the matrix
    \[
    a = \begin{pmatrix} 1 & 2 & 1 \\ 2 & 3 & 3 \\ 1 & 1 & 2 \end{pmatrix}.
    \]
    verify the result by checking if the product of \( a \) and its inverse equals the identity matrix.",determinants
"given the matrix \( a = \begin{pmatrix} 3 & 1 & 2 \\ 1 & 2 & 1 \\ 2 & 3 & 1 \end{pmatrix} \), compute the area of the parallelogram formed by the rows of \( a \) using the determinant. discuss the geometric interpretation of the determinant as the scaling factor for the area in \( \mathbb{r}^2 \) and how the matrix entries affect the area calculation.",determinants
consider the matrix \( a = \begin{pmatrix} 1 & 3 & 0 \\ 4 & 1 & 2 \\ 1 & 2 & 3 \end{pmatrix} \). compute the determinant of \( a \) and use it to find the volume of the parallelepiped formed by the rows of \( a \). interpret the result geometrically in terms of volume scaling in \( \mathbb{r}^3 \).,determinants
let \( a = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{pmatrix} \). calculate the volume of the parallelepiped formed by the rows of \( a \) using the determinant. discuss how the determinant relates to the scaling factors of the coordinate axes.,determinants
"given the matrix \( a = \begin{pmatrix} 1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & 0 & 6 \end{pmatrix} \), compute the determinant and use it to calculate the volume of the parallelepiped formed by the columns of \( a \). provide a detailed analysis of how the matrix structure influences the computation.",determinants
compute the area of the triangle formed by the vectors corresponding to the rows of the matrix \( a = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix} \). use the determinant to calculate the area and interpret the result in terms of geometric properties in \( \mathbb{r}^2 \).,determinants
consider the matrix \( b = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 1 & 0 & 1 \end{pmatrix} \). calculate the determinant of \( b \) and use it to determine the volume of the solid formed by the vectors corresponding to the rows of \( b \). analyze the result geometrically and discuss the interpretation of the determinant in this context.,determinants
"given the matrix \( a = \begin{pmatrix} 4 & 1 & 3 \\ 2 & 2 & 1 \\ 1 & 3 & 2 \end{pmatrix} \), compute the volume of the parallelepiped formed by the columns of \( a \). discuss how the determinant relates to the geometric volume and how row/column exchanges affect the determinant value.",determinants
"let \( a = \begin{pmatrix} 2 & 0 & 1 \\ 3 & 1 & 0 \\ 1 & 4 & 2 \end{pmatrix} \). using the determinant, calculate the volume of the parallelepiped formed by the vectors corresponding to the rows of \( a \). discuss the implications of the result in terms of spatial orientation and how the matrix entries influence the geometric outcome.",determinants
"given the matrix \( a = \begin{pmatrix} 1 & 2 & 3 \\ 2 & 3 & 4 \\ 5 & 6 & 7 \end{pmatrix} \), compute the determinant and explain how the value of the determinant corresponds to the volume of the parallelepiped formed by the rows of \( a \). discuss the properties of the matrix that affect the determinant and its relation to volume scaling.",determinants
consider the matrix \( a = \begin{pmatrix} 0 & 1 & 2 \\ 3 & 1 & 0 \\ 1 & 4 & 5 \end{pmatrix} \). calculate the volume of the parallelepiped formed by the columns of \( a \) using the determinant. explain how the determinant behaves as a measure of volume in three-dimensional space and the relationship between the rows and columns in the matrix.,determinants
let \( a = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix} \) be a matrix. compute the determinant of \( a \) and determine whether the permutation represented by the matrix is even or odd. discuss the relationship between the determinant and the parity of permutations.,determinants
"given a matrix \( a = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{pmatrix} \), determine the parity of the permutation that transforms the identity matrix into \( a \). explain how the determinant relates to the parity of the permutation and prove the conclusion using the properties of the determinant.",determinants
consider the matrix \( b = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix} \). compute the determinant of \( b \) and determine whether it represents an even or odd permutation. provide a detailed explanation of how row swaps affect the determinant and the parity of the permutation.,determinants
"let \( c = \begin{pmatrix} 3 & 4 & 5 \\ 6 & 7 & 8 \\ 9 & 10 & 11 \end{pmatrix} \). compute the determinant of \( c \) and classify the permutation as even or odd. discuss how the matrix structure relates to the permutation's parity, especially in the context of determinant properties.",determinants
"given the matrix \( d = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix} \), calculate the determinant and determine whether the permutation of rows in \( d \) is even or odd. how does the parity of the permutation affect the value of the determinant, and what conclusions can be drawn from this observation?",determinants
consider the matrix \( a = \begin{pmatrix} 2 & 3 \\ 5 & 7 \end{pmatrix} \). find the determinant of \( a \) and explain the significance of the permutation's parity in relation to the determinant's sign. discuss whether the matrix represents an even or odd permutation of the identity matrix.,determinants
let \( a = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{pmatrix} \). calculate the determinant of \( a \) and classify the permutation of rows as even or odd. provide a proof of your classification and explain how row exchanges affect the determinant in this case.,determinants
"given a matrix \( e = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix} \), compute the determinant and determine whether it corresponds to an even or odd permutation of rows. discuss the importance of the permutation's parity in determining the determinant’s value and the broader implications for matrix theory.",determinants
"consider the matrix \( f = \begin{pmatrix} 1 & 3 & 2 \\ 4 & 6 & 5 \\ 7 & 9 & 8 \end{pmatrix} \). compute the determinant of \( f \) and analyze the parity of the permutation it represents. discuss how the determinant sign and permutation parity are related, and how the order of row swaps influences the final result.",determinants
let \( g = \begin{pmatrix} 3 & 1 & 2 \\ 6 & 4 & 5 \\ 9 & 7 & 8 \end{pmatrix} \). calculate the determinant of \( g \) and determine whether the permutation is even or odd. explain the role of the determinant in determining the parity of a permutation and how this property can be extended to higher-dimensional matrices.,determinants
"given the matrix \( a = \begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix} \), compute the number of terms in the expansion of \( \det(a) \). also, determine how many of these terms will be zero and justify your reasoning using the properties of the matrix elements.",determinants
let \( a = \begin{pmatrix} 1 & 0 & 2 \\ 0 & 3 & 4 \\ 5 & 0 & 6 \end{pmatrix} \). find the number of non-zero terms in the determinant expansion of \( a \) and explain why certain terms cancel out to zero. discuss how the structure of \( a \) influences the number of non-zero terms in the expansion.,determinants
"for the matrix \( a = \begin{pmatrix} x & y & z \\ 2x & 2y & 2z \\ x & -y & z \end{pmatrix} \), calculate the number of terms in the determinant expansion. how many of these terms will be zero, and what can be inferred about the matrix’s rank and linear dependence of rows or columns?",determinants
"given a \( 4 \times 4 \) matrix \( a = \begin{pmatrix} a_1 & a_2 & a_3 & a_4 \\ b_1 & b_2 & b_3 & b_4 \\ c_1 & c_2 & c_3 & c_4 \\ d_1 & d_2 & d_3 & d_4 \end{pmatrix} \), determine the number of terms in the expansion of \( \det(a) \) and the number of zero terms in that expansion. what does this tell you about the number of independent rows or columns in the matrix?",determinants
let \( a = \begin{pmatrix} 2 & 1 & 0 \\ 3 & 5 & 0 \\ 4 & 2 & 1 \end{pmatrix} \). determine the number of terms in the determinant expansion for this matrix and identify which terms are zero. discuss the impact of the zeros on the rank of the matrix and how the determinant computation changes with such terms.,determinants
"for the matrix \( a = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ 9 & 10 & 11 & 12 \\ 13 & 14 & 15 & 16 \end{pmatrix} \), compute the number of terms in the determinant expansion and the number of terms that will be zero. analyze how the structure of this specific matrix, with rows forming an arithmetic progression, influences the determinant’s value and the cancellation of terms in the expansion.",determinants
"given the points \( a = (1, 2), b = (3, 5), c = (6, 7) \), calculate the area of the triangle formed by these points using determinants. show the step-by-step derivation of the area and discuss the geometric interpretation of the determinant in this context.",determinants
"consider the vectors \( \mathbf{v}_1 = (1, 2, 3), \mathbf{v}_2 = (4, 5, 6), \mathbf{v}_3 = (7, 8, 9) \). compute the volume of the parallelepiped formed by these vectors using the scalar triple product. discuss the geometric interpretation of the determinant in terms of the volume of the parallelepiped.",determinants
let \( a = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{pmatrix} \) be a matrix. use determinants to determine the volume of the parallelepiped formed by the row vectors of \( a \). discuss how the structure of the matrix affects the volume of the parallelepiped and its relationship to the determinant.,determinants
"given the points \( a = (1, 1, 1), b = (2, 3, 4), c = (4, 5, 6), d = (7, 8, 9) \), calculate the volume of the tetrahedron formed by these points using determinants. provide a detailed explanation of how to compute this volume from the determinant of a matrix.",determinants
"for the parallelogram defined by the vectors \( \mathbf{v}_1 = (3, 4), \mathbf{v}_2 = (1, 2) \), compute the area using the determinant of the matrix formed by these vectors. discuss the significance of the determinant as a measure of the area and how it relates to the geometry of the parallelogram.",determinants
"consider a parallelepiped in 3d space defined by the vectors \( \mathbf{v}_1 = (2, 0, 1), \mathbf{v}_2 = (1, 3, 0), \mathbf{v}_3 = (0, 1, 2) \). find the volume of this parallelepiped using the determinant of the matrix formed by these vectors. discuss how the determinant calculation captures the 3d nature of the volume.",determinants
"let the points \( a = (1, 1), b = (4, 2), c = (3, 5) \) define a triangle in the plane. use the determinant method to find the area of the triangle. also, show how this formula generalizes to triangles in higher dimensions.",determinants
"find the area of the triangle formed by the points \( a = (0, 0), b = (1, 2), c = (3, 3) \) in \( \mathbb{r}^2 \) using determinants. discuss the conditions under which the determinant-based formula for the area may yield zero and its interpretation in terms of the geometry of the triangle.",determinants
"given the vertices of a hypercube in \( \mathbb{r}^4 \), compute the volume of the hypercube using the determinant of a matrix formed by the vectors corresponding to its edges. provide an in-depth analysis of how the determinant method can be applied to higher-dimensional objects like the hypercube.",determinants
"consider a triangle in \( \mathbb{r}^3 \) with vertices \( a = (1, 0, 0), b = (0, 1, 0), c = (0, 0, 1) \). calculate the area of this triangle using determinants. discuss the relationship between the determinant and the geometry of the triangle in three-dimensional space.",determinants
"given a matrix \( a = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \), find the volume of the unit cube and explain how the determinant relates to the geometric interpretation of this volume. discuss what happens to the volume when the matrix is scaled or rotated.",determinants
let \( a = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix} \). compute the determinant of \( a \) and explain how it can be used to calculate the volume of the parallelepiped formed by the vectors corresponding to the rows of \( a \). discuss the impact of row operations on the volume.,determinants
"for the matrix \( b = \begin{pmatrix} 2 & 1 & 3 \\ 1 & 0 & 4 \\ 5 & 2 & 6 \end{pmatrix} \), compute the volume of the parallelepiped formed by its row vectors. discuss the effect of the matrix entries on the volume and how it can be interpreted geometrically in 3d space.",determinants
"given the vertices of a triangle in \( \mathbb{r}^3 \), \( a = (1, 2, 3), b = (4, 5, 6), c = (7, 8, 9) \), compute the area using determinants. analyze how the determinant-based method for computing the area can be extended to higher dimensions for simplex-like structures.",determinants
"consider the parallelogram formed by the vectors \( \mathbf{v}_1 = (2, 1) \) and \( \mathbf{v}_2 = (1, 3) \) in \( \mathbb{r}^2 \). calculate its area using the determinant of the matrix formed by these two vectors. discuss the significance of the cross product interpretation of the determinant for 2d areas.",determinants
let \( a = \begin{pmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \\ 2 & 3 & 4 \end{pmatrix} \) be the matrix representing the vectors forming a parallelepiped. compute the volume of the parallelepiped formed by the column vectors of \( a \) using the determinant. discuss the effects of scaling and rotations on the volume as represented by the determinant.,determinants
consider a matrix \( a \in \mathbb{r}^{n \times n} \) and a perturbation matrix \( e \in \mathbb{r}^{n \times n} \) such that the perturbed matrix is \( a + e \). derive an expression for the sensitivity of the determinant of \( a \) to perturbations in \( e \) using first-order approximation. discuss the implications of this result on numerical stability when calculating determinants in high-dimensional systems.,determinants
"investigate the condition number of a matrix \( a \) and how it is related to the sensitivity of its determinant under perturbation. provide a detailed proof that the condition number, defined as \( \kappa(a) = \|a\| \|a^{-1}\| \), can be used to bound the relative change in the determinant \( \det(a) \) due to small perturbations. how does the condition number influence the numerical stability of determinant calculations in computational algorithms?",determinants
"let \( a \) be a diagonalizable matrix with eigenvalues \( \lambda_1, \lambda_2, \dots, \lambda_n \). discuss the effect of a small perturbation \( e \) on the eigenvalues of \( a \) using perturbation theory. derive an expression for the change in the determinant \( \det(a + e) \) in terms of the eigenvalue shifts induced by \( e \), and explain the role of eigenvalue sensitivity in understanding the stability of determinants.",determinants
"in the context of eigenvalue perturbation theory, consider the matrix \( a + e \), where \( e \) is a small perturbation. show that, to first order, the determinant of \( a + e \) can be approximated as \( \det(a + e) \approx \det(a) \cdot \left( 1 + \text{tr}(a^{-1}e) \right) \). discuss the implications of this result for matrices with large condition numbers and provide a numerical example to illustrate the behavior.",determinants
"given a matrix \( a \in \mathbb{r}^{n \times n} \), define the matrix \( a + \delta a \) as a perturbation of \( a \), where \( \delta a \) represents a small change. using perturbation theory, derive a bound for the relative error in \( \det(a + \delta a) \) in terms of the norm of \( \delta a \) and the condition number of \( a \). discuss the computational techniques to mitigate the effects of large errors when calculating determinants in practical scenarios.",determinants
"investigate how the spectral properties of a matrix \( a \), including the eigenvalues and singular values, impact the perturbation behavior of its determinant. specifically, explore the relationship between the largest and smallest eigenvalues of \( a \) and the change in the determinant when subjected to small perturbations. discuss how these properties are used in numerical methods to estimate the stability of determinant calculations in large matrices.",determinants
"discuss the perturbation theory for the eigenvalues of symmetric matrices. derive a first-order approximation for the change in the determinant of a symmetric matrix \( a \) when it is perturbed by a small symmetric matrix \( e \). how does the spectral theorem help in analyzing the effects of such perturbations, and what insights can be gained about the stability of determinants in this specific case?",determinants
"consider a square matrix \( a \in \mathbb{c}^{n \times n} \) and its singular value decomposition \( a = u \sigma v^h \), where \( u \) and \( v \) are unitary matrices and \( \sigma \) is a diagonal matrix of singular values. derive the relationship between the determinant of \( a \) and the singular values in \( \sigma \). specifically, express \( \det(a) \) as the product of the singular values and discuss how this provides insight into the geometric properties of the matrix.",determinants
"let \( a \in \mathbb{r}^{n \times n} \) have singular value decomposition \( a = u \sigma v^t \). explore the impact of the rank of \( a \) on its determinant. show that if \( a \) is of full rank, the determinant is the product of the non-zero singular values, and if \( a \) is rank-deficient, the determinant is zero. provide a proof that the determinant of a rank-deficient matrix vanishes and relate this result to the properties of the svd.",determinants
"suppose \( a \in \mathbb{r}^{n \times n} \) has singular values \( \sigma_1, \sigma_2, \dots, \sigma_n \), where some of these singular values are zero. discuss the geometric interpretation of the determinant of \( a \) as it relates to the volume scaling factor of a transformation defined by \( a \). how does the presence of zero singular values affect the transformation, and what does it imply about the invertibility of the matrix?",determinants
"consider the case where \( a \in \mathbb{c}^{n \times n} \) is a complex matrix, and \( a = u \sigma v^h \) is its singular value decomposition. investigate how the determinant of \( a \) extends to complex matrices. discuss how the determinant can be interpreted geometrically in complex vector spaces, and show how the determinant relates to the volume scaling of the linear transformation \( a \) in a complex setting.",determinants
"let \( a \in \mathbb{c}^{n \times n} \) be a complex matrix with singular values \( \sigma_1, \sigma_2, \dots, \sigma_n \), and let \( \lambda_1, \lambda_2, \dots, \lambda_n \) be the eigenvalues of \( a \). discuss the connection between the singular values and eigenvalues of a complex matrix. prove that for a square matrix \( a \), the determinant can be written as the product of both the singular values and eigenvalues. how do these results extend to non-square matrices?",determinants
"in the context of rank decomposition, consider a matrix \( a \in \mathbb{r}^{m \times n} \) with rank \( r \), where \( r < \min(m, n) \). using the singular value decomposition of \( a \), discuss how the rank influences the determinant of \( a \). specifically, show that the determinant of a matrix with rank less than \( \min(m, n) \) is zero, and explain the significance of this result in terms of the matrix's singular values.",determinants
"investigate the extension of the determinant function to quaternionic matrices. let \( a \in \mathbb{h}^{n \times n} \) be a matrix with entries in the quaternions, and discuss how the determinant is defined for such matrices. explore the geometric interpretation of the determinant in quaternionic vector spaces, particularly in terms of scaling and orientation. provide examples of quaternionic matrices and their determinants, and compare the behavior of quaternionic determinants with those of complex matrices.",determinants
"find the eigenvalues and eigenvectors of the matrix 
    \[
    a = 
    \begin{bmatrix}
    1 & -1 \\
    2 & 4
    \end{bmatrix}
    \]
    verify that the trace equals the sum of the eigenvalues, and the determinant equals their product.",eigenvalues and eigenvectors
"with the same matrix $a$, solve the differential equation 
    \[
    \frac{du}{dt} = a u, \quad u(0) =
    \begin{bmatrix}
    0 \\
    6
    \end{bmatrix}
    \]
    what are the two pure exponential solutions?",eigenvalues and eigenvectors
"if we shift to $a - 7i$, what are the eigenvalues and eigenvectors, and how are they related to those of $a$?
    \[
    b = a - 7i =
    \begin{bmatrix}
    -6 & -1 \\
    2 & -3
    \end{bmatrix}
    \]",eigenvalues and eigenvectors
"solve $\frac{du}{dt} = p u$, when $p$ is a projection:
    \[
    \frac{du}{dt} =
    \begin{bmatrix}
    \frac{1}{2} & \frac{1}{2} \\
    \frac{1}{2} & \frac{1}{2}
    \end{bmatrix}
    u, \quad u(0) =
    \begin{bmatrix}
    5 \\
    3
    \end{bmatrix}
    \]
    part of $u(0)$ increases exponentially while the nullspace part stays fixed.",eigenvalues and eigenvectors
"find the eigenvalues and eigenvectors of
    \[
    a =
    \begin{bmatrix}
    3 & 4 & 2 \\
    0 & 1 & 2 \\
    0 & 0 & 0
    \end{bmatrix}
    \quad \text{and} \quad
    b =
    \begin{bmatrix}
    0 & 0 & 2 \\
    0 & 2 & 0 \\
    2 & 0 & 0
    \end{bmatrix}
    \]
    check that $\lambda_1 + \lambda_2 + \lambda_3$ equals the trace and $\lambda_1 \lambda_2 \lambda_3$ equals the determinant.",eigenvalues and eigenvectors
give an example to show that the eigenvalues can be changed when a multiple of one row is subtracted from another. why is a zero eigenvalue not changed by the steps of elimination?,eigenvalues and eigenvectors
"suppose that $\lambda$ is an eigenvalue of $a$, and $x$ is its eigenvector: $ax = \lambda x$.
    \begin{enumerate}",eigenvalues and eigenvectors
"show that this same $x$ is an eigenvector of $b = a - 7i$, and find the eigenvalue. this should confirm exercise 3.",eigenvalues and eigenvectors
"assuming $\lambda \neq 0$, show that $x$ is also an eigenvector of $a^{-1}$—and find the eigenvalue.",eigenvalues and eigenvectors
"show that the determinant equals the product of the eigenvalues by imagining that the characteristic polynomial is factored into
    \[
    \det(a - \lambda i) = (\lambda_1 - \lambda)(\lambda_2 - \lambda) \cdots (\lambda_n - \lambda),
    \]
    and making a clever choice of $\lambda$.\",eigenvalues and eigenvectors
"show that the trace equals the sum of the eigenvalues, in two steps. first, find the coefficient of $(-\lambda)^{n-1}$ on the right side of equation (16). next, find all the terms in 
    \[
    \det(a - \lambda i) = \det
    \begin{bmatrix}
    a_{11} - \lambda & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} - \lambda & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nn} - \lambda
    \end{bmatrix}
    \]
    that involve $(-\lambda)^{n-1}$. they all come from the main diagonal! find that coefficient of $(-\lambda)^{n-1}$ and compare.",eigenvalues and eigenvectors
\begin{enumerate},eigenvalues and eigenvectors
"construct $2 \times 2$ matrices such that the eigenvalues of $ab$ are not the products of the eigenvalues of $a$ and $b$, and the eigenvalues of $a + b$ are not the sums of the individual eigenvalues.",eigenvalues and eigenvectors
"verify, however, that the sum of the eigenvalues of $a + b$ equals the sum of all the individual eigenvalues of $a$ and $b$, and similarly for products. why is this true?",eigenvalues and eigenvectors
the eigenvalues of $a$ equal the eigenvalues of $a^t$. this is because $\det(a - \lambda i)$ equals $\det(a^t - \lambda i)$. that is true because $\underline{\hspace{3cm}}$. show by an example that the eigenvectors of $a$ and $a^t$ are not the same.,eigenvalues and eigenvectors
"find the eigenvalues and eigenvectors of
    \[
    a =
    \begin{bmatrix}
    3 & 4 \\
    4 & -3
    \end{bmatrix}
    \quad \text{and} \quad
    a =
    \begin{bmatrix}
    a & b \\
    b & a
    \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"if $b$ has eigenvalues $1, 2, 3$, $c$ has eigenvalues $4, 5, 6$, and $d$ has eigenvalues $7, 8, 9$, what are the eigenvalues of the $6 \times 6$ matrix 
    \[
    a =
    \begin{bmatrix}
    b & c \\
    0 & d
    \end{bmatrix}?
    \]",eigenvalues and eigenvectors
"find the rank and all four eigenvalues for both the matrix of ones and the checkerboard matrix:
    \[
    a =
    \begin{bmatrix}
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1
    \end{bmatrix}
    \quad \text{and} \quad
    c =
    \begin{bmatrix}
    0 & 1 & 0 & 1 \\
    1 & 0 & 1 & 0 \\
    0 & 1 & 0 & 1 \\
    1 & 0 & 1 & 0
    \end{bmatrix}.
    \]
    which eigenvectors correspond to nonzero eigenvalues?",eigenvalues and eigenvectors
what are the rank and eigenvalues when $a$ and $c$ in the previous exercise are $n \times n$? remember that the eigenvalue $\lambda = 0$ is repeated $n - r$ times.,eigenvalues and eigenvectors
"if $a$ is the $4 \times 4$ matrix of ones, find the eigenvalues and the determinant of $a - i$.",eigenvalues and eigenvectors
"choose the third row of the “companion matrix”
    \[
    a =
    \begin{bmatrix}
    0 & 1 & 0 \\
    0 & 0 & 1 \\
    \vdots
    \end{bmatrix}
    \]
    so that its characteristic polynomial $|a - \lambda i|$ is $-\lambda^3 + 4\lambda^2 + 5\lambda + 6$.",eigenvalues and eigenvectors
"suppose $a$ has eigenvalues $0, 3, 5$ with independent eigenvectors $u, v, w$.
    \begin{enumerate}",eigenvalues and eigenvectors
give a basis for the nullspace and a basis for the column space.,eigenvalues and eigenvectors
find a particular solution to $ax = v + w$. find all solutions.,eigenvalues and eigenvectors
"show that $ax = u$ has no solution. (if it had a solution, then $\underline{\hspace{4cm}}$ would be in the column space.)",eigenvalues and eigenvectors
"the powers $a^k$ of this matrix $a$ approach a limit as $k \to \infty$:
    \[
    a =
    \begin{bmatrix}
    0.8 & 0.3 \\
    0.2 & 0.7
    \end{bmatrix},
    \quad
    a^2 =
    \begin{bmatrix}
    0.70 & 0.45 \\
    0.30 & 0.55
    \end{bmatrix},
    \quad
    a^\infty =
    \begin{bmatrix}
    0.6 & 0.6 \\
    0.4 & 0.4
    \end{bmatrix}.
    \]
    the matrix $a^2$ is halfway between $a$ and $a^\infty$. explain why 
    \[
    a^2 = \frac{1}{2} (a + a^\infty)
    \]
    from the eigenvalues and eigenvectors of these three matrices.",eigenvalues and eigenvectors
"find the eigenvalues and eigenvectors of these two matrices:
    \[
    a =
    \begin{bmatrix}
    1 & 4 \\
    2 & 3
    \end{bmatrix}
    \quad \text{and} \quad
    a + i =
    \begin{bmatrix}
    2 & 4 \\
    2 & 4
    \end{bmatrix}.
    \]
    $a + i$ has the same eigenvectors as $a$. its eigenvalues are $\underline{\hspace{3cm}}$.",eigenvalues and eigenvectors
"compute the eigenvalues and eigenvectors of $a$ and $a^{-1}$:
    \[
    a =
    \begin{bmatrix}
    0 & 2 \\
    2 & 3
    \end{bmatrix}
    \quad \text{and} \quad
    a^{-1} =
    \begin{bmatrix}
    -3/4 & 1/2 \\
    1/2 & 0
    \end{bmatrix}.
    \]
    $a^{-1}$ has the same eigenvectors as $a$. when $a$ has eigenvalues $\lambda_1$ and $\lambda_2$, its inverse has eigenvalues $\underline{\hspace{3cm}}$.",eigenvalues and eigenvectors
"compute the eigenvalues and eigenvectors of $a$ and $a^2$:
    \[
    a =
    \begin{bmatrix}
    -1 & 3 \\
    2 & 0
    \end{bmatrix}
    \quad \text{and} \quad
    a^2 =
    \begin{bmatrix}
    7 & -3 \\
    -2 & 6
    \end{bmatrix}.
    \]
    $a^2$ has the same $\underline{\hspace{3cm}}$ as $a$. when $a$ has eigenvalues $\lambda_1$ and $\lambda_2$, $a^2$ has eigenvalues $\underline{\hspace{3cm}}$.",eigenvalues and eigenvectors
"if you know $x$ is an eigenvector, the way to find $\lambda$ is to $\underline{\hspace{3cm}}$.",eigenvalues and eigenvectors
"if you know $\lambda$ is an eigenvalue, the way to find $x$ is to $\underline{\text{\hspace{3cm}}}$.",eigenvalues and eigenvectors
"what do you do to $ax = \lambda x$, in order to prove the following?
    \begin{itemize}",eigenvalues and eigenvectors
"(a) $\lambda^2$ is an eigenvalue of $a^2$, as in problem 22.",eigenvalues and eigenvectors
"(b) $\lambda^{-1}$ is an eigenvalue of $a^{-1}$, as in problem 21.",eigenvalues and eigenvectors
"(c) $\lambda +1$ is an eigenvalue of $a+i$, as in problem 20.
    \end{itemize}",eigenvalues and eigenvectors
"from the unit vector $u = \begin{bmatrix} \frac{1}{6} \\ \frac{1}{6} \\ \frac{3}{6} \\ \frac{5}{6} \end{bmatrix}$, construct the rank-1 projection matrix $p = uu^t$.
    \begin{itemize}",eigenvalues and eigenvectors
(a) show that $pu = u$. then $u$ is an eigenvector with $\lambda = 1$.,eigenvalues and eigenvectors
"(b) if $v$ is perpendicular to $u$, show that $pv = \mathbf{0}$. then $\lambda = 0$.",eigenvalues and eigenvectors
"(c) find three independent eigenvectors of $p$, all with eigenvalue $\lambda = 0$.
    \end{itemize}",eigenvalues and eigenvectors
"solve $\det(q-\lambda i) = 0$ using the quadratic formula to reach $\lambda = \cos\theta \pm i\sin\theta$:
    \[
    q = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}
    \]
    which rotates the $xy$-plane by the angle $\theta$. find the eigenvectors of $q$ by solving $(q-\lambda i)x = 0$. use $i^2 = -1$.",eigenvalues and eigenvectors
"every permutation matrix leaves $x = (1,1,...,1)$ unchanged. then $\lambda = 1$. find two more eigenvalues for these permutations:
    \[
    p_1 = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix},
    \quad
    p_2 = \begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"if $a$ has $\lambda_1 = 4$ and $\lambda_2 = 5$, then
    \[
    \det(a-\lambda i) = (\lambda -4)(\lambda -5) = \lambda^2 -9\lambda +20.
    \]
    find three matrices that have trace $a+d = 9$, determinant $20$, and eigenvalues $\lambda = 4,5$.",eigenvalues and eigenvectors
"a $3 \times 3$ matrix $b$ is known to have eigenvalues $0, 1, 2$. this information is enough to find three of the following:
    \begin{itemize}",eigenvalues and eigenvectors
(a) the rank of $b$.,eigenvalues and eigenvectors
(b) the determinant of $b^t b$.,eigenvalues and eigenvectors
(c) the eigenvalues of $b^t b$.,eigenvalues and eigenvectors
"(d) the eigenvalues of $(b+i)^{-1}$.
    \end{itemize}",eigenvalues and eigenvectors
choose the second row of $a = \begin{bmatrix} 0 & 1 \\ * & * \end{bmatrix}$ so that $a$ has eigenvalues $4$ and $7$.,eigenvalues and eigenvectors
"choose $a, b, c$ so that $\det(a-\lambda i) = 9\lambda - \lambda^3$. then the eigenvalues are $-3, 0, 3$:
    \[
    a = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ a & b & c \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"construct any $3 \times 3$ markov matrix $m$: positive entries down each column add to 1. if $e = (1,1,1)$, verify that $m^t e = e$. by problem 11, $\lambda = 1$ is also an eigenvalue of $m$. 
    
    challenge: a $3 \times 3$ singular markov matrix with trace $\frac{1}{2}$ has eigenvalues $\lambda =$.",eigenvalues and eigenvectors
find three $2 \times 2$ matrices that have $\lambda_1 = \lambda_2 = 0$. the trace is zero and the determinant is zero. the matrix $a$ might not be 0 but check that $a^2 = 0$.,eigenvalues and eigenvectors
"this matrix is singular with rank 1. find three $\lambda$’s and three eigenvectors:
    \[
    a = \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix} \begin{bmatrix} 2 & 1 & 2 \end{bmatrix} = \begin{bmatrix} 2 & 1 & 2 \\ 4 & 2 & 4 \\ 2 & 1 & 2 \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"suppose $a$ and $b$ have the same eigenvalues $\lambda_1, \dots, \lambda_n$ with the same independent eigenvectors $x_1, \dots, x_n$. then $a = b$. reason: any vector $x$ is a combination $c_1 x_1 + \dots + c_n x_n$. what is $ax$? what is $bx$?",eigenvalues and eigenvectors
"(review) find the eigenvalues of $a$, $b$, and $c$:
    \[
    a = \begin{bmatrix} 1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & 0 & 6 \end{bmatrix}, \quad
    b = \begin{bmatrix} 0 & 0 & 1 \\ 0 & 2 & 0 \\ 3 & 0 & 0 \end{bmatrix}, \quad
    c = \begin{bmatrix} 2 & 2 & 2 \\ 2 & 2 & 2 \\ 2 & 2 & 2 \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"when $a+b = c+d$, show that $(1,1)$ is an eigenvector and find both eigenvalues:
    \[
    a = \begin{bmatrix} a & b \\ c & d \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"when $p$ exchanges rows 1 and 2 and columns 1 and 2, the eigenvalues don’t change.
    find eigenvectors of $a$ and $pap$ for $\lambda = 11$:
    \[
    a = \begin{bmatrix} 1 & 2 & 1 \\ 3 & 6 & 3 \\ 4 & 8 & 4 \end{bmatrix}, \quad
    pap = \begin{bmatrix} 6 & 3 & 3 \\ 2 & 1 & 1 \\ 8 & 4 & 4 \end{bmatrix}.
    \]",eigenvalues and eigenvectors
challenge problem: is there a real $2 \times 2$ matrix (other than $i$) with $a^3 = i$? its eigenvalues must satisfy $\lambda^3 = i$. they can be $e^{2\pi i / 3}$ and $e^{-2\pi i / 3}$. what trace and determinant would this give? construct $a$.,eigenvalues and eigenvectors
there are six $3 \times 3$ permutation matrices $p$. what numbers can be the determinants of $p$? what numbers can be pivots? what numbers can be the trace of $p$? what four numbers can be eigenvalues of $p$?,eigenvalues and eigenvectors
"factor the following matrices into \( s \lambda s^{-1} \):
    \begin{itemize}",eigenvalues and eigenvectors
\( a = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} \),eigenvalues and eigenvectors
"\( a = \begin{pmatrix} 2 & 1 \\ 0 & 0 \end{pmatrix} \)
    \end{itemize}",eigenvalues and eigenvectors
"find the matrix \( a \) whose eigenvalues are 1 and 4, and whose eigenvectors are \( \begin{pmatrix} 3 \\ 1 \end{pmatrix} \) and \( \begin{pmatrix} 2 \\ 1 \end{pmatrix} \), respectively. (hint: \( a = s \lambda s^{-1} \).)",eigenvalues and eigenvectors
"find all the eigenvalues and eigenvectors of
    \[
    a = \begin{pmatrix}
    1 & 1 & 1 \\
    1 & 1 & 1 \\
    1 & 1 & 1
    \end{pmatrix}
    \]
    and write two different diagonalizing matrices \( s \).",eigenvalues and eigenvectors
"if a 3 by 3 upper triangular matrix has diagonal entries 1, 2, 7, how do you know it can be diagonalized? what is \( \lambda \)?",eigenvalues and eigenvectors
"which of these matrices cannot be diagonalized?
    \begin{itemize}",eigenvalues and eigenvectors
\( a_1 = \begin{pmatrix} 2 & -2 \\ 2 & -2 \end{pmatrix} \),eigenvalues and eigenvectors
\( a_2 = \begin{pmatrix} 2 & 0 \\ 2 & -2 \end{pmatrix} \),eigenvalues and eigenvectors
"\( a_3 = \begin{pmatrix} 2 & 0 \\ 2 & 2 \end{pmatrix} \)
    \end{itemize}",eigenvalues and eigenvectors
"(a) if \( a^2 = i \), what are the possible eigenvalues of \( a \)?
    
    (b) if this \( a \) is 2 by 2, and not \( i \) or \( -i \), find its trace and determinant.
    
    (c) if the first row is \( (3, -1) \), what is the second row?",eigenvalues and eigenvectors
"if \( a = \begin{pmatrix} 4 & 3 \\ 1 & 2 \end{pmatrix} \), find \( a^{100} \) by diagonalizing \( a \).",eigenvalues and eigenvectors
"suppose \( a = uv^t \) is a column times a row (a rank-1 matrix).
    \begin{itemize}",eigenvalues and eigenvectors
"(a) by multiplying \( a \) times \( u \), show that \( u \) is an eigenvector. what is \( \lambda \)?",eigenvalues and eigenvectors
(b) what are the other eigenvalues of \( a \) (and why)?,eigenvalues and eigenvectors
"(c) compute \( \text{trace}(a) \) from the sum on the diagonal and the sum of \( \lambda \)'s.
    \end{itemize}",eigenvalues and eigenvectors
"show by direct calculation that \( \text{trace}(ab) \) and \( \text{trace}(ba) \) are the same when
    \[
    a = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \quad \text{and} \quad b = \begin{pmatrix} q & r \\ s & t \end{pmatrix}.
    \]
    deduce that \( ab - ba = i \) is impossible (except in infinite dimensions).",eigenvalues and eigenvectors
"suppose \( a \) has eigenvalues 1, 2, 4. what is the trace of \( a^2 \)? what is the determinant of \( (a^{-1})^t \)?",eigenvalues and eigenvectors
"if the eigenvalues of \( a \) are 1, 1, 2, which of the following are certain to be true? give a reason if true or a counterexample if false:
    \begin{itemize}",eigenvalues and eigenvectors
(a) \( a \) is invertible.,eigenvalues and eigenvectors
(b) \( a \) is diagonalizable.,eigenvalues and eigenvectors
"(c) \( a \) is not diagonalizable.
    \end{itemize}",eigenvalues and eigenvectors
"suppose the only eigenvectors of \( a \) are multiples of \( x = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} \). true or false:
    \begin{itemize}",eigenvalues and eigenvectors
(a) \( a \) is not invertible.,eigenvalues and eigenvectors
(b) \( a \) has a repeated eigenvalue.,eigenvalues and eigenvectors
diagonalize the matrix \( a = \begin{pmatrix} 5 & 4 \\ 4 & 5 \end{pmatrix} \) and find one of its square roots—a matrix such that \( r^2 = a \). how many square roots will there be?,eigenvalues and eigenvectors
suppose the eigenvector matrix \( s \) has \( s^t = s^{-1} \). show that \( a = s \lambda s^{-1} \) is symmetric and has orthogonal eigenvectors.,eigenvalues and eigenvectors
"factor these two matrices into \( a = s \lambda s^{-1} \):
    \begin{itemize}",eigenvalues and eigenvectors
\( a = \begin{pmatrix} 1 & 2 \\ 0 & 3 \end{pmatrix} \),eigenvalues and eigenvectors
"\( a = \begin{pmatrix} 1 & 1 \\ 2 & 2 \end{pmatrix} \)
    \end{itemize}",eigenvalues and eigenvectors
"if \( a = s \lambda s^{-1} \), then \( a^3 = ( ) ( ) ( ) \) and \( a^{-1} = ( ) ( ) ( ) \).",eigenvalues and eigenvectors
"if \( a \) has \( \lambda_1 = 2 \) with eigenvector \( x_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix} \) and \( \lambda_2 = 5 \) with \( x_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix} \), use \( s \lambda s^{-1} \) to find \( a \). no other matrix has the same \( \lambda \)'s and \( x \)'s.",eigenvalues and eigenvectors
suppose \( a = s \lambda s^{-1} \). what is the eigenvalue matrix for \( a + 2i \)? what is the eigenvector matrix? check that \( a + 2i = ( ) ( ) ( )^{-1} \).,eigenvalues and eigenvectors
"true or false: if the \( n \) columns of \( s \) (eigenvectors of \( a \)) are independent, then:
    \begin{itemize}",eigenvalues and eigenvectors
(c) \( s \) is invertible.,eigenvalues and eigenvectors
"(d) \( s \) is diagonalizable.
    \end{itemize}",eigenvalues and eigenvectors
"if the eigenvectors of \( a \) are the columns of \( i \), then \( a \) is a matrix. if the eigenvector matrix \( s \) is triangular, then \( s^{-1} \) is triangular and \( a \) is triangular.",eigenvalues and eigenvectors
"describe all matrices \( s \) that diagonalize this matrix \( a \):
    \[
    a = \begin{pmatrix} 4 & 0 \\ 1 & 2 \end{pmatrix}.
    \]
    then describe all matrices that diagonalize \( a^{-1} \).",eigenvalues and eigenvectors
write the most general matrix that has eigenvectors \( \begin{pmatrix} 1 \\ 1 \end{pmatrix} \) and \( \begin{pmatrix} 1 \\ -1 \end{pmatrix} \).,eigenvalues and eigenvectors
"find the eigenvalues of \( a \) and \( b \) and \( a + b \):
    \[
    a = \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}, \quad b = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}, \quad a + b = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}.
    \]
    eigenvalues of \( a + b \) (are equal to) (are not equal to) eigenvalues of \( a \) plus eigenvalues of \( b \).",eigenvalues and eigenvectors
"find the eigenvalues of \( a \), \( b \), \( ab \), and \( ba \):
    \[
    a = \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}, \quad b = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}, \quad ab = \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix}, \quad ba = \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix}.
    \]
    eigenvalues of \( ab \) (are equal to) (are not equal to) eigenvalues of \( a \) times eigenvalues of \( b \). eigenvalues of \( ab \) (are) (are not) equal to eigenvalues of \( ba \).",eigenvalues and eigenvectors
"true or false: if the eigenvalues of \( a \) are 2, 2, 5, then the matrix is certainly:
    \begin{itemize}",eigenvalues and eigenvectors
(a) invertible.,eigenvalues and eigenvectors
(b) diagonalizable.,eigenvalues and eigenvectors
"(c) not diagonalizable.
    \end{itemize}",eigenvalues and eigenvectors
"if the eigenvalues of \( a \) are 1 and 0, write everything you know about the matrices \( a \) and \( a^2 \).",eigenvalues and eigenvectors
"complete these matrices so that \( \text{det}(a) = 25 \). then trace = 10, and \( \lambda = 5 \) is repeated! find an eigenvector with \( a x = 5x \). these matrices will not be diagonalizable because there is no second line of eigenvectors.
    \[
    a = \begin{pmatrix} 8 & 2 \\ \end{pmatrix}, \quad a = \begin{pmatrix} 9 & 4 \\ 1 & \end{pmatrix}, \quad a = \begin{pmatrix} 10 & 5 \\ -5 & \end{pmatrix}.
    \]",eigenvalues and eigenvectors
the matrix \( a = \begin{pmatrix} 3 & 1 \\ 0 & 3 \end{pmatrix} \) is not diagonalizable because the rank of \( a - 3i \) is $\underline{\hspace{2cm}}$. change one entry to make \( a \) diagonalizable. which entries could you change?,eigenvalues and eigenvectors
"\( a^k = s \lambda^k s^{-1} \) approaches the zero matrix as \( k \to \infty \) if and only if every \( \lambda \) has absolute value less than 1. does \( a^k \to 0 \) or \( b^k \to 0 \)?
    \[
    a = \begin{pmatrix} 0.6 & 0.4 \\ 0.4 & 0.6 \end{pmatrix}, \quad b = \begin{pmatrix} 0.6 & 0.9 \\ 0.1 & 0.6 \end{pmatrix}.
    \]",eigenvalues and eigenvectors
(recommended) find \( \lambda \) and \( s \) to diagonalize \( a \) in problem 29. what is the limit of \( \lambda^k \) as \( k \to \infty \)? what is the limit of \( s \lambda^k s^{-1} \)? in the columns of this limiting matrix you see the \_.,eigenvalues and eigenvectors
"find \( \lambda \) and \( s \) to diagonalize \( b \) in problem 29. what is \( b^{10} u_0 \) for these \( u_0 \)?
    \[
    u_0 = \begin{pmatrix} 3 \\ 1 \end{pmatrix}, \quad u_0 = \begin{pmatrix} 3 \\ -1 \end{pmatrix}, \quad u_0 = \begin{pmatrix} 6 \\ 0 \end{pmatrix}.
    \]",eigenvalues and eigenvectors
"diagonalize \( a \) and compute \( s \lambda^k s^{-1} \) to prove this formula for \( a^k \):
    \[
    a = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \quad a^k = \frac{1}{2} \begin{pmatrix} 3k+1 & 3k-1 \\ 3k-1 & 3k+1 \end{pmatrix}.
    \]",eigenvalues and eigenvectors
"diagonalize \( b \) and compute \( s \lambda^k s^{-1} \) to prove this formula for \( b^k \):
    \[
    b = \begin{pmatrix} 3 & 1 \\ 0 & 2 \end{pmatrix}, \quad b^k = \begin{pmatrix} 3^k & 3^{k-2} \\ k & 2^k \end{pmatrix}.
    \]",eigenvalues and eigenvectors
suppose that \( a = s \lambda s^{-1} \). take determinants to prove that \( \text{det}(a) = \lambda_1 \lambda_2 \cdots \lambda_n \) = product of \( \lambda \)’s. this quick proof only works when \( a \) is \underline{\hspace{2cm}}.,eigenvalues and eigenvectors
"the trace of \( s \times \lambda s^{-1} \) equals the trace of \( \lambda s^{-1} \times s \). so the trace of a diagonalizable \( a \) equals the trace of \( \lambda \), which is \underline{\hspace{2cm}}.",eigenvalues and eigenvectors
"if \( a = s \lambda s^{-1} \), diagonalize the block matrix
    \[
    b = \begin{pmatrix} a & 0 \\ 0 & 2a \end{pmatrix}.
    \]
    find its eigenvalue and eigenvector matrices.",eigenvalues and eigenvectors
"consider all \( 4 \times 4 \) matrices \( a \) that are diagonalized by the same fixed eigenvector matrix \( s \). show that the \( a \)’s form a subspace (i.e., \( ca \) and \( a_1 + a_2 \) have this same \( s \)). what is this subspace when \( s = i \)? what is its dimension?",eigenvalues and eigenvectors
"suppose \( a^2 = a \). on the left side, \( a \) multiplies each column of \( a \). which of our four subspaces contains eigenvectors with \( \lambda = 1 \)? which subspace contains eigenvectors with \( \lambda = 0 \)? from the dimensions of those subspaces, \( a \) has a full set of independent eigenvectors and can be diagonalized.",eigenvalues and eigenvectors
"suppose \( a x = \lambda x \). if \( \lambda = 0 \), then \( x \) is in the nullspace. if \( \lambda \neq 0 \), then \( x \) is in the column space. those spaces have dimensions \( (n - r) + r = n \). so why doesn’t every square matrix have \( n \) linearly independent eigenvectors?",eigenvalues and eigenvectors
"substitute \( a = s \lambda s^{-1} \) into the product \( (a - \lambda_1 i)(a - \lambda_2 i) \cdots (a - \lambda_n i) \) and explain why this produces the zero matrix. we are substituting the matrix \( a \) for the number \( \lambda \) in the polynomial \( p(\lambda) = \text{det}(a - \lambda i) \). the cayley-hamilton theorem says that this product is always \( p(a) = \) zero matrix, even if \( a \) is not diagonalizable.",eigenvalues and eigenvectors
"test the cayley-hamilton theorem on fibonacci’s matrix
    \[
    a = \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix}.
    \]
    the theorem predicts that \( a^2 - a - i = 0 \), since \( \text{det}(a - \lambda i) \) is \( \lambda^2 - \lambda - 1 \).",eigenvalues and eigenvectors
"if \( a = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \), then \( \text{det}(a - \lambda i) \) is \( (\lambda - a)(\lambda - d) \). check the cayley-hamilton statement that \( (a - ai)(a - di) = \) zero matrix.",eigenvalues and eigenvectors
"if \( a = \begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix} \) and \( ab = ba \), show that \( b = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \) is also diagonal. \( b \) has the same eigen\underline{\hspace{2cm}} as \( a \), but different eigen\underline{\hspace{2cm}}. these diagonal matrices \( b \) form a two-dimensional subspace of matrix space. \( ab - ba = 0 \) gives four equations for the unknowns \( a, b, c, d \)—find the rank of the \( 4 \times 4 \) matrix.",eigenvalues and eigenvectors
"if \( a \) is \( 5 \times 5 \), then \( ab - ba = 0 \) gives 25 equations for the 25 entries in \( b \). show that the \( 25 \times 25 \) matrix is singular by noticing a simple nonzero solution for \( b \).",eigenvalues and eigenvectors
"find the eigenvalues and eigenvectors for both of these markov matrices \( a \) and \( a_{\infty} \). explain why \( a^{100} \) is close to \( a_{\infty} \):
    \[
    a = \begin{pmatrix} 0.6 & 0.2 \\ 0.4 & 0.8 \end{pmatrix}, \quad a_{\infty} = \begin{pmatrix} \frac{1}{3} & \frac{1}{3} \\ \frac{2}{3} & \frac{2}{3} \end{pmatrix}.
    \]",eigenvalues and eigenvectors
"prove that every third fibonacci number in \( 0, 1, 1, 2, 3, \dots \) is even.",eigenvalues and eigenvectors
"bernadelli studied a beetle “which lives three years only and propagates in its third year.” they survive the first year with probability \( \frac{1}{2} \), and the second with probability \( \frac{1}{3} \), and then produce six females on the way out. the beetle matrix \( a \) is
    \[
    a = \begin{pmatrix} 0 & 0 & 6 \\ \frac{1}{2} & 0 & 0 \\ 0 & \frac{1}{3} & 0 \end{pmatrix}.
    \]
    show that \( a^3 = i \), and follow the distribution of 3000 beetles for six years.",eigenvalues and eigenvectors
"for the fibonacci matrix
    \[
    a = \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix},
    \]
    compute \( a^2 \), \( a^3 \), and \( a^4 \). then use the text and a calculator to find \( f_{20} \).",eigenvalues and eigenvectors
"suppose each “gibonacci” number \( g_{k+2} \) is the average of the two previous numbers, \( g_{k+1} \) and \( g_k \). then
    \[
    g_{k+2} = \frac{1}{2} (g_{k+1} + g_k) = \frac{1}{2} g_{k+1} + \frac{1}{2} g_k.
    \]
    this can be written as
    \[
    \begin{pmatrix} g_{k+2} \\ g_{k+1} \end{pmatrix} = a \begin{pmatrix} g_{k+1} \\ g_k \end{pmatrix},
    \]
    where \( a \) is the transformation matrix.
    \begin{enumerate}",eigenvalues and eigenvectors
"diagonalize the fibonacci matrix by completing \( s^{-1} \):
    \[
    \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} = \begin{pmatrix} \lambda_1 & \lambda_2 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix}.
    \]
    do the multiplication \( s \lambda^k s^{-1} \begin{pmatrix} 1 \\ 0 \end{pmatrix} \) to find its second component. this is the \( k \)-th fibonacci number \( f_k = \frac{\lambda_1^k - \lambda_2^k}{\lambda_1 - \lambda_2} \).",eigenvalues and eigenvectors
"the numbers \( \lambda_1^k \) and \( \lambda_2^k \) satisfy the fibonacci rule
    \[
    f_{k+2} = f_{k+1} + f_k,
    \]
    where
    \[
    \lambda_{k+2}^1 = \lambda_{k+1}^1 + \lambda_k^1 \quad \text{and} \quad \lambda_{k+2}^2 = \lambda_{k+1}^2 + \lambda_k^2.
    \]
    prove this by using the original equation for the \( \lambda \)’s (multiply it by \( \lambda_k \)). then any combination of \( \lambda_1^k \) and \( \lambda_2^k \) satisfies the rule. the combination
    \[
    f_k = \frac{\lambda_1^k - \lambda_2^k}{\lambda_1 - \lambda_2}
    \]
    gives the right start of \( f_0 = 0 \) and \( f_1 = 1 \).",eigenvalues and eigenvectors
"lucas started with \( l_0 = 2 \) and \( l_1 = 1 \). the rule \( l_{k+2} = l_{k+1} + l_k \) is the same, so \( a \) is still fibonacci’s matrix. add its eigenvectors \( \mathbf{x_1} + \mathbf{x_2} \):
    \[
    \begin{pmatrix} \lambda_1 \\ 1 \end{pmatrix} + \begin{pmatrix} \lambda_2 \\ 1 \end{pmatrix} = \begin{pmatrix} \frac{1}{2}(1+\sqrt{5}) \\ 1 \end{pmatrix} + \begin{pmatrix} \frac{1}{2}(1-\sqrt{5}) \\ 1 \end{pmatrix} = \begin{pmatrix} 1 \\ 2 \end{pmatrix}.
    \]
    multiplying by \( a^k \), the second component is \( l_k = \lambda_1^k + \lambda_2^k \). compute the lucas number \( l_{10} \) slowly by \( l_{k+2} = l_{k+1} + l_k \), and compute approximately by \( \lambda_1^{10} \).",eigenvalues and eigenvectors
"suppose there is an epidemic in which every month half of those who are well become sick, and a quarter of those who are sick become dead. find the steady state for the corresponding markov process:
    \[
    \begin{pmatrix} d_{k+1} \\ s_{k+1} \\ w_{k+1} \end{pmatrix} = \begin{pmatrix} \frac{1}{4} & 0 & 0 \\ \frac{1}{2} & \frac{3}{4} & 0 \\ 0 & \frac{1}{2} & 1 \end{pmatrix} \begin{pmatrix} d_k \\ s_k \\ w_k \end{pmatrix}.
    \]",eigenvalues and eigenvectors
"write the 3 by 3 transition matrix for a chemistry course that is taught in two sections, if every week \( \frac{1}{4} \) of those in section a and \( \frac{1}{3} \) of those in section b drop the course, and \( \frac{1}{6} \) of each section transfer to the other section.",eigenvalues and eigenvectors
"find the limiting values of \( y_k \) and \( z_k \) as \( k \to \infty \) if
    \[
    y_{k+1} = 0.8 y_k + 0.3 z_k, \quad y_0 = 0
    \]
    \[
    z_{k+1} = 0.2 y_k + 0.7 z_k, \quad z_0 = 5.
    \]
    also, find formulas for \( y_k \) and \( z_k \) from \( a^k = s \lambda^k s^{-1} \).",eigenvalues and eigenvectors
"(a) from the fact that column 1 + column 2 = 2(column 3), so the columns are linearly dependent, find one eigenvalue and one eigenvector of \( a \):
    \[
    a = \begin{pmatrix} 0.2 & 0.4 & 0.3 \\ 0.4 & 0.2 & 0.3 \\ 0.4 & 0.4 & 0.4 \end{pmatrix}.
    \]
    (b) find the other eigenvalues of \( a \) (it is markov). 
    (c) if \( u_0 = \begin{pmatrix} 0 \\ 10 \\ 0 \end{pmatrix} \), find the limit of \( a^k u_0 \) as \( k \to \infty \).",eigenvalues and eigenvectors
"suppose there are three major centers for move-it-yourself trucks. every month half of those in boston and in los angeles go to chicago, the other half stay where they are, and the trucks in chicago are split equally between boston and los angeles. set up the 3 by 3 transition matrix \( a \), and find the steady state \( u_\infty \) corresponding to the eigenvalue \( \lambda = 1 \).",eigenvalues and eigenvectors
"(a) in what range of \( a \) and \( b \) is the following equation a markov process?
    \[
    u_{k+1} = a u_k = \begin{pmatrix} a & b \\ 1-a & 1-b \end{pmatrix} u_k, \quad u_0 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
    \]
    (b) compute \( u_k = s \lambda^k s^{-1} u_0 \) for any \( a \) and \( b \).
    (c) under what condition on \( a \) and \( b \) does \( u_k \) approach a finite limit as \( k \to \infty \), and what is the limit? does \( a \) have to be a markov matrix?",eigenvalues and eigenvectors
"multinational companies in the americas, asia, and europe have assets of \$4 trillion. at the start, \$2 trillion are in the americas and \$2 trillion in europe. each year \( \frac{1}{2} \) of the american money stays home, and \( \frac{1}{4} \) goes to each of asia and europe. for asia and europe, \( \frac{1}{2} \) stays home and \( \frac{1}{2} \) is sent to the americas.
    \begin{enumerate}",eigenvalues and eigenvectors
"find the matrix \( a \) that gives
        \[
        \begin{pmatrix} \text{americas}_{k+1} \\ \text{asia}_{k+1} \\ \text{europe}_{k+1} \end{pmatrix} = a \begin{pmatrix} \text{americas}_k \\ \text{asia}_k \\ \text{europe}_k \end{pmatrix}.
        \]",eigenvalues and eigenvectors
find the eigenvalues and eigenvectors of \( a \).,eigenvalues and eigenvectors
find the limiting distribution of the \$4 trillion as the world ends.,eigenvalues and eigenvectors
find the distribution of the \$4 trillion at year \( k \).,eigenvalues and eigenvectors
"if \( a \) is a markov matrix, show that the sum of the components of \( a \mathbf{x} \) equals the sum of the components of \( \mathbf{x} \). deduce that if \( a \mathbf{x} = \lambda \mathbf{x} \) with \( \lambda \neq 1 \), the components of the eigenvector add to zero.",eigenvalues and eigenvectors
"the solution to \( \frac{du}{dt} = a u \) with
    \[
    a = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix},
    \]
    where the eigenvalues are \( i \) and \( -i \), goes around in a circle:
    \[
    u = (\cos t, \sin t).
    \]
    suppose we approximate \( \frac{du}{dt} \) by forward, backward, and centered differences:
    \begin{itemize}",eigenvalues and eigenvectors
(f) \( u_{n+1} - u_n = a u_n \) or \( u_{n+1} = (i + a) u_n \) (this is euler’s method).,eigenvalues and eigenvectors
(b) \( u_{n+1} - u_n = a u_{n+1} \) or \( u_{n+1} = (i - a)^{-1} u_n \) (backward euler).,eigenvalues and eigenvectors
"(c) \( u_{n+1} - u_n = \frac{1}{2} a (u_{n+1} + u_n) \) or \( u_{n+1} = (i - \frac{1}{2} a)^{-1} (i + \frac{1}{2} a) u_n \).
    \end{itemize}
    find the eigenvalues of \( i + a \), \( (i - a)^{-1} \), and \( (i - \frac{1}{2} a)^{-1} (i + \frac{1}{2} a) \). for which difference equation does the solution \( u_n \) stay on a circle?",eigenvalues and eigenvectors
"what values of \( \alpha \) produce instability in
    \[
    v_{n+1} = \alpha (v_n + w_n), \quad w_{n+1} = \alpha (v_n + w_n)?
    \]",eigenvalues and eigenvectors
"find the largest values of \( a \), \( b \), and \( c \) for which these matrices are stable or neutrally stable:
    \[
    \begin{pmatrix} a & -0.8 \\ 0.8 & 0.2 \end{pmatrix}, \quad
    \begin{pmatrix} b & 0.8 \\ 0 & 0.2 \end{pmatrix}, \quad
    \begin{pmatrix} 0.8 & 0.2 \\ c & 0.8 \end{pmatrix}.
    \]",eigenvalues and eigenvectors
"multiplying term by term, check that \( (i - a)(i + a + a^2 + \cdots) = i \). this series represents \( (i - a)^{-1} \). it is nonnegative when \( a \) is nonnegative, provided it has a finite sum; the condition for that is \( \lambda_{\text{max}} < 1 \). add up the infinite series and confirm that it equals \( (i - a)^{-1} \), for the consumption matrix
    \[
    a = \begin{pmatrix} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix},
    \]
    which has \( \lambda_{\text{max}} = 0 \).",eigenvalues and eigenvectors
"for \( a = \begin{pmatrix} 0 & 0.2 \\ 0 & 0.5 \end{pmatrix} \), find the powers \( a^k \) (including \( a^0 \)) and show explicitly that their sum agrees with \( (i - a)^{-1} \).",eigenvalues and eigenvectors
explain by mathematics or economics why increasing the “consumption matrix” \( a \) must increase \( t_{\text{max}} = \lambda_1 \) (and slow down the expansion).,eigenvalues and eigenvectors
"what are the limits as \( k \to \infty \) (the steady states) of the following?
    \[
    \begin{pmatrix} 0.4 & 0.2 \\ 0.6 & 0.8 \end{pmatrix}^k \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \quad
    \begin{pmatrix} 0.4 & 0.2 \\ 0.6 & 0.8 \end{pmatrix}^k \begin{pmatrix} 0 \\ 1 \end{pmatrix}, \quad
    \begin{pmatrix} 0.4 & 0.2 \\ 0.6 & 0.8 \end{pmatrix}^k.
    \]",eigenvalues and eigenvectors
"diagonalize \( a = \begin{pmatrix} 3 & 2 \\ 2 & 3 \end{pmatrix} \) and compute \( s \lambda^k s^{-1} \) to prove this formula for \( a^k \):
    \[
    a^k = \frac{1}{2} \begin{pmatrix} 5^{k+1} & 5^k - 1 \\ 5^k - 1 & 5^{k+1} \end{pmatrix}.
    \]",eigenvalues and eigenvectors
"diagonalize \( b = \begin{pmatrix} 3 & 1 \\ 0 & 2 \end{pmatrix} \) and compute \( s \lambda^k s^{-1} \) to prove this formula for \( b^k \):
    \[
    b^k = \begin{pmatrix} 3^k & 3^k - 2^k \\ 0 & 2^k \end{pmatrix}.
    \]",eigenvalues and eigenvectors
"the eigenvalues of \( a \) are 1 and 9, and the eigenvalues of \( b \) are \( -1 \) and 9:
    \[
    a = \begin{pmatrix} 5 & 4 \\ 4 & 5 \end{pmatrix}, \quad
    b = \begin{pmatrix} 4 & 5 \\ 5 & 4 \end{pmatrix}.
    \]
    find a matrix square root of \( a \) from \( r = s \sqrt{\lambda} s^{-1} \). why is there no real matrix square root of \( b \)?",eigenvalues and eigenvectors
"if \( a \) and \( b \) have the same eigenvalues with the same full set of independent eigenvectors, their factorizations into \( a = s \lambda_1 s^{-1} \) and \( b = s \lambda_2 s^{-1} \) are the same. so, \( a = b \).",eigenvalues and eigenvectors
"suppose \( a \) and \( b \) have the same full set of eigenvectors, so that \( a = s \lambda_1 s^{-1} \) and \( b = s \lambda_2 s^{-1} \). prove that \( ab = ba \).",eigenvalues and eigenvectors
when do the eigenvectors for \( \lambda = 0 \) span the nullspace \( n(a) \)?,eigenvalues and eigenvectors
when do all the eigenvectors for \( \lambda \neq 0 \) span the column space \( c(a) \)?,eigenvalues and eigenvectors
"the powers \( a^k \) approach zero if all \( |\lambda_i| < 1 \), and they blow up if any \( |\lambda_i| > 1 \). peter lax gives four striking examples in his book *linear algebra*:
    \[
    a = \begin{pmatrix} 3 & 2 \\ 1 & 4 \end{pmatrix}, \quad
    b = \begin{pmatrix} 3 & 2 \\ -5 & -3 \end{pmatrix}, \quad
    c = \begin{pmatrix} 5 & 7 \\ -3 & -4 \end{pmatrix}, \quad
    d = \begin{pmatrix} 5 & 6.9 \\ -3 & -4 \end{pmatrix}.
    \]
    \[
     ||a^{1024}|| \approx 10^{700}, \quad b^{1024} = i, \quad c^{1024} = -c, \quad ||d^{1024}||\approx 10^{-78}.

    \]
    find the eigenvalues \( \lambda = e^{i\theta} \) of \( b \) and \( c \) to show that \( b^4 = i \) and \( c^3 = -i \).",eigenvalues and eigenvectors
"following the first example in this section, find the eigenvalues and eigenvectors, and the exponential $e^{at}$, for 
    \[
    a = \begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"for the previous matrix, write the general solution to $\frac{du}{dt} = au$, and the specific solution that matches $u(0) = \begin{bmatrix}3 \\ 1\end{bmatrix}$. what is the steady state as $t \to \infty$? (this is a continuous markov process; $\lambda = 0$ in a differential equation corresponds to $\lambda = 1$ in a difference equation, since $e^{0t} = 1$.)",eigenvalues and eigenvectors
"suppose the time direction is reversed to give the matrix $-a$:
    \[
    \frac{du}{dt} = \begin{bmatrix} 1 & -1 \\ -1 & 1 \end{bmatrix} u \quad \text{with} \quad u(0) = \begin{bmatrix}3 \\ 1\end{bmatrix}.
    \]
    find $u(t)$ and show that it blows up instead of decaying as $t \to \infty$. (diffusion is irreversible, and the heat equation cannot run backward.)",eigenvalues and eigenvectors
"if $p$ is a projection matrix, show from the infinite series that
    \[
    e^p \approx i + 1.718p.
    \]",eigenvalues and eigenvectors
"a diagonal matrix like $\lambda = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix}$ satisfies the usual rule $e^{\lambda(t+t)} = e^{\lambda t} e^{\lambda t}$, because the rule holds for each diagonal entry.
    
    \begin{itemize}",eigenvalues and eigenvectors
"the higher-order equation $y'' + y = 0$ can be written as a first-order system by introducing the velocity $y'$ as another unknown:
    \[
    \frac{d}{dt} \begin{bmatrix} y \\ y' \end{bmatrix} = \begin{bmatrix} y' \\ y'' \end{bmatrix} = \begin{bmatrix} y' \\ -y \end{bmatrix}.
    \]
    if this is $\frac{du}{dt} = au$, what is the $2 \times 2$ matrix $a$? find its eigenvalues and eigenvectors, and compute the solution that starts from $y(0) = 2, y'(0) = 0$.",eigenvalues and eigenvectors
"convert $y'' = 0$ to a first-order system $\frac{du}{dt} = au$:
    \[
    \frac{d}{dt} \begin{bmatrix} y \\ y' \end{bmatrix} = \begin{bmatrix} y' \\ 0 \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} y \\ y' \end{bmatrix}.
    \]
    this $2 \times 2$ matrix $a$ has only one eigenvector and cannot be diagonalized. compute $e^{at}$ from the series $i + at + \dots$ and write the solution $e^{at} u(0)$ starting from $y(0) = 3, y'(0) = 4$. check that your $(y, y')$ satisfies $y'' = 0$.",eigenvalues and eigenvectors
"suppose the rabbit population $r$ and the wolf population $w$ are governed by
    \[
    \frac{dr}{dt} = 4r - 2w, \quad \frac{dw}{dt} = r + w.
    \]
    \begin{itemize}",eigenvalues and eigenvectors
"decide the stability of $u' = au$ for the following matrices:
    \begin{itemize}",eigenvalues and eigenvectors
"decide on the stability or instability of $\frac{dv}{dt} = w$, $\frac{dw}{dt} = v$. is there a solution that decays?",eigenvalues and eigenvectors
"from their trace and determinant, at what time $t$ do the following matrices change between stable with real eigenvalues, stable with complex eigenvalues, and unstable?
    \begin{itemize}",eigenvalues and eigenvectors
$a_1 = \begin{bmatrix} 1 & -1 \\ t & -1 \end{bmatrix}$,eigenvalues and eigenvectors
$a_2 = \begin{bmatrix} 0 & 4 - t \\ 1 & -2 \end{bmatrix}$,eigenvalues and eigenvectors
"$a_3 = \begin{bmatrix} t & -1 \\ 1 & t \end{bmatrix}$
    \end{itemize}",eigenvalues and eigenvectors
"find the eigenvalues and eigenvectors for
    \[
        \frac{du}{dt} = au = \begin{bmatrix} 0 & 3 & 0 \\ -3 & 0 & 4 \\ 0 & -4 & 0 \end{bmatrix} u.
    \]
    why do you know, without computing, that $e^{at}$ will be an orthogonal matrix and $||u(t)||^2 = u_1^2 + u_2^2 + u_3^2$ will be constant?",eigenvalues and eigenvectors
"for the skew-symmetric equation
    \[
        \frac{du}{dt} = au = \begin{bmatrix} 0 & c & -b \\ -c & 0 & a \\ b & -a & 0 \end{bmatrix} \begin{bmatrix} u_1 \\ u_2 \\ u_3 \end{bmatrix},
    \]
    \begin{itemize}",eigenvalues and eigenvectors
"what are the eigenvalues $\lambda$ and frequencies $\omega$, and the general solution, of the following equation?
    \[
        \frac{d^2 u}{dt^2} = \begin{bmatrix} -5 & 4 \\ 4 & -5 \end{bmatrix} u.
    \]",eigenvalues and eigenvectors
"solve the second-order equation
    \[
        \frac{d^2 u}{dt^2} = \begin{bmatrix} -5 & -1 \\ -1 & -5 \end{bmatrix} u
    \]
    with $u(0) = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and $u'(0) = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$.",eigenvalues and eigenvectors
"in most applications, the second-order equation looks like $m u'' + k u = 0$, with a mass matrix multiplying the second derivatives. substitute the pure exponential $u = e^{i \omega t} x$ and find the “generalized eigenvalue problem” that must be solved for the frequency $\omega$ and the vector $x$.",eigenvalues and eigenvectors
"with a friction matrix $f$ in the equation $u'' + f u' - a u = 0$, substitute a pure exponential $u = e^{\lambda t} x$ and find a quadratic eigenvalue problem for $\lambda$.",eigenvalues and eigenvectors
"for equation (16) in the text, with $\omega = 1$ and $\sqrt{3}$, find the motion if the first mass is hit at $t = 0$; $u(0) = (0,0)$ and $u'(0) = (1,0)$.",eigenvalues and eigenvectors
"every $2 \times 2$ matrix with trace zero can be written as
    \[
        a = \begin{bmatrix} a & b+c \\ b-c & -a \end{bmatrix}.
    \]
    show that its eigenvalues are real exactly when $a^2 + b^2 \geq c^2$.",eigenvalues and eigenvectors
"by back-substitution or by computing eigenvectors, solve
    \[
        \frac{du}{dt} = \begin{bmatrix} 1 & 2 & 1 \\ 0 & 3 & 6 \\ 0 & 0 & 4 \end{bmatrix} u
    \]
    with $u(0) = \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}$.",eigenvalues and eigenvectors
"find $\lambda$'s and $x$'s so that $u = e^{\lambda t} x$ solves
    \[
        \frac{du}{dt} = \begin{bmatrix} 4 & 3 \\ 0 & 1 \end{bmatrix} u.
    \]
    what combination $u = c_1 e^{\lambda_1 t} x_1 + c_2 e^{\lambda_2 t} x_2$ starts from $u(0) = (5, -2)$?",eigenvalues and eigenvectors
"solve problem 21 for $u(t) = (y(t),z(t))$ by back-substitution:
    \begin{itemize}",eigenvalues and eigenvectors
"first solve $\frac{dz}{dt} = z$, starting from $z(0) = -2$.",eigenvalues and eigenvectors
"then solve $\frac{dy}{dt} = 4y+3z$, starting from $y(0) = 5$.",eigenvalues and eigenvectors
"the solution for $y$ will be a combination of $e^{4t}$ and $e^{t}$.
    \end{itemize}",eigenvalues and eigenvectors
"find $a$ to change $y'' = 5y' +4y$ into a vector equation for $u(t) = (y(t),y'(t))$:
    \begin{itemize}",eigenvalues and eigenvectors
$\frac{du}{dt} = \begin{bmatrix} y' \\ y'' \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 4 & 5 \end{bmatrix} \begin{bmatrix} y \\ y' \end{bmatrix} = au.$,eigenvalues and eigenvectors
"what are the eigenvalues of $a$? find them also by substituting $y = e^{\lambda t}$ into the scalar equation $y'' = 5y' +4y$.
    \end{itemize}",eigenvalues and eigenvectors
"a door is opened between rooms that hold $v(0) = 30$ people and $w(0) = 10$ people.
    \begin{itemize}",eigenvalues and eigenvectors
"the movement between rooms is proportional to the difference $v-w$: \[ \frac{dv}{dt} = w-v, \quad \frac{dw}{dt} = v-w. \]",eigenvalues and eigenvectors
show that the total $v+w$ is constant (40 people).,eigenvalues and eigenvectors
"find the matrix in $\frac{du}{dt} = au$, and its eigenvalues and eigenvectors.",eigenvalues and eigenvectors
"what are $v$ and $w$ at $t = 1$?
    \end{itemize}",eigenvalues and eigenvectors
"reverse the diffusion of people in problem 24 to $\frac{du}{dt} = -au$:
    \begin{itemize}",eigenvalues and eigenvectors
"$\frac{dv}{dt} = v-w$, $\frac{dw}{dt} = w-v$.",eigenvalues and eigenvectors
the total $v+w$ still remains constant.,eigenvalues and eigenvectors
how are the $\lambda$'s changed now that $a$ is changed to $-a$?,eigenvalues and eigenvectors
"show that $v(t)$ grows to infinity from $v(0) = 30$.
    \end{itemize}",eigenvalues and eigenvectors
"the solution to $y'' = 0$ is a straight line $y = c +dt$. convert to a matrix equation:
    \begin{itemize}",eigenvalues and eigenvectors
$\frac{d}{dt} \begin{bmatrix} y \\ y' \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} y \\ y' \end{bmatrix}$,eigenvalues and eigenvectors
compute $e^{at} = i +at + \frac{1}{2} a^2 t^2 + \dots$.,eigenvalues and eigenvectors
"multiply your $e^{at}$ times $(y(0),y'(0))$ to check the straight line $y(t) = y(0) + y'(0)t$.
    \end{itemize}",eigenvalues and eigenvectors
"substitute $y = e^{\lambda t}$ into $y'' = 6y' - 9y$ to show that $\lambda = 3$ is a repeated root.
    \begin{itemize}",eigenvalues and eigenvectors
convert to matrix equation: \[ \frac{d}{dt} \begin{bmatrix} y \\ y' \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ -9 & 6 \end{bmatrix} \begin{bmatrix} y \\ y' \end{bmatrix}. \],eigenvalues and eigenvectors
"show that this matrix has $\lambda = 3,3$ and only one line of eigenvectors.",eigenvalues and eigenvectors
"show that the second solution is $y = te^{3t}$.
    \end{itemize}",eigenvalues and eigenvectors
figure out how to write $my'' + by' + ky = 0$ as a vector equation $mu' = au$.,eigenvalues and eigenvectors
"find two familiar functions that solve the equation $\frac{d^2 y}{dt^2} = -y$.
    \begin{itemize}",eigenvalues and eigenvectors
which one starts with $y(0) = 1$ and $y'(0) = 0$?,eigenvalues and eigenvectors
"convert to a vector equation: \[ \frac{du}{dt} = au, \quad a = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}. \]",eigenvalues and eigenvectors
"solve problem 6 again using this formulation.
    \end{itemize}",eigenvalues and eigenvectors
"a particular solution to $\frac{du}{dt} = au - b$ is $u_p = a^{-1}b$, if $a$ is invertible. the solutions to $\frac{du}{dt} = au$ give $u_n$. find the complete solution $u_p + u_n$ to:
    \begin{itemize}",eigenvalues and eigenvectors
$\frac{du}{dt} = 2u - 8$.,eigenvalues and eigenvectors
"$\frac{du}{dt} = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix} u - \begin{bmatrix} 8 \\ 6 \end{bmatrix}$.
    \end{itemize}",eigenvalues and eigenvectors
"if $c$ is not an eigenvalue of $a$, substitute $u = e^{ct}v$ and find $v$ to solve $\frac{du}{dt} = au - e^{ct}b$. this $u = e^{ct}v$ is a particular solution. how does it break down when $c$ is an eigenvalue?",eigenvalues and eigenvectors
"find a matrix $a$ to illustrate each of the unstable regions in figure 5.2:
    \begin{itemize}",eigenvalues and eigenvectors
$\lambda_1 < 0$ and $\lambda_2 > 0$.,eigenvalues and eigenvectors
$\lambda_1 > 0$ and $\lambda_2 > 0$.,eigenvalues and eigenvectors
"complex $\lambda$'s with real part $a > 0$.
    \end{itemize}",eigenvalues and eigenvectors
write five terms of the infinite series for $e^{at}$. take the $t$ derivative of each term. show that you have four terms of $ae^{at}$. conclusion: $e^{at}u(0)$ solves $u' = au$.,eigenvalues and eigenvectors
the matrix $b = \begin{bmatrix} 0 & -1 \\ 0 & 0 \end{bmatrix}$ has $b^2 = 0$. find $e^{bt}$ from a (short) infinite series. check that the derivative of $e^{bt}$ is $be^{bt}$.,eigenvalues and eigenvectors
"starting from $u(0)$, the solution at time $t$ is $e^{at} u(0)$. go an additional time $t$ to reach $e^{at}(e^{at} u(0))$. this solution at time $t + t$ can also be written as $e^{a(t+t)}$. conclusion: $e^{at} e^{at} = e^{a(t+t)}$.",eigenvalues and eigenvectors
write $a = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$ in the form $s\lambda s^{-1}$. find $e^{at}$ from $s e^{\lambda t} s^{-1}$.,eigenvalues and eigenvectors
"if $a^2 = a$, show that the infinite series produces $e^{at} = i + (e^t -1)a$. for $a = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$ in problem 36, this gives $e^{at}$.",eigenvalues and eigenvectors
"generally, $e^a e^b$ is different from $e^b e^a$. they are both different from $e^{a+b}$. check this using problems 36--37 and 34:
    \begin{itemize}",eigenvalues and eigenvectors
$a = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$,eigenvalues and eigenvectors
$b = \begin{bmatrix} 0 & -1 \\ 0 & 0 \end{bmatrix}$,eigenvalues and eigenvectors
"$a+b = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$
    \end{itemize}",eigenvalues and eigenvectors
write $a = \begin{bmatrix} 1 & 1 \\ 0 & 3 \end{bmatrix}$ as $s\lambda s^{-1}$. multiply $s e^{\lambda t} s^{-1}$ to find the matrix exponential $e^{at}$. check $e^{at} = i$ when $t = 0$.,eigenvalues and eigenvectors
"put $a = \begin{bmatrix} 1 & 3 \\ 0 & 0 \end{bmatrix}$ into the infinite series to find $e^{at}$. first compute $a^2$:
    \[
    e^{at} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + \begin{bmatrix} t & 3t \\ 0 & 0 \end{bmatrix} + \frac{1}{2} \begin{bmatrix} ? & ? \\ ? & ? \end{bmatrix} + \cdots = \begin{bmatrix} e^t & 0 \\ 0 & 1 \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"give two reasons why the matrix exponential $e^{at}$ is never singular:
    \begin{itemize}",eigenvalues and eigenvectors
(a) write its inverse.,eigenvalues and eigenvectors
"(b) write its eigenvalues. if $ax = \lambda x$ then $e^{at}x = e^{\lambda t} x$.
    \end{itemize}",eigenvalues and eigenvectors
"find a solution $x(t), y(t)$ of the first system that gets large as $t \to \infty$. to avoid this instability a scientist thought of exchanging the two equations!
    \begin{itemize}",eigenvalues and eigenvectors
"original system:
        \[
        \frac{dx}{dt} = 0x - 4y, \quad \frac{dy}{dt} = -2x + 2y.
        \]",eigenvalues and eigenvectors
"exchanged system:
        \[
        \frac{dy}{dt} = -2x + 2y, \quad \frac{dx}{dt} = 0x - 4y.
        \]
    \end{itemize}
    now the matrix $\begin{bmatrix} -2 & 2 \\ 0 & -4 \end{bmatrix}$ is stable. it has $\lambda < 0$. comment on this craziness.",eigenvalues and eigenvectors
"from this general solution to $\frac{du}{dt} = au$, find the matrix $a$:
    \[
    u(t) = c_1 e^{2t} \begin{bmatrix} 2 \\ 1 \end{bmatrix} + c_2 e^{5t} \begin{bmatrix} 1 \\ 1 \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"for the complex numbers \( 3 + 4i \) and \( 1 - i \),
    \begin{enumerate}",eigenvalues and eigenvectors
find their positions in the complex plane.,eigenvalues and eigenvectors
find their sum and product.,eigenvalues and eigenvectors
find their conjugates and their absolute values.,eigenvalues and eigenvectors
do the original numbers lie inside or outside the unit circle?,eigenvalues and eigenvectors
"what can you say about
    \begin{enumerate}",eigenvalues and eigenvectors
the sum of a complex number and its conjugate?,eigenvalues and eigenvectors
the conjugate of a number on the unit circle?,eigenvalues and eigenvectors
the product of two numbers on the unit circle?,eigenvalues and eigenvectors
the sum of two numbers on the unit circle?,eigenvalues and eigenvectors
"if \( x = 2 + i \) and \( y = 1 + 3i \), find \( x \), \( x^2 \), \( \frac{1}{x} \), and \( \frac{x}{y} \). check that the absolute value \( |xy| \) equals \( |x| \) times \( |y| \), and the absolute value \( \left| \frac{1}{x} \right| \) equals \( \frac{1}{|x|} \).",eigenvalues and eigenvectors
"find \( a \) and \( b \) for the complex numbers \( a + ib \) at the angles \( \theta = 30^\circ, 60^\circ, 90^\circ \) on the unit circle. verify by direct multiplication that the square of the first is the second, and the cube of the first is the third.",eigenvalues and eigenvectors
"if \( x = re^{i\theta} \), what are \( x^2 \), \( x^{-1} \), and \( x \) in polar coordinates? where are the complex numbers that have \( x^{-1} = x \)?",eigenvalues and eigenvectors
"at \( t = 0 \), the complex number \( e^{(-1 + i)t} \) equals one. sketch its path in the complex plane as \( t \) increases from \( 0 \) to \( 2\pi \).",eigenvalues and eigenvectors
"find the lengths and the inner product of
    \[
    x = \begin{bmatrix} 2 - 4i \\ 4i \end{bmatrix}, \quad y = \begin{bmatrix} 2 + 4i \\ 4i \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"write out the matrix \( a^h \) and compute \( c = a^h a \) if
    \[
    a = \begin{bmatrix} 1 & i & 0 \\ i & 0 & 1 \end{bmatrix}.
    \]
    what is the relation between \( c \) and \( c^h \)? does it hold whenever \( c \) is constructed from some \( a^h a \)?",eigenvalues and eigenvectors
"with the preceding \( a \), use elimination to solve \( ax = 0 \).",eigenvalues and eigenvectors
"show that the nullspace you just computed is orthogonal to \( c(a^h) \) and not to the usual row space \( c(a^t) \). the four fundamental spaces in the complex case are \( n(a) \) and \( c(a) \) as before, and then \( n(a^h) \) and \( c(a^h) \).",eigenvalues and eigenvectors
how is the determinant of \( a^h \) related to the determinant of \( a \)?,eigenvalues and eigenvectors
prove that the determinant of any hermitian matrix is real.,eigenvalues and eigenvectors
"how many degrees of freedom are there in a real symmetric matrix, a real diagonal matrix, and a real orthogonal matrix? the first answer is the sum of the other two, because \( a = q \lambda q^t \).",eigenvalues and eigenvectors
show that \( 3 \times 3 \) hermitian matrices \( a \) and also unitary \( u \) have 9 real degrees of freedom (columns of \( u \) can be multiplied by any \( e^{i\theta} \)).,eigenvalues and eigenvectors
"write \( p \), \( q \), and \( r \) in the form \( \lambda_1 x_1 x_1^h + \lambda_2 x_2 x_2^h \) of the spectral theorem:
    \begin{itemize}",eigenvalues and eigenvectors
\( p = \begin{bmatrix} 1 & 2 \\ 1 & 2 \end{bmatrix} \),eigenvalues and eigenvectors
\( q = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \),eigenvalues and eigenvectors
"\( r = \begin{bmatrix} 3 & 4 \\ 4 & -3 \end{bmatrix} \)
    \end{itemize}",eigenvalues and eigenvectors
"give a reason if true or a counterexample if false:
    \begin{itemize}",eigenvalues and eigenvectors
"(a) if \( a \) is hermitian, then \( a + ii \) is invertible.",eigenvalues and eigenvectors
"(b) if \( q \) is orthogonal, then \( q^{+1/2} i \) is invertible.",eigenvalues and eigenvectors
"(c) if \( a \) is real, then \( a + ii \) is invertible.
    \end{itemize}",eigenvalues and eigenvectors
"suppose \( a \) is a symmetric \( 3 \times 3 \) matrix with eigenvalues 0, 1, 2.
    \begin{itemize}",eigenvalues and eigenvectors
"(a) what properties can be guaranteed for the corresponding unit eigenvectors \( u \), \( v \), \( w \)?",eigenvalues and eigenvectors
"(b) in terms of \( u \), \( v \), \( w \), describe the nullspace, left nullspace, row space, and column space of \( a \).",eigenvalues and eigenvectors
(c) find a vector \( x \) that satisfies \( a x = v + w \). is \( x \) unique?,eigenvalues and eigenvectors
(d) under what conditions on \( b \) does \( a x = b \) have a solution?,eigenvalues and eigenvectors
"(e) if \( u \), \( v \), \( w \) are the columns of \( s \), what are \( s^{-1} \) and \( s^{-1} a s \)?
    \end{itemize}",eigenvalues and eigenvectors
"in the list below, which classes of matrices contain \( a \) and which contain \( b \)?
    \begin{itemize}",eigenvalues and eigenvectors
\( a = \begin{bmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0 \end{bmatrix} \),eigenvalues and eigenvectors
"\( b = \frac{1}{4} \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \end{bmatrix} \)
    \end{itemize}
    classes to check: orthogonal, invertible, projection, permutation, hermitian, rank-1, diagonalizable, markov.
    find the eigenvalues of \( a \) and \( b \).",eigenvalues and eigenvectors
"what is the dimension of the space \( s \) of all \( n \times n \) real symmetric matrices? the spectral theorem says that every symmetric matrix is a combination of \( n \) projection matrices. since the dimension exceeds \( n \), how is this difference explained?",eigenvalues and eigenvectors
"write one significant fact about the eigenvalues of each of the following:
    \begin{itemize}",eigenvalues and eigenvectors
(a) a real symmetric matrix.,eigenvalues and eigenvectors
(b) a stable matrix: all solutions to \( \frac{du}{dt} = a u \) approach zero.,eigenvalues and eigenvectors
(c) an orthogonal matrix.,eigenvalues and eigenvectors
(d) a markov matrix.,eigenvalues and eigenvectors
(e) a defective matrix (nondiagonalizable).,eigenvalues and eigenvectors
"(f) a singular matrix.
    \end{itemize}",eigenvalues and eigenvectors
"show that if $u$ and $v$ are unitary, so is $uv$. use the criterion $u^h u = i$.",eigenvalues and eigenvectors
"show that a unitary matrix has $|\det u| = 1$, but possibly $\det u$ is different from $\det u^h$. describe all $2 \times 2$ matrices that are unitary.",eigenvalues and eigenvectors
"find a third column so that $u$ is unitary. how much freedom in column 3?
    \[
    u = \begin{bmatrix} 
    \frac{1}{\sqrt{3}} & \frac{i}{\sqrt{2}} \\
    \frac{1}{\sqrt{3}} & 0 \\
    \frac{i}{\sqrt{3}} & \frac{1}{\sqrt{2}} 
    \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"diagonalize the $2 \times 2$ skew-hermitian matrix 
    \[
    k = \begin{bmatrix} i & i \\ i & i \end{bmatrix},
    \]
    whose entries are all $\sqrt{-1}$. compute $e^{kt} = s e^{\lambda t} s^{-1}$, and verify that $e^{kt}$ is unitary. what is the derivative of $e^{kt}$ at $t = 0$?",eigenvalues and eigenvectors
"describe all $3 \times 3$ matrices that are simultaneously hermitian, unitary, and diagonal. how many are there?",eigenvalues and eigenvectors
"every matrix $z$ can be split into a hermitian and a skew-hermitian part, $z = a + k$, just as a complex number $z$ is split into $a + ib$. the real part of $z$ is half of $z + \bar{z}$, and the ""real part"" of $z$ is half of $z + z^h$. find a similar formula for the ""imaginary part"" $k$, and split these matrices into $a+k$:
    \[
    z = \begin{bmatrix} 3+i & 4+2i \\ 0 & 5 \end{bmatrix}, \quad z = \begin{bmatrix} i & i \\ -i & i \end{bmatrix}.
    \]",eigenvalues and eigenvectors
show that the columns of the $4 \times 4$ fourier matrix $f$ in example 5 are eigenvectors of the permutation matrix $p$ in example 6.,eigenvalues and eigenvectors
"for the permutation of example 6, write out the circulant matrix 
    \[
    c = c_0 i + c_1 p + c_2 p^2 + c_3 p^3.
    \]
    (its eigenvector matrix is again the fourier matrix.) write out also the four components of the matrix-vector product $cx$, which is the convolution of $c = (c_0, c_1, c_2, c_3)$ and $x = (x_0, x_1, x_2, x_3)$.",eigenvalues and eigenvectors
"for a circulant $c = f \lambda f^{-1}$, why is it faster to multiply by $f^{-1}$, then $\lambda$, then $f$ (the convolution rule), than to multiply directly by $c$?",eigenvalues and eigenvectors
"find the lengths of $u = (1+i, 1-i, 1+2i)$ and $v = (i, i, i)$. also find $u^h v$ and $v^h u$.",eigenvalues and eigenvectors
"prove that $a^h a$ is always a hermitian matrix. compute $a^h a$ and $a a^h$:
    \[
    a = \begin{bmatrix} i & 1 & i \\ 1 & i & i \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"if $a z = 0$, then $a^h a z = 0$. if $a^h a z = 0$, multiply by $z^h$ to prove that $a z = 0$. the nullspaces of $a$ and $a^h a$ are \dots $a^h a$ is an invertible hermitian matrix when the nullspace of $a$ contains only $z = \dots$.",eigenvalues and eigenvectors
"when you multiply a hermitian matrix by a real number $c$, is $ca$ still hermitian? if $c = i$, show that $ia$ is skew-hermitian. the $3 \times 3$ hermitian matrices form a subspace, provided that the “scalars” are real numbers.",eigenvalues and eigenvectors
"which classes of matrices does $p$ belong to: orthogonal, invertible, hermitian, unitary, factorizable into lu, factorizable into qr?
    \[
    p = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"compute $p^2$, $p^3$, and $p^{100}$ in problem 30. what are the eigenvalues of $p$?",eigenvalues and eigenvectors
"find the unit eigenvectors of $p$ in problem 30, and put them into the columns of a unitary matrix $u$. what property of $p$ makes these eigenvectors orthogonal?",eigenvalues and eigenvectors
write down the $3 \times 3$ circulant matrix $c = 2i + 5p + 4p^2$. it has the same eigenvectors as $p$ in problem 30. find its eigenvalues.,eigenvalues and eigenvectors
"if $u$ is unitary and $q$ is a real orthogonal matrix, show that $u^{-1}$ is unitary and also $uq$ is unitary. start from $u^h u = i$ and $q^t q = i$.",eigenvalues and eigenvectors
"diagonalize $a$ (real $\lambda$’s) and $k$ (imaginary $\lambda$’s) to reach $u \lambda u^h$:
    \[
    a = \begin{bmatrix} 0 & 1-i \\ i+1 & 1 \end{bmatrix}, \quad k = \begin{bmatrix} 0 & -1+i \\ 1+i & i \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"diagonalize this orthogonal matrix to reach $q = u \lambda u^h$. now all $\lambda$’s are:
    \[
    q = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"diagonalize this unitary matrix $v$ to reach $v = u \lambda u^h$. again all $|\lambda| = 1$:
    \[
    v = \frac{1}{\sqrt{3}} \begin{bmatrix} 1 & 1-i \\ 1+i & -1 \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"if $v_1,\dots,v_n$ is an orthonormal basis for $\mathbb{c}^n$, the matrix with those columns is a unitary matrix. show that any vector $z$ equals $(v_1^h z)v_1 + \dots + (v_n^h z)v_n$.",eigenvalues and eigenvectors
the functions $e^{-ix}$ and $e^{-ix}$ are orthogonal on the interval $0 \leq x \leq 2\pi$ because their complex inner product is $\int_0^{2\pi} e^{-ix} e^{ix}dx = 0$.,eigenvalues and eigenvectors
"the vectors $v = (1,i,1)$, $w = (i,1,0)$ and $z = \begin{bmatrix} a & b & c \end{bmatrix}^t$ are an orthogonal basis for $\mathbb{c}^n$.",eigenvalues and eigenvectors
"if $a = r + is$ is a hermitian matrix, are the real matrices $r$ and $s$ symmetric?",eigenvalues and eigenvectors
the (complex) dimension of $\mathbb{c}^n$ is $n$. find a nonreal basis for $\mathbb{c}^n$.,eigenvalues and eigenvectors
describe all $1 \times 1$ matrices that are hermitian and also unitary. do the same for $2 \times 2$ matrices.,eigenvalues and eigenvectors
how are the eigenvalues of $a^h$ (square matrix) related to the eigenvalues of $a$?,eigenvalues and eigenvectors
"if $u^h u = 1$, show that $i -2uu^h$ is hermitian and also unitary. the rank-1 matrix $uu^h$ is the projection onto what line in $\mathbb{c}^n$?",eigenvalues and eigenvectors
"if $a+ib$ is a unitary matrix ($a$ and $b$ are real), show that 
    \[
        q = \begin{bmatrix} a & -b \\ b & a \end{bmatrix}
    \]
    is an orthogonal matrix.",eigenvalues and eigenvectors
"if $a+ib$ is a hermitian matrix ($a$ and $b$ are real), show that 
    \[
        \begin{bmatrix} a & -b \\ b & a \end{bmatrix}
    \]
    is symmetric.",eigenvalues and eigenvectors
prove that the inverse of a hermitian matrix is again a hermitian matrix.,eigenvalues and eigenvectors
"diagonalize this matrix by constructing its eigenvalue matrix $\lambda$ and its eigenvector matrix $s$:
    \[
        a = \begin{bmatrix} 2 & 1-i \\ 1+i & 3 \end{bmatrix} = a^h.
    \]",eigenvalues and eigenvectors
a matrix with orthonormal eigenvectors has the form $a = u\lambda u^{-1} = u\lambda u^h$. prove that $aa^h = a^h a$. these are exactly the normal matrices.,eigenvalues and eigenvectors
"if $b$ is similar to $a$ and $c$ is similar to $b$, show that $c$ is similar to $a$. (let $b = m^{-1}am$ and $c = n^{-1}bn$.) which matrices are similar to $i$?",eigenvalues and eigenvectors
"describe in words all matrices that are similar to $\begin{bmatrix}1 & 0 \\ 0 & -1\end{bmatrix}$, and find two of them.",eigenvalues and eigenvectors
explain why $a$ is never similar to $a+i$.,eigenvalues and eigenvectors
"find a diagonal $m$, made up of $1$s and $-1$s, to show that
    \begin{itemize}",eigenvalues and eigenvectors
$a = \begin{bmatrix} 2 & 1 & 0 \\ 1 & 2 & 1 \\ 0 & 1 & 2 \end{bmatrix}$,eigenvalues and eigenvectors
"is similar to $b = \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}$.
    \end{itemize}",eigenvalues and eigenvectors
show (if $b$ is invertible) that $ba$ is similar to $ab$.,eigenvalues and eigenvectors
\begin{itemize},eigenvalues and eigenvectors
"consider any $a$ and a “givens rotation” $m$ in the 1–2 plane:
    \begin{itemize}",eigenvalues and eigenvectors
"$a = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix}$, $m = \begin{bmatrix} \cos\theta & -\sin\theta & 0 \\ \sin\theta & \cos\theta & 0 \\ 0 & 0 & 1 \end{bmatrix}$.",eigenvalues and eigenvectors
"choose the rotation angle $\theta$ to produce zero in the $(3,1)$ entry of $m^{-1}am$.
    \end{itemize}",eigenvalues and eigenvectors
"what matrix $m$ changes the basis $v_1 = (1,1)$, $v_2 = (1,4)$ to the basis $v_1 = (2,5)$, $v_2 = (1,4)$? the columns of $m$ come from expressing $v_1$ and $v_2$ as combinations $\sum m_{ij} v_j$ of the $v$’s.",eigenvalues and eigenvectors
"for the same two bases, express the vector $(3,9)$ as a combination $c_1v_1 + c_2v_2$ and also as $d_1v_1 + d_2v_2$. check numerically that $m$ connects $c$ to $d$: $mc = d$.",eigenvalues and eigenvectors
"confirm the last exercise: if $v_1 = m_{11}v_1 + m_{21}v_2$ and $v_2 = m_{12}v_1 + m_{22}v_2$, and $m_{11}c_1 + m_{12}c_2 = d_1$ and $m_{21}c_1 + m_{22}c_2 = d_2$, the vectors $c_1v_1 + c_2v_2$ and $d_1v_1 + d_2v_2$ are the same. this is the “change of basis formula” $mc = d$.",eigenvalues and eigenvectors
"if the transformation $t$ is a reflection across the $45^\circ$ line in the plane, find its matrix with respect to the standard basis $v_1 = (1,0)$, $v_2 = (0,1)$, and also with respect to $v_1 = (1,1)$, $v_2 = (1,-1)$. show that those matrices are similar.",eigenvalues and eigenvectors
"the identity transformation takes every vector to itself: $tx = x$. find the corresponding matrix, if the first basis is $v_1 = (1,2)$, $v_2 = (3,4)$ and the second basis is $w_1 = (1,0)$, $w_2 = (0,1)$. (it is not the identity matrix!)",eigenvalues and eigenvectors
"the derivative of $a + bx + cx^2$ is $b + 2cx + 0x^2$.
    \begin{itemize}",eigenvalues and eigenvectors
"show that every number is an eigenvalue for $t f(x) = \frac{d f}{dx}$, but the transformation $t f(x) = \int_{0}^{x} f(t) dt$ has no eigenvalues (here $-\infty < x < \infty$).",eigenvalues and eigenvectors
"on the space of $2 \times 2$ matrices, let $t$ be the transformation that transposes every matrix. find the eigenvalues and ""eigenmatrices"" for $a^t = \lambda a$.",eigenvalues and eigenvectors
"prove that every unitary matrix $a$ is diagonalizable, in two steps:
    \begin{itemize}",eigenvalues and eigenvectors
"find a normal matrix ($n n^h = n^h n$) that is not hermitian, skew-hermitian, unitary, or diagonal. show that all permutation matrices are normal.",eigenvalues and eigenvectors
"suppose $t$ is a $3 \times 3$ upper triangular matrix, with entries $t_{ij}$. compare the entries of $t t^h$ and $t^h t$, and show that if they are equal, then $t$ must be diagonal. all normal triangular matrices are diagonal.",eigenvalues and eigenvectors
"if $n$ is normal, show that $\|n x\| = \|n^h x\|$ for every vector $x$. deduce that the $i$th row of $n$ has the same length as the $i$th column. note: if $n$ is also upper triangular, this leads again to the conclusion that it must be diagonal.",eigenvalues and eigenvectors
"prove that a matrix with orthonormal eigenvectors must be normal, as claimed: if $u^{-1} n u = a$, or $n = u \lambda u^h$, then $n n^h = n^h n$.",eigenvalues and eigenvectors
"find a unitary $u$ and triangular $t$ so that $u^{-1} a u = t$, for 
    \[
    a = \begin{bmatrix} 5 & -3 \\ 4 & -2 \end{bmatrix}, \quad 
    a = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \\ 1 & 0 & 0 \end{bmatrix}.
    \]",eigenvalues and eigenvectors
"if $a$ has eigenvalues $0, 1, 2$, what are the eigenvalues of $a(a - i)(a - 2i)$?",eigenvalues and eigenvectors
"the characteristic polynomial of 
    \[
    a = \begin{bmatrix} a & b \\ c & d \end{bmatrix}
    \]
    is $\lambda^2 - (a+d)\lambda + (ad - bc)$. by direct substitution, verify cayley-hamilton: 
    \[
    a^2 - (a+d)a + (ad - bc)i = 0.
    \]",eigenvalues and eigenvectors
"if $a_{ij} = 1$ above the main diagonal and $a_{ij} = 0$ elsewhere, find the jordan form (say $4 \times 4$) by finding all the eigenvectors.",eigenvalues and eigenvectors
"show, by trying for an $m$ and failing, that no two of the three jordan forms in equation (8) are similar: 
    \[
    j_1 \neq m^{-1} j_2 m, \quad j_1 \neq m^{-1} j_3 m, \quad \text{and} \quad j_2 \neq m^{-1} j_3 m.
    \]",eigenvalues and eigenvectors
"solve $ u' = ju $ by back-substitution, solving first for $ u_2(t) $:
    \begin{itemize}",eigenvalues and eigenvectors
$ \frac{du}{dt} = ju = \begin{bmatrix} 5 & 1 \\ 0 & 5 \end{bmatrix} \begin{bmatrix} u_1 \\ u_2 \end{bmatrix} $ with initial value $ u(0) = \begin{bmatrix} 1 \\ 2 \end{bmatrix} $.,eigenvalues and eigenvectors
"notice $ te^{5t} $ in the first component $ u_1(t) $.
    \end{itemize}",eigenvalues and eigenvectors
"compute $ a^{10} $ and $ e^a $ if $ a = mjm^{-1} $:
    \begin{itemize}",eigenvalues and eigenvectors
"$ a = \begin{bmatrix} 14 & 9 \\ -16 & -10 \end{bmatrix} = \begin{bmatrix} 3 & -2 \\ -4 & 3 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 3 & 2 \\ 4 & 3 \end{bmatrix} $.
    \end{itemize}",eigenvalues and eigenvectors
"show that $ a $ and $ b $ are similar by finding $ m $ so that $ b = m^{-1}am $:
    \begin{itemize}",eigenvalues and eigenvectors
(a) $ a = \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} $ and $ b = \begin{bmatrix} 0 & 1 \\ 0 & 1 \end{bmatrix} $.,eigenvalues and eigenvectors
(b) $ a = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} $ and $ b = \begin{bmatrix} 1 & -1 \\ -1 & 1 \end{bmatrix} $.,eigenvalues and eigenvectors
"(c) $ a = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} $ and $ b = \begin{bmatrix} 4 & 3 \\ 2 & 1 \end{bmatrix} $.
    \end{itemize}",eigenvalues and eigenvectors
"which of these matrices $ a_1 $ to $ a_6 $ are similar? check their eigenvalues.
    \begin{itemize}",eigenvalues and eigenvectors
"$ \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}, \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}, \begin{bmatrix} 0 & 0 \\ 1 & 1 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix}, \begin{bmatrix} 0 & 1 \\ 0 & 1 \end{bmatrix} $.
    \end{itemize}",eigenvalues and eigenvectors
"there are sixteen $ 2 \times 2 $ matrices whose entries are 0s and 1s. similar matrices go into the same family.
    \begin{itemize}",eigenvalues and eigenvectors
"how many families? how many matrices (total 16) in each family?
    \end{itemize}",eigenvalues and eigenvectors
"(a) if $ x $ is in the nullspace of $ a $, show that $ m^{-1} x $ is in the nullspace of $ m^{-1}am $.",eigenvalues and eigenvectors
(b) the nullspaces of $ a $ and $ m^{-1}am $ have the same (vectors)(basis)(dimension).,eigenvalues and eigenvectors
"if $ a $ and $ b $ have exactly the same eigenvalues and eigenvectors, does $ a = b $? with $ n $ independent eigenvectors, we do have $ a = b $. find $ a \neq b $ when $ \lambda = 0,0 $ (repeated), but there is only one line of eigenvectors $ (x_1,0) $.",eigenvalues and eigenvectors
"by direct multiplication, find $ j^2 $ and $ j^3 $ when $ j = \begin{bmatrix} c & 1 \\ 0 & c \end{bmatrix} $. guess the form of $ j^k $. set $ k = 0 $ to find $ j^0 $. set $ k = -1 $ to find $ j^{-1} $.",eigenvalues and eigenvectors
"if $ j $ is the $ 5 \times 5 $ jordan block with $ \lambda = 0 $, find $ j^2 $ and count its eigenvectors, and find its jordan form (two blocks).",eigenvalues and eigenvectors
"the text solved $ \frac{du}{dt} = ju $ for a $ 3 \times 3 $ jordan block $ j $. add a fourth equation $ \frac{dw}{dt} = 5w + x $. follow the pattern of solutions for $ z, y, x $ to find $ w $.",eigenvalues and eigenvectors
"these jordan matrices have eigenvalues 0, 0, 0, 0. they have two eigenvectors (find them). but the block sizes don’t match and $ j $ is not similar to $ k $:
    \begin{itemize}",eigenvalues and eigenvectors
"$ j = \begin{bmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 \end{bmatrix} $,",eigenvalues and eigenvectors
"$ k = \begin{bmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} $.
    \end{itemize}",eigenvalues and eigenvectors
"for any matrix $ m $, compare $ jm $ with $ mk $. if they are equal, show that $ m $ is not invertible. then $ m^{-1} jm = k $ is impossible.",eigenvalues and eigenvectors
"prove in three steps that $a^t$ is always similar to $a$ (we know that the $\lambda$'s are the same, the eigenvectors are the problem):
    \begin{itemize}",eigenvalues and eigenvectors
"which pairs are similar? choose $a, b, c, d$ to prove that the other pairs aren’t:
    \begin{itemize}",eigenvalues and eigenvectors
$\begin{bmatrix} a & b \\ c & d \end{bmatrix}$,eigenvalues and eigenvectors
$\begin{bmatrix} b & a \\ d & c \end{bmatrix}$,eigenvalues and eigenvectors
$\begin{bmatrix} c & d \\ a & b \end{bmatrix}$,eigenvalues and eigenvectors
"$\begin{bmatrix} d & c \\ b & a \end{bmatrix}$
    \end{itemize}",eigenvalues and eigenvectors
"true or false, with a good reason:
    \begin{itemize}",eigenvalues and eigenvectors
prove that $ab$ has the same eigenvalues as $ba$.,eigenvalues and eigenvectors
"if $a$ is $6 \times 4$ and $b$ is $4 \times 6$, $ab$ and $ba$ have different sizes. nevertheless,
    \begin{align*}
        \begin{bmatrix} i & -a \\ 0 & i \end{bmatrix} 
        \begin{bmatrix} ab & 0 \\ b & 0 \end{bmatrix} 
        \begin{bmatrix} i & a \\ 0 & i \end{bmatrix} 
        = 
        \begin{bmatrix} 0 & 0 \\ b & ba \end{bmatrix} = g.
    \end{align*}
    \begin{itemize}",eigenvalues and eigenvectors
"why is each of these statements true?
    \begin{itemize}",eigenvalues and eigenvectors
"find the eigenvalues and eigenvectors, and the diagonalizing matrix $s$, for
    \begin{itemize}",eigenvalues and eigenvectors
$a = \begin{bmatrix} 1 & 0 \\ 2 & 3 \end{bmatrix}$,eigenvalues and eigenvectors
"$b = \begin{bmatrix} 7 & 2 \\ -15 & -4 \end{bmatrix}$
    \end{itemize}",eigenvalues and eigenvectors
"find the determinants of $a$ and $a^{-1}$ if
    \[ a = s \begin{bmatrix} \lambda_1 & 2 \\ 0 & \lambda_2 \end{bmatrix} s^{-1}. \]",eigenvalues and eigenvectors
"if $a$ has eigenvalues 0 and 1, corresponding to the eigenvectors
    \[ \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad \begin{bmatrix} 2 \\ -1 \end{bmatrix}, \]
    how can you tell in advance that $a$ is symmetric? what are its trace and determinant? what is $a$?",eigenvalues and eigenvectors
"in the previous problem, what will be the eigenvalues and eigenvectors of $a^2$? what is the relation of $a^2$ to $a$?",eigenvalues and eigenvectors
does there exist a matrix $a$ such that the entire family $a + ci$ is invertible for all complex numbers $c$? find a real matrix with $a + ri$ invertible for all real $r$.,eigenvalues and eigenvectors
"solve for both initial values and then find $e^{at}$:
    \[ \frac{du}{dt} = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix} u, \]
    if $u(0) = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and if $u(0) = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$.",eigenvalues and eigenvectors
"would you prefer to have interest compounded quarterly at 40\% per year, or annually at 50\%?",eigenvalues and eigenvectors
"true or false (with counterexample if false):
    \begin{itemize}",eigenvalues and eigenvectors
"what happens to the fibonacci sequence if we go backward in time, and how is $f_{-k}$ related to $f_k$? the law $f_{k+2} = f_{k+1} + f_k$ is still in force, so $f_{-1} = 1$.",eigenvalues and eigenvectors
"find the general solution to $\frac{du}{dt} = au$ if
    \[ a = \begin{bmatrix} 0 & -1 & 0 \\ 1 & 0 & -1 \\ 0 & 1 & 0 \end{bmatrix}. \]
    can you find a time $t$ at which the solution $u(t)$ is guaranteed to return to the initial value $u(0)$?",eigenvalues and eigenvectors
"if $p$ is the matrix that projects $\mathbb{r}^n$ onto a subspace $s$, explain why every vector in $s$ is an eigenvector, and so is every vector in $s^{\perp}$. what are the eigenvalues? (note the connection to $p^2 = p$, which means that $\lambda^2 = \lambda$.)",eigenvalues and eigenvectors
show that every matrix of order $> 1$ is the sum of two singular matrices.,eigenvalues and eigenvectors
"if the eigenvalues of $a$ are $1$ and $3$ with eigenvectors $\begin{bmatrix} 5 \\ 2 \end{bmatrix}$ and $\begin{bmatrix} 2 \\ 1 \end{bmatrix}$, find the solutions to $\frac{du}{dt} = au$ and $u_{k+1} = au_k$, starting from $u = \begin{bmatrix} 9 \\ 4 \end{bmatrix}$.",eigenvalues and eigenvectors
"find the eigenvalues and eigenvectors of 
    \[
    a = \begin{bmatrix} 0 & -i & 0 \\ i & 1 & i \\ 0 & -i & 0 \end{bmatrix}.
    \]
    what property do you expect for the eigenvectors, and is it true?",eigenvalues and eigenvectors
"by trying to solve 
    \[
    \begin{bmatrix} a & b \\ c & d \end{bmatrix} \begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} = a
    \]
    show that $a$ has no square root. change the diagonal entries of $a$ to $4$ and find a square root.",eigenvalues and eigenvectors
"true or false, with reason if true and counterexample if false:
    \begin{itemize}",eigenvalues and eigenvectors
"if $k$ is a skew-symmetric matrix, show that $q = (i - k)(i + k)^{-1}$ is an orthogonal matrix. find $q$ if $k = \begin{bmatrix} 0 & 2 \\ -2 & 0 \end{bmatrix}$.",eigenvalues and eigenvectors
"if $k^h = -k$ (skew-hermitian), the eigenvalues are imaginary and the eigenvectors are orthogonal.
    \begin{itemize}",eigenvalues and eigenvectors
"if $m$ is the diagonal matrix with entries $d, d^2, d^3$, what is $m^{-1}am$? what are its eigenvalues in the following case?
    \[
    a = \begin{bmatrix}
    1 & 1 & 1 \\
    1 & 1 & 1 \\
    1 & 1 & 1
    \end{bmatrix}
    \]",eigenvalues and eigenvectors
"if $a^2 = -i$, what are the eigenvalues of $a$? if $a$ is a real $n \times n$ matrix, show that $n$ must be even, and give an example.",eigenvalues and eigenvectors
"if $ax = \lambda_1x$ and $a^t y = \lambda_2y$ (all real), show that $x^t y = 0$.",eigenvalues and eigenvectors
"a variation on the fourier matrix is the ""sine matrix"":
    \[
    s = \frac{1}{\sqrt{2}} \begin{bmatrix}
    \sin \theta & \sin 2\theta & \sin 3\theta \\
    \sin 2\theta & \sin 4\theta & \sin 6\theta \\
    \sin 3\theta & \sin 6\theta & \sin 9\theta
    \end{bmatrix}
    \]
    with $\theta = \frac{\pi}{4}$. verify that $s^t = s^{-1}$. (the columns are the eigenvectors of the tridiagonal matrix $-1, 2, -1$.)",eigenvalues and eigenvectors
"suppose the first row of $a$ is 7, 6, and its eigenvalues are $i, -i$. find $a$.",eigenvalues and eigenvectors
"if the vectors $x_1$ and $x_2$ are in the columns of $s$, what are the eigenvalues and eigenvectors of 
    \[
    a = s \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix} s^{-1}
    \quad \text{and} \quad b = s \begin{bmatrix} 2 & 3 \\ 0 & 1 \end{bmatrix} s^{-1}?
    \]",eigenvalues and eigenvectors
"what is the limit as $k \to \infty$ (the markov steady state) of
    \[
    \begin{bmatrix}
    0.4 & 0.3 \\
    0.6 & 0.7
    \end{bmatrix}^k
    \begin{bmatrix}
    a \\
    b
    \end{bmatrix}
    ?
    \]",eigenvalues and eigenvectors
"let \( a = \begin{bmatrix} 4 & 1 \\ 2 & 3 \end{bmatrix} \) and \( b = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \). determine if \( a \) and \( b \) are similar matrices. if they are, find the matrix \( p \) such that \( p^{-1}ap = b \).",eigenvalues and eigenvectors
"given the matrix \( a = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \), compute the eigenvalues and eigenvectors of \( a \), and verify if the matrix is diagonalizable.",eigenvalues and eigenvectors
let \( a = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \) and \( b = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} \). show that the two matrices are similar and compute the eigenvalues and eigenvectors of both matrices.,eigenvalues and eigenvectors
"for the matrix \( a = \begin{bmatrix} 5 & 4 \\ 1 & 3 \end{bmatrix} \), find its eigenvalues and eigenvectors, and then find a similarity transformation that diagonalizes \( a \).",eigenvalues and eigenvectors
"prove that two similar matrices have the same eigenvalues. provide an example using \( a = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix} \) and \( b = \begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix} \), and compute their eigenvectors.",eigenvalues and eigenvectors
"given the matrix \( a = \begin{bmatrix} 3 & 4 \\ 4 & 3 \end{bmatrix} \), find its eigenvalues and eigenvectors. then, determine if \( a \) is diagonalizable and find the matrix that diagonalizes it.",eigenvalues and eigenvectors
consider the matrix \( a = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix} \). find its eigenvalues and eigenvectors and show that \( a \) is not diagonalizable.,eigenvalues and eigenvectors
"let \( a = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix} \) and \( p = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \). verify that \( a \) and \( p^{-1}ap \) are similar matrices. then, compute the eigenvalues and eigenvectors of \( p^{-1}ap \).",eigenvalues and eigenvectors
"let \( a = \begin{bmatrix} 5 & 4 & 2 \\ 0 & 3 & 1 \\ 0 & 0 & 2 \end{bmatrix} \). compute the jordan canonical form of \( a \) and find the corresponding jordan chains. determine the eigenvalues, eigenvectors, and generalized eigenvectors of \( a \), and verify the completeness of the jordan basis.",eigenvalues and eigenvectors
"consider the matrix \( a = \begin{bmatrix} 2 & 1 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{bmatrix} \). find the eigenvalues and eigenvectors of \( a \), and then use the algebraic and geometric multiplicities to determine whether \( a \) is diagonalizable. if it is not diagonalizable, compute its jordan canonical form and the jordan chains associated with each eigenvalue.",eigenvalues and eigenvectors
"let \( a = \begin{bmatrix} 4 & 1 & 0 & 0 \\ 0 & 4 & 1 & 0 \\ 0 & 0 & 4 & 1 \\ 0 & 0 & 0 & 4 \end{bmatrix} \). find the eigenvalues of \( a \), and then compute the jordan canonical form of \( a \). determine whether \( a \) is diagonalizable, and if not, find the jordan basis and describe the structure of the jordan blocks.",eigenvalues and eigenvectors
"consider the matrix \( a = \begin{bmatrix} 1 & 1 & 0 & 0 \\ 0 & 1 & 1 & 0 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix} \). find the jordan canonical form of \( a \), and calculate the eigenvalues and eigenvectors of \( a \). show that \( a \) is not diagonalizable and compute the generalized eigenvectors that complete the jordan chains.",eigenvalues and eigenvectors
"let \( a = \begin{bmatrix} 1 & 2 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix} \). find the eigenvalues, eigenvectors, and generalized eigenvectors of \( a \), and compute its jordan canonical form. discuss the conditions under which a matrix has a unique jordan form and the role of the jordan chains in the diagonalization process. analyze how the structure of the jordan blocks affects the transformation properties of \( a \).",eigenvalues and eigenvectors
"consider the transition matrix \( p = \begin{bmatrix} 0.3 & 0.7 \\ 0.6 & 0.4 \end{bmatrix} \). find the steady-state vector for this markov chain by solving for the eigenvector corresponding to the eigenvalue \( \lambda = 1 \). verify that the steady-state vector is a probability distribution, and explain the significance of the steady-state distribution in the long-term behavior of the chain.",eigenvalues and eigenvectors
"let \( p = \begin{bmatrix} 0.5 & 0.5 & 0 \\ 0.2 & 0.3 & 0.5 \\ 0.3 & 0.2 & 0.5 \end{bmatrix} \) be a transition matrix of a markov chain. determine the steady-state vector \( \mathbf{v} \) that satisfies \( p \mathbf{v} = \mathbf{v} \). solve for the eigenvalue and eigenvector associated with the steady-state, and interpret the meaning of the result in the context of the long-term behavior of the chain.",eigenvalues and eigenvectors
"consider a markov chain with the transition matrix \( p = \begin{bmatrix} 0.4 & 0.6 \\ 0.5 & 0.5 \end{bmatrix} \). compute the eigenvalue and eigenvector associated with the steady-state solution of the chain. show that the steady-state distribution is unique and explain how it relates to the long-term behavior of the chain. additionally, compute the limit of the powers of \( p \) as \( n \to \infty \) and discuss the convergence to the steady state.",eigenvalues and eigenvectors
"let \( p = \begin{bmatrix} 0.8 & 0.2 \\ 0.1 & 0.9 \end{bmatrix} \) represent the transition matrix of a markov chain. compute the steady-state vector \( \mathbf{v} \) that satisfies \( p \mathbf{v} = \mathbf{v} \), and determine the eigenvalue and eigenvector corresponding to this steady state. analyze how the steady-state distribution evolves over multiple iterations and provide an explanation of the convergence process to the steady state.",eigenvalues and eigenvectors
"consider the transition matrix \( p = \begin{bmatrix} 0.7 & 0.3 & 0 \\ 0.2 & 0.6 & 0.2 \\ 0 & 0.5 & 0.5 \end{bmatrix} \). find the steady-state vector by solving \( p \mathbf{v} = \mathbf{v} \). then, determine the eigenvalue associated with the steady-state and explain its significance. discuss the convergence properties of the markov chain and the long-term behavior of the system, including any transient states if applicable.",eigenvalues and eigenvectors
"let \( a = \begin{bmatrix} 4 & 1 & 2 \\ 1 & 5 & 3 \\ 2 & 3 & 6 \end{bmatrix} \) be a symmetric matrix. compute the eigenvalues and eigenvectors of \( a \), and diagonalize \( a \) by finding an orthogonal matrix \( q \) such that \( q^t a q = d \), where \( d \) is the diagonal matrix of eigenvalues. show that the matrix \( a \) is diagonalizable and explain the significance of the orthogonal transformation.",eigenvalues and eigenvectors
"consider the hermitian matrix \( a = \begin{bmatrix} 3 & 2 - i \\ 2 + i & 4 \end{bmatrix} \). find the eigenvalues and eigenvectors of \( a \), and diagonalize \( a \) by finding a unitary matrix \( u \) such that \( u^\dagger a u = d \), where \( d \) is a diagonal matrix of eigenvalues. discuss the properties of hermitian matrices and the implications for the eigenvalues and eigenvectors.",eigenvalues and eigenvectors
"let \( a = \begin{bmatrix} 5 & 1 & 2 \\ 1 & 3 & 1 \\ 2 & 1 & 4 \end{bmatrix} \) be a symmetric matrix. find the eigenvalues and eigenvectors of \( a \), and diagonalize \( a \) using the spectral theorem. verify that the eigenvectors are orthogonal and explain why this orthogonality is a crucial property for diagonalization of symmetric matrices.",eigenvalues and eigenvectors
"consider the hermitian matrix \( a = \begin{bmatrix} 7 & 3 + 2i \\ 3 - 2i & 5 \end{bmatrix} \). find the eigenvalues and eigenvectors of \( a \), and diagonalize \( a \) by finding a unitary matrix \( u \) such that \( u^\dagger a u = d \), where \( d \) is the diagonal matrix of eigenvalues. discuss how the eigenvalues of a hermitian matrix are always real and how this property affects the diagonalization process.",eigenvalues and eigenvectors
"let \( a = \begin{bmatrix} 6 & 2 & 1 \\ 2 & 5 & 4 \\ 1 & 4 & 3 \end{bmatrix} \) be a symmetric matrix. find the eigenvalues and eigenvectors of \( a \), and diagonalize \( a \) using the gram-schmidt process to orthonormalize the eigenvectors. verify that the matrix \( a \) is diagonalizable and discuss how the diagonalization relates to the spectral theorem for symmetric matrices.",eigenvalues and eigenvectors
"let \( a = \begin{bmatrix} 2 + i & 3 - 2i \\ 1 + i & 4 - i \end{bmatrix} \) be a complex matrix. compute the eigenvalues and eigenvectors of \( a \), and determine whether \( a \) is diagonalizable. if it is diagonalizable, find a matrix \( p \) such that \( p^{-1} a p \) is diagonal. discuss the geometric interpretation of the eigenvectors in the complex plane.",eigenvalues and eigenvectors
"consider the complex matrix \( a = \begin{bmatrix} 1 & -i & 0 \\ i & 2 & -i \\ 0 & i & 3 \end{bmatrix} \). find the characteristic polynomial of \( a \) and determine its eigenvalues. compute the corresponding eigenvectors and check whether they form a linearly independent set. if the matrix is diagonalizable, explicitly construct the diagonalization.",eigenvalues and eigenvectors
"let \( a = \begin{bmatrix} 2 + i & 1 - i & 0 \\ 3 + 2i & 4 - i & 1 + i \\ 0 & 2 - i & 5 + i \end{bmatrix} \). compute the eigenvalues and eigenvectors of \( a \), and determine whether the eigenvectors form an orthonormal basis with respect to the standard inner product in \( \mathbb{c}^3 \). if not, apply the gram-schmidt process to obtain an orthonormal basis.",eigenvalues and eigenvectors
"given the complex matrix \( a = \begin{bmatrix} 0 & i & -1 \\ -i & 0 & i \\ 1 & -i & 0 \end{bmatrix} \), find its eigenvalues and eigenvectors. show that \( a \) is a normal matrix by verifying \( a a^\dagger = a^\dagger a \). using the spectral theorem, find a unitary matrix \( u \) such that \( u^\dagger a u \) is diagonal. discuss the significance of normal matrices in the context of diagonalization.",eigenvalues and eigenvectors
"consider the complex matrix \( a = \begin{bmatrix} 1 + i & 2 \\ -i & 3 - i \end{bmatrix} \). compute its eigenvalues and eigenvectors, and determine whether \( a \) is diagonalizable. if \( a \) is not diagonalizable, compute its jordan form. additionally, explain how the jordan form helps in understanding the structure of non-diagonalizable complex matrices and their generalized eigenvectors.",eigenvalues and eigenvectors
"sketch the feasible set with constraints \( x + 2y \geq 6 \), \( 2x + y \geq 6 \), \( x \geq 0 \), \( y \geq 0 \). what points lie at the three ``corners'' of this set?",linear programming and game theory
"on the preceding feasible set, what is the minimum value of the cost function \( x + y \)? draw the line \( x + y = \text{constant} \) that first touches the feasible set. what points minimize the cost functions \( 3x + y \) and \( x - y \)?",linear programming and game theory
"show that the feasible set constrained by \( 2x + 5y \leq 3 \), \( -3x + 8y \leq -5 \), \( x \geq 0 \), \( y \geq 0 \), is empty.",linear programming and game theory
"show that the following problem is feasible but unbounded, so it has no optimal solution: maximize \( x + y \), subject to \( x \geq 0 \), \( y \geq 0 \), \( -3x + 2y \leq -1 \), \( x - y \leq 2 \).",linear programming and game theory
"add a single inequality constraint to \( x \geq 0 \), \( y \geq 0 \) such that the feasible set contains only one point.",linear programming and game theory
"what shape is the feasible set \( x \geq 0 \), \( y \geq 0 \), \( z \geq 0 \), \( x + y + z = 1 \), and what is the maximum of \( x + 2y + 3z \)?",linear programming and game theory
solve the portfolio problem at the end of the preceding section.,linear programming and game theory
"in the feasible set for the general motors problem, the nonnegativity \( x, y, z \geq 0 \) leaves an eighth of three-dimensional space (the positive octant). how is this cut by the two planes from the constraints, and what shape is the feasible set? how do its corners show that, with only these two constraints, there will be only two kinds of cars in the optimal solution?",linear programming and game theory
"(transportation problem) suppose texas, california, and alaska each produce a million barrels of oil; 800,000 barrels are needed in chicago at a distance of 1000, 2000, and 3000 miles from the three producers, respectively; and 2,200,000 barrels are needed in new england 1500, 3000, and 3700 miles away. if shipments cost one unit for each barrel-mile, what linear program with five equality constraints must be solved to minimize the shipping cost?",linear programming and game theory
"minimize $x_1 + x_2 - x_3$, subject to:
    \begin{align*}
        2x_1 - 4x_2 + x_3 + x_4 &= 4, \\
        3x_1 + 5x_2 + x_3 + x_5 &= 2.
    \end{align*}
    which of $x_1$, $x_2$, $x_3$ should enter the basis, and which of $x_4$, $x_5$ should leave? compute the new pair of basic variables, and find the cost at the new corner.",linear programming and game theory
"after the preceding simplex step, prepare for and decide on the next step.",linear programming and game theory
"in example 3, suppose the cost is $3x + y$. with rearrangement, the cost vector is $c = (0,1,3,0)$. show that $r \geq 0$ and, therefore, that corner $p$ is optimal.",linear programming and game theory
"suppose the cost function in example 3 is $x - y$, so that after rearrangement $c = (0,-1,1,0)$ at the corner $p$. compute $r$ and decide which column $u$ should enter the basis. then compute $b^{-1}u$ and show from its sign that you will never meet another corner. we are climbing the $y$-axis in figure 8.3, and $x - y$ goes to $-\infty$.",linear programming and game theory
"again in example 3, change the cost to $x + 3y$. verify that the simplex method takes you from $p$ to $q$ to $r$, and that the corner $r$ is optimal.",linear programming and game theory
"phase i finds a basic feasible solution to $ax = b$ (a corner). after changing signs to make $b \geq 0$, consider the auxiliary problem of minimizing $w_1 + w_2 + \cdots + w_m$, subject to $x \geq 0$, $w \geq 0$, $ax + w = b$. whenever $ax = b$ has a nonnegative solution, the minimum cost in this problem will be zero—with $w^* = 0$.
    \begin{itemize}",linear programming and game theory
"if we wanted to maximize instead of minimize the cost (with $ax = b$ and $x \geq 0$), what would be the stopping test on $r$, and what rules would choose the column of $n$ to make basic and the column of $b$ to make free?",linear programming and game theory
"minimize $2x_1 + x_2$, subject to $x_1 + x_2 \geq 4$, $x_1 + 3x_2 \geq 12$, $x_1 - x_2 \geq 0$, $x \geq 0$.",linear programming and game theory
"verify the inverse in equation (5), and show that $be$ has $bv = u$ in its $k$th column. then $be$ is the correct basis matrix for the next step, $e^{-1}b^{-1}$ is its inverse, and $e^{-1}$ updates the basis matrix correctly.",linear programming and game theory
"suppose we want to minimize $cx = x_1 - x_2$, subject to:
    \begin{align*}
        2x_1 - 4x_2 + x_3 &= 6, \\
        3x_1 + 6x_2 + x_4 &= 12
    \end{align*}
    (all $x_1, x_2, x_3, x_4 \geq 0$). starting from $x = (0,0,6,12)$, should $x_1$ or $x_2$ be increased from its current value of zero? how far can it be increased until the equations force $x_3$ or $x_4$ down to zero? at that point, what is the new $x$?",linear programming and game theory
"for the matrix $p = i - a^t (a a^t)^{-1} a$, show that if $x$ is in the nullspace of $a$, then $px = x$. the nullspace stays unchanged under this projection.",linear programming and game theory
"(a) minimize the cost $c^t x = 5x_1 + 4x_2 + 8x_3$ on the plane $x_1 + x_2 + x_3 = 3$, by testing the vertices $p$, $q$, $r$, where the triangle is cut off by the requirement $x \geq 0$.",linear programming and game theory
"what is the dual of the following problem: minimize \( x_1 + x_2 \), subject to \( x_1 \geq 0 \),
    \( x_2 \geq 0 \), \( 2x_1 \geq 4 \), \( x_1 + 3x_2 \geq 11 \)? find the solution to both this problem and its dual,
    and verify that minimum equals maximum.",linear programming and game theory
"what is the dual of the following problem: maximize \( y_2 \) subject to \( y_1 \geq 0 \), \( y_2 \geq 0 \),
    \( y_1 + y_2 \leq 3 \)? solve both this problem and its dual.",linear programming and game theory
"suppose \( a \) is the identity matrix (so that \( m = n \)), and the vectors \( b \) and \( c \) are nonnegative. explain why \( x^* = b \) is optimal in the minimum problem, find \( y^* \) in the maximum
    problem, and verify that the two values are the same. if the first component of \( b \) is
    negative, what are \( x^* \) and \( y^* \)?",linear programming and game theory
"construct a 1 by 1 example in which \( ax \geq b \), \( x \geq 0 \) is unfeasible, and the dual problem
    is unbounded.",linear programming and game theory
"starting with the 2 by 2 matrix \( a =
    \begin{pmatrix}
    1 & 0 \\
    0 & -1
    \end{pmatrix} \), choose \( b \) and \( c \) so that both of the feasible
    sets \( ax \geq b \), \( x \geq 0 \) and \( ya \leq c \), \( y \geq 0 \) are empty.",linear programming and game theory
"if all entries of \( a \), \( b \), and \( c \) are positive, show that both the primal and the dual are
    feasible.",linear programming and game theory
"show that \( x = (1,1,1,0) \) and \( y = (1,1,0,1) \) are feasible in the primal and dual, with
    \[
    a =
    \begin{pmatrix}
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 1 & 1 & 1 \\
    1 & 0 & 0 & 1
    \end{pmatrix}
    , \quad
    b =
    \begin{pmatrix}
    1 \\
    1 \\
    1 \\
    1
    \end{pmatrix}
    , \quad
    c =
    \begin{pmatrix}
    1 \\
    1 \\
    1 \\
    3
    \end{pmatrix}
    .
    \]
    then, after computing \( cx \) and \( yb \), explain how you know they are optimal.",linear programming and game theory
"what is the dual of the following problem: minimize \( x_1 + x_2 \), subject to \( x_1 \geq 0 \), \( x_2 \geq 0 \), \( 2x_1 \geq 4 \), \( x_1 + 3x_2 \geq 11 \)? find the solution to both this problem and its dual, and verify that minimum equals maximum.",linear programming and game theory
"what is the dual of the following problem: maximize \( y_2 \) subject to \( y_1 \geq 0 \), \( y_2 \geq 0 \), \( y_1 + y_2 \leq 3 \)? solve both this problem and its dual.",linear programming and game theory
"suppose \( a \) is the identity matrix (so that \( m = n \)), and the vectors \( b \) and \( c \) are nonnegative. explain why \( x^* = b \) is optimal in the minimum problem, find \( y^* \) in the maximum problem, and verify that the two values are the same. if the first component of \( b \) is negative, what are \( x^* \) and \( y^* \)?",linear programming and game theory
"construct a 1 by 1 example in which \( ax \geq b \), \( x \geq 0 \) is unfeasible, and the dual problem is unbounded.",linear programming and game theory
"starting with the 2 by 2 matrix \( a = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \), choose \( b \) and \( c \) so that both of the feasible sets \( ax \geq b \), \( x \geq 0 \) and \( ya \leq c \), \( y \geq 0 \) are empty.",linear programming and game theory
"if all entries of \( a \), \( b \), and \( c \) are positive, show that both the primal and the dual are feasible.",linear programming and game theory
"show that \( x = (1,1,1,0) \) and \( y = (1,1,0,1) \) are feasible in the primal and dual, with
    \[
    a = \begin{bmatrix}
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 1 & 1 & 1 \\
    1 & 0 & 0 & 1
    \end{bmatrix}, \quad
    b = \begin{bmatrix}
    1 \\
    1 \\
    1 \\
    1
    \end{bmatrix}, \quad
    c = \begin{bmatrix}
    1 \\
    1 \\
    1 \\
    3
    \end{bmatrix}.
    \]
    then, after computing \( cx \) and \( yb \), explain how you know they are optimal.",linear programming and game theory
"verify that the vectors in the previous exercise satisfy the complementary slackness conditions, and find the one slack inequality in both the primal and the dual.",linear programming and game theory
"suppose that \( a = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \), \( b = \begin{bmatrix} 1 \\ -1 \end{bmatrix} \), and \( c = \begin{bmatrix} 1 \\ 1 \end{bmatrix} \). find the optimal \( x \) and \( y \), and verify the complementary slackness conditions (as well as \( yb = cx \)).",linear programming and game theory
if the primal problem is constrained by equations instead of inequalities—minimize \( cx \) subject to \( ax = b \) and \( x \geq 0 \)—then the requirement \( y \geq 0 \) is left out of the dual: maximize \( yb \) subject to \( ya \leq c \). show that the one-sided inequality \( yb \leq cx \) still holds. why was \( y \geq 0 \) needed in equation (1) but not here? this weak duality can be completed to full duality.,linear programming and game theory
"without the simplex method, minimize the cost \( 5x_1 + 3x_2 + 4x_3 \), subject to \( x_1 + x_2 + x_3 \geq 1 \), \( x_1 \geq 0 \), \( x_2 \geq 0 \), \( x_3 \geq 0 \).
    \begin{enumerate}",linear programming and game theory
what is the shape of the feasible set?,linear programming and game theory
"what is the dual problem, and what is its solution \( y \)?",linear programming and game theory
"if the primal has a unique optimal solution \( x^* \), and then \( c \) is changed a little, explain why \( x^* \) still remains the optimal solution.",linear programming and game theory
"write the dual of the following problem: maximize \( x_1 + x_2 + x_3 \) subject to \( 2x_1 + x_2 \leq 4 \), \( x_3 \leq 6 \). what are the optimal \( x^* \) and \( y^* \) (if they exist!)?",linear programming and game theory
"if \( a = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \), describe the cone of nonnegative combinations of the columns. if \( b \) lies inside that cone, say \( b = (3,2) \), what is the feasible vector \( x \)? if \( b \) lies outside, say \( b = (0,1) \), what vector \( y \) will satisfy the alternative?",linear programming and game theory
"in three dimensions, can you find a set of six vectors whose cone of nonnegative combinations fills the whole space? what about four vectors?",linear programming and game theory
"use the concept of alternatives to show that the following equation has no solution:
    \[
    \begin{bmatrix}
    2 & 2 \\
    4 & 4
    \end{bmatrix}
    x =
    \begin{bmatrix}
    1 \\
    1
    \end{bmatrix}.
    \]",linear programming and game theory
"use the concept of alternatives to show that there is no solution \( x \geq 0 \) for the system:
    \[
    \begin{bmatrix}
    1 & 3 & -5 \\
    1 & -4 & -7
    \end{bmatrix}
    x =
    \begin{bmatrix}
    2 \\
    3
    \end{bmatrix}.
    \]",linear programming and game theory
"if you could increase the capacity of any one pipe in the network above, which change would produce the largest increase in the maximal flow?",linear programming and game theory
draw a 5-node network with capacity $|i - j|$ between node $i$ and node $j$. find the largest possible flow from node 1 to node 4.,linear programming and game theory
"in a graph, the maximum number of paths from $s$ to $t$ with no common edges equals the minimum number of edges whose removal disconnects $s$ from $t$. relate this to the max flow-min cut theorem.",linear programming and game theory
"find a maximal set of marriages (a complete matching, if possible) for the following matrices:
    \[
    a = \begin{bmatrix}
    0 & 0 & 1 & 0 & 0 \\
    1 & 1 & 0 & 1 & 1 \\
    0 & 1 & 1 & 0 & 1 \\
    0 & 0 & 1 & 1 & 0 \\
    0 & 0 & 0 & 1 & 0
    \end{bmatrix}
    \]
    \[
    b = \begin{bmatrix}
    1 & 1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 1 & 0 \\
    0 & 0 & 1 & 0 & 1 \\
    1 & 1 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 & 0
    \end{bmatrix}
    \]
    sketch the network for $b$, with heavier lines on the edges in your matching.",linear programming and game theory
"for the matrix $a$ in problem 4, which rows violate hall’s condition—by having all their 1s in too few columns? which $p \times q$ submatrix of zeros has $p+q > n$?",linear programming and game theory
"how many lines (horizontal and vertical) are needed to cover all the 1s in $a$ in problem 4? for any matrix, explain why weak duality is true: if $k$ marriages are possible, then it takes at least $k$ lines to cover all the 1s.",linear programming and game theory
suppose every row and every column contains exactly two 1s. prove that a complete matching is possible. (show that the 1s cannot be covered by less than $n$ lines.),linear programming and game theory
"find an example with two or more 1s in each row and column, for which a complete matching is impossible.",linear programming and game theory
"if a $7 \times 7$ matrix has 15 1s, prove that it allows at least 3 marriages.",linear programming and game theory
"for infinite sets, a complete matching may be impossible even if hall’s condition is passed. if the first row is all 1s and then every $a_{i,i-1} = 1$, show that any $p$ rows have 1s in at least $p$ columns—and yet there is no complete matching.",linear programming and game theory
"if you could increase the capacity of any one pipe in a network, which change would produce the largest increase in the maximal flow?",linear programming and game theory
"find a maximal set of marriages (a complete matching, if possible) for the matrix $a$:
    \[
    a = \begin{pmatrix}
    0 & 0 & 1 & 0 & 0 \\
    1 & 1 & 0 & 1 & 1 \\
    0 & 1 & 1 & 0 & 1 \\
    0 & 0 & 1 & 1 & 0 \\
    0 & 0 & 0 & 1 & 0
    \end{pmatrix}
    \]
    and for the matrix $b$:
    \[
    b = \begin{pmatrix}
    1 & 1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 1 & 0 \\
    0 & 0 & 1 & 0 & 1 \\
    1 & 1 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 & 0
    \end{pmatrix}
    \]
    sketch the network for $b$, with heavier lines on the edges in your matching.",linear programming and game theory
"for the matrix $a$ in problem 4, which rows violate hall’s condition by having all their 1s in too few columns? which $p \times q$ submatrix of zeros has $p + q > n$?",linear programming and game theory
\begin{enumerate},linear programming and game theory
"if a $7 \times 7$ matrix has 15 ones, prove that it allows at least 3 marriages.",linear programming and game theory
"for infinite sets, a complete matching may be impossible even if hall’s condition is passed. if the first row is all 1s and then every $a_{i, i-1} = 1$, show that any $p$ rows have 1s in at least $p$ columns—and yet there is no complete matching.",linear programming and game theory
"if figure 8.5 shows lengths instead of capacities, find the shortest path from $s$ to $t$, and a minimal spanning tree.",linear programming and game theory
apply algorithms 1 and 2 to find a shortest spanning tree for the network of problem 2.,linear programming and game theory
why does the greedy algorithm work for the spanning tree problem?,linear programming and game theory
"show by example that the greedy algorithm could fail to find the shortest path from $s$ to $t$, by starting with the shortest edge.",linear programming and game theory
"if $a$ is the $5 \times 5$ matrix with 1s just above and just below the main diagonal, find:
    \begin{enumerate}",linear programming and game theory
a set of rows with 1s in too few columns.,linear programming and game theory
a set of columns with 1s in too few rows.,linear programming and game theory
a $p \times q$ submatrix of zeros with $p + q > 5$.,linear programming and game theory
four lines that cover all the 1s.,linear programming and game theory
the maximal flow problem has slack variables $w_{ij} = c_{ij} - x_{ij}$ for the difference between capacities and flows. state the problem of figure 8.5 as a linear program.,linear programming and game theory
how will the optimal strategies in the game that opens this section be affected if the \$20 is increased to \$70? what is the value (the average win for x) of this new game?,linear programming and game theory
"with payoff matrix $a = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$, explain the calculation by x of the maximin and by y of the minimax. what strategies $x^*$ and $y^*$ are optimal?",linear programming and game theory
"if $a_{ij}$ is the largest entry in its row and the smallest in its column, why will x always choose column $j$ and y always choose row $i$ (regardless of the rest of the matrix)? show that the preceding problem had such an entry, and then construct an $a$ without one.",linear programming and game theory
"compute y's best strategy by weighting the rows of $a = \begin{pmatrix} 3 & 4 & 1 \\ 2 & 0 & 3 \end{pmatrix}$ with $y$ and $1 - y$. x will concentrate on the largest of the components $3y + 2(1 - y)$, $4y$, and $y + 3(1 - y)$. find the largest of those three (depending on $y$) and then find the $y^*$ between 0 and 1 that makes this largest component as small as possible.",linear programming and game theory
"with the same $a$ as in problem 4, find the best strategy for x. show that x uses only the two columns (the first and third) that meet at the minimax point in the graph.",linear programming and game theory
"find both optimal strategies, and the value, if
    \[
    a = \begin{pmatrix}
    1 & 0 & -1 \\
    -2 & -1 & 2
    \end{pmatrix}.
    \]",linear programming and game theory
"suppose $a = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$. what weights $x_1$ and $1 - x_1$ will give a column of the form $\begin{pmatrix} u \\ u \end{pmatrix}$, and what weights $y_1$ and $1 - y_1$ on the two rows will give a new row $[v \; v]$? \\ show that  v=u",linear programming and game theory
"with payoff matrix \( a = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix} \), explain the calculation by x of the maximin and by y of the minimax. what strategies \( x^* \) and \( y^* \) are optimal?",linear programming and game theory
"if \( a_{ij} \) is the largest entry in its row and the smallest in its column, why will x always choose column j and y always choose row i (regardless of the rest of the matrix)? show that the preceding problem had such an entry, and then construct an \( a \) without one.",linear programming and game theory
"compute y’s best strategy by weighting the rows of \( a = \begin{pmatrix} 3 & 4 & 1 \\ 2 & 0 & 3 \end{pmatrix} \) with \( y \) and \( 1 - y \). x will concentrate on the largest of the components \( 3y + 2(1 - y) \), \( 4y \), and \( y + 3(1 - y) \). find the largest of those three (depending on \( y \)) and then find the \( y^* \) between 0 and 1 that makes this largest component as small as possible.",linear programming and game theory
"with the same \( a \) as in problem 4, find the best strategy for x. show that x uses only the two columns (the first and third) that meet at the minimax point in the graph.",linear programming and game theory
"find both optimal strategies, and the value, if
    \[
    a = \begin{pmatrix} 1 & 0 & -1 \\ -2 & -1 & 2 \end{pmatrix}.
    \]",linear programming and game theory
"suppose \( a = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \). what weights \( x_1 \) and \( 1 - x_1 \) will give a column of the form \( \begin{pmatrix} u \\ u \end{pmatrix} \), and what weights \( y_1 \) and \( 1 - y_1 \) on the two rows will give a new row \( \begin{pmatrix} v & v \end{pmatrix} \)?",linear programming and game theory
"find \( x^* \), \( y^* \), and the value \( v \) for
    \[
    a = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{pmatrix}.
    \]",linear programming and game theory
"compute
    \[
    \min_{y_i \geq 0, y_1 + y_2 = 1} \max_{x_1 \geq 0, x_1 + x_2 = 1} (x_1 y_1 + x_2 y_2).
    \]",linear programming and game theory
"explain each of the inequalities in equation (5). then, once the minimax theorem has turned them into equalities, derive (again in words) the saddle point equations (4).",linear programming and game theory
"show that \( x^* = \left( \frac{1}{2}, \frac{1}{2}, 0, 0 \right) \) and \( y^* = \left( \frac{1}{2}, \frac{1}{2} \right) \) are optimal strategies in our simplified version of poker, by computing \( y a x^* \) and \( y^* a x \) and verifying the conditions (4) for a saddle point.",linear programming and game theory
"has it been proved that no chess strategy always wins for black? this is certainly true when the players are given two moves at a time; if black had a winning strategy, white could move a knight out and back and then follow that strategy, leading to the impossible conclusion that both would win.",linear programming and game theory
"if x chooses a prime number and simultaneously y guesses whether it is odd or even (with gain or loss of \$1), who has the advantage?",linear programming and game theory
"if x is a quarterback, with the choice of run or pass, and y can defend against a run or a pass, suppose the payoff (in yards) is
    \[
    a = \begin{pmatrix} 2 & 8 \\ 6 & -6 \end{pmatrix}
    \]
    \[
    \text{defense against run} \quad \text{defense against pass}
    \]
    \[
    \text{run} \quad \text{pass}
    \]
    what are the optimal strategies and the average gain on each play?",linear programming and game theory
define a linear inequality. provide at least two examples of linear inequalities and discuss the significance of each example in linear programming.,linear programming and game theory
how do linear inequalities serve as constraints in linear programming? illustrate with an example where linear inequalities define a feasible region and explain how these inequalities limit the solution set.,linear programming and game theory
"discuss the concept of a feasible region in the context of linear inequalities. what geometric shape does this region take in two-dimensional space, and what are the implications for finding the optimal solution?",linear programming and game theory
"given the system of inequalities, \(x + 2y \leq 5\), \(3x - y \geq 4\), and \(x \geq 0\), graphically represent the feasible region and discuss how the feasible solutions are constrained by these inequalities.",linear programming and game theory
explain the significance of slack and surplus variables in linear inequalities. how can they be used to convert inequality constraints into equality constraints for solving optimization problems?,linear programming and game theory
prove that the solution set of a system of linear inequalities is convex. show this by considering a system with two variables and proving that any convex combination of two feasible points remains within the feasible region.,linear programming and game theory
derive the conditions for a point to be inside the feasible region of a system of linear inequalities. what steps would you follow to check if a given point satisfies the system of inequalities?,linear programming and game theory
prove the fundamental theorem of linear programming that the optimal solution of a linear programming problem occurs at a vertex of the feasible region. provide a proof using geometric arguments and linear inequalities.,linear programming and game theory
"given a system of two linear inequalities \(x + y \leq 5\) and \(2x - y \geq 3\), solve the system graphically and identify the feasible region. show all steps of the solution.",linear programming and game theory
"using the graphical method, solve a system of linear inequalities and find the optimal solution. specifically, consider the system \(x + 2y \leq 6\), \(3x + y \geq 3\), and \(x \geq 0\), and determine the optimal values of \(x\) and \(y\) that maximize the objective function \(z = 2x + 3y\).",linear programming and game theory
"discuss the steps involved in converting a linear inequality into an equation using slack variables. provide a detailed example to illustrate this process, including the conversion of inequalities into equalities for optimization problems.",linear programming and game theory
"given a word problem where a factory produces two products, a and b, subject to resource constraints such as machine hours and labor hours, formulate a system of linear inequalities that model the problem. determine the feasible production levels for each product.",linear programming and game theory
"consider a linear programming problem where you are asked to allocate resources between two projects, subject to budget constraints. formulate the problem as a system of linear inequalities and discuss the feasibility of the solution space.",linear programming and game theory
"prove that for any system of linear inequalities, the set of feasible solutions is a convex set. include a discussion of the properties of convex sets and provide a formal proof using linear combinations of feasible points.",linear programming and game theory
"given the system of inequalities \(2x + 3y \leq 10\), \(x - y \geq 1\), and \(y \geq 2\), find the feasible region and determine if the point \( (3, 4) \) satisfies all the inequalities.",linear programming and game theory
explain the role of linear inequalities in multi-objective optimization problems. how do these inequalities help in identifying a solution that balances multiple objectives? provide an example.,linear programming and game theory
derive the graphical method for solving two-variable linear inequalities. use an example to show how the intersection of half-planes can help identify the feasible region.,linear programming and game theory
"in the context of a diet problem, formulate a set of linear inequalities to represent nutritional constraints such as calorie, protein, and fat intake. solve the inequalities graphically to determine the feasible diet plan.",linear programming and game theory
discuss the interpretation of the optimal solution in linear programming. how does the feasible region relate to the optimal values of decision variables? provide a graphical example to support your explanation.,linear programming and game theory
show how to derive the feasible solution from a system of linear inequalities using the simplex method. provide a detailed example that involves multiple inequalities and explain each step of the algorithm.,linear programming and game theory
"how do we use linear inequalities to model economic problems, such as profit maximization and cost minimization? provide an example where linear inequalities are used to represent supply and demand constraints.",linear programming and game theory
discuss the concept of the dual problem in linear programming. how can the dual of a system of linear inequalities help us understand the relationship between constraints and objective functions?,linear programming and game theory
"solve a system of linear inequalities graphically, and use it to determine the optimal solution for a linear programming problem with an objective function of the form \(z = 4x + 5y\).",linear programming and game theory
"given a real-world scenario involving resource allocation in a manufacturing plant, formulate a system of linear inequalities to represent machine time, labor hours, and production constraints. solve the system to find the feasible production plan.",linear programming and game theory
show how linear inequalities can be used in network flow problems. formulate a set of inequalities that describe the constraints of a network and solve for the optimal flow values.,linear programming and game theory
"in a transportation problem, constraints are often expressed as linear inequalities. formulate a system of inequalities for a transportation problem where the objective is to minimize shipping costs, given constraints on supply and demand.",linear programming and game theory
"derive the relationship between linear inequalities and convex polyhedra. prove that the feasible region of a system of linear inequalities is a convex polyhedron, and explain how this result can be used in optimization problems.",linear programming and game theory
"discuss the use of the graphical method for solving three-variable systems of linear inequalities. how can we extend the method used for two variables to higher dimensions, and what are the challenges involved?",linear programming and game theory
"explain how linear inequalities can be applied to problems involving market equilibrium. formulate a system of inequalities to represent supply and demand conditions, and determine the equilibrium price and quantity.",linear programming and game theory
"using the graphical method, solve the following system of inequalities: \(4x + y \leq 12\), \(x - 2y \geq 3\), and \(x \geq 0\). identify the feasible region and determine the optimal solution for \(z = 2x + 3y\).",linear programming and game theory
discuss the importance of sensitivity analysis in linear programming. how can the feasible region and optimal solutions be affected by changes in the coefficients of the linear inequalities?,linear programming and game theory
explain the significance of integer programming when solving systems of linear inequalities. provide an example of a problem where integer solutions are required and explain how this modifies the solution process.,linear programming and game theory
how can linear inequalities be used to model traffic flow in urban planning? formulate a set of inequalities to represent traffic congestion constraints and determine the optimal traffic management plan.,linear programming and game theory
"solve a word problem involving the allocation of funds for multiple investment options, each with different returns and risk levels. formulate the problem as a system of linear inequalities and find the feasible investment allocation that maximizes the return.",linear programming and game theory
prove that the optimal solution of a linear programming problem with a non-empty feasible region is always a vertex of the feasible region. provide a detailed proof using the convexity of the feasible region.,linear programming and game theory
"given a system of linear inequalities, explain how to interpret the results of a linear programming problem graphically. how can the objective function help in identifying the optimal solution within the feasible region?",linear programming and game theory
discuss how to handle infeasible solutions in a linear programming problem. what methods can be used to identify and resolve infeasible solutions in systems of linear inequalities?,linear programming and game theory
"using the simplex method, solve the following linear programming problem: maximize \(z = 3x + 4y\), subject to the constraints \(x + 2y \leq 6\), \(2x + y \leq 6\), and \(x, y \geq 0\).",linear programming and game theory
"given the constraints \(2x + y \leq 8\), \(x + 3y \geq 5\), and \(x \leq 4\), solve the system of linear inequalities algebraically to find the feasible region.",linear programming and game theory
discuss the geometric interpretation of the simplex method in the context of linear inequalities. how does the method move from one vertex of the feasible region to another to find the optimal solution?,linear programming and game theory
how can linear inequalities be used to model transportation problems in logistics? formulate a system of inequalities to represent supply and demand constraints and solve for the optimal transportation plan.,linear programming and game theory
derive a general procedure for converting a system of linear inequalities into a set of linear equations using slack variables. provide a detailed example that demonstrates this procedure for a resource allocation problem.,linear programming and game theory
"explain in detail the steps involved in the simplex method. starting with a general linear programming problem in standard form, describe how the initial simplex tableau is constructed. provide an example with a system of two constraints and explain how the tableau evolves after the first pivot operation.",linear programming and game theory
"what are the conditions for optimality in linear programming using the simplex method? prove that if the solution is optimal, no further improvements can be made in terms of the objective function. use an example involving a two-variable linear programming problem to demonstrate the application of these conditions.",linear programming and game theory
define a pivot in the context of the simplex method. explain the role of the pivot operation in progressing towards the optimal solution. provide a detailed example of how a pivot operation is carried out in a simplex tableau and show the transformation of the tableau after the operation.,linear programming and game theory
what is the difference between basic and non-basic variables in the simplex method? discuss their roles in determining the solution to a linear programming problem. explain how basic variables correspond to the current solution and how non-basic variables are used to find the next iteration in the simplex method.,linear programming and game theory
"derive the algorithm for the simplex method. starting from the general linear programming problem, explain the sequence of steps involved in solving the problem. discuss how the method transitions from one basic feasible solution to another and eventually reaches the optimal solution.",linear programming and game theory
"given the linear programming problem: 
    \[
    \text{maximize } z = 3x + 2y
    \]
    subject to the constraints
    \[
    x + y \leq 4, \quad 2x + y \leq 5, \quad x \geq 0, \quad y \geq 0,
    \]
    formulate the initial simplex tableau and solve for the optimal solution using the simplex method. show all steps involved, including identifying the entering and leaving variables.",linear programming and game theory
"consider the linear programming problem: 
    \[
    \text{minimize } z = -2x + 3y
    \]
    subject to the constraints
    \[
    x + 2y \geq 4, \quad 3x - y \leq 5, \quad x \geq 0, \quad y \geq 0.
    \]
    solve this problem using the simplex method, starting from the initial tableau. include all steps of the algorithm, including the pivoting process and optimality check.",linear programming and game theory
"discuss the concept of degeneracy in the simplex method. provide an example where degeneracy occurs, and explain how to handle degenerate solutions during the pivoting process. illustrate with a numerical example, such as the linear programming problem:
    \[
    \text{maximize } z = 4x + 3y
    \]
    subject to the constraints
    \[
    x + y \leq 5, \quad 2x + 2y \leq 10, \quad x \geq 0, \quad y \geq 0.
    \]
    explain why degeneracy arises in this example and discuss the potential issues it causes in the simplex method.",linear programming and game theory
"given a linear programming problem in standard form:
    \[
    \text{maximize } z = x + 2y + 3z
    \]
    subject to the constraints
    \[
    x + y + z \leq 4, \quad 2x + y + 3z \geq 6, \quad x \geq 0, \quad y \geq 0, \quad z \geq 0,
    \]
    construct the initial simplex tableau and solve the problem using the simplex method. explain each step in detail, including the identification of entering and leaving variables and the pivoting procedure.",linear programming and game theory
"explain the role of duality in the simplex method. for a given linear programming problem, formulate its dual problem and solve both the primal and dual problems using the simplex method. provide an example with the following linear programming problem:
    \[
    \text{maximize } z = 2x + 3y
    \]
    subject to the constraints
    \[
    x + 2y \leq 6, \quad 3x + y \leq 7, \quad x \geq 0, \quad y \geq 0.
    \]
    formulate the dual of this problem and solve both problems, showing the relationship between the primal and dual solutions.",linear programming and game theory
"describe in detail how the simplex method can be applied to a linear programming problem with more than two variables. using a three-variable example, construct the initial simplex tableau and demonstrate the steps involved in performing the pivot operations to solve for the optimal solution.",linear programming and game theory
"consider the following linear programming problem:
    \[
    \text{maximize } z = 5x + 4y
    \]
    subject to the constraints
    \[
    2x + y \leq 8, \quad x + 2y \geq 6, \quad x \geq 0, \quad y \geq 0.
    \]
    solve this problem using the simplex method. provide the initial tableau, pivot steps, and the final solution, including the interpretation of the basic and non-basic variables at each step.",linear programming and game theory
"explain the concept of a feasible region in the context of the simplex method. how does the simplex method navigate through the feasible region, and how does this relate to the movement from one basic feasible solution to another? use a geometric example of a two-variable linear programming problem to illustrate the concept.",linear programming and game theory
"define degeneracy in the simplex method and explain its impact on the algorithm’s efficiency. provide a numerical example in which degeneracy causes cycling, and discuss the potential strategies to handle it, such as bland's rule.",linear programming and game theory
"given the linear programming problem:
    \[
    \text{maximize } z = 4x + 5y
    \]
    subject to the constraints
    \[
    3x + 2y \leq 12, \quad x + y \geq 4, \quad x \geq 0, \quad y \geq 0,
    \]
    formulate the initial simplex tableau, apply the simplex method to solve for the optimal solution, and interpret the results in the context of the problem.",linear programming and game theory
"prove that the simplex method terminates after a finite number of iterations. consider the case where the objective function coefficients are strictly positive, and explain why this ensures the algorithm will not cycle indefinitely. illustrate the proof with a simple two-variable linear programming problem.",linear programming and game theory
"solve the following linear programming problem using the simplex method:
    \[
    \text{maximize } z = 2x + 3y
    \]
    subject to the constraints
    \[
    x + y \leq 4, \quad x + 2y \leq 6, \quad x \geq 0, \quad y \geq 0.
    \]
    construct the initial tableau and explain the steps for identifying the entering and leaving variables. demonstrate the pivoting process and discuss the final optimal solution.",linear programming and game theory
explain how artificial variables are used in the simplex method to handle linear programming problems that do not start with an initial basic feasible solution. provide an example where artificial variables are required and explain how they are removed during the solution process.,linear programming and game theory
"for the following linear programming problem:
    \[
    \text{maximize } z = 3x + 2y + 5z
    \]
    subject to the constraints
    \[
    x + y + z \leq 7, \quad x + 2y + 3z \geq 8, \quad x, y, z \geq 0,
    \]
    formulate the initial simplex tableau and solve it using the simplex method. discuss the process of performing the pivot operations and how the optimal solution is reached.",linear programming and game theory
describe the concept of the dual simplex method and explain how it differs from the regular simplex method. provide an example where the dual simplex method is necessary and describe the procedure for solving the problem.,linear programming and game theory
"given the linear programming problem:
    \[
    \text{minimize } z = -4x + 6y
    \]
    subject to the constraints
    \[
    x + 2y \geq 5, \quad 2x + y \leq 7, \quad x \geq 0, \quad y \geq 0,
    \]
    solve the problem using the simplex method. include the formulation of the initial tableau, pivoting process, and final interpretation of the optimal solution.",linear programming and game theory
"solve the following linear programming problem using the simplex method:
    \[
    \text{maximize } z = 6x + 3y
    \]
    subject to the constraints
    \[
    2x + y \leq 6, \quad x + 2y \geq 3, \quad x \geq 0, \quad y \geq 0.
    \]
    formulate the initial simplex tableau and solve for the optimal solution. discuss how to handle situations where both constraints are active.",linear programming and game theory
"explain the relationship between the primal and dual linear programming problems. given the following primal problem:
    \[
    \text{minimize } z = 5x + 4y
    \]
    subject to the constraints
    \[
    x + 2y \geq 6, \quad 2x + y \leq 8, \quad x \geq 0, \quad y \geq 0,
    \]
    formulate its dual problem, solve both the primal and dual problems using the simplex method, and explain the connection between their solutions.",linear programming and game theory
"describe the two-phase simplex method. why is it used, and how does it help in solving linear programming problems that do not have an obvious basic feasible solution? provide a detailed step-by-step application of the two-phase method to a sample problem.",linear programming and game theory
"given the linear programming problem:
    \[
    \text{maximize } z = 7x + 4y
    \]
    subject to the constraints
    \[
    3x + 2y \leq 12, \quad x + 3y \geq 9, \quad x \geq 0, \quad y \geq 0,
    \]
    solve the problem using the simplex method. discuss the interpretation of the basic and non-basic variables and explain how to identify the optimal solution.",linear programming and game theory
discuss the significance of the objective function's coefficients in the simplex method. what happens if one or more coefficients are negative or zero? use a practical example to demonstrate how changes in the objective function can affect the solution process.,linear programming and game theory
explain how the simplex method handles unbounded solutions in linear programming. provide an example where the solution is unbounded and discuss the steps that lead to this conclusion.,linear programming and game theory
"given the following linear programming problem:
    \[
    \text{maximize } z = 2x + 5y
    \]
    subject to the constraints
    \[
    x + 3y \leq 9, \quad 2x + y \geq 4, \quad x \geq 0, \quad y \geq 0,
    \]
    solve the problem using the simplex method. include all steps from the formulation of the initial tableau to the final optimal solution.",linear programming and game theory
explain the role of the optimality test in the simplex method. at what point is the algorithm considered to have found an optimal solution? illustrate this with a numerical example where the algorithm converges to the optimal solution.,linear programming and game theory
"solve the following linear programming problem using the simplex method:
    \[
    \text{maximize } z = 3x + 2y
    \]
    subject to the constraints
    \[
    2x + 3y \leq 12, \quad x + 2y \geq 5, \quad x \geq 0, \quad y \geq 0.
    \]
    discuss the procedure for pivoting and the identification of the entering and leaving variables. provide the optimal solution and interpret its meaning in the context of the problem.",linear programming and game theory
"given the linear programming problem:
    \[
    \text{maximize } z = 10x + 6y
    \]
    subject to the constraints
    \[
    x + y \leq 7, \quad 2x + 3y \leq 12, \quad x \geq 0, \quad y \geq 0,
    \]
    formulate the initial simplex tableau and solve for the optimal solution. explain the decision-making process during each pivot operation and the final interpretation of the results.",linear programming and game theory
"define the dual problem in linear programming. provide a detailed explanation of how to derive the dual from a given primal linear programming problem. demonstrate this by formulating the dual of the following primal problem:
    \[
    \text{maximize } z = 5x_1 + 3x_2
    \]
    subject to the constraints
    \[
    2x_1 + x_2 \leq 8, \quad x_1 + 3x_2 \geq 6, \quad x_1 \geq 0, \quad x_2 \geq 0.
    \]",linear programming and game theory
"what is the economic interpretation of the dual variables in the context of resource allocation? provide a specific example, using a linear programming problem, and explain the role of dual variables in terms of shadow prices.",linear programming and game theory
"explain the relationship between the primal and dual problems in linear programming. given the primal problem:
    \[
    \text{minimize } z = 4x_1 + 3x_2
    \]
    subject to the constraints
    \[
    x_1 + x_2 \geq 3, \quad x_1 + 2x_2 \leq 6, \quad x_1 \geq 0, \quad x_2 \geq 0,
    \]
    formulate the dual and discuss the connection between the primal and dual optimal solutions.",linear programming and game theory
"derive the dual of the following linear programming problem:
    \[
    \text{maximize } z = 3x_1 + 2x_2
    \]
    subject to the constraints
    \[
    x_1 + 2x_2 \leq 4, \quad 2x_1 + x_2 \geq 5, \quad x_1, x_2 \geq 0.
    \]
    explain the steps involved in deriving the dual problem and discuss the primal-dual relationships.",linear programming and game theory
"prove the strong duality theorem in linear programming. provide a detailed proof showing that if the primal problem has an optimal solution, then the dual problem also has an optimal solution, and vice versa.",linear programming and game theory
"derive the complementary slackness conditions for the following linear programming problem:
    \[
    \text{maximize } z = 4x_1 + 3x_2
    \]
    subject to the constraints
    \[
    x_1 + x_2 \leq 5, \quad 2x_1 + x_2 \geq 6, \quad x_1 \geq 0, \quad x_2 \geq 0.
    \]
    explain the significance of the complementary slackness conditions and provide an interpretation in terms of the dual variables.",linear programming and game theory
"given the following primal linear programming problem:
    \[
    \text{maximize } z = 5x_1 + 7x_2
    \]
    subject to the constraints
    \[
    3x_1 + 2x_2 \leq 12, \quad x_1 + 3x_2 \geq 6, \quad x_1 \geq 0, \quad x_2 \geq 0,
    \]
    formulate the dual problem and solve the primal and dual problems simultaneously. verify that the solutions satisfy the duality conditions.",linear programming and game theory
"solve the following primal-dual pair of problems:
    \[
    \text{maximize } z = 4x_1 + 3x_2
    \]
    subject to the constraints
    \[
    x_1 + x_2 \leq 6, \quad 2x_1 + x_2 \geq 5, \quad x_1 \geq 0, \quad x_2 \geq 0,
    \]
    and
    \[
    \text{minimize } w = 6y_1 + 5y_2
    \]
    subject to the constraints
    \[
    y_1 + 2y_2 \geq 4, \quad y_1 + y_2 \leq 3, \quad y_1 \geq 0, \quad y_2 \geq 0.
    \]
    verify the duality conditions and interpret the solutions in terms of the primal and dual variables.",linear programming and game theory
"using the dual problem, interpret the shadow prices in the context of the following resource allocation problem:
    \[
    \text{maximize } z = 3x_1 + 2x_2
    \]
    subject to the constraints
    \[
    2x_1 + x_2 \leq 5, \quad x_1 + 3x_2 \geq 6, \quad x_1 \geq 0, \quad x_2 \geq 0.
    \]
    explain how the dual variables correspond to the shadow prices and how they reflect the impact of resource availability on the objective function.",linear programming and game theory
"formulate the dual problem for the following linear programming model:
    \[
    \text{maximize } z = 6x_1 + 5x_2
    \]
    subject to the constraints
    \[
    3x_1 + 2x_2 \leq 12, \quad x_1 + x_2 \geq 6, \quad x_1 \geq 0, \quad x_2 \geq 0.
    \]
    explain the duality gap and how it relates to the primal and dual solutions.",linear programming and game theory
"prove the weak duality theorem in linear programming. show that for any feasible solution to the primal and dual problems, the objective function value of the primal is always greater than or equal to the objective function value of the dual.",linear programming and game theory
"given the following primal problem:
    \[
    \text{maximize } z = 8x_1 + 4x_2
    \]
    subject to the constraints
    \[
    3x_1 + x_2 \leq 10, \quad x_1 + 2x_2 \geq 6, \quad x_1, x_2 \geq 0,
    \]
    formulate the dual problem and explain how to solve both problems simultaneously. discuss the primal-dual relationships.",linear programming and game theory
"derive the dual of the following linear programming problem:
    \[
    \text{maximize } z = 5x_1 + 2x_2
    \]
    subject to the constraints
    \[
    2x_1 + x_2 \leq 7, \quad x_1 + 3x_2 \geq 9, \quad x_1, x_2 \geq 0.
    \]
    show the steps involved in the dual formulation and explain the dual variables and their interpretation.",linear programming and game theory
discuss the concept of complementary slackness in linear programming. provide an example of a linear programming problem and show how the complementary slackness conditions hold for the primal and dual solutions.,linear programming and game theory
"consider the following primal problem:
    \[
    \text{minimize } w = 3x_1 + 4x_2
    \]
    subject to the constraints
    \[
    x_1 + 2x_2 \geq 5, \quad 2x_1 + x_2 \leq 7, \quad x_1 \geq 0, \quad x_2 \geq 0.
    \]
    derive the dual problem and solve the primal and dual problems. verify the optimality conditions and interpret the solutions.",linear programming and game theory
show how the dual problem is related to the primal problem’s sensitivity analysis. use an example to illustrate how the dual variables represent the change in the objective value of the primal problem when the right-hand side of the constraints is modified.,linear programming and game theory
"given the following primal problem:
    \[
    \text{minimize } z = 2x_1 + 5x_2
    \]
    subject to the constraints
    \[
    3x_1 + 4x_2 \geq 10, \quad x_1 + 2x_2 \leq 6, \quad x_1, x_2 \geq 0,
    \]
    formulate the dual problem and solve it. compare the primal and dual optimal solutions and explain the duality gap.",linear programming and game theory
"prove the strong duality theorem in linear programming and use it to explain why the optimal value of the primal problem equals the optimal value of the dual problem, assuming both problems have feasible solutions.",linear programming and game theory
"derive the complementary slackness conditions and apply them to solve the following linear programming problem:
    \[
    \text{maximize } z = 6x_1 + 3x_2
    \]
    subject to the constraints
    \[
    x_1 + x_2 \leq 4, \quad 2x_1 + x_2 \geq 6, \quad x_1, x_2 \geq 0.
    \]",linear programming and game theory
"solve the following primal-dual pair of linear programming problems and verify the duality conditions:
    \[
    \text{maximize } z = 7x_1 + 4x_2
    \]
    subject to the constraints
    \[
    x_1 + 2x_2 \leq 8, \quad x_1 + 3x_2 \geq 7, \quad x_1, x_2 \geq 0,
    \]
    and
    \[
    \text{minimize } w = 8y_1 + 7y_2
    \]
    subject to the constraints
    \[
    y_1 + y_2 \geq 7, \quad 2y_1 + y_2 \leq 7, \quad y_1, y_2 \geq 0.
    \]
    verify that the solutions satisfy the duality conditions.",linear programming and game theory
"derive the dual of the following linear programming problem:
    \[
    \text{maximize } z = 5x_1 + 3x_2
    \]
    subject to the constraints
    \[
    x_1 + x_2 \leq 4, \quad 2x_1 + x_2 \geq 6, \quad x_1 \geq 0, \quad x_2 \geq 0.
    \]
    interpret the dual variables and discuss how they affect the primal solution.",linear programming and game theory
"using the primal-dual relationships, solve the following pair of linear programming problems:
    \[
    \text{maximize } z = 4x_1 + 2x_2
    \]
    subject to the constraints
    \[
    2x_1 + x_2 \leq 6, \quad x_1 + 3x_2 \geq 4, \quad x_1 \geq 0, \quad x_2 \geq 0,
    \]
    and
    \[
    \text{minimize } w = 6y_1 + 4y_2
    \]
    subject to the constraints
    \[
    y_1 + 2y_2 \geq 4, \quad y_1 + y_2 \leq 3, \quad y_1, y_2 \geq 0.
    \]
    verify that the solutions satisfy the primal-dual relationships.",linear programming and game theory
"consider the following primal linear programming problem:
    \[
    \text{minimize } w = 3x_1 + 2x_2
    \]
    subject to the constraints
    \[
    x_1 + 2x_2 \geq 5, \quad 2x_1 + x_2 \leq 7, \quad x_1, x_2 \geq 0.
    \]
    derive the dual problem and solve both the primal and dual problems. verify the optimality conditions and discuss the economic interpretation of the dual variables.",linear programming and game theory
"show that the strong duality theorem holds for the following linear programming problem:
    \[
    \text{maximize } z = 7x_1 + 4x_2
    \]
    subject to the constraints
    \[
    x_1 + x_2 \leq 6, \quad x_1 + 3x_2 \geq 5, \quad x_1, x_2 \geq 0.
    \]
    derive the dual problem and prove that the optimal values of the primal and dual problems are equal.",linear programming and game theory
"solve the following primal and dual problems simultaneously and verify the duality conditions:
    \[
    \text{maximize } z = 5x_1 + 3x_2
    \]
    subject to the constraints
    \[
    x_1 + x_2 \leq 4, \quad 2x_1 + x_2 \geq 6, \quad x_1, x_2 \geq 0,
    \]
    and
    \[
    \text{minimize } w = 4y_1 + 6y_2
    \]
    subject to the constraints
    \[
    y_1 + 2y_2 \geq 5, \quad y_1 + y_2 \leq 3, \quad y_1, y_2 \geq 0.
    \]
    verify that the primal-dual solutions satisfy the duality conditions.",linear programming and game theory
"discuss the transportation problem in the context of network models. derive the equations representing the problem and explain how it can be solved using linear programming. include a numerical example where transportation costs between various origins and destinations are given, and you need to minimize the total transportation cost.",linear programming and game theory
explain the maximum flow problem and its relevance in network models. derive the formulation of the maximum flow problem as a linear programming problem. consider a flow network where the capacities of edges between nodes are provided. solve for the maximum flow using the ford-fulkerson algorithm.,linear programming and game theory
"what is the minimum cost flow problem in network theory? explain how this problem can be modeled using linear programming and its significance in real-world applications. solve a numerical example where the demand and supply at various nodes are specified, and the transportation costs between nodes are given.",linear programming and game theory
derive the transportation problem equations for a multi-commodity transportation problem. how can the simplex method be used to solve this? illustrate with a numerical example involving multiple commodities and multiple origins and destinations.,linear programming and game theory
"solve a minimum cost flow problem using the network simplex method. provide a detailed step-by-step solution to a numerical example where you are given a flow network with a set of supply and demand constraints, and each edge has a cost associated with it. optimize the total cost of satisfying the demand.",linear programming and game theory
"use dijkstra’s algorithm to solve the shortest path problem for a weighted directed graph. provide a graph with 6 nodes and weighted edges, and use dijkstra’s algorithm to find the shortest path from a given source to all other nodes. include the algorithm's step-by-step procedure.",linear programming and game theory
derive the linear programming formulation for the maximum flow problem. solve a network flow problem with 4 nodes and multiple directed edges where you need to maximize the flow from a source to a sink under capacity constraints. use the ford-fulkerson method to determine the maximum flow.,linear programming and game theory
discuss the concept of the minimum cut in a network flow problem. provide a numerical example where you need to identify the minimum cut of a flow network and explain the relationship between the maximum flow and minimum cut according to the max-flow min-cut theorem.,linear programming and game theory
"solve a transportation problem using the modified distribution method (modi method) with a given cost matrix, supply, and demand vectors. explain the steps involved and derive the optimal solution that minimizes the total transportation cost. provide a numerical example with 4 origins and 3 destinations.",linear programming and game theory
derive the equations for the shortest path problem in a directed graph and solve it using bellman-ford's algorithm. provide a graph with negative weight edges and use bellman-ford to find the shortest path from a specified source node to all other nodes.,linear programming and game theory
"solve a maximum flow problem with the edmonds-karp algorithm. given a network with 5 nodes and capacity constraints on the edges, apply the edmonds-karp method to determine the maximum flow from the source to the sink. explain each iteration of the algorithm in detail.",linear programming and game theory
discuss the differences between the primal and dual formulations of the transportation problem. provide an example of a transportation problem and solve it using both formulations. compare the results obtained and explain the significance of the dual variables in the context of the transportation problem.,linear programming and game theory
"using linear programming, derive the optimality conditions for the minimum cost flow problem. solve a numerical example where the objective is to minimize the total cost of flowing goods through a network with multiple supply and demand nodes.",linear programming and game theory
"explain the application of the transportation problem in supply chain management. formulate the problem as a linear programming problem, and solve a specific example where you are given transportation costs, supply at each source, and demand at each destination.",linear programming and game theory
derive the formulation of the shortest path problem as a linear program. solve a numerical example using the network flow approach to find the shortest path in a graph with weighted edges. explain the relationship between the shortest path and the minimum cost flow problem.,linear programming and game theory
"using the ford-fulkerson algorithm, solve a maximum flow problem in a network with 6 nodes and directed edges with specified capacities. illustrate the steps involved and explain how the algorithm handles cycles and residual networks.",linear programming and game theory
"solve a transportation problem using the stepping-stone method. provide a numerical example where you have supply, demand, and a cost matrix. explain the method step-by-step and derive the optimal transportation plan that minimizes the total cost.",linear programming and game theory
derive the equations for a transportation problem with constraints on the number of shipments that can be made along each route. solve the problem using linear programming and explain how the constraints affect the optimal solution.,linear programming and game theory
discuss how the concept of duality applies to the transportation problem. provide a numerical example and formulate both the primal and dual problems. solve both problems and verify the duality theorem by comparing the objective values.,linear programming and game theory
"discuss the maximum flow problem in the context of a flow network with parallel edges. solve a maximum flow problem where there are parallel edges between two nodes, and explain how the flow can be distributed across the edges while respecting capacity constraints.",linear programming and game theory
"use the simplex method to solve a transportation problem with a non-degenerate solution. provide a specific numerical example and explain the steps in the simplex algorithm, including the initialization of the basic feasible solution and the optimality condition.",linear programming and game theory
show how to formulate a shortest path problem as a network flow problem and solve it using linear programming. provide a graph with multiple paths and demonstrate how network flow techniques can be applied to find the shortest path.,linear programming and game theory
"solve the following transportation problem using the least cost method:
    \[
    \text{minimize } z = 4x_1 + 2x_2 + 3x_3 + 6x_4
    \]
    subject to the constraints:
    \[
    x_1 + x_2 + x_3 = 10, \quad x_4 + x_2 = 5, \quad x_3 + x_4 = 7
    \]
    where \(x_1, x_2, x_3, x_4\) represent the amount of goods transported from four different sources to destinations with given supply and demand.",linear programming and game theory
solve the minimum cost flow problem using the network simplex method for a given network with 5 nodes and multiple arcs. derive the optimal flow and demonstrate the reduction in cost as the network adjusts.,linear programming and game theory
"solve a maximum flow problem with 4 nodes, where you are given the capacity of each edge and need to determine the maximum possible flow from the source to the sink. use both the ford-fulkerson method and the edmonds-karp method to find the solution and compare their performance.",linear programming and game theory
"derive the equations for a transportation problem with multiple objectives. solve the problem where you are given multiple cost matrices for different objectives, and you need to minimize the weighted sum of these objectives.",linear programming and game theory
apply dijkstra's algorithm to find the shortest path from node a to node b in a graph with 7 nodes and weighted edges. provide the step-by-step calculation and verify the correctness of the solution.,linear programming and game theory
"discuss the significance of the dual variables in the context of the transportation problem. provide a numerical example, solve the primal problem, and interpret the dual variables in terms of shadow prices for supply and demand constraints.",linear programming and game theory
"derive the conditions for optimality in the minimum cost flow problem. solve the problem with 6 nodes and multiple supply and demand nodes, and verify the conditions using the dual variables.",linear programming and game theory
explain the importance of the max-flow min-cut theorem in the context of network optimization. solve a numerical example where you need to identify the maximum flow and the minimum cut of a network and show the connection between these two concepts.,linear programming and game theory
solve a transportation problem with unbalanced supply and demand using the modi method. provide a numerical example where the supply does not equal the demand and show how to balance the system with dummy nodes.,linear programming and game theory
"apply the ford-fulkerson method to a maximum flow problem where the capacity of each edge is an integer. provide a flow network with 5 nodes and 6 edges, and solve for the maximum flow from the source to the sink.",linear programming and game theory
solve a shortest path problem in a directed graph with negative edge weights using bellman-ford's algorithm. provide a graph with 5 nodes and negative weights and solve for the shortest path from the source to all other nodes.,linear programming and game theory
"derive and solve a transportation problem with fixed supply and demand quantities using the primal-dual method. provide a specific example with supply, demand, and cost matrices, and show how the primal-dual method helps achieve the optimal solution.",linear programming and game theory
"solve the minimum cost flow problem using the primal-dual algorithm in a flow network. provide a numerical example with 4 nodes and 5 edges, and show how the primal-dual method efficiently solves the flow optimization problem.",linear programming and game theory
explain the difference between the simplex method and the network simplex method in solving the transportation problem. provide a numerical example and demonstrate how both methods are applied to solve the same transportation problem.,linear programming and game theory
derive the dual of the minimum cost flow problem and solve it using linear programming. provide a flow network with supply and demand constraints and solve for the dual variables and the optimal flow distribution.,linear programming and game theory
"solve a maximum flow problem with a multi-commodity flow network, where you are tasked with maximizing the flow of multiple commodities through a network. provide a numerical example with 3 commodities and multiple flow constraints.",linear programming and game theory
"define a nash equilibrium in the context of game theory. discuss its significance and provide an example of a game where players have conflicting interests. solve for the nash equilibrium in a two-player game with a 3x3 payoff matrix, explaining the steps involved in the process.",linear programming and game theory
what is the difference between cooperative and non-cooperative games? explain the concept of a cooperative game using the shapley value and illustrate it with a real-world example. contrast it with a non-cooperative game and discuss the implications for players' strategies.,linear programming and game theory
explain the concept of a zero-sum game. provide a mathematical formulation of a zero-sum game and derive the optimal strategies for both players in a two-player game. solve for the equilibrium strategies using linear programming techniques.,linear programming and game theory
"discuss the role of mixed strategies in game theory. derive the mixed strategy nash equilibrium for a simple game, such as the game of matching pennies, and explain the significance of randomization in players' strategies.",linear programming and game theory
"using the concept of evolutionary game theory, explain how populations of individuals evolve in response to strategic interactions. discuss the concepts of evolutionarily stable strategies (ess) and their applications in biology, economics, and social sciences.",linear programming and game theory
derive the conditions for a nash equilibrium in a two-player game where both players have two strategies. use a 2x2 matrix to demonstrate how to find the equilibrium and explain the conditions under which the game has a pure strategy nash equilibrium.,linear programming and game theory
explain how the concept of nash equilibrium can be extended to games with more than two players. discuss the necessary conditions for a nash equilibrium in multiplayer games and illustrate with an example from auction theory.,linear programming and game theory
"solve a zero-sum game using linear programming. given a 3x3 payoff matrix, formulate the problem as a linear programming model and solve for the optimal strategies for both players. discuss how the simplex method can be used to find the solution.",linear programming and game theory
discuss the concept of a dominant strategy in game theory. provide an example of a game where one player has a dominant strategy and explain how the existence of a dominant strategy affects the outcome of the game.,linear programming and game theory
"define and explain the concept of a mixed strategy nash equilibrium. derive the mixed strategy nash equilibrium for a 2x2 game matrix, where each player has two strategies. provide a detailed step-by-step solution, including the probabilities associated with each strategy.",linear programming and game theory
explain the application of game theory to pricing strategies in competitive markets. solve a problem where two firms compete in setting prices for similar products and analyze the nash equilibrium in this pricing game. discuss the economic implications of the equilibrium prices.,linear programming and game theory
discuss the application of game theory in oligopolistic markets. use the cournot competition model to analyze the strategic interaction between two firms in an oligopoly. solve for the nash equilibrium output levels and discuss the implications for market prices.,linear programming and game theory
"derive the nash equilibrium for a coordination game with two players. provide a numerical example of a coordination game where the players can either cooperate or defect, and show the different possible nash equilibria in this game.",linear programming and game theory
solve for the nash equilibrium in a 3x3 game matrix. explain the steps involved in identifying the best responses for each player and show how the equilibrium can be found using dominated strategy elimination.,linear programming and game theory
"explain the concept of mixed strategy equilibria in zero-sum games. derive the mixed strategy nash equilibrium for a zero-sum game with two players, where each player has two strategies, and solve the corresponding linear programming problem.",linear programming and game theory
"discuss the concept of subgame perfect nash equilibrium (spne) in dynamic games. provide an example of a sequential game, such as the ultimatum game, and solve for the spne using backward induction.",linear programming and game theory
"solve a zero-sum game with a payoff matrix using the minimax theorem. given a matrix with payoffs for two players, apply the minimax strategy and explain the reasoning behind the optimal strategy for each player.",linear programming and game theory
"apply game theory to analyze a competitive advertising problem. assume two firms must decide on their advertising strategies in a market, and analyze the nash equilibrium of the game. use a game matrix to solve for the equilibrium and discuss the strategic behavior of the firms.",linear programming and game theory
"define the concept of evolutionary stability in game theory. discuss the application of evolutionary game theory to the evolution of cooperation in social dilemmas, and derive the conditions under which a strategy becomes evolutionarily stable.",linear programming and game theory
derive the nash equilibrium in a 2x2 game with strictly dominated strategies. use the concept of iterated elimination of strictly dominated strategies and show how it simplifies the process of finding the equilibrium in the game.,linear programming and game theory
"solve a game theory problem involving mixed strategies where the players' payoffs depend on the actions of both players. given a 2x2 game matrix, solve for the mixed strategy nash equilibrium and explain the significance of the solution in the context of the game.",linear programming and game theory
derive the nash equilibrium for a 2x2 zero-sum game using linear programming. provide a numerical example where both players have two strategies and show the optimal strategies for both players using the simplex method.,linear programming and game theory
"explain the application of game theory in environmental policy-making. analyze a game between a government and a corporation concerning pollution control, and solve for the nash equilibrium in this context, discussing the economic and social implications of the outcome.",linear programming and game theory
discuss the concept of the prisoners' dilemma in the context of game theory. provide a detailed explanation of the game and solve for the nash equilibrium. analyze the social and economic consequences of the equilibrium strategy in a real-world scenario.,linear programming and game theory
"solve a competitive bidding problem using game theory. given a situation where two firms submit bids for a contract, analyze the nash equilibrium of the bidding game and determine the optimal bidding strategies for both players.",linear programming and game theory
solve a mixed strategy game where both players have two strategies and the payoffs are non-symmetric. use the linear programming method to derive the mixed strategy nash equilibrium and explain the significance of each player's mixed strategy.,linear programming and game theory
"explain the application of game theory in political campaigns. analyze a scenario where two candidates are competing for votes, and use game theory to solve for the nash equilibrium in terms of campaign strategies.",linear programming and game theory
discuss the role of information asymmetry in game theory. provide an example of a signaling game and derive the conditions under which one player will send a signal to another player. solve for the equilibrium strategy in a game with asymmetric information.,linear programming and game theory
derive the nash equilibrium for a public goods game where players must decide how much to contribute to the provision of a public good. explain the challenges of finding a cooperative solution in this game and discuss the implications for social welfare.,linear programming and game theory
"explain the concept of the centipede game and analyze its nash equilibrium. provide a detailed explanation of the game’s structure, and derive the equilibrium strategies for both players in a finite version of the game.",linear programming and game theory
derive the solution to a zero-sum game with mixed strategies and explain how linear programming can be used to solve the game. provide a specific 3x3 payoff matrix and solve for the mixed strategy equilibrium.,linear programming and game theory
"analyze a game of chicken and derive the nash equilibrium. explain the strategic decisions involved in the game, and discuss the implications of the equilibrium in the context of decision-making under risk.",linear programming and game theory
"solve for the nash equilibrium in a sequential game using backward induction. consider a game where two players move in turn, and each player must decide their strategy based on the previous player's move. explain the process of backward induction to find the equilibrium strategy.",linear programming and game theory
discuss the application of game theory in auction theory. analyze a sealed-bid auction and solve for the nash equilibrium bidding strategy for the participants. compare the equilibrium strategy to the strategy in an open-bid auction.,linear programming and game theory
"solve for the nash equilibrium in a game with incomplete information. given a game where players have private information about their payoffs, use the bayesian nash equilibrium concept to derive the optimal strategies for each player.",linear programming and game theory
"apply game theory to analyze the behavior of firms in a price war. assume two firms are competing on prices, and use game theory to find the nash equilibrium in the pricing game. discuss how the equilibrium price affects both firms' profits.",linear programming and game theory
derive the nash equilibrium for a public goods game where players decide whether to contribute to a common fund. explain the challenges of achieving the socially optimal outcome and the role of nash equilibrium in this context.,linear programming and game theory
solve for the mixed strategy nash equilibrium in a 2x2 game with unequal payoffs for both players. use a mathematical approach to derive the probabilities of choosing each strategy and explain how the mixed strategy resolves the conflict between players.,linear programming and game theory
explain the concept of a weakly dominant strategy and solve for the nash equilibrium in a game where one player has a weakly dominant strategy. discuss how this affects the players' choices and the equilibrium outcome.,linear programming and game theory
"solve a zero-sum game using the minimax strategy. given a game with two players and a 2x2 payoff matrix, apply the minimax theorem to determine the optimal strategies for both players and explain the reasoning behind the minimax solution.",linear programming and game theory
"derive the nash equilibrium for a game with three players, each of whom can either cooperate or defect. use a payoff matrix to show how the players' strategies interact and determine the equilibrium outcomes.",linear programming and game theory
solve a real-world problem using evolutionary game theory. consider a scenario in biology where two species are competing for resources. use evolutionary game theory to model the interaction between the species and determine the evolutionarily stable strategies.,linear programming and game theory
discuss the concept of a mixed strategy nash equilibrium in a zero-sum game with three strategies for each player. use linear programming to derive the equilibrium strategies for both players and explain the solution process in detail.,linear programming and game theory
"given the following payoff matrix for a two-player game, where player a chooses between strategies $s_1$ and $s_2$, and player b chooses between strategies $t_1$ and $t_2$, find the nash equilibrium of the game.

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (3, 2) & (1, 1) \\
    s_2 & (0, 0) & (2, 3) \\
    \end{matrix}
    \]",linear programming and game theory
"consider a game where two players can each choose between two actions, $a$ and $b$. the payoff matrix is given by:

    \[
    \begin{matrix}
      & a & b \\
    a & (4, 3) & (1, 4) \\
    b & (2, 2) & (3, 1) \\
    \end{matrix}
    \]

    find the mixed strategy nash equilibrium for this game.",linear programming and game theory
"in a zero-sum game with the following payoff matrix, where player a and player b both have two strategies, compute the optimal mixed strategy for player a using linear programming:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (4, -4) & (-1, 1) \\
    s_2 & (3, -3) & (2, -2) \\
    \end{matrix}
    \]",linear programming and game theory
"solve the following zero-sum game using the minimax theorem. find the optimal strategy for both players:

    \[
    \begin{matrix}
      & t_1 & t_2 & t_3 \\
    s_1 & (2, -2) & (3, -3) & (-1, 1) \\
    s_2 & (0, 0) & (-2, 2) & (4, -4) \\
    \end{matrix}
    \]",linear programming and game theory
"for a game with the following payoff matrix, determine whether a pure strategy nash equilibrium exists, and if so, find it:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (2, 3) & (3, 2) \\
    s_2 & (1, 1) & (2, 3) \\
    \end{matrix}
    \]",linear programming and game theory
"solve for the mixed strategy nash equilibrium for the following game, where the players choose between strategies $x$ and $y$:

    \[
    \begin{matrix}
      & x & y \\
    x & (2, 1) & (1, 2) \\
    y & (3, 0) & (0, 3) \\
    \end{matrix}
    \]",linear programming and game theory
"in a zero-sum game where player a has strategies $s_1$, $s_2$, and $s_3$, and player b has strategies $t_1$ and $t_2$, the payoff matrix is given by:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (5, -5) & (3, -3) \\
    s_2 & (2, -2) & (4, -4) \\
    s_3 & (1, -1) & (0, 0) \\
    \end{matrix}
    \]

    use linear programming to determine the optimal strategies for both players.",linear programming and game theory
"consider a two-player game where player a and player b each have two strategies, $s_1$ and $s_2$. the payoff matrix is given by:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (4, 1) & (2, 3) \\
    s_2 & (1, 2) & (3, 4) \\
    \end{matrix}
    \]

    find the nash equilibrium for this game using the concept of best responses.",linear programming and game theory
"given the following game matrix, where the players choose between strategies $x$ and $y$, compute the mixed strategy nash equilibrium:

    \[
    \begin{matrix}
      & x & y \\
    x & (1, 1) & (2, 3) \\
    y & (3, 2) & (0, 4) \\
    \end{matrix}
    \]",linear programming and game theory
"in the game represented by the following payoff matrix, where player a has three strategies and player b has two strategies, find the nash equilibrium using linear programming:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (6, -6) & (1, -1) \\
    s_2 & (2, -2) & (5, -5) \\
    s_3 & (4, -4) & (3, -3) \\
    \end{matrix}
    \]",linear programming and game theory
"consider a game where two players can choose between strategies $a$ and $b$. the following payoff matrix is given:

    \[
    \begin{matrix}
      & a & b \\
    a & (3, 2) & (0, 1) \\
    b & (1, 0) & (2, 3) \\
    \end{matrix}
    \]

    find the mixed strategy nash equilibrium for this game.",linear programming and game theory
"for the following payoff matrix, determine whether a pure strategy nash equilibrium exists:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (3, 2) & (1, 0) \\
    s_2 & (0, 1) & (2, 3) \\
    \end{matrix}
    \]",linear programming and game theory
"given the payoff matrix below, solve for the nash equilibrium using the iterated elimination of strictly dominated strategies:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (5, 5) & (1, 0) \\
    s_2 & (2, 1) & (4, 4) \\
    \end{matrix}
    \]",linear programming and game theory
"solve the following game using the minimax strategy, and determine the optimal strategies for both players:

    \[
    \begin{matrix}
      & t_1 & t_2 & t_3 \\
    s_1 & (2, -2) & (1, -1) & (0, 0) \\
    s_2 & (4, -4) & (3, -3) & (5, -5) \\
    \end{matrix}
    \]",linear programming and game theory
"find the mixed strategy nash equilibrium for the following game, where player a has strategies $s_1$, $s_2$, and $s_3$, and player b has strategies $t_1$ and $t_2$:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (1, -1) & (3, -3) \\
    s_2 & (4, -4) & (2, -2) \\
    s_3 & (0, 0) & (5, -5) \\
    \end{matrix}
    \]",linear programming and game theory
"given the following payoff matrix, find the nash equilibrium of the game and analyze the strategies used by the players:

    \[
    \begin{matrix}
      & t_1 & t_2 & t_3 \\
    s_1 & (5, 4) & (2, 1) & (1, 2) \\
    s_2 & (3, 3) & (4, 5) & (2, 1) \\
    s_3 & (2, 2) & (1, 3) & (5, 4) \\
    \end{matrix}
    \]",linear programming and game theory
"solve the following game using linear programming to determine the optimal mixed strategy for player a:

    \[
    \begin{matrix}
      & t_1 & t_2 & t_3 \\
    s_1 & (4, -4) & (3, -3) & (2, -2) \\
    s_2 & (1, -1) & (2, -2) & (3, -3) \\
    s_3 & (5, -5) & (4, -4) & (3, -3) \\
    \end{matrix}
    \]",linear programming and game theory
"in a zero-sum game where both players choose between two strategies, find the mixed strategy nash equilibrium for the following game:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (3, -3) & (1, -1) \\
    s_2 & (2, -2) & (0, 0) \\
    \end{matrix}
    \]",linear programming and game theory
"find the nash equilibrium for a game in which player a can choose between $s_1$ and $s_2$, and player b can choose between $t_1$ and $t_2$, with the following payoff matrix:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (2, 2) & (3, 1) \\
    s_2 & (1, 3) & (4, 4) \\
    \end{matrix}
    \]",linear programming and game theory
"in a strategic game with the following payoff matrix, compute the nash equilibrium for the two players:

    \[
    \begin{matrix}
      & t_1 & t_2 & t_3 \\
    s_1 & (2, 3) & (1, 1) & (4, 2) \\
    s_2 & (3, 2) & (2, 4) & (5, 5) \\
    s_3 & (4, 1) & (3, 3) & (1, 4) \\
    \end{matrix}
    \]",linear programming and game theory
"given a two-player game where the payoff matrix is as follows, solve for the nash equilibrium using the concept of dominant strategies:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (4, 3) & (2, 1) \\
    s_2 & (1, 2) & (3, 4) \\
    \end{matrix}
    \]",linear programming and game theory
"determine the mixed strategy nash equilibrium for the following two-player game:

    \[
    \begin{matrix}
      & a & b \\
    a & (2, 3) & (1, 1) \\
    b & (0, 1) & (3, 2) \\
    \end{matrix}
    \]",linear programming and game theory
"in a two-player game where the following payoff matrix is given, find the nash equilibrium using the best response function:

    \[
    \begin{matrix}
      & t_1 & t_2 & t_3 \\
    s_1 & (1, 2) & (3, 1) & (2, 3) \\
    s_2 & (2, 1) & (4, 3) & (1, 2) \\
    s_3 & (3, 2) & (1, 4) & (3, 1) \\
    \end{matrix}
    \]",linear programming and game theory
"solve for the nash equilibrium using the concept of best responses for the following game:

    \[
    \begin{matrix}
      & t_1 & t_2 \\
    s_1 & (5, 6) & (2, 1) \\
    s_2 & (3, 2) & (6, 5) \\
    \end{matrix}
    \]",linear programming and game theory
"consider the following linear programming problem:

    maximize \( z = 3x_1 + 2x_2 \)

    subject to the constraints:
    \[
    2x_1 + x_2 \leq 8
    \]
    \[
    x_1 + 2x_2 \geq 6
    \]
    \[
    x_1, x_2 \geq 0
    \]

    use the simplex method to find the optimal solution.",linear programming and game theory
"solve the following linear programming problem using the duality theorem:

    minimize \( z = 4x_1 + 5x_2 \)

    subject to:
    \[
    x_1 + 2x_2 \geq 10
    \]
    \[
    2x_1 + x_2 \geq 8
    \]
    \[
    x_1, x_2 \geq 0
    \]
    
    determine both the primal and dual optimal solutions.",linear programming and game theory
"given the following linear programming problem, use the method of linear programming with slack variables to convert it into standard form:

    maximize \( z = 6x_1 + 5x_2 \)

    subject to:
    \[
    3x_1 + 2x_2 \leq 12
    \]
    \[
    x_1 + x_2 \geq 4
    \]
    \[
    x_1, x_2 \geq 0
    \]

    then, solve the problem using the simplex method.",linear programming and game theory
"formulate the following real-world problem as a linear programming problem: a company produces two products, \( p_1 \) and \( p_2 \). each product requires time on two machines, machine 1 and machine 2. the time required for each product on each machine is given below:

    \[
    \text{machine 1:} \quad \text{product } p_1 \text{ requires } 3 \text{ hours}, \text{ product } p_2 \text{ requires } 2 \text{ hours}
    \]
    \[
    \text{machine 2:} \quad \text{product } p_1 \text{ requires } 2 \text{ hours}, \text{ product } p_2 \text{ requires } 3 \text{ hours}
    \]
    
    the company has a total of 12 hours available on machine 1 and 15 hours available on machine 2. the profit from each unit of \( p_1 \) is $4 and the profit from each unit of \( p_2 \) is $5. formulate this as a linear programming problem to maximize profit.",linear programming and game theory
"solve the following linear programming problem using the simplex method:

    maximize \( z = 3x_1 + 2x_2 + x_3 \)

    subject to the constraints:
    \[
    x_1 + x_2 + x_3 \leq 6
    \]
    \[
    2x_1 + x_2 \geq 4
    \]
    \[
    x_2 - x_3 \leq 2
    \]
    \[
    x_1, x_2, x_3 \geq 0
    \]",linear programming and game theory
"formulate the following transportation problem as a linear programming problem:

    a company has 3 warehouses and 4 retailers. the supply at each warehouse and the demand at each retailer are given below, along with the cost per unit transported from each warehouse to each retailer:

    \[
    \text{supply:} \quad 50 \text{ units at warehouse 1}, 60 \text{ units at warehouse 2}, 40 \text{ units at warehouse 3}
    \]
    \[
    \text{demand:} \quad 40 \text{ units at retailer 1}, 50 \text{ units at retailer 2}, 30 \text{ units at retailer 3}, 30 \text{ units at retailer 4}
    \]
    \[
    \text{cost matrix:} \quad \begin{matrix}
    4 & 2 & 5 & 3 \\
    3 & 4 & 1 & 2 \\
    6 & 7 & 4 & 2
    \end{matrix}
    \]

    use linear programming to minimize the transportation cost.",linear programming and game theory
"given the following linear programming problem, apply the big-m method to find the optimal solution:

    maximize \( z = 4x_1 + 3x_2 \)

    subject to:
    \[
    x_1 + x_2 \leq 8
    \]
    \[
    x_1 - x_2 \geq 2
    \]
    \[
    x_1, x_2 \geq 0
    \]",linear programming and game theory
"solve the following integer linear programming problem using branch and bound method:

    maximize \( z = 5x_1 + 4x_2 \)

    subject to the constraints:
    \[
    x_1 + 2x_2 \leq 8
    \]
    \[
    3x_1 + x_2 \geq 6
    \]
    \[
    x_1, x_2 \text{ are integers, and } x_1, x_2 \geq 0
    \]",linear programming and game theory
"apply the dual simplex method to solve the following linear programming problem:

    minimize \( z = 2x_1 + 3x_2 \)

    subject to the constraints:
    \[
    2x_1 + x_2 \geq 6
    \]
    \[
    x_1 + 3x_2 \geq 8
    \]
    \[
    x_1, x_2 \geq 0
    \]",linear programming and game theory
"solve the following linear programming problem using graphical methods:

    maximize \( z = x_1 + x_2 \)

    subject to the constraints:
    \[
    x_1 + 2x_2 \leq 8
    \]
    \[
    3x_1 + x_2 \leq 6
    \]
    \[
    x_1, x_2 \geq 0
    \]",linear programming and game theory
"formulate the following problem as a linear programming problem: a company has 3 types of workers: senior, junior, and trainee. each worker can produce a certain number of units of a product per day as follows:

    \[
    \text{senior:} \quad 10 \text{ units per day}
    \]
    \[
    \text{junior:} \quad 8 \text{ units per day}
    \]
    \[
    \text{trainee:} \quad 5 \text{ units per day}
    \]

    the company needs to produce at least 300 units per day. senior workers are paid $50 per day, junior workers $30 per day, and trainees $20 per day. the company wants to minimize the total cost while satisfying the production requirement. formulate this as a linear programming problem.",linear programming and game theory
"given the following primal linear programming problem, find the dual using the duality theorem:

    maximize \( z = 3x_1 + 2x_2 \)

    subject to the constraints:
    \[
    2x_1 + x_2 \leq 10
    \]
    \[
    x_1 + 3x_2 \leq 15
    \]
    \[
    x_1, x_2 \geq 0
    \]",linear programming and game theory
"use the simplex method to solve the following linear programming problem:

    maximize \( z = 5x_1 + 4x_2 + 3x_3 \)

    subject to the constraints:
    \[
    x_1 + x_2 + x_3 \leq 10
    \]
    \[
    3x_1 + 2x_2 \geq 12
    \]
    \[
    x_1, x_2, x_3 \geq 0
    \]",linear programming and game theory
"solve the following linear programming problem using the simplex method:

    maximize \( z = 4x_1 + 5x_2 \)

    subject to the constraints:
    \[
    x_1 + x_2 \leq 10
    \]
    \[
    2x_1 + 3x_2 \geq 15
    \]
    \[
    x_1, x_2 \geq 0
    \]",linear programming and game theory
"formulate the following real-world transportation problem as a linear programming problem: a retailer needs to transport goods from 3 suppliers to 4 customers. the supply, demand, and transportation costs are as follows:

    \[
    \text{supplier 1:} 50 \text{ units, supplier 2:} 60 \text{ units, supplier 3:} 40 \text{ units}
    \]
    \[
    \text{customer 1:} 40 \text{ units, customer 2:} 50 \text{ units, customer 3:} 30 \text{ units, customer 4:} 30 \text{ units}
    \]
    \[
    \text{cost matrix:} \quad \begin{matrix}
    4 & 6 & 8 & 5 \\
    2 & 4 & 5 & 6 \\
    7 & 9 & 5 & 3
    \end{matrix}
    \]

    minimize the total transportation cost.",linear programming and game theory
"apply the primal-dual interior-point method to solve the following linear programming problem:

    maximize \( z = 2x_1 + 3x_2 \)

    subject to:
    \[
    3x_1 + 2x_2 \leq 10
    \]
    \[
    x_1 + 2x_2 \geq 4
    \]
    \[
    x_1, x_2 \geq 0
    \]",linear programming and game theory
"solve the following linear programming problem using the two-phase simplex method:

    maximize \( z = 5x_1 + 3x_2 \)

    subject to the constraints:
    \[
    3x_1 + x_2 \geq 8
    \]
    \[
    x_1 + 4x_2 \leq 10
    \]
    \[
    x_1, x_2 \geq 0
    \]",linear programming and game theory
"consider a game theory problem in which two players, player 1 and player 2, have the following payoff matrices:

    player 1's payoff matrix:
    \[
    \begin{matrix}
    2 & -1 \\
    4 & 3
    \end{matrix}
    \]

    player 2's payoff matrix:
    \[
    \begin{matrix}
    3 & 2 \\
    1 & -2
    \end{matrix}
    \]

    find the mixed strategy nash equilibrium using linear programming. determine the probabilities with which each player should choose their strategies to maximize their respective payoffs.",linear programming and game theory
"for the equations \(x + y = 4\), \(2x - 2y = 4\), draw the row picture (two intersecting
          lines) and the column picture (combination of two columns equal to the column
          vector \((4,4)\) on the right side).",matrices and gaussian elimination
"solve to find a combination of the columns that equals $b$ for the triangular system:
    \begin{align*}
    u - v - w &= b_1 \\
    v + w &= b_2 \\
    w &= b_3.
    \end{align*}",matrices and gaussian elimination
"(recommended) describe the intersection of the three planes:
    \begin{align*}
    u + v + w + z &= 6 \\
    u + w + z &= 4 \\
    u + w &= 2
    \end{align*}
    (all in four-dimensional space). is it a line, a point, or an empty set? what is the intersection if the fourth plane $u = -1$ is included? find a fourth equation that leaves us with no solution.",matrices and gaussian elimination
"sketch these three lines and decide if the equations are solvable for the 3 by 2 system:
    \begin{align*}
    x + 2y &= 2 \\
    x - y &= 2 \\
    y &= 1.
    \end{align*}
    what happens if all right-hand sides are zero? is there any nonzero choice of right-hand sides that allows the three lines to intersect at the same point?",matrices and gaussian elimination
"find two points on the line of intersection of the three planes:
    \begin{align*}
    t &= 0 \\
    z &= 0 \\
    x + y + z + t &= 1
    \end{align*}
    in four-dimensional space.",matrices and gaussian elimination
"when $b = (2,5,7)$, find a solution $(u,v,w)$ to equation (4) different from the solution $(1,0,1)$ mentioned in the text.",matrices and gaussian elimination
"give two more right-hand sides in addition to $b = (2,5,7)$ for which equation (4) can be solved. give two more right-hand sides in addition to $b = (2,5,6)$ for which it cannot be solved.",matrices and gaussian elimination
"explain why the system
    \begin{align*}
    u + v + w &= 2 \\
    u + 2v + 3w &= 1 \\
    v + 2w &= 0
    \end{align*}
    is singular by finding a combination of the three equations that adds up to $0 = 1$. what value should replace the last zero on the right side to allow the equations to have solutions—and what is one of the solutions?",matrices and gaussian elimination
"the column picture for the previous exercise (singular system) is
    $$u \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} + v \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix} + w \begin{bmatrix} 1 \\ 3 \\ 2 \end{bmatrix} = b.$$
    show that the three columns on the left lie in the same plane by expressing the third column as a combination of the first two. what are all the solutions $(u,v,w)$ if $b$ is the zero vector $(0,0,0)$?",matrices and gaussian elimination
"(recommended) under what condition on $y_1$, $y_2$, $y_3$ do the points $(0,y_1)$, $(1,y_2)$, $(2,y_3)$ lie on a straight line?",matrices and gaussian elimination
"these equations are certain to have the solution $x = y = 0$. for which values of $a$ is there a whole line of solutions?
    \begin{align*}
    ax + 2y &= 0 \\
    2x + ay &= 0
    \end{align*}",matrices and gaussian elimination
"starting with $x + 4y = 7$, find the equation for the parallel line through $x = 0$, $y = 0$. find the equation of another line that meets the first at $x = 3$, $y = 1$.",matrices and gaussian elimination
draw the two pictures in two planes for the equations $x - 2y = 0$ and $x + y = 6$.,matrices and gaussian elimination
"for two linear equations in three unknowns $x$, $y$, $z$, the row picture will show (2 or 3) (lines or planes) in (two or three)-dimensional space. the column picture is in (two or three)-dimensional space. the solutions normally lie on a \_\_\_\_\_.",matrices and gaussian elimination
"for four linear equations in two unknowns $x$ and $y$, the row picture shows four \_\_\_\_\_. the column picture is in \_\_\_\_\-dimensional space. the equations have no solution unless the vector on the right-hand side is a combination of \_\_\_\_\_.",matrices and gaussian elimination
find a point with $z = 2$ on the intersection line of the planes $x + y + 3z = 6$ and $x - y + z = 4$. find the point with $z = 0$ and a third point halfway between.,matrices and gaussian elimination
"the first of these equations plus the second equals the third:
    \begin{align*}
    x + y + z &= 2 \\
    x + 2y + z &= 3 \\
    2x + 3y + 2z &= 5.
    \end{align*}
    the first two planes meet along a line. the third plane contains that line, because if $x$, $y$, $z$ satisfy the first two equations then they also \_\_\_\_\_. the equations have infinitely many solutions (the whole line $l$). find three solutions.",matrices and gaussian elimination
"move the third plane in problem 17 to a parallel plane $2x + 3y + 2z = 9$. now the three equations have no solution—why not? the first two planes meet along the line $l$, but the third plane doesn’t intersect that line.",matrices and gaussian elimination
"in problem 17, the columns are $(1,1,2)$, $(1,2,3)$, and $(1,1,2)$. this is a “singular case” because the third column is \_\_\_\_\_. find two combinations of the columns that give $b = (2,3,5)$. this is only possible for $b = (4,6,c)$ if $c = \_\_\_\_\_.",matrices and gaussian elimination
"normally, 4 “planes” in four-dimensional space meet at a \_\_\_\_\_. normally, 4 column vectors in four-dimensional space can combine to produce \(b$. what combination of $(1,0,0,0)$, $(1,1,0,0)$, $(1,1,1,0)$, $(1,1,1,1)$ produces $b = (3,3,3,2)$? what 4 equations for $x$, $y$, $z$, $t$ are you solving?",matrices and gaussian elimination
"when equation 1 is added to equation 2, which of these are changed: the planes in the row picture, the column picture, the coefficient matrix, the solution?",matrices and gaussian elimination
"if $(a,b)$ is a multiple of $(c,d)$ with $abcd \neq 0$, show that $(a,c)$ is a multiple of $(b,d)$. this is surprisingly important: call it a challenge question. you could use numbers first to see how $a$, $b$, $c$, and $d$ are related. the question will lead to: if 
    $$a = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$$
    has dependent rows then it has dependent columns.",matrices and gaussian elimination
"in these equations, the third column (multiplying $w$) is the same as the right side $b$. the column form of the equations immediately gives what solution for $(u,v,w)$?
    \begin{align*}
    6u + 7v + 8w &= 8 \\
    4u + 5v + 9w &= 9 \\
    2u - 2v + 7w &= 7.
    \end{align*}",matrices and gaussian elimination
"what multiple $k$ of equation 1 should be subtracted from equation 2?
    \begin{align*}
    2x + 3y &= 1 \\
    10x + 9y &= 11.
    \end{align*}
    after this elimination step, write down the upper triangular system and circle the two pivots. the numbers 1 and 11 have no influence on those pivots.",matrices and gaussian elimination
"solve the triangular system of problem 1 by back-substitution, $y$ before $x$. verify that $x$ times $(2,10)$ plus $y$ times $(3,9)$ equals $(1,11)$. if the right-hand side changes to $(4,44)$, what is the new solution?",matrices and gaussian elimination
"what multiple $k$ of equation 2 should be subtracted from equation 3?
    \begin{align*}
    2x - 4y &= 6 \\
    -x + 5y &= 0.
    \end{align*}
    after this elimination step, solve the triangular system. if the right-hand side changes to $(-6,0)$, what is the new solution?",matrices and gaussian elimination
"what multiple $k$ of equation 1 should be subtracted from equation 2?
    \begin{align*}
    ax + by &= f \\
    cx + dy &= g.
    \end{align*}
    the first pivot is $a$ (assumed nonzero). elimination produces what formula for the second pivot? what is $y$? the second pivot is missing when $ad = bc$.",matrices and gaussian elimination
"choose a right-hand side which gives no solution and another right-hand side which gives infinitely many solutions. what are two of those solutions?
    \begin{align*}
    3x + 2y &= 10 \\
    6x + 4y &= \_\_\_\_\_.
    \end{align*}",matrices and gaussian elimination
"choose a coefficient $b$ that makes this system singular. then choose a right-hand side $g$ that makes it solvable. find two solutions in that singular case.
    \begin{align*}
    2x + by &= 16 \\
    4x + 8y &= g.
    \end{align*}",matrices and gaussian elimination
"for which numbers $a$ does elimination break down (a) permanently, and (b) temporarily?
    \begin{align*}
    ax + 3y &= -3 \\
    4x + 6y &= 6.
    \end{align*}
    solve for $x$ and $y$ after fixing the second breakdown by a row exchange.",matrices and gaussian elimination
"for which three numbers $k$ does elimination break down? which is fixed by a row exchange? in each case, is the number of solutions $0$, $1$, or $\infty$?
    \begin{align*}
    kx + 3y &= 6 \\
    3x + ky &= -6.
    \end{align*}",matrices and gaussian elimination
"what test on $b_1$ and $b_2$ decides whether these two equations allow a solution? how many solutions will they have? draw the column picture.
    \begin{align*}
    3x - 2y &= b_1 \\
    6x - 4y &= b_2.
    \end{align*}",matrices and gaussian elimination
"reduce this system to upper triangular form by two row operations:
    \begin{align*}
    2x + 3y + z &= 8 \\
    4x + 7y + 5z &= 20 \\
    -2y + 2z &= 0.
    \end{align*}
    circle the pivots. solve by back-substitution for $z$, $y$, $x$.",matrices and gaussian elimination
"apply elimination (circle the pivots) and back-substitution to solve:
    \begin{align*}
    2x - 3y &= 3 \\
    4x - 5y + z &= 7 \\
    2x - y - 3z &= 5.
    \end{align*}
    list the three row operations: subtract $k$ times row from row.",matrices and gaussian elimination
"which number $d$ forces a row exchange, and what is the triangular system (not singular) for that $d$? which $d$ makes this system singular (no third pivot)?
    \begin{align*}
    2x + 5y + z &= 0 \\
    4x + dy + z &= 2 \\
    y - z &= 3.
    \end{align*}",matrices and gaussian elimination
"which number $b$ leads later to a row exchange? which $b$ leads to a missing pivot? in that singular case, find a nonzero solution $x$, $y$, $z$.
    \begin{align*}
    x + by &= 0 \\
    x - 2y - z &= 0 \\
    y + z &= 0.
    \end{align*}",matrices and gaussian elimination
"(a) construct a $3 \times 3$ system that needs two row exchanges to reach a triangular form and a solution.
    
    (b) construct a $3 \times 3$ system that needs a row exchange to keep going, but breaks down later.",matrices and gaussian elimination
"if rows 1 and 2 are the same, how far can you get with elimination (allowing row exchange)? if columns 1 and 2 are the same, which pivot is missing?
    \begin{align*}
    2x - y + z &= 0 \\
    2x - y + z &= 0 \\
    4x + y + z &= 2 \\
    2x + 2y + z &= 0 \\
    4x + 4y + z &= 0 \\
    6x + 6y + z &= 2.
    \end{align*}",matrices and gaussian elimination
"construct a $3 \times 3$ example that has 9 different coefficients on the left-hand side, but rows 2 and 3 become zero in elimination. how many solutions to your system with $b = (1,10,100)$ and how many with $b = (0,0,0)$?",matrices and gaussian elimination
"which number $q$ makes this system singular and which right-hand side $t$ gives it infinitely many solutions? find the solution that has $z = 1$.
    \begin{align*}
    x + 4y - 2z &= 1 \\
    x + 7y - 6z &= 6 \\
    3y + qz &= t.
    \end{align*}",matrices and gaussian elimination
"(recommended) it is impossible for a system of linear equations to have exactly two solutions. explain why.
    \begin{enumerate}",matrices and gaussian elimination
"if $(x,y,z)$ and $(x,y,z)$ are two solutions, what is another one?",matrices and gaussian elimination
"if 25 planes meet at two points, where else do they meet?",matrices and gaussian elimination
three planes can fail to have an intersection point when no two planes are parallel. the system is singular if row 3 of $a$ is a linear combination of the first two rows. find a third equation that can’t be solved if $x+y+z = 0$ and $x-2y-z = 1$.,matrices and gaussian elimination
"find the pivots and the solution for these four equations:
    \begin{align*}
    2x + y &= 0 \\
    x + 2y + z &= 0 \\
    y + 2z + t &= 0 \\
    z + 2t &= 5.
    \end{align*}",matrices and gaussian elimination
"if you extend problem 20 following the $1, 2, 1$ pattern or the $-1, 2, -1$ pattern, what is the fifth pivot? what is the $n$th pivot?",matrices and gaussian elimination
"apply elimination and back-substitution to solve:
    \begin{align*}
    2u + 3v &= 0 \\
    4u + 5v + w &= 3 \\
    2u - v - 3w &= 5.
    \end{align*}
    what are the pivots? list the three operations in which a multiple of one row is subtracted from another.",matrices and gaussian elimination
"for the system
    \begin{align*}
    u + v + w &= 2 \\
    u + 3v + 3w &= 0 \\
    u + 3v + 5w &= 2,
    \end{align*}
    what is the triangular system after forward elimination, and what is the solution?",matrices and gaussian elimination
"solve the system and find the pivots when
    \begin{align*}
    2u - v &= 0 \\
    -u + 2v - w &= 0 \\
    -v + 2w - z &= 0 \\
    -w + 2z &= 5.
    \end{align*}
    you may carry the right-hand side as a fifth column (and omit writing $u$, $v$, $w$, $z$ until the solution at the end).",matrices and gaussian elimination
"apply elimination to the system
    \begin{align*}
    u + v + w &= -2 \\
    3u + 3v - w &= 6 \\
    u - v + w &= -1.
    \end{align*}
    when a zero arises in the pivot position, exchange that equation for the one below it and proceed. what coefficient of $v$ in the third equation, in place of the present $-1$, would make it impossible to proceed—and force elimination to break down?",matrices and gaussian elimination
"solve by elimination the system of two equations
    \begin{align*}
    x - y &= 0 \\
    3x + 6y &= 18.
    \end{align*}
    draw a graph representing each equation as a straight line in the $x-y$ plane; the lines intersect at the solution. also, add one more line—the graph of the new second equation which arises after elimination.",matrices and gaussian elimination
"find three values of $ a $ for which elimination breaks down, temporarily or permanently, in the following system of equations:
    \begin{align*}
        au + u &= 1 \\
        4u + av &= 2.
    \end{align*}
    breakdown at the first step can be fixed by exchanging rows—but not breakdown at the last step.",matrices and gaussian elimination
"true or false:
    \begin{enumerate}",matrices and gaussian elimination
"if the third equation starts with a zero coefficient (it begins with $ 0u $), then no multiple of equation 1 will be subtracted from equation 3.",matrices and gaussian elimination
"if the third equation has zero as its second coefficient (it contains $ 0v $), then no multiple of equation 2 will be subtracted from equation 3.",matrices and gaussian elimination
"if the third equation contains $ 0u $ and $ 0v $, then no multiple of equation 1 or equation 2 will be subtracted from equation 3.",matrices and gaussian elimination
"(very optional) normally the multiplication of two complex numbers
    $$(a+ib)(c+id) = (ac−bd) +i(bc+ad)$$
    involves the four separate multiplications $ ac, bd, bc, $ and $ ad $. ignoring $ i $, can you compute $ ac−bd $ and $ bc+ad $ with only three multiplications? (you may do additions, such as forming $ a+b $ before multiplying, without any penalty.)",matrices and gaussian elimination
"use elimination to solve the following systems:
    \begin{align*}
        u + v + w &= 6 \\
        u + 2v + 2w &= 11 \\
        2u + 3v - 4w &= 3
    \end{align*}
    and
    \begin{align*}
        u + v + w &= 7 \\
        u + 2v + 2w &= 10 \\
        2u + 3v - 4w &= 3.
    \end{align*}",matrices and gaussian elimination
"for which three numbers $ a $ will elimination fail to give three pivots in the following system?
    $$ax + 2y + 3z = b_1 \\     ax + ay + 4z = b_2 \\     ax + ay + az = b_3.$$",matrices and gaussian elimination
"find experimentally the average size (absolute value) of the first, second, and third pivots for matlab’s $ \text{lu}(\text{rand}(3,3)) $. the average of the first pivot from $ \text{abs}(a(1,1)) $ should be 0.5.",matrices and gaussian elimination
"compute the products:
    $$\begin{bmatrix}         4 & 0 & 1 \\         0 & 1 & 0 \\         4 & 0 & 1     \end{bmatrix}     \begin{bmatrix}         3 \\         4 \\         5     \end{bmatrix}$$
    and
    $$\begin{bmatrix}         1 & 0 & 0 \\         0 & 1 & 0 \\         0 & 0 & 1     \end{bmatrix}     \begin{bmatrix}         5 \\         -2 \\         3     \end{bmatrix}$$
    and
    $$\begin{bmatrix}         2 & 0 \\         1 & 3     \end{bmatrix}     \begin{bmatrix}         1 \\         1     \end{bmatrix}.$$
    for the third one, draw the column vectors $ (2,1) $ and $ (0,3) $. multiplying by $ (1,1) $ just adds the vectors (do it graphically).",matrices and gaussian elimination
"working a column at a time, compute the products:
    $$\begin{bmatrix}         4 & 1 \\         5 & 1 \\         6 & 1     \end{bmatrix}     \begin{bmatrix}         1 \\         3     \end{bmatrix}$$
    and
    $$\begin{bmatrix}         1 & 2 & 3 \\         4 & 5 & 6 \\         7 & 8 & 9     \end{bmatrix}     \begin{bmatrix}         0 \\         1 \\         0     \end{bmatrix}$$
    and
    $$\begin{bmatrix}         4 & 3 \\         6 & 6 \\         8 & 9     \end{bmatrix}     \begin{bmatrix}         1 \\         2 \\         1 \\         3     \end{bmatrix}.$$",matrices and gaussian elimination
"find two inner products and a matrix product:
    $$\langle 1, -2, 7 \rangle     \begin{bmatrix}         1 \\         -2 \\         7     \end{bmatrix}$$
    and
    $$\langle 1, -2, 7 \rangle     \begin{bmatrix}         3 \\         5 \\         1     \end{bmatrix}$$
    and
    $$\begin{bmatrix}         1 \\         -2 \\         7     \end{bmatrix}     \langle 3, 5, 1 \rangle.$$
    the first gives the length of the vector (squared).",matrices and gaussian elimination
"if an $ m \times n $ matrix $ a $ multiplies an $ n $-dimensional vector $ x $, how many separate multiplications are involved? what if $ a $ multiplies an $ n \times p $ matrix $ b $?",matrices and gaussian elimination
"multiply $ ax $ to find a solution vector $ x $ to the system $ ax = \text{zero vector} $. can you find more solutions to $ ax = 0 $?
    $$ax =      \begin{bmatrix}         3 & -6 & 0 \\         0 & 2 & -2 \\         1 & -1 & -1     \end{bmatrix}     \begin{bmatrix}         2 \\         1 \\         1     \end{bmatrix}.$$",matrices and gaussian elimination
"write down the $ 2 \times 2 $ matrices $ a $ and $ b $ that have entries $ a_{ij} = i + j $ and $ b_{ij} = (-1)^{i+j} $.
    multiply them to find $ ab $ and $ ba $.",matrices and gaussian elimination
"give $ 3 \times 3 $ examples (not just the zero matrix) of:
    \begin{enumerate}",matrices and gaussian elimination
a diagonal matrix: $ a_{ij} = 0 $ if $ i \neq j $.,matrices and gaussian elimination
a symmetric matrix: $ a_{ij} = a_{ji} $ for all $ i $ and $ j $.,matrices and gaussian elimination
an upper triangular matrix: $ a_{ij} = 0 $ if $ i > j $.,matrices and gaussian elimination
a skew-symmetric matrix: $ a_{ij} = -a_{ji} $ for all $ i $ and $ j $.,matrices and gaussian elimination
"do these subroutines multiply $ ax $ by rows or columns? start with $ b(i) = 0 $:
    \begin{verbatim}
    do 10 i = 1, n
        do 10 j = 1, n
            b(i) = b(i) + a(i,j) * x(j)
    10 continue
    \end{verbatim}
    and
    \begin{verbatim}
    do 10 j = 1, n
        do 10 i = 1, n
            b(i) = b(i) + a(i,j) * x(j)
    10 continue
    \end{verbatim}
    the outputs $ bx = ax $ are the same. the second code is slightly more efficient in fortran and much more efficient on a vector machine (the first changes single entries $ b(i) $, the second can update whole vectors).",matrices and gaussian elimination
"if the entries of $ a $ are $ a_{ij} $, use subscript notation to write:
    \begin{enumerate}",matrices and gaussian elimination
the first pivot.,matrices and gaussian elimination
the multiplier $ m_{i1} $ of row 1 to be subtracted from row $ i $.,matrices and gaussian elimination
the new entry that replaces $ a_{ij} $ after that subtraction.,matrices and gaussian elimination
the second pivot.,matrices and gaussian elimination
"true or false? give a specific counterexample when false.
    \begin{enumerate}",matrices and gaussian elimination
"if columns 1 and 3 of $ b $ are the same, so are columns 1 and 3 of $ ab $.",matrices and gaussian elimination
"if rows 1 and 3 of $ b $ are the same, so are rows 1 and 3 of $ ab $.",matrices and gaussian elimination
"if rows 1 and 3 of $ a $ are the same, so are rows 1 and 3 of $ ab $.",matrices and gaussian elimination
$ (ab)^2 = a^2b^2 $.,matrices and gaussian elimination
"the first row of $ ab $ is a linear combination of all the rows of $ b $. what are the coefficients in this combination, and what is the first row of $ ab $, if
    $$a = \begin{bmatrix}         2 & 1 & 4 \\         0 & -1 & 1     \end{bmatrix}$$
    and
    $$b = \begin{bmatrix}         1 & 1 \\         0 & 1 \\         1 & 0     \end{bmatrix}?$$",matrices and gaussian elimination
"the product of two lower triangular matrices is again lower triangular (all its entries above the main diagonal are zero). confirm this with a $ 3 \times 3 $ example, and then explain how it follows from the laws of matrix multiplication.",matrices and gaussian elimination
"by trial and error find examples of $ 2 \times 2 $ matrices such that:
    \begin{enumerate}",matrices and gaussian elimination
"$ a^2 = -i $, $ a $ having only real entries.",matrices and gaussian elimination
"$ b^2 = 0 $, although $ b \neq 0 $.",matrices and gaussian elimination
"$ cd = -dc $, not allowing the case $ cd = 0 $.",matrices and gaussian elimination
"$ ef = 0 $, although no entries of $ e $ or $ f $ are zero.",matrices and gaussian elimination
"describe the rows of $ ea $ and the columns of $ ae $ if
    $$e = \begin{bmatrix}         1 & 7 \\         0 & 1     \end{bmatrix}.$$",matrices and gaussian elimination
"suppose $ a $ commutes with every $ 2 \times 2 $ matrix (i.e., $ ab = ba $), and in particular
    $$a = \begin{pmatrix}     a & b \\     c & d     \end{pmatrix}$$
    commutes with 
    $$b_1 = \begin{pmatrix}     1 & 0 \\     0 & 0     \end{pmatrix}$$
    and 
    $$b_2 = \begin{pmatrix}     0 & 1 \\     0 & 0     \end{pmatrix}.$$
    show that $ a = d $ and $ b = c = 0 $. if $ ab = ba $ for all matrices $ b $, then $ a $ is a multiple of the identity.",matrices and gaussian elimination
"let $ x $ be the column vector $ (1, 0, \ldots, 0)^t $. show that the rule $ (ab)x = a(bx) $ forces the first column of $ ab $ to equal $ a $ times the first column of $ b $.",matrices and gaussian elimination
"which of the following matrices are guaranteed to equal $ (a+b)^2 $?
    \begin{itemize}",matrices and gaussian elimination
$ a^2 + 2ab + b^2 $,matrices and gaussian elimination
$ a(a+b) + b(a+b) $,matrices and gaussian elimination
$ (a+b)(b+a) $,matrices and gaussian elimination
"$ a^2 + ab + ba + b^2 $
    \end{itemize}",matrices and gaussian elimination
"if $ a $ and $ b $ are $ n \times n $ matrices with all entries equal to 1, find $ (ab)_{ij} $. summation notation turns the product $ ab $, and the law $ (ab)c = a(bc) $, into
    $$(ab)_{ij} = \sum_k a_{ik} b_{kj} \quad \text{and} \quad \sum_j \left( \sum_k a_{ik} b_{kj} \right) c_{jl} = \sum_k a_{ik} \left( \sum_j b_{kj} c_{jl} \right).$$
    compute both sides if $ c $ is also $ n \times n $, with every $ c_{jl} = 2 $.",matrices and gaussian elimination
"a fourth way to multiply matrices is columns of $ a $ times rows of $ b $:
    $$ab = (\text{column 1})(\text{row 1}) + \cdots + (\text{column } n)(\text{row } n) = \text{sum of simple matrices}.$$
    give a $ 2 \times 2 $ example of this important rule for matrix multiplication.",matrices and gaussian elimination
"the matrix that rotates the $ x-y $ plane by an angle $ \theta $ is
    $$a(\theta) = \begin{pmatrix}     \cos \theta & -\sin \theta \\     \sin \theta & \cos \theta     \end{pmatrix}.$$
    verify that $ a(\theta_1)a(\theta_2) = a(\theta_1 + \theta_2) $ from the identities for $ \cos(\theta_1 + \theta_2) $ and $ \sin(\theta_1 + \theta_2) $. what is $ a(\theta) $ times $ a(-\theta) $?",matrices and gaussian elimination
"find the powers $ a^2, a^3 $ ($ a^2 \times a $), $ b^2, b^3, c^2, c^3 $. what are $ a_k, b_k, $ and $ c_k $?
    \[
    a = \begin{pmatrix}
    \frac{1}{2} & \frac{1}{2} \\
    \frac{1}{2} & \frac{1}{2}
    \end{pmatrix}, \quad
    b = \begin{pmatrix}
    1 & 0 \\
    0 & -1
    \end{pmatrix}, \quad
    c = ab = \begin{pmatrix}       
    \frac{1}{2} & -\frac{1}{2} \\
    \frac{1}{2} & -\frac{1}{2}
    \end{pmatrix}.
    \]",matrices and gaussian elimination
"write down the $3 \times 3$ matrices that produce these elimination steps:
    \begin{enumerate}",matrices and gaussian elimination
$e_{21}$ subtracts 5 times row 1 from row 2.,matrices and gaussian elimination
$e_{32}$ subtracts $-7$ times row 2 from row 3.,matrices and gaussian elimination
"$p$ exchanges rows 1 and 2, then rows 2 and 3.",matrices and gaussian elimination
"in problem 22, applying $e_{21}$ and then $e_{32}$ to the column $b = (1,0,0)$ gives $e_{32}e_{21}b =$ . applying $e_{32}$ before $e_{21}$ gives $e_{21}e_{32}b =$ . when $e_{32}$ comes first, row \_\_\_ feels no effect from row \_\_\_.",matrices and gaussian elimination
"which three matrices $e_{21}$, $e_{31}$, $e_{32}$ put $a$ into triangular form $u$?
    \[
    a = \begin{bmatrix} 1 & 1 & 0 \\ 4 & 6 & 1 \\ -2 & 2 & 0 \end{bmatrix}
    \]
    and $e_{32}e_{31}e_{21}a = u$. multiply those $e$’s to get one matrix $m$ that does elimination: $ma = u$.",matrices and gaussian elimination
"suppose $a_{33} = 7$ and the third pivot is 5. if you change $a_{33}$ to 11, the third pivot is \_\_\_. if you change $a_{33}$ to \_\_\_, there is zero in the pivot position.",matrices and gaussian elimination
"if every column of $a$ is a multiple of $(1,1,1)$, then $ax$ is always a multiple of $(1,1,1)$. do a $3 \times 3$ example. how many pivots are produced by elimination?",matrices and gaussian elimination
"what matrix $e_{31}$ subtracts 7 times row 1 from row 3? to reverse that step, $r_{31}$ should \_\_\_ 7 times row \_\_\_ to row \_\_\_. multiply $e_{31}$ by $r_{31}$.",matrices and gaussian elimination
\begin{enumerate},matrices and gaussian elimination
$e_{21}$ subtracts row 1 from row 2 and then $p_{23}$ exchanges rows 2 and 3. what matrix $m = p_{23}e_{21}$ does both steps at once?,matrices and gaussian elimination
$p_{23}$ exchanges rows 2 and 3 and then $e_{31}$ subtracts row 1 from row 3. what matrix $m = e_{31}p_{23}$ does both steps at once? explain why the $m$’s are the same but the $e$’s are different.,matrices and gaussian elimination
what $3 \times 3$ matrix $e_{13}$ will add row 3 to row 1?,matrices and gaussian elimination
what matrix adds row 1 to row 3 and at the same time adds row 3 to row 1?,matrices and gaussian elimination
what matrix adds row 1 to row 3 and then adds row 3 to row 1?,matrices and gaussian elimination
"multiply these matrices:
    \[
    \begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \end{bmatrix}
    \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}
    \begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \end{bmatrix}
    \]
    and
    \[
    \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ -1 & 0 & 1 \end{bmatrix}
    \begin{bmatrix} 1 & 2 & 3 \\ 1 & 3 & 1 \\ 1 & 4 & 0 \end{bmatrix}
    \]",matrices and gaussian elimination
"this $4 \times 4$ matrix needs which elimination matrices $e_{21}$, $e_{32}$, and $e_{43}$?
    \[
    a = \begin{bmatrix} 2 & -1 & 0 & 0 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ 0 & 0 & -1 & 2 \end{bmatrix}
    \]",matrices and gaussian elimination
"write these ancient problems in a 2 by 2 matrix form \(ax = b\) and solve them:
    \begin{enumerate}",matrices and gaussian elimination
\(x\) is twice as old as \(y\) and their ages add to 39.,matrices and gaussian elimination
"\((x, y) = (2, 5)\) and \((3, 7)\) lie on the line \(y = mx + c\). find \(m\) and \(c\).",matrices and gaussian elimination
"the parabola \(y = a + bx + cx^2\) goes through the points \((x, y) = (1, 4)\), \((2, 8)\), and \((3, 14)\). find and solve a matrix equation for the unknowns \((a, b, c)\).",matrices and gaussian elimination
"multiply these matrices in the orders \(ef\) and \(fe\) and \(e^2\):
    \[
    e = \begin{bmatrix} 
    1 & 0 & 0 \\
    a & 1 & 0 \\
    b & 0 & 1
    \end{bmatrix}, \quad
    f = \begin{bmatrix} 
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & c & 1
    \end{bmatrix}
    \]",matrices and gaussian elimination
"suppose all columns of \(b\) are the same. then all columns of \(eb\) are the same, because each one is \(e\) times \(\dots\).",matrices and gaussian elimination
suppose all rows of \(b\) are \([1 \ 2 \ 4]\). show by example that all rows of \(eb\) are not \([1 \ 2 \ 4]\). it is true that those rows are \(\dots\),matrices and gaussian elimination
"if \(e\) adds row 1 to row 2 and \(f\) adds row 2 to row 1, does \(ef\) equal \(fe\)?",matrices and gaussian elimination
"the first component of $ax$ is $\sum a_{1j}x_j = a_{11}x_1 + \dots + a_{1n}x_n$. write formulas for:
    \begin{enumerate}",matrices and gaussian elimination
the third component of $ax$.,matrices and gaussian elimination
"the $(1,1)$ entry of $a^2$.",matrices and gaussian elimination
"if $ab = i$ and $bc = i$, use the associative law to prove that $a = c$.",matrices and gaussian elimination
"let $a$ be a $3 \times 5$ matrix, $b$ a $5 \times 3$ matrix, $c$ a $5 \times 1$ matrix, and $d$ a $3 \times 1$ matrix, where all entries are $1$. determine which of the following matrix operations are allowed and compute the results:
    \begin{enumerate}",matrices and gaussian elimination
$ba$,matrices and gaussian elimination
$ab$,matrices and gaussian elimination
$abd$,matrices and gaussian elimination
$dba$,matrices and gaussian elimination
$a(b+c)$,matrices and gaussian elimination
"what rows or columns or matrices do you multiply to find:
    \begin{enumerate}",matrices and gaussian elimination
the third column of $ab$?,matrices and gaussian elimination
the first row of $ab$?,matrices and gaussian elimination
"the entry in row 3, column 4 of $ab$?",matrices and gaussian elimination
"the entry in row 1, column 1 of $cde$?",matrices and gaussian elimination
"(for $3 \times 3$ matrices) choose the only $b$ such that for every matrix $a$:
    \begin{enumerate}",matrices and gaussian elimination
$ba = 4a$.,matrices and gaussian elimination
$ba = 4b$.,matrices and gaussian elimination
$ba$ has rows 1 and 3 of $a$ reversed and row 2 unchanged.,matrices and gaussian elimination
all rows of $ba$ are the same as row 1 of $a$.,matrices and gaussian elimination
"true or false?
    \begin{enumerate}",matrices and gaussian elimination
if $a^2$ is defined then $a$ is necessarily square.,matrices and gaussian elimination
if $ab$ and $ba$ are defined then $a$ and $b$ are square.,matrices and gaussian elimination
if $ab$ and $ba$ are defined then $ab$ and $ba$ are square.,matrices and gaussian elimination
if $ab = b$ then $a = i$.,matrices and gaussian elimination
"if $a$ is $m \times n$, how many separate multiplications are involved when:
    \begin{enumerate}",matrices and gaussian elimination
$a$ multiplies a vector $x$ with $n$ components?,matrices and gaussian elimination
$a$ multiplies an $n \times p$ matrix $b$? then $ab$ is $m \times p$.,matrices and gaussian elimination
$a$ multiplies itself to produce $a^2$? here $m = n$.,matrices and gaussian elimination
"to prove that $(ab)c = a(bc)$, use the column vectors $b_1, ..., b_n$ of $b$. first, suppose that $c$ has only one column $c$ with entries $c_1, ..., c_n$:
    \begin{itemize}",matrices and gaussian elimination
"$ab$ has columns $ab_1, ..., ab_n$, and $bc$ has one column $c_1b_1 + \cdots + c_n b_n$.",matrices and gaussian elimination
"then $(ab)c = c_1 ab_1 + \cdots + c_n ab_n$ equals $a(c_1 b_1 + \cdots + c_n b_n) = a(bc)$.
    \end{itemize}
    linearity gives equality of those two sums, and $(ab)c = a(bc)$. the same is true for all other columns of $c$. therefore, $(ab)c = a(bc)$.",matrices and gaussian elimination
"multiply $ab$ using columns times rows:
    \[
    ab = \begin{bmatrix} 1 & 0 \\ 2 & 4 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} 3 & 3 & 0 \\ 1 & 2 & 1 \end{bmatrix}
    \]
    compute the result.",matrices and gaussian elimination
"block multiplication separates matrices into blocks (submatrices). if their shapes make block multiplication possible, then it is allowed. replace these $x$'s by numbers and confirm that block multiplication succeeds:
    \[
    \begin{bmatrix} a & b \end{bmatrix} \begin{bmatrix} c \\ d \end{bmatrix} = ac + bd
    \]
    and 
    \[
    \begin{bmatrix} x & x & x \\ x & x & x \\ x & x & x \end{bmatrix} \begin{bmatrix} x & x & x \\ x & x & x \\ x & x & x \end{bmatrix}
    \]",matrices and gaussian elimination
"draw the cuts in $a$ and $b$ and $ab$ to show how each of the four multiplication rules is really a block multiplication to find $ab$:
    \begin{enumerate}",matrices and gaussian elimination
matrix $a$ times columns of $b$.,matrices and gaussian elimination
rows of $a$ times matrix $b$.,matrices and gaussian elimination
rows of $a$ times columns of $b$.,matrices and gaussian elimination
columns of $a$ times rows of $b$.,matrices and gaussian elimination
"block multiplication says that elimination on column 1 produces:
    \begin{equation}
        ea = \begin{bmatrix} 1 & 0 \\ -c/a & i \end{bmatrix} \begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} a & b \\ 0 & s \end{bmatrix}.
    \end{equation}",matrices and gaussian elimination
"elimination for a $2 \times 2$ block matrix: when $a^{-1}a = i$, multiply the first block row by $ca^{-1}$ and subtract from the second row to find the \textit{schur complement} $s$:
    \begin{equation}
        \begin{bmatrix} i & 0 \\ -ca^{-1} & i \end{bmatrix} \begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} a & b \\ 0 & s \end{bmatrix}.
    \end{equation}",matrices and gaussian elimination
"with $i^2 = -1$, the product $(a + ib)(x + iy)$ is given by:
    \begin{equation}
        ax + ibx + iay - by.
    \end{equation}
    use blocks to separate the real part from the imaginary part that multiplies $i$:
    \begin{equation}
        \begin{bmatrix} a & -b \\ ? & ? \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} ax - by \\ ? \end{bmatrix}.
    \end{equation}",matrices and gaussian elimination
"suppose you solve $ax = b$ for three special right-hand sides $b$:
    \begin{align*}
        ax_1 &= \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, & ax_2 &= \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, & ax_3 &= \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}.
    \end{align*}
    if the solutions $x_1, x_2, x_3$ are the columns of a matrix $x$, what is $ax$?",matrices and gaussian elimination
"if the three solutions in question 51 are:
    \begin{align*}
        x_1 &= (1,1,1), & x_2 &= (0,1,1), & x_3 &= (0,0,1),
    \end{align*}
    solve $ax = b$ when $b = (3,5,8)$. \textbf{challenge problem}: what is $a$?",matrices and gaussian elimination
"find all matrices \( a = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \) that satisfy 
    \[
    a \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} a.
    \]",matrices and gaussian elimination
"if you multiply a northwest matrix \( a \) and a southeast matrix \( b \), what type of matrices are \( ab \) and \( ba \)? 
    
    \textit{note: “northwest” and “southeast” mean zeros below and above the antidiagonal going from \((1,n)\) to \((n,1)\).}",matrices and gaussian elimination
"write the equation \( 2x+3y+z+5t = 8 \) as a matrix \( a \) (how many rows?) multiplying the column vector \( (x,y,z,t) \) to produce \( b \). 
    
    \textit{the solutions fill a plane in four-dimensional space. the plane is three-dimensional with no 4d volume.}",matrices and gaussian elimination
"what \( 2 \times 2 \) matrix \( p_1 \) projects the vector \( (x,y) \) onto the x-axis to produce \( (x,0) \)? what matrix \( p_2 \) projects onto the y-axis to produce \( (0,y) \)? 
    
    \textit{if you multiply \((5,7)\) by \( p_1 \) and then multiply by \( p_2 \), you get \(( )\) and \(( )\).}",matrices and gaussian elimination
"write the inner product of \( (1,4,5) \) and \( (x,y,z) \) as a matrix multiplication \( ax \). \( a \) has one row. 
    
    \textit{the solutions to \( ax = 0 \) lie on a perpendicular to the vector. the columns of \( a \) are only in -dimensional space.}",matrices and gaussian elimination
"in matlab notation, write the commands that define the matrix \( a \) and the column vectors \( x \) and \( b \). what command would test whether or not \( ax = b \)?
    \[
    a = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}, \quad x = \begin{bmatrix} 5 \\ -2 \end{bmatrix}, \quad b = \begin{bmatrix} 1 \\ 7 \end{bmatrix}.
    \]",matrices and gaussian elimination
"the matlab commands \texttt{a = eye(3)} and \texttt{v = [3:5]'} produce the \( 3 \times 3 \) identity matrix and the column vector \( (3,4,5) \). what are the outputs from \( a * v \) and \( v' * v \)? (computer not needed!) if you ask for \( v * a \), what happens?",matrices and gaussian elimination
"if you multiply the \( 4 \times 4 \) all-ones matrix \( a = \texttt{ones(4,4)} \) and the column \( v = \texttt{ones(4,1)} \), what is \( a * v \)? (computer not needed.) if you multiply \( b = \texttt{eye(4)} + \texttt{ones(4,4)} \) times \( w = \texttt{zeros(4,1)} + 2 * \texttt{ones(4,1)} \), what is \( b * w \)?",matrices and gaussian elimination
"invent a \( 3 \times 3 \) magic matrix \( m \) with entries \( 1,2,...,9 \). all rows and columns and diagonals add to 15. the first row could be \( 8, 3, 4 \). what is \( m \) times \( (1,1,1) \)? what is the row vector \( \begin{bmatrix} 1 & 1 & 1 \end{bmatrix} \) times \( m \)?",matrices and gaussian elimination
when is an upper triangular matrix nonsingular (a full set of pivots)?,matrices and gaussian elimination
"what multiple \( m \) of row 2 of \( a \) will elimination subtract from row 3 of \( a \)? use the factored form:
    \[
    a = \begin{bmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ 1 & 4 & 1 \end{bmatrix} 
    \begin{bmatrix} 5 & 7 & 8 \\ 0 & 2 & 3 \\ 0 & 0 & 6 \end{bmatrix}.
    \]
    what will be the pivots? will a row exchange be required?",matrices and gaussian elimination
"multiply the matrix \( l = e^{-1}f^{-1}g^{-1} \) in equation (6) by \( gfe \) in equation (3):
    \[
    \begin{bmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix} 
    \times 
    \begin{bmatrix} 1 & 0 & 0 \\ -2 & 1 & 0 \\ -1 & 1 & 1 \end{bmatrix}.
    \]
    multiply also in the opposite order. why are the answers what they are?",matrices and gaussian elimination
"apply elimination to produce the factors \( l \) and \( u \) for:
    \[
    a = \begin{bmatrix} 2 & 1 \\ 8 & 7 \end{bmatrix},
    \quad a = \begin{bmatrix} 3 & 1 & 1 \\ 1 & 3 & 1 \\ 1 & 1 & 3 \end{bmatrix},
    \quad a = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 4 & 4 \\ 1 & 4 & 8 \end{bmatrix}.
    \]",matrices and gaussian elimination
"factor \( a \) into \( lu \), and write down the upper triangular system \( ux = c \) which appears after elimination, for:
    \[
    ax = \begin{bmatrix} 2 & 3 & 3 \\ 0 & 5 & 7 \\ 6 & 9 & 8 \end{bmatrix} 
    \begin{bmatrix} u \\ v \\ w \end{bmatrix} = 
    \begin{bmatrix} 2 \\ 2 \\ 5 \end{bmatrix}.
    \]",matrices and gaussian elimination
"find \( e^2 \), \( e^8 \), and \( e^{-1} \) if:
    \[
    e = \begin{bmatrix} 1 & 0 \\ 6 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"find the products $fgh$ and $hgf$ if (with upper triangular zeros omitted):
    \[
    f = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 2 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}, \quad
    g = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 2 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}, \quad
    h = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 2 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"(second proof of $a = lu$) the third row of $u$ comes from the third row of $a$ by subtracting multiples of rows 1 and 2 (of $u$!):
    \[
    \text{row 3 of } u = \text{row 3 of } a - \ell_{31}(\text{row 1 of } u) - \ell_{32}(\text{row 2 of } u).
    \]
    \begin{enumerate}",matrices and gaussian elimination
why are rows of $u$ subtracted off and not rows of $a$?,matrices and gaussian elimination
"the equation above is the same as:
        \[
        \text{row 3 of } a = \ell_{31}(\text{row 1 of } u) + \ell_{32}(\text{row 2 of } u) + 1(\text{row 3 of } u).
        \]
        which rule for matrix multiplication makes this row 3 of $l$ times $u$?",matrices and gaussian elimination
"(a) under what conditions is the following product nonsingular?
    \[
    a = \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}
    \begin{bmatrix} d_1 & 0 & 0 \\ 0 & d_2 & 0 \\ 0 & 0 & d_3 \end{bmatrix}
    \begin{bmatrix} 1 & -1 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix}.
    \]
    (b) solve the system $ax = b$ starting with $lc = b$:
    \[
    \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}
    \begin{bmatrix} c_1 \\ c_2 \\ c_3 \end{bmatrix} =
    \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} = b.
    \]",matrices and gaussian elimination
"(a) why does it take approximately $\frac{n^2}{2}$ multiplication-subtraction steps to solve each of $lc = b$ and $ux = c$?
    (b) how many steps does elimination use in solving 10 systems with the same $60 \times 60$ coefficient matrix $a$?",matrices and gaussian elimination
"solve as two triangular systems, without multiplying $lu$ to find $a$:
    \[
    lux = \begin{bmatrix} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 0 & 1 \end{bmatrix}
    \begin{bmatrix} 2 & 4 & 4 \\ 0 & 1 & 2 \\ 0 & 0 & 1 \end{bmatrix}
    \begin{bmatrix} u \\ v \\ w \end{bmatrix} =
    \begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix}.
    \]",matrices and gaussian elimination
"how could you factor $a$ into a product $ul$, upper triangular times lower triangular? would they be the same factors as in $a = lu$?",matrices and gaussian elimination
"solve by elimination, exchanging rows when necessary:
    \begin{align*}
        u + 4v + 2w &= -2 \\
        -2u - 8v + 3w &= 32 \\
        v + w &= 1
    \end{align*}
    and
    \begin{align*}
        v + w &= 0 \\
        u + v &= 0 \\
        u + v + w &= 1.
    \end{align*}
    which permutation matrices are required?",matrices and gaussian elimination
"write down all six of the $3 \times 3$ permutation matrices, including $p = i$. identify their inverses, which are also permutation matrices. the inverses satisfy $pp^{-1} = i$ and are on the same list.",matrices and gaussian elimination
"find the $pa = ldu$ factorizations (and check them) for
    \[
    a = \begin{bmatrix} 0 & 1 & 1 \\ 1 & 0 & 1 \\ 2 & 3 & 4 \end{bmatrix}
    \]
    and
    \[
    a = \begin{bmatrix} 1 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 1 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
find a $4 \times 4$ permutation matrix that requires three row exchanges to reach the end of elimination (which is $u = i$).,matrices and gaussian elimination
"the less familiar form $a = lpu$ exchanges rows only at the end:
    \[
    a = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 3 \\ 2 & 5 & 8 \end{bmatrix} 
    \rightarrow l^{-1} a = \begin{bmatrix} 1 & 1 & 1 \\ 0 & 0 & 2 \\ 0 & 3 & 6 \end{bmatrix} = pu = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\ 0 & 3 & 6 \\ 0 & 0 & 2 \end{bmatrix}.
    \]
    what is $l$ in this case? comparing with $pa = lu$ in box 1j, the multipliers now stay in place ($\ell_{21}$ is 1 and $\ell_{31}$ is 2 when $a = lpu$).",matrices and gaussian elimination
"decide whether the following systems are singular or nonsingular, and whether they have no solution, one solution, or infinitely many solutions:
    \begin{align*}
        v - w &= 2 \\
        u - v &= 2 \\
        u - w &= 2
    \end{align*}
    and
    \begin{align*}
        v - w &= 0 \\
        u - v &= 0 \\
        u - w &= 0
    \end{align*}
    and
    \begin{align*}
        v + w &= 1 \\
        u + v &= 1 \\
        u + w &= 1.
    \end{align*}",matrices and gaussian elimination
"which numbers \( a \), \( b \), \( c \) lead to row exchanges? which make the matrix singular?
    \[
    a = \begin{bmatrix}
    1 & 2 & 0 \\
    a & 8 & 3 \\
    0 & b & 5
    \end{bmatrix}
    \quad \text{and} \quad
    a = \begin{bmatrix}
    c & 2 \\
    6 & 4
    \end{bmatrix}
    \]",matrices and gaussian elimination
"forward elimination changes
    \[
    \begin{bmatrix}
    1 & 1 \\
    1 & 2
    \end{bmatrix}
    x = b \quad \text{to a triangular} \quad
    \begin{bmatrix}
    1 & 1 \\
    0 & 1
    \end{bmatrix}
    x = c:
    \]
    \[
    \text{step 1: } x + y = 5, \quad x + 2y = 7 \quad \longrightarrow \quad x + y = 5, \quad y = 2
    \]
    \[
    \begin{bmatrix}
    1 & 1 & 5 \\
    1 & 2 & 7
    \end{bmatrix}
    \quad \longrightarrow \quad
    \begin{bmatrix}
    1 & 1 & 5 \\
    0 & 1 & 2
    \end{bmatrix}
    \]
    that step subtracted \( 2 \times \) row 1 from row 2. the reverse step adds \( 2 \times \) row 1 to row 2. the matrix for that reverse step is \( l \). multiply this \( l \) times the triangular system
    \[
    \begin{bmatrix}
    1 & 1 \\
    0 & 1
    \end{bmatrix}
    x =
    \begin{bmatrix}
    5 \\
    2
    \end{bmatrix}
    \quad \text{to get} \quad
    \begin{bmatrix}
    \cdots
    \end{bmatrix}
    \]
    in letters, \( l \) multiplies \( ux = c \) to give \( \cdots \).",matrices and gaussian elimination
"(move to 3 by 3) forward elimination changes \( ax = b \) to a triangular \( ux = c \):
    \[
    x + y + z = 5
    \]
    \[
    x + 2y + 3z = 7
    \]
    \[
    x + 3y + 6z = 11
    \quad \longrightarrow \quad
    x + y + z = 5
    \]
    \[
    y + 2z = 2 \quad
    2y + 5z = 6
    \]
    \[
    x + y + z = 5
    \]
    \[
    y + 2z = 2 \quad
    z = 2
    \]
    the equation \( z = 2 \) in \( ux = c \) comes from the original \( x + 3y + 6z = 11 \) in \( ax = b \) by subtracting \( 3 \times \) equation 1 and \( 2 \times \) equation 2.
    reverse that to recover:
    \[
    [1, 3, 6, 11] \quad \text{in} \quad [a, b]
    \quad \text{from the final} \quad
    [1, 1, 1, 5], \quad [0, 1, 2, 2], \quad [0, 0, 1, 2]
    \quad \text{in} \quad [u, c]
    \]
    row 3 of \( a, b \) is:
    \[
    (\lambda_{31} \, \text{row 1} + \lambda_{32} \, \text{row 2} + \text{row 3}) \quad \text{of} \quad [u, c].
    \]
    in matrix notation, this is multiplication by \( l \). so \( a = lu \) and \( b = lc \).",matrices and gaussian elimination
"what are the 3 by 3 triangular systems \( lc = b \) and \( ux = c \) from problem 21? check that \( c = (5, 2, 2) \) solves the first one. which \( x \) solves the second one?",matrices and gaussian elimination
"what two elimination matrices \( e_{21} \) and \( e_{32} \) put \( a \) into upper triangular form \( e_{32} e_{21} a = u \)? multiply by \( e_{31}^{-1} \) and \( e_{21}^{-1} \) to factor \( a \) into \( lu = e_{21}^{-1} e_{32}^{-1} u \):
    \[
    a = \begin{bmatrix}
    1 & 1 & 1 \\
    2 & 4 & 5 \\
    0 & 4 & 0
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"what three elimination matrices \( e_{21} \), \( e_{31} \), \( e_{32} \) put \( a \) into upper triangular form \( e_{32} e_{31} e_{21} a = u \)? multiply by \( e_{32}^{-1} \), \( e_{31}^{-1} \), and \( e_{21}^{-1} \) to factor \( a \) into \( lu \), where \( l = e_{21}^{-1} e_{31}^{-1} e_{32}^{-1} \). find \( l \) and \( u \):
    \[
    a = \begin{bmatrix}
    1 & 0 & 1 \\
    2 & 2 & 2 \\
    3 & 4 & 5
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"when zero appears in a pivot position, a = lu is not possible! (we need nonzero pivots \( d, f, i \) in \( u \).) show directly why these are both impossible:
    \[
    \begin{bmatrix}
    0 & 1 \\
    2 & 3
    \end{bmatrix}
    = 
    \begin{bmatrix}
    1 & 0 \\
    \ell & 1
    \end{bmatrix}
    \begin{bmatrix}
    d & e & g \\
    f & h & i
    \end{bmatrix}.
    \]
    \[
    a = \begin{bmatrix}
    1 & 1 & 0 \\
    1 & 1 & 2 \\
    1 & 2 & 1
    \end{bmatrix}
    = 
    \begin{bmatrix}
    1 & \ell & m \\
    1 & n & 1
    \end{bmatrix}
    \begin{bmatrix}
    d & e & g \\
    f & h & i
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"which number \( c \) leads to zero in the second pivot position? a row exchange is needed and \( a = lu \) is not possible. which \( c \) produces zero in the third pivot position? then a row exchange can't help and elimination fails:
    \[
    a = \begin{bmatrix}
    1 & c & 0 \\
    2 & 4 & 1 \\
    3 & 5 & 1
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"what are \( l \) and \( d \) for this matrix \( a \)? what is \( u \) in \( a = lu \) and what is the new \( u \) in \( a = ldu \)?
    \[
    a = \begin{bmatrix}
    2 & 4 & 8 \\
    0 & 3 & 9 \\
    0 & 0 & 7
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"a and b are symmetric across the diagonal (because \( 4 = 4 \)). find their triple factorizations \( ldu \) and say how \( u \) is related to \( l \) for these symmetric matrices:
    \[
    a = \begin{bmatrix}
    2 & 4 \\
    4 & 11
    \end{bmatrix},
    \quad
    b = \begin{bmatrix}
    1 & 4 & 0 \\
    4 & 12 & 4 \\
    0 & 4 & 0
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"(recommended) compute \( l \) and \( u \) for the symmetric matrix
    \[
    a = \begin{bmatrix}
    a & a & a & a \\
    a & b & b & b \\
    a & b & c & c \\
    a & b & c & d
    \end{bmatrix}.
    \]
    find four conditions on \( a, b, c, d \) to get \( a = lu \) with four pivots.",matrices and gaussian elimination
"find \( l \) and \( u \) for the nonsymmetric matrix
    \[
    a = \begin{bmatrix}
    a & r & r & r \\
    a & b & s & s \\
    a & b & c & t \\
    a & b & c & d
    \end{bmatrix}.
    \]
    find the four conditions on \( a, b, c, d, r, s, t \) to get \( a = lu \) with four pivots.",matrices and gaussian elimination
"tridiagonal matrices have zero entries except on the main diagonal and the two adjacent diagonals. factor these into \( a = lu \) and \( a = ldv \):
    \[
    a = \begin{bmatrix}
    1 & 1 & 0 \\
    1 & 2 & 1 \\
    0 & 1 & 2
    \end{bmatrix},
    \quad
    a = \begin{bmatrix}
    a & a & 0 \\
    a & a+b & b \\
    0 & b & b+c
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"solve the triangular system \( lc = b \) to find \( c \). then solve \( ux = c \) to find \( x \):
    \[
    l = \begin{bmatrix}
    1 & 0 \\
    4 & 1
    \end{bmatrix},
    \quad
    u = \begin{bmatrix}
    2 & 4 \\
    0 & 1
    \end{bmatrix},
    \quad
    b = \begin{bmatrix}
    2 \\
    11
    \end{bmatrix}.
    \]
    for safety, find \( a = lu \) and solve \( ax = b \) as usual. circle \( c \) when you see it.",matrices and gaussian elimination
"solve \( lc = b \) to find \( c \). then solve \( ux = c \) to find \( x \). what was \( a \)?
    \[
    l = \begin{bmatrix}
    1 & 0 & 0 \\
    1 & 1 & 0 \\
    1 & 1 & 1
    \end{bmatrix},
    \quad
    u = \begin{bmatrix}
    1 & 1 & 1 \\
    0 & 1 & 1 \\
    0 & 0 & 1
    \end{bmatrix},
    \quad
    b = \begin{bmatrix}
    4 \\
    5 \\
    6
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"if \( a \) and \( b \) have nonzeros in the positions marked by \( x \), which zeros are still zero in their factors \( l \) and \( u \)?
    \[
    a = \begin{bmatrix}
    x & x & x & x \\
    x & x & x & 0 \\
    0 & x & x & x \\
    0 & 0 & x & x
    \end{bmatrix},
    \quad
    b = \begin{bmatrix}
    x & x & x & 0 \\
    x & x & 0 & x \\
    x & 0 & x & x \\
    0 & x & x & x
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"(important) if \( a \) has pivots 2, 7, 6 with no row exchanges, what are the pivots for the upper left 2 by 2 submatrix \( b \) (without row 3 and column 3)? explain why.",matrices and gaussian elimination
"starting from a 3 by 3 matrix \( a \) with pivots 2, 7, 6, add a fourth row and column to produce \( m \). what are the first three pivots for \( m \), and why? what fourth row and column are sure to produce 9 as the fourth pivot?",matrices and gaussian elimination
"use \( \text{chol}(\text{pascal}(5)) \) to find the triangular factors of matlab’s \( \text{pascal}(5) \). row exchanges in \( [l, u] = \text{lu}(\text{pascal}(5)) \) spoil pascal’s pattern!",matrices and gaussian elimination
"(review) for which numbers \( c \) is \( a = lu \) impossible—with three pivots?
    \[
    a = \begin{bmatrix}
    1 & 2 & 0 \\
    3 & c & 1 \\
    0 & 1 & 1
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"estimate the time difference for each new right-hand side \( b \) when \( n = 800 \). create \( a = \text{rand}(800) \) and \( b = \text{rand}(800,1) \) and \( b = \text{rand}(800,9) \). compare the times from
    \[
    \text{tic}; a \backslash b; \text{toc}
    \]
    and
    \[
    \text{tic}; a \backslash b; \text{toc}
    \]
    (which solves for 9 right sides).",matrices and gaussian elimination
"there are 12 “even” permutations of \( (1,2,3,4) \), with an even number of exchanges. two of them are \( (1,2,3,4) \) with no exchanges and \( (4,3,2,1) \) with two exchanges. list the other ten. instead of writing each 4 by 4 matrix, use the numbers 4, 3, 2, 1 to give the position of the 1 in each row.",matrices and gaussian elimination
"how many exchanges will permute \( (5,4,3,2,1) \) back to \( (1,2,3,4,5) \)? how many exchanges to change \( (6,5,4,3,2,1) \) to \( (1,2,3,4,5,6) \)? one is even and the other is odd. for \( (n, \ldots, 1) \) to \( (1, \ldots, n) \), show that \( n = 100 \) and 101 are even, \( n = 102 \) and 103 are odd.",matrices and gaussian elimination
"if \( p_1 \) and \( p_2 \) are permutation matrices, so is \( p_1p_2 \). this still has the rows of \( i \) in some order. give examples with \( p_1p_2 \neq p_2p_1 \) and \( p_3p_4 = p_4p_3 \).",matrices and gaussian elimination
"(try this question.) which permutation makes \( pa \) upper triangular? which permutations make \( p_1ap_2 \) lower triangular? multiplying \( a \) on the right by \( p_2 \) exchanges the rows of \( a \).
    \[
    a = \begin{bmatrix}
    0 & 0 & 6 \\
    1 & 2 & 3 \\
    0 & 4 & 5
    \end{bmatrix}
    \]",matrices and gaussian elimination
find a 3 by 3 permutation matrix with \( p^3 = i \) (but not \( p = i \)). find a 4 by 4 permutation \( p_b \) with \( p_b^4 \neq i \).,matrices and gaussian elimination
"if you take powers of a permutation, why is some \( p^k \) eventually equal to \( i \)? find a 5 by 5 permutation \( p \) so that the smallest power to equal \( i \) is \( p^6 \). (this is a challenge question. combine a 2 by 2 block with a 3 by 3 block.)",matrices and gaussian elimination
"the matrix \( p \) that multiplies \( (x,y,z) \) to give \( (z,x,y) \) is also a rotation matrix. find \( p \) and \( p^3 \). the rotation axis \( a = (1,1,1) \) doesn’t move, it equals \( pa \). what is the angle of rotation from \( v = (2,3,-5) \) to \( pv = (-5,2,3) \)?",matrices and gaussian elimination
"if \( p \) is any permutation matrix, find a nonzero vector \( x \) so that \( (i - p)x = 0 \). (this will mean that \( i - p \) has no inverse, and has determinant zero.)",matrices and gaussian elimination
"if \( p \) has 1s on the antidiagonal from \( (1,n) \) to \( (n,1) \), describe \( pap \).",matrices and gaussian elimination
"find the inverses (no special system required) of
    \[
    a_1 = \begin{bmatrix} 0 & 2 \\ 3 & 0 \end{bmatrix}, \quad a_2 = \begin{bmatrix} 2 & 0 \\ 4 & 2 \end{bmatrix}, \quad a_3 = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}.
    \]",matrices and gaussian elimination
"find the inverses of the permutation matrices
        \[
        p_1 = \begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \end{bmatrix} \quad \text{and} \quad p_2 = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}.
        \]",matrices and gaussian elimination
explain for permutations why \( p^{-1} \) is always the same as \( p^t \). show that the 1s are in the right places to give \( p p^t = i \).,matrices and gaussian elimination
"from \( ab = c \), find a formula for \( a^{-1} \). also find \( a^{-1} \) from \( pa = lu \).",matrices and gaussian elimination
"if \( a \) is invertible and \( ab = ac \), prove quickly that \( b = c \).",matrices and gaussian elimination
"if \( a = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \), find an example with \( ab = ac \) but \( b \neq c \).",matrices and gaussian elimination
"if the inverse of \( a^2 \) is \( b \), show that the inverse of \( a \) is \( ab \). (thus \( a \) is invertible whenever \( a^2 \) is invertible.)",matrices and gaussian elimination
"use the gauss-jordan method to invert
    \[
    a_1 = \begin{bmatrix} 1 & 0 & 0 \\ 1 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix}, \quad
    a_2 = \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}, \quad
    a_3 = \begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"find three 2 by 2 matrices, other than \( a = i \) and \( a = -i \), that are their own inverses: \( a^2 = i \).",matrices and gaussian elimination
"show that \( a = \begin{bmatrix} 1 & 1 \\ 3 & 3 \end{bmatrix} \) has no inverse by solving \( ax = 0 \), and by failing to solve
    \[
    \begin{bmatrix} 1 & 1 \\ 3 & 3 \end{bmatrix} \begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"suppose elimination fails because there is no pivot in column 3:
    \[
    a = \begin{bmatrix} 
    2 & 1 & 4 & 6 \\
    0 & 3 & 8 & 5 \\
    0 & 0 & 0 & 7 \\
    0 & 0 & 0 & 9
    \end{bmatrix}.
    \]
    show that \( a \) cannot be invertible. the third row of \( a^{-1} \), multiplying \( a \), should give the third row \( \begin{bmatrix} 0 & 0 & 1 & 0 \end{bmatrix} \) of \( a^{-1}a = i \). why is this impossible?",matrices and gaussian elimination
"find the inverses (in any legal way) of
    \[
    a_1 = \begin{bmatrix} 
    0 & 0 & 0 & 1 \\
    0 & 0 & 2 & 0 \\
    0 & 3 & 0 & 0 \\
    4 & 0 & 0 & 0
    \end{bmatrix}, \quad
    a_2 = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    -\frac{1}{2} & 1 & 0 & 0 \\
    0 & -\frac{2}{3} & 1 & 0 \\
    0 & 0 & -\frac{3}{4} & 1
    \end{bmatrix}, \quad
    a_3 = \begin{bmatrix}
    a & b & 0 & 0 \\
    c & d & 0 & 0 \\
    0 & 0 & a & b \\
    0 & 0 & c & d
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"give examples of \( a \) and \( b \) such that
    \begin{enumerate}",matrices and gaussian elimination
\( a + b \) is not invertible although \( a \) and \( b \) are invertible.,matrices and gaussian elimination
\( a + b \) is invertible although \( a \) and \( b \) are not invertible.,matrices and gaussian elimination
"all of \( a \), \( b \), and \( a + b \) are invertible.",matrices and gaussian elimination
"in the last case, use \( a^{-1}(a+b)b^{-1} = b^{-1} + a^{-1} \) to show that \( c = b^{-1} + a^{-1} \) is also invertible—and find a formula for \( c^{-1} \).",matrices and gaussian elimination
"if \( a \) is invertible, which properties of \( a \) remain true for \( a^{-1} \)?
    \begin{enumerate}",matrices and gaussian elimination
\( a \) is triangular.,matrices and gaussian elimination
\( a \) is symmetric.,matrices and gaussian elimination
\( a \) is tridiagonal.,matrices and gaussian elimination
all entries are whole numbers.,matrices and gaussian elimination
all entries are fractions (including numbers like \( \frac{3}{1} \)).,matrices and gaussian elimination
"if \( a = \begin{bmatrix} 3 \\ 1 \end{bmatrix} \) and \( b = \begin{bmatrix} 2 \\ 2 \end{bmatrix} \), compute \( a^t b \), \( b^t a \), \( ab^t \), and \( ba^t \).",matrices and gaussian elimination
"if \( b \) is square, show that \( a = b + b^t \) is always symmetric and \( k = b - b^t \) is always skew-symmetric—which means that \( k^t = -k \). find these matrices \( a \) and \( k \) when
    \[
    b = \begin{bmatrix} 1 & 3 \\ 1 & 1 \end{bmatrix},
    \]
    and write \( b \) as the sum of a symmetric matrix and a skew-symmetric matrix.",matrices and gaussian elimination
how many entries can be chosen independently in a symmetric matrix of order \( n \)?,matrices and gaussian elimination
how many entries can be chosen independently in a skew-symmetric matrix \( (k^t = -k) \) of order \( n \)? the diagonal of \( k \) is zero!,matrices and gaussian elimination
"if \( a = ldu \), with 1s on the diagonals of \( l \) and \( u \), what is the corresponding factorization of \( a^t \)? note that \( a \) and \( a^t \) (square matrices with no row exchanges) share the same pivots.",matrices and gaussian elimination
what triangular systems will give the solution to \( a^t y = b \)?,matrices and gaussian elimination
"if \( a = l_1 d_1 u_1 \) and \( a = l_2 d_2 u_2 \), prove that \( l_1 = l_2 \), \( d_1 = d_2 \), and \( u_1 = u_2 \). if \( a \) is invertible, the factorization is unique.
    \begin{enumerate}",matrices and gaussian elimination
"derive the equation \( l_1^{-1} l_2 d_2 = d_1 u_1 u_2^{-1} \), and explain why one side is lower triangular and the other side is upper triangular.",matrices and gaussian elimination
compare the main diagonals and then compare the off-diagonals.,matrices and gaussian elimination
"under what conditions on their entries are $a$ and $b$ invertible?
    \[
    a = \begin{bmatrix} 
    a & b & c \\
    d & e & 0 \\
    f & 0 & 0
    \end{bmatrix}, \quad
    b = \begin{bmatrix} 
    a & b & 0 \\
    c & d & 0 \\
    0 & 0 & e
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"compute the symmetric $ldl^t$ factorization of
    \[
    a = \begin{bmatrix} 
    1 & 3 & 5 \\
    3 & 12 & 18 \\
    5 & 18 & 30 
    \end{bmatrix}
    \]
    and 
    \[
    a = \begin{bmatrix} a & b \\ b & d \end{bmatrix}.
    \]",matrices and gaussian elimination
"find the inverse of
    \[
    a = \begin{bmatrix} 
    1 & 0 & 0 & 0 \\
    \frac{1}{4} & 1 & 0 & 0 \\
    \frac{1}{3} & \frac{1}{3} & 1 & 0 \\
    \frac{1}{2} & \frac{1}{2} & \frac{1}{2} & 1
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"if $a$ and $b$ are square matrices, show that $i - ba$ is invertible if $i - ab$ is invertible. start from
    \[
    b(i - ab) = (i - ba)b.
    \]",matrices and gaussian elimination
"find the inverses (directly or from the $2 \times 2$ formula) of $a$, $b$, $c$:
    \[
    a = \begin{bmatrix} 0 & 3 \\ 4 & 6 \end{bmatrix}, \quad
    b = \begin{bmatrix} a & b \\ b & 0 \end{bmatrix}, \quad
    c = \begin{bmatrix} 3 & 4 \\ 5 & 7 \end{bmatrix}.
    \]",matrices and gaussian elimination
"solve for the columns of $a^{-1}$:
    \[
    a^{-1} = \begin{bmatrix} x & t \\ y & z \end{bmatrix}.
    \]
    solve:
    \[
    \begin{bmatrix} 10 & 20 \\ 20 & 50 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix},
    \]
    \[
    \begin{bmatrix} 10 & 20 \\ 20 & 50 \end{bmatrix} \begin{bmatrix} t \\ z \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"show that \(
    \begin{bmatrix} 1 & 2 \\ 3 & 6 \end{bmatrix}
    \) has no inverse by trying to solve for the column \((x,y)\):
    \[
    \begin{bmatrix} 1 & 2 \\ 3 & 6 \end{bmatrix} \begin{bmatrix} x & t \\ y & z \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"if $a$ has row 1 + row 2 = row 3, show that $a$ is not invertible:
    \begin{enumerate}",matrices and gaussian elimination
"explain why $ax = (1,0,0)$ cannot have a solution.",matrices and gaussian elimination
"which right-hand sides $(b_1, b_2, b_3)$ might allow a solution to $ax = b$?",matrices and gaussian elimination
what happens to row 3 in elimination?,matrices and gaussian elimination
"if $a$ has column 1 + column 2 = column 3, show that $a$ is not invertible:
    \begin{enumerate}",matrices and gaussian elimination
find a nonzero solution $x$ to $ax = 0$. the matrix is $3 \times 3$.,matrices and gaussian elimination
elimination keeps column 1 + column 2 = column 3. explain why there is no third pivot.,matrices and gaussian elimination
suppose $a$ is invertible and you exchange its first two rows to reach $b$. is the new matrix $b$ invertible? how would you find $b^{-1}$ from $a^{-1}$?,matrices and gaussian elimination
"if the product $m = abc$ of three square matrices is invertible, then $a, b, c$ are invertible. find a formula for $b^{-1}$ that involves $m^{-1}$ and $a$ and $c$.",matrices and gaussian elimination
prove that a matrix with a column of zeros cannot have an inverse.,matrices and gaussian elimination
multiply $\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ times $\begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$. what is the inverse of each matrix if $ad \neq bc$?,matrices and gaussian elimination
"what matrix $e$ has the same effect as these three steps? subtract row 1 from row 2, subtract row 1 from row 3, then subtract row 2 from row 3.",matrices and gaussian elimination
"what single matrix $l$ has the same effect as these three reverse steps? add row 2 to row 3, add row 1 to row 3, then add row 1 to row 2.",matrices and gaussian elimination
"find the numbers $a$ and $b$ that give the inverse of $5 \cdot i_4 - \mathbf{1}_{4 \times 4}$:
    \[
    \begin{bmatrix}
        4 & -1 & -1 & -1 \\
        -1 & 4 & -1 & -1 \\
        -1 & -1 & 4 & -1 \\
        -1 & -1 & -1 & 4 
    \end{bmatrix}^{-1} = 
    \begin{bmatrix}
        a & b & b & b \\
        b & a & b & b \\
        b & b & a & b \\
        b & b & b & a
    \end{bmatrix}
    \]
    what are $a$ and $b$ in the inverse of $6 \cdot i_5 - \mathbf{1}_{5 \times 5}$?",matrices and gaussian elimination
show that $a = 4 \cdot i_4 - \mathbf{1}_{4 \times 4}$ is not invertible: multiply $a \cdot \mathbf{1}_{4 \times 1}$.,matrices and gaussian elimination
there are sixteen $2 \times 2$ matrices whose entries are 1s and 0s. how many of them are invertible?,matrices and gaussian elimination
"change $i$ into $a^{-1}$ as you reduce $a$ to $i$ (by row operations):
    \[
    \left[ a \mid i \right] = \begin{bmatrix} 1 & 3 & | & 1 & 0 \\ 2 & 7 & | & 0 & 1 \end{bmatrix}
    \]
    and
    \[
    \left[ a \mid i \right] = \begin{bmatrix} 1 & 4 & | & 1 & 0 \\ 3 & 9 & | & 0 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"follow the 3 by 3 text example but with plus signs in $a$. eliminate above and below the pivots to reduce $[a \mid i]$ to $[i \mid a^{-1}]$:
    \[
    \left[ a \mid i \right] = \begin{bmatrix}
        2 & 1 & 0 & | & 1 & 0 & 0 \\
        1 & 2 & 1 & | & 0 & 1 & 0 \\
        0 & 1 & 2 & | & 0 & 0 & 1
    \end{bmatrix}.
    \]",matrices and gaussian elimination
multiply $\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ by $\begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$. what is the inverse of each matrix if $ad \neq bc$?,matrices and gaussian elimination
"find the numbers $a$ and $b$ that give the inverse of $5 \cdot i_4 - \mathbf{1}_{4,4}$:
    \[
    \begin{bmatrix} 4 & -1 & -1 & -1 \\ -1 & 4 & -1 & -1 \\ -1 & -1 & 4 & -1 \\ -1 & -1 & -1 & 4 \end{bmatrix}^{-1} = 
    \begin{bmatrix} a & b & b & b \\ b & a & b & b \\ b & b & a & b \\ b & b & b & a \end{bmatrix}
    \]
    what are $a$ and $b$ in the inverse of $6 \cdot i_5 - \mathbf{1}_{5,5}$?",matrices and gaussian elimination
"show that $a = 4 \cdot i_4 - \mathbf{1}_{4,4}$ is not invertible: multiply $a \cdot \mathbf{1}_{4,1}$.",matrices and gaussian elimination
"change $i$ into $a^{-1}$ as you reduce $a$ to $i$ (by row operations):
    \[
    \begin{bmatrix} 1 & 3 & 1 & 0 \\ 2 & 7 & 0 & 1 \end{bmatrix}, \quad 
    \begin{bmatrix} 1 & 4 & 1 & 0 \\ 3 & 9 & 0 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"follow the 3 by 3 text example but with plus signs in $a$. eliminate above and below the pivots to reduce $[a \ i]$ to $[i \ a^{-1}]$:
    \[
    \begin{bmatrix} 2 & 1 & 0 & 1 & 0 & 0 \\ 1 & 2 & 1 & 0 & 1 & 0 \\ 0 & 1 & 2 & 0 & 0 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"use gauss-jordan elimination on $[a \ i]$ to solve $aa^{-1} = i$:
    \[
    \begin{bmatrix} 1 & a & b \\ 0 & 1 & c \\ 0 & 0 & 1 \end{bmatrix} 
    \begin{bmatrix} x_1 & x_2 & x_3 \end{bmatrix} = 
    \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"invert these matrices $a$ by the gauss-jordan method starting with $[a \ i]$:
    \[
    a = \begin{bmatrix} 1 & 0 & 0 \\ 2 & 1 & 3 \\ 0 & 0 & 1 \end{bmatrix}, \quad
    a = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 2 \\ 1 & 2 & 3 \end{bmatrix}.
    \]",matrices and gaussian elimination
"exchange rows and continue with gauss-jordan to find $a^{-1}$:
    \[
    \begin{bmatrix} 0 & 2 & 1 & 0 \\ 2 & 2 & 0 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"true or false (with a counterexample if false and a reason if true):
    \begin{enumerate}",matrices and gaussian elimination
a $4 \times 4$ matrix with a row of zeros is not invertible.,matrices and gaussian elimination
a matrix with $i$s down the main diagonal is invertible.,matrices and gaussian elimination
if $a$ is invertible then $a^{-1}$ is invertible.,matrices and gaussian elimination
if $a^t$ is invertible then $a$ is invertible.,matrices and gaussian elimination
"for which three numbers $c$ is this matrix not invertible, and why not?
    \[
    a = \begin{bmatrix} 2 & c & c \\ c & c & c \\ 8 & 7 & c \end{bmatrix}.
    \]",matrices and gaussian elimination
"prove that $a$ is invertible if $a \neq 0$ and $a \neq b$ (find the pivots and $a^{-1}$):
    \[
    a = \begin{bmatrix} a & b & b \\ a & a & b \\ a & a & a \end{bmatrix}.
    \]",matrices and gaussian elimination
"this matrix has a remarkable inverse. find $a^{-1}$ by elimination on $[a \ i]$. extend to a $5 \times 5$ “alternating matrix” and guess its inverse:
    \[
    a = \begin{bmatrix} 1 & -1 & 1 & -1 \\ 0 & 1 & -1 & 1 \\ 0 & 0 & 1 & -1 \\ 0 & 0 & 0 & 1 \end{bmatrix}.
    \]",matrices and gaussian elimination
"if $b$ has the columns of $a$ in reverse order, solve $(a - b)x = 0$ to show that $a - b$ is not invertible. an example will lead you to $x$.",matrices and gaussian elimination
"find and check the inverses (assuming they exist) of these block matrices:
    \begin{equation*}
    \begin{bmatrix} i & 0 \\ c & i \end{bmatrix}
    \begin{bmatrix} a & 0 \\ c & d \end{bmatrix}
    \begin{bmatrix} 0 & i \\ i & d \end{bmatrix}.
    \end{equation*}",matrices and gaussian elimination
"use \texttt{inv(s)} to invert matlab’s $4 \times 4$ symmetric matrix $s = \texttt{pascal(4)}$. create pascal’s lower triangular matrix $a = \texttt{abs(pascal(4,1))}$ and test $\texttt{inv(s) = inv(a') * inv(a)}$.",matrices and gaussian elimination
"if $a = \texttt{ones}(4,4)$ and $b = \texttt{rand}(4,1)$, how does matlab tell you that $ax = b$ has no solution? if $b = \texttt{ones}(4,1)$, which solution to $ax = b$ is found by $a \backslash b$?",matrices and gaussian elimination
"$m^{-1}$ shows the change in $a^{-1}$ (useful to know) when a matrix is subtracted from $a$. check part 3 by carefully multiplying $mm^{-1}$ to get $i$:
    
    \begin{itemize}",matrices and gaussian elimination
$m = i - uv^t$ and $m^{-1} = i + uv^t/(1 - v^t u)$.,matrices and gaussian elimination
$m = a - uv^t$ and $m^{-1} = a^{-1} + a^{-1} uv^t a^{-1}/(1 - v^t a^{-1} u)$.,matrices and gaussian elimination
$m = i - uv$ and $m^{-1} = i_n + u(i_m - vu)^{-1} v$.,matrices and gaussian elimination
"$m = a - uw^{-1}v$ and $m^{-1} = a^{-1} + a^{-1} u(w - v a^{-1} u)^{-1} v a^{-1}$.
    \end{itemize}",matrices and gaussian elimination
"find $a^t$, $a^{-1}$, $(a^{-1})^t$, and $(a^t)^{-1}$ for:
    \begin{equation*}
    a = \begin{bmatrix} 1 & 0 \\ 9 & 3 \end{bmatrix} \quad \text{and} \quad a = \begin{bmatrix} 1 & c \\ c & 0 \end{bmatrix}.
    \end{equation*}",matrices and gaussian elimination
"verify that $(ab)^t = b^t a^t$ but that these are different from $a^t b^t$:
    \begin{equation*}
    a = \begin{bmatrix} 1 & 0 \\ 2 & 1 \end{bmatrix}, \quad b = \begin{bmatrix} 1 & 3 \\ 0 & 1 \end{bmatrix}, \quad ab = \begin{bmatrix} 1 & 3 \\ 2 & 7 \end{bmatrix}.
    \end{equation*}
    in case $ab = ba$ (not generally true!), how do you prove that $b^t a^t = a^t b^t$?",matrices and gaussian elimination
\begin{itemize},matrices and gaussian elimination
show that $a^2 = 0$ is possible but $a^t a = 0$ is not possible (unless $a$ is the zero matrix).,matrices and gaussian elimination
"the row vector $x^t$ times $a$ times the column $y$ produces what number?
        \[ x^t a y = \begin{bmatrix} 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} = . \]",matrices and gaussian elimination
"this is the row $x^t a =$ times the column $y = (0,1,0)$.",matrices and gaussian elimination
this is the row $x^t = [0 \ 1]$ times the column $ay = .$,matrices and gaussian elimination
"when you transpose a block matrix $m = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$, the result is $m^t = $. test it. under what conditions on $a, b, c, d$ is the block matrix symmetric?",matrices and gaussian elimination
"explain why the inner product of $x$ and $y$ equals the inner product of $px$ and $py$. then $(px)^t(py) = x^t y$ says that $p^t p = i$ for any permutation. with $x = (1,2,3)$ and $y = (1,4,2)$, choose $p$ to show that $(px)^t y$ is not always equal to $x^t (p^t y)$.",matrices and gaussian elimination
"if $a = a^t$ and $b = b^t$, which of these matrices are certainly symmetric?
    \begin{enumerate}",matrices and gaussian elimination
$a^2 - b^2$,matrices and gaussian elimination
$(a+b)(a-b)$,matrices and gaussian elimination
$aba$,matrices and gaussian elimination
$abab$,matrices and gaussian elimination
"if $a = a^t$ needs a row exchange, then it also needs a column exchange to stay symmetric. in matrix language, $pa$ loses the symmetry of $a$ but recovers the symmetry.",matrices and gaussian elimination
"how many entries of $a$ can be chosen independently, if $a = a^t$ is $5 \times 5$?",matrices and gaussian elimination
how do $l$ and $d$ ($5 \times 5$) give the same number of choices in $ldl^t$?,matrices and gaussian elimination
"suppose $r$ is rectangular ($m \times n$) and $a$ is symmetric ($m \times m$).
    \begin{enumerate}",matrices and gaussian elimination
transpose $r^t a r$ to show its symmetry. what shape is this matrix?,matrices and gaussian elimination
show why $r^t r$ has no negative numbers on its diagonal.,matrices and gaussian elimination
"factor these symmetric matrices into $a = ldl^t$. the matrix $d$ is diagonal:
    \[ a = \begin{bmatrix} 1 & 3 \\ 3 & 2 \end{bmatrix}, \quad a = \begin{bmatrix} 1 & b \\ b & c \end{bmatrix}, \quad a = \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}. \]",matrices and gaussian elimination
"wires go between boston, chicago, and seattle. those cities are at voltages $x_b, x_c, x_s$. with unit resistances between cities, the three currents are in $y$:
    \[
    y = a x = \begin{bmatrix} y_{bc} \\ y_{cs} \\ y_{bs} \end{bmatrix} = \begin{bmatrix} 1 & -1 & 0 \\ 0 & 1 & -1 \\ 1 & 0 & -1 \end{bmatrix} \begin{bmatrix} x_b \\ x_c \\ x_s \end{bmatrix}.
    \]
    \begin{enumerate}",matrices and gaussian elimination
find the total currents $a^t y$ out of the three cities.,matrices and gaussian elimination
verify that $(ax)^t y$ agrees with $x^t (a^t y)$—six terms in both.,matrices and gaussian elimination
"producing $x_1$ trucks and $x_2$ planes requires $x_1 + 50x_2$ tons of steel, $40x_1 + 1000x_2$ pounds of rubber, and $2x_1+50x_2$ months of labor. if the unit costs $y_1, y_2, y_3$ are \$700 per ton, \$3 per pound, and \$3000 per month:
    \begin{itemize}",matrices and gaussian elimination
"what are the values of one truck and one plane? those are the components of $a^t y$.
    \end{itemize}",matrices and gaussian elimination
"$ax$ gives the amounts of steel, rubber, and labor to produce $x$ in problem 62. 
    \begin{itemize}",matrices and gaussian elimination
find $a$.,matrices and gaussian elimination
"then $(ax)^t y$ is the cost of inputs while $x^t (a^t y)$ is the value of the outputs.
    \end{itemize}",matrices and gaussian elimination
"a new factorization of $a$ into triangular and symmetric matrices:
    \begin{itemize}",matrices and gaussian elimination
start from $a = ldu$. then $a$ equals $l(u^t)^{-1} u^t d u$.,matrices and gaussian elimination
why is $l(u^t)^{-1}$ triangular? its diagonal is all 1s.,matrices and gaussian elimination
"why is $u^t d u$ symmetric?
    \end{itemize}",matrices and gaussian elimination
"a group of matrices includes $ab$ and $a^{-1}$ if it includes $a$ and $b$. “products and inverses stay in the group.”
    \begin{itemize}",matrices and gaussian elimination
"which of these sets are groups? lower triangular matrices $l$ with ones on the diagonal, symmetric matrices $s$, positive matrices $m$, diagonal invertible matrices $d$, permutation matrices $p$.",matrices and gaussian elimination
"invent two more matrix groups.
    \end{itemize}",matrices and gaussian elimination
"if every row of a $4 \times 4$ matrix contains the numbers $0, 1, 2, 3$ in some order:
    \begin{itemize}",matrices and gaussian elimination
can the matrix be symmetric?,matrices and gaussian elimination
"can it be invertible?
    \end{itemize}",matrices and gaussian elimination
prove that no reordering of rows and reordering of columns can transpose a typical matrix.,matrices and gaussian elimination
"a square northwest matrix $b$ is zero in the southeast corner, below the antidiagonal that connects $(1,n)$ to $(n,1)$. 
    \begin{itemize}",matrices and gaussian elimination
will $b^t$ and $b^2$ be northwest matrices?,matrices and gaussian elimination
will $b^{-1}$ be northwest or southeast?,matrices and gaussian elimination
"what is the shape of $bc$ if $b$ is northwest and $c$ is southeast?
    \end{itemize}",matrices and gaussian elimination
"compare \texttt{tic; inv(a); toc} for $a = \texttt{rand}(500)$ and $a = \texttt{rand}(1000)$. 
    \begin{itemize}",matrices and gaussian elimination
"the $n^3$ count suggests that computing time (measured by \texttt{tic; toc}) should multiply by 8 when $n$ is doubled. do you expect these random $a$ to be invertible?
    \end{itemize}",matrices and gaussian elimination
"given $i = \texttt{eye}(1000)$, $a = \texttt{rand}(1000)$, and $b = \texttt{triu}(a)$ producing a random triangular matrix $b$:
    \begin{itemize}",matrices and gaussian elimination
compare the times for \texttt{inv(b)} and \texttt{b\i}.,matrices and gaussian elimination
"compare also \texttt{inv(a)} and \texttt{a\i} for the full matrix $a$.
    \end{itemize}",matrices and gaussian elimination
"show that $l^{-1}$ has entries $j/i$ for $i \leq j$ for the given matrix $l$:
    \[
    l = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    -\frac{1}{2} & 1 & 0 & 0 \\
    0 & -\frac{2}{3} & 1 & 0 \\
    0 & 0 & -\frac{3}{4} & 1
    \end{bmatrix}
    \]
    
    \[
    l^{-1} = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    \frac{1}{2} & 1 & 0 & 0 \\
    \frac{1}{3} & \frac{2}{3} & 1 & 0 \\
    \frac{1}{4} & \frac{2}{4} & \frac{3}{4} & 1
    \end{bmatrix}
    \]
    
    \begin{itemize}",matrices and gaussian elimination
"test this pattern for \texttt{l = eye(5) - diag(1:5)\diag(1:4,-1)} and \texttt{inv(l)}.
    \end{itemize}",matrices and gaussian elimination
write out the $ldu = ldl^t$ factors of $a$ in equation (6) when $n = 4$. find the determinant as the product of the pivots in $d$.,matrices and gaussian elimination
"modify $a_{11}$ in equation (6) from $a_{11} = 2$ to $a_{11} = 1$, and find the $ldu$ factors of this new tridiagonal matrix.",matrices and gaussian elimination
"find the $5 \times 5$ matrix $a_0$ ($h = \frac{1}{6}$) that approximates 
    \[
        -\frac{d^2 u}{dx^2} = f(x), \quad \frac{du}{dx}(0) = \frac{du}{dx}(1) = 0,
    \]
    replacing these boundary conditions by $u_0 = u_1$ and $u_6 = u_5$. check that your $a_0$ times the constant vector $(c, c, c, c, c)$ yields zero; $a_0$ is singular. analogously, if $u(x)$ is a solution of the continuous problem, then so is $u(x) + c$.",matrices and gaussian elimination
"write down the $3 \times 3$ finite-difference matrix equation ($h = \frac{1}{4}$) for
    \[
        -\frac{d^2 u}{dx^2} + u = x, \quad u(0) = u(1) = 0.
    \]",matrices and gaussian elimination
"with $h = \frac{1}{4}$ and $f(x) = 4\pi^2 \sin 2\pi x$, the difference equation (5) is
    \[
        \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}
        \begin{bmatrix} u_1 \\ u_2 \\ u_3 \end{bmatrix} = \pi^2 4
        \begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}.
    \]
    solve for $u_1, u_2, u_3$ and find their error in comparison with the true solution $u = \sin 2\pi x$ at $x = \frac{1}{4}, x = \frac{1}{2},$ and $x = \frac{3}{4}$.",matrices and gaussian elimination
"what $5 \times 5$ system replaces (6) if the boundary conditions are changed to $u(0) = 1, u(1) = 0$?",matrices and gaussian elimination
"compute $h^{-1}$ in two ways for the $3 \times 3$ hilbert matrix:
    \begin{equation*}
        h = \begin{bmatrix}
            1 & \frac{1}{2} & \frac{1}{3} \\
            \frac{1}{2} & \frac{1}{3} & \frac{1}{4} \\
            \frac{1}{3} & \frac{1}{4} & \frac{1}{5}
        \end{bmatrix}
    \end{equation*}
    first, compute the exact inverse. second, round off each number to three significant figures and compute again. discuss the impact of rounding errors on the results.",matrices and gaussian elimination
"for the same matrix $h$, compare the right-hand sides of $hx = b$ when the solutions are $x = (1,1,1)$ and $x = (0.6,-3.6)$.",matrices and gaussian elimination
"solve $hx = b$ with $b = (1,0,\dots,0)^t$ for the $10 \times 10$ hilbert matrix with $h_{ij} = \frac{1}{i+j-1}$, using any computational method for solving linear systems. then, change one entry of $b$ by $0.0001$ and compare the solutions.",matrices and gaussian elimination
"compare the pivots in direct elimination to those with partial pivoting for the matrix:
    \begin{equation*}
        a = \begin{bmatrix} 0.001 & 0 \\ 1 & 1000 \end{bmatrix}.
    \end{equation*}
    discuss the effects of scaling on numerical stability.",matrices and gaussian elimination
"explain why partial pivoting produces multipliers $\ell_{ij}$ in $l$ that satisfy $|\ell_{ij}| \leq 1$. can you construct a $3 \times 3$ example where all $|a_{ij}| \leq 1$ but the last pivot is $4$? discuss why this is the worst case, considering how each entry is at most doubled when $|\ell_{ij}| \leq 1$.",matrices and gaussian elimination
"write down the 3 by 3 matrices with entries
        \[
        a_{ij} = i - j \quad \text{and} \quad b_{ij} = \frac{i}{j}.
        \]",matrices and gaussian elimination
"compute the products \(ab\), \(ba\), and \(a^2\).",matrices and gaussian elimination
"for the matrices 
    \[
    a = \begin{bmatrix} 1 & 0 \\ 2 & 1 \end{bmatrix} \quad \text{and} \quad b = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix},
    \]
    compute \(ab\), \(ba\), \(a^{-1}\), \(b^{-1}\), and \((ab)^{-1}\).",matrices and gaussian elimination
"find examples of 2 by 2 matrices with \(a_{12} = \frac{1}{2}\) for which:
    \begin{enumerate}",matrices and gaussian elimination
"\(a^2 = i\),",matrices and gaussian elimination
"\(a^{-1} = a^t\),",matrices and gaussian elimination
\(a^2 = a\).,matrices and gaussian elimination
"solve by elimination and back-substitution:
    \[
    u + w = 4 \quad
    u + v = 3 \quad
    u + v + w = 6
    \]
    and
    \[
    v + w = 0 \quad
    u + w = 0 \quad
    u + v = 6.
    \]",matrices and gaussian elimination
factor the preceding matrices into \(a = lu\) or \(pa = lu\).,matrices and gaussian elimination
there are sixteen 2 by 2 matrices whose entries are 1s and 0s. how many are invertible?,matrices and gaussian elimination
"(much harder!) if you put 1s and 0s at random into the entries of a 10 by 10 matrix, is it more likely to be invertible or singular?",matrices and gaussian elimination
there are sixteen 2 by 2 matrices whose entries are 1s and −1s. how many are invertible?,matrices and gaussian elimination
"how are the rows of \(ea\) related to the rows of \(a\) in the following cases?
    \[
    e = \begin{bmatrix} 
    1 & 0 & 0 \\
    0 & 2 & 0 \\
    4 & 0 & 1 
    \end{bmatrix}
    \]
    or
    \[
    e = \begin{bmatrix} 
    1 & 1 & 1 \\
    0 & 0 & 0 
    \end{bmatrix}
    \]
    or
    \[
    e = \begin{bmatrix} 
    0 & 0 & 1 \\
    0 & 1 & 0 \\
    1 & 0 & 0 
    \end{bmatrix}.
    \]",matrices and gaussian elimination
write down a 2 by 2 system with infinitely many solutions.,matrices and gaussian elimination
"find inverses if they exist, by inspection or by gauss-jordan:
    \[
    a = \begin{bmatrix} 
    1 & 0 & 1 \\
    1 & 1 & 0 \\
    0 & 1 & 1 
    \end{bmatrix}, \quad
    a = \begin{bmatrix} 
    2 & 1 & 0 \\
    1 & 2 & 1 \\
    0 & 1 & 2 
    \end{bmatrix}, \quad
    a = \begin{bmatrix} 
    1 & 1 & -2 \\
    1 & -2 & 1 \\
    -2 & 1 & 1 
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"if \(e\) is 2 by 2 and it adds the first equation to the second, what are \(e^2\), \(e^8\), and \(8e\)?",matrices and gaussian elimination
"true or false, with reason if true or counterexample if false:
    \begin{enumerate}",matrices and gaussian elimination
"if \(a\) is invertible and its rows are in reverse order in \(b\), then \(b\) is invertible.",matrices and gaussian elimination
if \(a\) and \(b\) are symmetric then \(ab\) is symmetric.,matrices and gaussian elimination
if \(a\) and \(b\) are invertible then \(ba\) is invertible.,matrices and gaussian elimination
every nonsingular matrix can be factored into the product \(a = lu\) of a lower triangular \(l\) and an upper triangular \(u\).,matrices and gaussian elimination
"solve \(ax = b\) by solving the triangular systems \(lc = b\) and \(ux = c\):
    \[
    a = lu = \begin{bmatrix} 
    1 & 0 & 0 \\
    4 & 1 & 0 \\
    1 & 0 & 1 
    \end{bmatrix} 
    \begin{bmatrix} 
    2 & 2 & 4 \\
    0 & 1 & 3 \\
    0 & 0 & 1 
    \end{bmatrix}, \quad 
    b = \begin{bmatrix} 
    0 \\
    0 \\
    1 
    \end{bmatrix}.
    \]
    what part of \(a^{-1}\) have you found, with this particular \(b\)?",matrices and gaussian elimination
"if possible, find 3 by 3 matrices \(b\) such that:
    \begin{enumerate}",matrices and gaussian elimination
\(ba = 2a\) for every \(a\).,matrices and gaussian elimination
\(ba = 2b\) for every \(a\).,matrices and gaussian elimination
\(ba\) has the first and last rows of \(a\) reversed.,matrices and gaussian elimination
\(ba\) has the first and last columns of \(a\) reversed.,matrices and gaussian elimination
"find the value for \(c\) in the following \(n\) by \(n\) inverse:
    \[
    a = \begin{bmatrix} 
    n & -1 & \cdots & -1 \\
    -1 & n & \cdots & -1 \\
    \vdots & \vdots & \ddots & \vdots \\
    -1 & -1 & \cdots & n 
    \end{bmatrix}, \quad 
    a^{-1} = \frac{1}{n+1} \begin{bmatrix} 
    c & 1 & \cdots & 1 \\
    1 & c & \cdots & 1 \\
    \vdots & \vdots & \ddots & \vdots \\
    1 & 1 & \cdots & c 
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"for which values of \(k\) does the system:
    \[
    kx + y = 1 \\
    x + ky = 1
    \]
    have no solution, one solution, or infinitely many solutions?",matrices and gaussian elimination
"find the symmetric factorization \(a = ldl^t\) of:
    \[
    a = \begin{bmatrix} 
    1 & 2 & 0 \\
    2 & 6 & 4 \\
    0 & 4 & 11 
    \end{bmatrix}, \quad 
    a = \begin{bmatrix} 
    a & b \\
    b & c 
    \end{bmatrix}.
    \]",matrices and gaussian elimination
"suppose \(a\) is the 4 by 4 identity matrix except for a vector \(v\) in column 2:
    \[
    a = \begin{bmatrix} 
    1 & v_1 & 0 & 0 \\
    0 & v_2 & 0 & 0 \\
    0 & v_3 & 1 & 0 \\
    0 & v_4 & 0 & 1 
    \end{bmatrix}.
    \]
    \begin{enumerate}",matrices and gaussian elimination
"factor \(a\) into \(lu\), assuming \(v_2 \neq 0\).",matrices and gaussian elimination
"find \(a^{-1}\), which has the same form as \(a\).",matrices and gaussian elimination
"solve by elimination, or show that there is no solution:
    \[
    u + v + w = 0 \\
    u + 2v + 3w = 0 \\
    3u + 5v + 7w = 1
    \]
    and
    \[
    u + v + w = 0 \\
    u + u + 3w = 0 \\
    3u + 5v + 7w = 1.
    \]",matrices and gaussian elimination
"the \(n \times n\) permutation matrices are an important example of a “group.” if you multiply them you stay inside the group; they have inverses in the group; the identity is in the group; and the law \(p_1(p_2p_3) = (p_1p_2)p_3\) is true—because it is true for all matrices.
    \begin{enumerate}",matrices and gaussian elimination
how many members belong to the groups of \(4 \times 4\) and \(n \times n\) permutation matrices?,matrices and gaussian elimination
find a power \(k\) so that all \(3 \times 3\) permutation matrices satisfy \(p^k = i\).,matrices and gaussian elimination
describe the rows of \(da\) and the columns of \(ad\) if \(d = \begin{bmatrix} 2 & 0 \\ 0 & 5 \end{bmatrix}\).,matrices and gaussian elimination
"if \(a\) is invertible, what is the inverse of \(a^t\)?",matrices and gaussian elimination
"if \(a\) is also symmetric, what is the transpose of \(a^{-1}\)?",matrices and gaussian elimination
illustrate both formulas when \(a = \begin{bmatrix} 2 & 1 \\ 1 & 1 \end{bmatrix}\).,matrices and gaussian elimination
"by experiment with \(n = 2\) and \(n = 3\), find:
    \[
    \begin{bmatrix} 2 & 3 \\ 0 & 0 \end{bmatrix}^n, \quad 
    \begin{bmatrix} 2 & 3 \\ 0 & 1 \end{bmatrix}^n, \quad
    \begin{bmatrix} 2 & 3 \\ 0 & 1 \end{bmatrix}^{-1}.
    \]",matrices and gaussian elimination
"starting with a first plane \(u + 2v - w = 6\), find the equation for:
    \begin{enumerate}",matrices and gaussian elimination
the parallel plane through the origin.,matrices and gaussian elimination
"a second plane that also contains the points \((6, 0, 0)\) and \((2, 2, 0)\).",matrices and gaussian elimination
"a third plane that meets the first and second in the point \((4, 1, 0)\).",matrices and gaussian elimination
"what multiple of row 2 is subtracted from row 3 in forward elimination of \(a\)?
    \[
    a = \begin{bmatrix} 
    1 & 0 & 0 \\
    2 & 1 & 0 \\
    0 & 5 & 1 
    \end{bmatrix} \quad
    \begin{bmatrix} 
    1 & 2 & 0 \\
    0 & 1 & 5 \\
    0 & 0 & 1 
    \end{bmatrix}.
    \]
    how do you know (without multiplying those factors) that \(a\) is invertible, symmetric, and tridiagonal? what are its pivots?",matrices and gaussian elimination
"(a) what vector \( x \) will make \( ax = \text{column 1 of } a + 2(\text{column 3}) \), for a \( 3 \times 3 \) matrix \( a \)?

    (b) construct a matrix that has \( \text{column 1} + 2(\text{column 3}) = 0 \). check that \( a \) is singular (fewer than 3 pivots) and explain why that must be the case.",matrices and gaussian elimination
"true or false, with reason if true and counterexample if false:
    \begin{enumerate}",matrices and gaussian elimination
"if \( l_1u_1 = l_2u_2 \) (upper triangular \( u \)’s with nonzero diagonal, lower triangular \( l \)’s with unit diagonal), then \( l_1 = l_2 \) and \( u_1 = u_2 \). the lu factorization is unique.",matrices and gaussian elimination
"if \( a^2 + a = i \), then \( a^{-1} = a + i \).",matrices and gaussian elimination
"if all diagonal entries of \( a \) are zero, then \( a \) is singular.",matrices and gaussian elimination
"by experiment or the gauss-jordan method, compute:
    \[
    \begin{bmatrix} 
    1 & 0 & 0 \\
    ` & 1 & 0 \\
    m & 0 & 1 
    \end{bmatrix}^n,
    \quad
    \begin{bmatrix} 
    1 & 0 & 0 \\
    ` & 1 & 0 \\
    m & 0 & 1 
    \end{bmatrix}^{-1},
    \quad
    \begin{bmatrix} 
    1 & 0 & 0 \\
    ` & 1 & 0 \\
    0 & m & 1 
    \end{bmatrix}^{-1}.
    \]",matrices and gaussian elimination
"write down the \( 2 \times 2 \) matrices that:
    \begin{enumerate}",matrices and gaussian elimination
reverse the direction of every vector.,matrices and gaussian elimination
project every vector onto the \( x_2 \)-axis.,matrices and gaussian elimination
turn every vector counterclockwise through 90°.,matrices and gaussian elimination
reflect every vector through the 45° line \( x_1 = x_2 \).,matrices and gaussian elimination
let $a$ be an $n \times n$ real symmetric matrix. prove that $a$ is always diagonalizable and that its eigenvalues are real.,matrices and gaussian elimination
"suppose $a$ is a real symmetric matrix. show that there exists an orthogonal matrix $q$ such that $a = q \lambda q^t$, where $\lambda$ is a diagonal matrix. what does this decomposition tell us about the geometric and algebraic multiplicities of eigenvalues?",matrices and gaussian elimination
"consider the symmetric matrix $a = \begin{bmatrix} 4 & 1 & 2 \\ 1 & 3 & 0 \\ 2 & 0 & 5 \end{bmatrix}$. compute its eigenvalues and eigenvectors, and verify the spectral decomposition $a = q \lambda q^t$.",matrices and gaussian elimination
let $a$ be a positive definite symmetric matrix. show that there exists a unique lower triangular matrix $l$ with positive diagonal entries such that $a = ll^t$ (cholesky decomposition). prove that this decomposition is unique.,matrices and gaussian elimination
"if $a$ is a symmetric matrix, prove that $a$ can be written as $a = u d u^t$ where $u$ is an orthogonal matrix and $d$ is a diagonal matrix. explain how this factorization is related to the principal component analysis (pca) in data science.",matrices and gaussian elimination
"let $a$ be an $n \times n$ invertible matrix with integer entries. show that if all entries of $a^{-1}$ are also integers, then $\det(a) = \pm 1$. use the gauss-jordan method to justify your argument.",matrices and gaussian elimination
"given an arbitrary $3 \times 3$ matrix $a$, derive conditions on its entries such that the gauss-jordan method fails to produce an inverse. provide an example of such a matrix and justify your reasoning.",matrices and gaussian elimination
"suppose $a$ is an $n \times n$ matrix with rational entries, and the gauss-jordan elimination process involves only rational row operations. prove that if $a$ is invertible, then $a^{-1}$ has only rational entries. how does this result relate to field properties?",matrices and gaussian elimination
"let $a = \begin{bmatrix} 1 & a & b \\ 0 & 1 & c \\ 0 & 0 & 1 \end{bmatrix}$ be an upper triangular matrix. use the gauss-jordan method to find $a^{-1}$ explicitly in terms of $a, b,$ and $c$, and determine necessary conditions for its existence.",matrices and gaussian elimination
"consider an $n \times n$ matrix $a$ whose entries are chosen randomly from $\{0,1\}$. show that the probability of $a$ being invertible is nonzero. use properties of the gauss-jordan method to discuss how random row operations affect invertibility.",matrices and gaussian elimination
"let $a$ be a $3 \times 3$ matrix with distinct nonzero entries. prove that if $a$ can be factored as $a = lu$ with $l$ a lower triangular matrix and $u$ an upper triangular matrix, then $a$ must be nonsingular. provide a counterexample if $a$ is singular.",matrices and gaussian elimination
"compute the $lu$ and $ldu$ factorizations for the matrix $a = \begin{bmatrix} 4 & 3 & -1 \\ 2 & 1 & 3 \\ -6 & 2 & 5 \end{bmatrix}$, and verify your result by matrix multiplication.",matrices and gaussian elimination
"suppose $a$ is an $n \times n$ matrix with an $lu$ decomposition where $l$ is unit lower triangular and $u$ is upper triangular. show that if $a$ is invertible, then $l$ and $u$ are also invertible, and express $a^{-1}$ in terms of $l^{-1}$ and $u^{-1}$.",matrices and gaussian elimination
"given a symmetric positive definite matrix $a$, prove that its $lu$ decomposition satisfies $l = u^t$. show how this leads to the cholesky decomposition $a = ll^t$, and compare it to the $ldu$ factorization.",matrices and gaussian elimination
"consider the permutation matrix $p$ obtained by row interchanges during gaussian elimination. show that for certain matrices $a$, an $lu$ decomposition may not exist without row exchanges, leading to a $pa = lu$ factorization instead. provide an example and justify why row exchanges are necessary.",matrices and gaussian elimination
"solve the following system of linear equations using gaussian elimination:
    \[
    \begin{aligned}
    2x + y - 3z &= 9, \\
    4x + 3y + 2z &= 20, \\
    -x + 2y + z &= 2.
    \end{aligned}
    \]",matrices and gaussian elimination
"use back substitution to solve the following upper triangular system:
    \[
    \begin{aligned}
    x + 2y + 3z &= 9, \\
    0x + 4y + 5z &= 20, \\
    0x + 0y + 7z &= 14.
    \end{aligned}
    \]",matrices and gaussian elimination
"given the augmented matrix for a system of linear equations:
    \[
    \left[\begin{array}{ccc|c}
    1 & 2 & -1 & 3 \\
    2 & 4 & -1 & 7 \\
    1 & 3 & 2 & 6
    \end{array}\right],
    \]
    use gaussian elimination to find the solution to the system.",matrices and gaussian elimination
"solve the following system of equations using gaussian elimination and back substitution:
    \[
    \begin{aligned}
    3x - y + 2z &= 5, \\
    2x + 4y - 3z &= 7, \\
    x - 2y + 3z &= 4.
    \end{aligned}
    \]",matrices and gaussian elimination
"use elimination to solve the following system of equations:
    \[
    \begin{aligned}
    x + 3y - z &= 2, \\
    2x + 5y + 3z &= 3, \\
    3x + 2y - 4z &= 5.
    \end{aligned}
    \]",matrices and gaussian elimination
"compute the product of the following column and row vectors:
    \[
    \begin{bmatrix} 1 \\ 3 \\ 5 \end{bmatrix} \cdot \begin{bmatrix} 2 & 4 & 6 \end{bmatrix}.
    \]",matrices and gaussian elimination
let \( a = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix} \) and \( b = \begin{bmatrix} 10 \\ 11 \\ 12 \end{bmatrix} \). compute the matrix product \( ab \) using column-row multiplication.,matrices and gaussian elimination
"consider the block matrix multiplication of the following matrices:
    \[
    a = \begin{bmatrix} 
    1 & 2 \\ 
    3 & 4 
    \end{bmatrix}, 
    b = \begin{bmatrix} 
    5 & 6 \\ 
    7 & 8 
    \end{bmatrix}.
    \]
    compute the product \( ab \) using block matrix multiplication.",matrices and gaussian elimination
let \( a = \begin{bmatrix} 1 & 3 & 5 \\ 2 & 4 & 6 \end{bmatrix} \) and \( b = \begin{bmatrix} 7 \\ 8 \\ 9 \end{bmatrix} \). perform the column-row multiplication \( ab \) and provide the resulting vector.,matrices and gaussian elimination
"given the block matrices \( a = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix} \) and \( b = \begin{bmatrix} b_1 & b_2 \\ b_3 & b_4 \end{bmatrix} \), where each \( a_i \) and \( b_i \) are \( 2 \times 2 \) matrices, compute the product \( ab \) using block matrix multiplication.",matrices and gaussian elimination
"let \( a = \begin{bmatrix} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ 9 & 10 & 11 & 12 \end{bmatrix} \) and \( b = \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix} \). compute the matrix product \( ab \) using column-row multiplication, and discuss the result in terms of linear transformations.",matrices and gaussian elimination
"find the lengths and the inner product of $x = (1,4,0,2)$ and $y = (2,-2,1,3)$.",orthogonality
"give an example in $\mathbb{r}^2$ of linearly independent vectors that are not orthogonal. also, give an example of orthogonal vectors that are not independent.",orthogonality
"two lines in the plane are perpendicular when the product of their slopes is $-1$. apply this to the vectors $x = (x_1, x_2)$ and $y = (y_1, y_2)$, whose slopes are $\frac{x_2}{x_1}$ and $\frac{y_2}{y_1}$, to derive again the orthogonality condition $x^t y = 0$.",orthogonality
"how do we know that the $i$th row of an invertible matrix $b$ is orthogonal to the $j$th column of $b^{-1}$, if $i \neq j$?",orthogonality
"determine which pairs are orthogonal among the vectors:
    \begin{align*}
        v_1 &= \begin{bmatrix} 1 \\ 2 \\ -2 \\ 1 \end{bmatrix}, \quad
        v_2 = \begin{bmatrix} 4 \\ 0 \\ 4 \\ 0 \end{bmatrix}, \\ 
        v_3 &= \begin{bmatrix} 1 \\ -1 \\ -1 \\ -1 \end{bmatrix}, \quad
        v_4 = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}.
    \end{align*}",orthogonality
"find all vectors in $\mathbb{r}^3$ that are orthogonal to $(1,1,1)$ and $(1,-1,0)$. produce an orthonormal basis from these vectors (mutually orthogonal unit vectors).",orthogonality
"find a vector $x$ orthogonal to the row space of $a$, a vector $y$ orthogonal to the column space, and a vector $z$ orthogonal to the null space of:
    \begin{equation*}
        a = \begin{bmatrix} 1 & 2 & 1 \\ 2 & 4 & 3 \\ 3 & 6 & 4 \end{bmatrix}.
    \end{equation*}",orthogonality
"if $v$ and $w$ are orthogonal subspaces, show that the only vector they have in common is the zero vector: $v \cap w = \{0\}$.",orthogonality
"find the orthogonal complement of the plane spanned by the vectors $(1,1,2)$ and $(1,2,3)$, by taking these to be the rows of $a$ and solving $ax = 0$. remember that the complement is a whole line.",orthogonality
"construct a homogeneous equation in three unknowns whose solutions are the linear combinations of the vectors $(1,1,2)$ and $(1,2,3)$. this is the reverse of the previous exercise, but the two problems are really the same.",orthogonality
"the fundamental theorem is often stated in the form of fredholm’s alternative: for any $a$ and $b$, one and only one of the following systems has a solution:
    \begin{enumerate}",orthogonality
$ax = b$.,orthogonality
"$a^t y = 0$, $y^t b \neq 0$.",orthogonality
"find a basis for the orthogonal complement of the row space of $a$:
    \[
        a = \begin{bmatrix} 1 & 0 & 2 \\ 1 & 1 & 4 \end{bmatrix}.
    \]
    split $x = (3,3,3)$ into a row space component $x_r$ and a null space component $x_n$.",orthogonality
"illustrate the action of $a^t$ by a picture corresponding to figure 3.4, sending $c(a)$ back to the row space and the left null space to zero.",orthogonality
show that $x - y$ is orthogonal to $x + y$ if and only if $\|x\| = \|y\|$.,orthogonality
"find a matrix whose row space contains $(1,2,1)$ and whose null space contains $(1,-2,1)$, or prove that there is no such matrix.",orthogonality
"find all vectors that are perpendicular to $(1,4,4,1)$ and $(2,9,8,2)$.",orthogonality
"if $v$ is the orthogonal complement of $w$ in $\mathbb{r}^n$, is there a matrix with row space $v$ and null space $w$? starting with a basis for $v$, construct such a matrix.",orthogonality
"if $s = \{0\}$ is the subspace of $\mathbb{r}^4$ containing only the zero vector, what is $s^\perp$? if $s$ is spanned by $(0,0,0,1)$, what is $s^\perp$? what is $(s^\perp)^\perp$?",orthogonality
"why are these statements false?
    \begin{enumerate}",orthogonality
"if $v$ is orthogonal to $w$, then $v^\perp$ is orthogonal to $w^\perp$.",orthogonality
$v$ orthogonal to $w$ and $w$ orthogonal to $z$ makes $v$ orthogonal to $z$.,orthogonality
let $s$ be a subspace of $\mathbb{r}^n$. explain what $(s^\perp)^\perp = s$ means and why it is true.,orthogonality
"let $p$ be the plane in $\mathbb{r}^3$ with equation $x + 2y - z = 0$. find a vector perpendicular to $p$. what matrix has the plane $p$ as its null space, and what matrix has $p$ as its row space?",orthogonality
"let $s$ be the subspace of $\mathbb{r}^4$ containing all vectors with $x_1 + x_2 + x_3 + x_4 = 0$. find a basis for the space $s^\perp$, containing all vectors orthogonal to $s$.",orthogonality
construct an unsymmetric $2 \times 2$ matrix of rank $1$. copy figure 3.4 and put one vector in each subspace. which vectors are orthogonal?,orthogonality
redraw figure 3.4 for a $3 \times 2$ matrix of rank $r = 2$. which subspace is $z$ (zero vector only)? the nullspace part of any vector $x$ in $\mathbb{r}^2$ is $x_n =$ .,orthogonality
"construct a matrix with the required property or say why that is impossible.
    \begin{enumerate}",orthogonality
"column space contains $\begin{bmatrix} 1 \\ 2 \\ -3 \end{bmatrix}$ and $\begin{bmatrix} 2 \\ -3 \\ 5 \end{bmatrix}$, nullspace contains $\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$.",orthogonality
"row space contains $\begin{bmatrix} 1 \\ 2 \\ -3 \end{bmatrix}$ and $\begin{bmatrix} 2 \\ -3 \\ 5 \end{bmatrix}$, nullspace contains $\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$.",orthogonality
$ax = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$ has a solution and $a^t \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$.,orthogonality
every row is orthogonal to every column ($a$ is not the zero matrix).,orthogonality
"the columns add up to a column of 0s, the rows add to a row of 1s.",orthogonality
if $ab = 0$ then the columns of $b$ are in the of $a$. the rows of $a$ are in the of $b$. why can’t $a$ and $b$ be $3 \times 3$ matrices of rank $2$?,orthogonality
\begin{enumerate},orthogonality
"if $ax = b$ has a solution and $a^t y = 0$, then $y$ is perpendicular to .",orthogonality
"if $a^t y = c$ has a solution and $ax = 0$, then $x$ is perpendicular to .",orthogonality
"this is a system of equations $ax = b$ with no solution:
    \begin{align*}
        x + 2y + 2z &= 5 \\
        2x + 2y + 3z &= 5 \\
        3x + 4y + 5z &= 9.
    \end{align*}
    find numbers $y_1, y_2, y_3$ to multiply the equations so they add to $0 = 1$. you have found a vector $y$ in which subspace? the inner product $y^t b$ is 1.",orthogonality
"in figure 3.4, how do we know that $ax_r$ is equal to $ax$? how do we know that this vector is in the column space? if $a = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$ and $x = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ what is $x_r$?",orthogonality
if $ax$ is in the nullspace of $a^t$ then $ax = 0$. reason: $ax$ is also in the of $a$ and the spaces are . conclusion: $a^t a$ has the same nullspace as $a$.,orthogonality
"suppose $a$ is a symmetric matrix ($a^t = a$).
    \begin{enumerate}",orthogonality
why is its column space perpendicular to its nullspace?,orthogonality
"if $ax = 0$ and $az = 5z$, which subspaces contain these “eigenvectors” $x$ and $z$? symmetric matrices have perpendicular eigenvectors (see section 5.5).",orthogonality
"(recommended) draw figure 3.4 to show each subspace for
    \begin{align*}
        a &= \begin{bmatrix} 1 & 2 \\ 3 & 6 \end{bmatrix}, \quad b = \begin{bmatrix} 1 & 0 \\ 3 & 0 \end{bmatrix}.
    \end{align*}",orthogonality
"find the pieces $x_r$ and $x_n$, and draw figure 3.4 properly, if
    \[
    a = \begin{bmatrix} 1 & -1 \\ 0 & 0 \\ 0 & 0 \end{bmatrix}, \quad x = \begin{bmatrix} 2 \\ 0 \end{bmatrix}.
    \]",orthogonality
put bases for the orthogonal subspaces $v$ and $w$ into the columns of matrices $v$ and $w$. why does $v^t w = 0$ matrix? this matches $v^t w = 0$ for vectors.,orthogonality
"the floor and the wall are not orthogonal subspaces because they share a nonzero vector (along the line where they meet). two planes in $\mathbb{r}^3$ cannot be orthogonal! find a vector in both column spaces $c(a)$ and $c(b)$:
    \[
    a = \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 2 \end{bmatrix}, \quad b = \begin{bmatrix} 5 & 4 \\ 6 & 3 \\ 5 & 1 \end{bmatrix}.
    \]",orthogonality
extend problem 35 to a $p$-dimensional subspace $v$ and a $q$-dimensional subspace $w$ of $\mathbb{r}^n$. what inequality on $p+q$ guarantees that $v$ intersects $w$ in a nonzero vector? these subspaces cannot be orthogonal.,orthogonality
"prove that every $y$ in $n(a^t)$ is perpendicular to every $ax$ in the column space, using the matrix shorthand of equation (8). start from $a^t y = 0$.",orthogonality
"if $s$ is the subspace of $\mathbb{r}^3$ containing only the zero vector, what is $s^\perp$? if $s$ is spanned by $(1,1,1)$, what is $s^\perp$? if $s$ is spanned by $(2,0,0)$ and $(0,0,3)$, what is $s^\perp$?",orthogonality
"suppose $s$ only contains $(1,5,1)$ and $(2,2,2)$ (not a subspace). then $s^\perp$ is the nullspace of the matrix $a = \cdots$. $s^\perp$ is a subspace even if $s$ is not.",orthogonality
"suppose $l$ is a one-dimensional subspace (a line) in $\mathbb{r}^3$. its orthogonal complement $l^\perp$ is the perpendicular to $l$. then $(l^\perp)^\perp$ is a perpendicular to $l^\perp$. in fact, $(l^\perp)^\perp$ is the same as $\cdots$.",orthogonality
suppose $v$ is the whole space $\mathbb{r}^4$. then $v^\perp$ contains only the vector $\cdots$. then $(v^\perp)^\perp$ is $\cdots$. so $(v^\perp)^\perp$ is the same as $\cdots$.,orthogonality
"suppose $s$ is spanned by the vectors $(1,2,2,3)$ and $(1,3,3,2)$. find two vectors that span $s^\perp$. this is the same as solving $ax = 0$ for which $a$?",orthogonality
"if $p$ is the plane of vectors in $\mathbb{r}^4$ satisfying $x_1 + x_2 + x_3 + x_4 = 0$, write a basis for $p^\perp$. construct a matrix that has $p$ as its nullspace.",orthogonality
"if a subspace $s$ is contained in a subspace $v$, prove that $s^\perp$ contains $v^\perp$.",orthogonality
suppose an \(n \times n\) matrix is invertible: \(aa^{-1} = i\). then the first column of \(a^{-1}\) is orthogonal to the space spanned by which rows of \(a\)?,orthogonality
"find \(a^t a\) if the columns of \(a\) are unit vectors, all mutually perpendicular.",orthogonality
construct a \(3 \times 3\) matrix \(a\) with no zero entries whose columns are mutually perpendicular. compute \(a^t a\). why is it a diagonal matrix?,orthogonality
"the lines \(3x + y = b_1\) and \(6x + 2y = b_2\) are the same line if... in that case \((b_1, b_2)\) is perpendicular to the vector... the nullspace of the matrix is the line \(3x + y = \). one particular vector in that nullspace is...",orthogonality
"why is each of these statements false?
    \begin{enumerate}",orthogonality
"\((1, 1, 1)\) is perpendicular to \((1, 1, -2)\), so the planes \(x + y + z = 0\) and \(x + y - 2z = 0\) are orthogonal subspaces.",orthogonality
"the subspace spanned by \((1, 1, 0, 0, 0)\) and \((0, 0, 0, 1, 1)\) is the orthogonal complement of the subspace spanned by \((1, -1, 0, 0, 0)\) and \((2, -2, 3, 4, -4)\).",orthogonality
two subspaces that meet only in the zero vector are orthogonal.,orthogonality
"find a matrix with \(v = (1, 2, 3)\) in the row space and column space. find another matrix with \(v\) in the nullspace and column space. which pairs of subspaces can \(v\) not be in?",orthogonality
"suppose \(a\) is \(3 \times 4\), \(b\) is \(4 \times 5\), and \(ab = 0\). prove \(\text{rank}(a) + \text{rank}(b) \leq 4\).
    \\item the command \(n = \text{null}(a)\) will produce a basis for the nullspace of \(a\). then the command \(b = \text{null}(n')\) will produce a basis for the \rule{3cm}{0.5pt} of \(a\).",orthogonality
"(a) given any two positive numbers \(x\) and \(y\), choose the vector \(\mathbf{b} = (\sqrt{x}, \sqrt{y})\), and choose \(\mathbf{a} = (\sqrt{y}, \sqrt{x})\). apply the schwarz inequality to compare the arithmetic mean \(\frac{1}{2}(x + y)\) with the geometric mean \(\sqrt{xy}\).
    
    (b) suppose we start with a vector from the origin to the point \(\mathbf{x}\), and then add a vector of length \(\|\mathbf{y}\|\) connecting \(\mathbf{x}\) to \(\mathbf{x} + \mathbf{y}\). the third side of the triangle goes from the origin to \(\mathbf{x} + \mathbf{y}\). the triangle inequality asserts that this distance cannot be greater than the sum of the first two:
    \[
    \|\mathbf{x} + \mathbf{y}\| \leq \|\mathbf{x}\| + \|\mathbf{y}\|.
    \]
    after squaring both sides, and expanding \((\mathbf{x} + \mathbf{y})^t(\mathbf{x} + \mathbf{y})\), reduce this to the schwarz inequality.",orthogonality
"verify that the length of the projection in figure 3.7 is \(\|\mathbf{p}\| = \|\mathbf{b}\|\cos\theta\), using formula (5).",orthogonality
"what multiple of \(\mathbf{a} = (1,1,1)\) is closest to the point \(\mathbf{b} = (2,4,4)\)? find also the point closest to \(\mathbf{a}\) on the line through \(\mathbf{b}\).",orthogonality
"explain why the schwarz inequality becomes an equality in the case that \(\mathbf{a}\) and \(\mathbf{b}\) lie on the same line through the origin, and only in that case. what if they lie on opposite sides of the origin?",orthogonality
"in \(n\) dimensions, what angle does the vector \((1, 1, \dots, 1)\) make with the coordinate axes? what is the projection matrix \(p\) onto that vector?",orthogonality
"the schwarz inequality has a one-line proof if \(\mathbf{a}\) and \(\mathbf{b}\) are normalized ahead of time to be unit vectors:
    \[
    |\mathbf{a}^t \mathbf{b}| = \left| \sum a_j b_j \right| \leq \sum |a_j| |b_j| \leq \sum |a_j|^2 + |b_j|^2 = \frac{1}{2} + \frac{1}{2} = \|\mathbf{a}\| \|\mathbf{b}\|.
    \]
    which previous problem justifies the middle step?",orthogonality
"by choosing the correct vector \(\mathbf{b}\) in the schwarz inequality, prove that
    \[
    \left( a_1 + \cdots + a_n \right)^2 \leq n \left( a_1^2 + \cdots + a_n^2 \right).
    \]
    when does equality hold?",orthogonality
"the methane molecule \(\text{ch}_4\) is arranged as if the carbon atom were at the center of a regular tetrahedron with four hydrogen atoms at the vertices. if vertices are placed at \((0,0,0)\), \((1,1,0)\), \((1,0,1)\), and \((0,1,1)\)—note that all six edges have length \(\sqrt{2}\), so the tetrahedron is regular—what is the cosine of the angle between the rays going from the center \(\left(\frac{1}{2}, \frac{1}{2}, \frac{1}{2}\right)\) to the vertices? (the bond angle itself is about 109.5°, an old friend of chemists.)",orthogonality
"square the matrix \(p = \frac{\mathbf{a}\mathbf{a}^t}{\mathbf{a}^t \mathbf{a}}\), which projects onto a line, and show that \(p^2 = p\).
    (note the number \(\mathbf{a}^t \mathbf{a}\) in the middle of the matrix \(\frac{\mathbf{a}\mathbf{a}^t}{\mathbf{a}^t \mathbf{a}}\).)",orthogonality
is the projection matrix \(p\) invertible? why or why not?,orthogonality
"(a) find the projection matrix \(p_1\) onto the line through \(\mathbf{a} = \begin{bmatrix} 1 \\ 3 \end{bmatrix}\) and also the matrix \(p_2\) that projects onto the line perpendicular to \(\mathbf{a}\).
    
    (b) compute \(p_1 + p_2\) and \(p_1 p_2\) and explain.",orthogonality
find the matrix that projects every point in the plane onto the line \(x + 2y = 0\).,orthogonality
prove that the trace of \(p = \frac{\mathbf{a} \mathbf{a}^t}{\mathbf{a}^t \mathbf{a}}\)—which is the sum of its diagonal entries—always equals 1.,orthogonality
what matrix \(p\) projects every point in \(\mathbb{r}^3\) onto the line of intersection of the planes \(x + y + t = 0\) and \(x - t = 0\)?,orthogonality
show that the length of \(\mathbf{a} \mathbf{x}\) equals the length of \(\mathbf{a}^t \mathbf{x}\) if \(\mathbf{a}\mathbf{a}^t = \mathbf{a}^t \mathbf{a}\).,orthogonality
"suppose \(p\) is the projection matrix onto the line through \(\mathbf{a}\).
    (a) why is the inner product of \(\mathbf{x}\) with \(p\mathbf{y}\) equal to the inner product of \(p\mathbf{x}\) with \(\mathbf{y}\)?
    
    (b) are the two angles the same? find their cosines if \(\mathbf{a} = (1, 1, -1)\), \(\mathbf{x} = (2, 0, 1)\), \(\mathbf{y} = (2, 1, 2)\).
    
    (c) why is the inner product of \(p\mathbf{x}\) with \(p\mathbf{y}\) again the same? what is the angle between those two?",orthogonality
"project the vector \(\mathbf{b}\) onto the line through \(\mathbf{a}\). check that \(\mathbf{e}\) is perpendicular to \(\mathbf{a}\):
    \begin{itemize}",orthogonality
(a) \(\mathbf{b} = \begin{bmatrix} 1 \\ 2 \\ 2 \end{bmatrix}\) and \(\mathbf{a} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}\).,orthogonality
"(b) \(\mathbf{b} = \begin{bmatrix} 1 \\ 3 \\ 1 \end{bmatrix}\) and \(\mathbf{a} = \begin{bmatrix} -1 \\ -3 \\ -1 \end{bmatrix}\).
    \end{itemize}",orthogonality
"draw the projection of \(\mathbf{b}\) onto \(\mathbf{a}\) and also compute it from \(\mathbf{p} = x \mathbf{a} \mathbf{b} \):
    \begin{itemize}",orthogonality
(a) \(\mathbf{b} = \begin{bmatrix} \cos \theta \\ \sin \theta \end{bmatrix}\) and \(\mathbf{a} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\).,orthogonality
"(b) \(\mathbf{b} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\) and \(\mathbf{a} = \begin{bmatrix} 1 \\ -1 \end{bmatrix}\).
    \end{itemize}",orthogonality
"in problem 17, find the projection matrix \(p = \frac{\mathbf{a} \mathbf{a}^t}{\mathbf{a}^t \mathbf{a}}\) onto the line through each vector \(\mathbf{a}\). verify in both cases that \(p^2 = p\). multiply \(\mathbf{p} \mathbf{b}\) in each case to compute the projection \(\mathbf{p}\).",orthogonality
construct the projection matrices \(p_1\) and \(p_2\) onto the lines through the \(\mathbf{a}\)'s in problem 18. is it true that \((p_1 + p_2)^2 = p_1 + p_2\)? this would be true if \(p_1 p_2 = 0\).,orthogonality
"compute the projection matrices \(\frac{\mathbf{a} \mathbf{a}^t}{\mathbf{a}^t \mathbf{a}}\) onto the lines through \(\mathbf{a_1} = (-1, 2, 2)\) and \(\mathbf{a_2} = (2, 2, -1)\). multiply those projection matrices and explain why their product \(p_1 p_2\) is what it is.",orthogonality
"project \(\mathbf{b} = (1, 0, 0)\) onto the lines through \(\mathbf{a_1}\) and \(\mathbf{a_2}\) in problem 21 and also onto \(\mathbf{a_3} = (2, -1, 2)\). add the three projections \(\mathbf{p_1} + \mathbf{p_2} + \mathbf{p_3}\).",orthogonality
"continuing problems 21–22, find the projection matrix \(p_3\) onto \(\mathbf{a_3} = (2, -1, 2)\). verify that \(p_1 + p_2 + p_3 = i\). the basis \(\mathbf{a_1}, \mathbf{a_2}, \mathbf{a_3}\) is orthogonal!",orthogonality
"project the vector \(\mathbf{b} = (1, 1)\) onto the lines through \(\mathbf{a_1} = (1, 0)\) and \(\mathbf{a_2} = (1, 2)\). draw the projections \(\mathbf{p_1}\) and \(\mathbf{p_2}\) and add \(\mathbf{p_1} + \mathbf{p_2}\). the projections do not add to \(\mathbf{b}\) because the \(\mathbf{a}\)'s are not orthogonal.",orthogonality
"in problem 24, the projection of \(\mathbf{b}\) onto the plane of \(\mathbf{a_1}\) and \(\mathbf{a_2}\) will equal \(\mathbf{b}\). find \(p = a(a^t a)^{-1} a^t\) for \(a = \begin{bmatrix} \mathbf{a_1} & \mathbf{a_2} \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}\).",orthogonality
"project \(\mathbf{a_1} = (1, 0)\) onto \(\mathbf{a_2} = (1, 2)\). then project the result back onto \(\mathbf{a_1}\). draw these projections and multiply the projection matrices \(p_1 p_2\). is this a projection?",orthogonality
"find the best least-squares solution \(\mathbf{x}_b\) to:
    \[
    3x = 10, \quad 4x = 5.
    \]
    what error \(e^2\) is minimized? check that the error vector \((10 - 3\mathbf{x}_b, 5 - 4\mathbf{x}_b)\) is perpendicular to the column \((3, 4)\).",orthogonality
"suppose the values \(b_1 = 1\) and \(b_2 = 7\) at times \(t_1 = 1\) and \(t_2 = 2\) are fitted by a line \(\mathbf{b} = d\mathbf{t}\) through the origin. solve \(d = 1\) and \(2d = 7\) by least squares, and sketch the best line.",orthogonality
"solve \(\mathbf{a}\mathbf{x} = \mathbf{b}\) by least squares, and find \(\mathbf{p} = \mathbf{a}\mathbf{x}_b\) if
    \[
    \mathbf{a} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}.
    \]
    verify that the error \(\mathbf{b} - \mathbf{p}\) is perpendicular to the columns of \(\mathbf{a}\).",orthogonality
"write out \(e^2 = \| \mathbf{a}\mathbf{x} - \mathbf{b} \|^2\) and set to zero its derivatives with respect to \(u\) and \(v\), if
    \[
    \mathbf{a} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} u \\ v \end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix} 1 \\ 3 \\ 4 \end{bmatrix}.
    \]
    compare the resulting equations with \(\mathbf{a}^t \mathbf{a} \mathbf{x}_b = \mathbf{a}^t \mathbf{b}\), confirming that calculus as well as geometry gives the normal equations. find the solution \(\mathbf{x}_b\) and the projection \(\mathbf{p} = \mathbf{a}\mathbf{x}_b\). why is \(\mathbf{p} = \mathbf{b}\)?",orthogonality
"the following system has no solution:
    \[
    \mathbf{a} \mathbf{x} = \begin{bmatrix} 1 & -1 \\ 1 & 0 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} c \\ d \end{bmatrix} = \begin{bmatrix} 4 \\ 5 \\ 9 \end{bmatrix} = \mathbf{b}.
    \]
    sketch and solve a straight-line fit that leads to the minimization of the quadratic
    \[
    (c - d - 4)^2 + (c - 5)^2 + (c + d - 9)^2.
    \]
    what is the projection of \(\mathbf{b}\) onto the column space of \(\mathbf{a}\)?",orthogonality
"find the projection of \(\mathbf{b}\) onto the column space of \(\mathbf{a}\):
    \[
    \mathbf{a} = \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ -2 & 4 \end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix} 1 \\ 2 \\ 7 \end{bmatrix}.
    \]
    split \(\mathbf{b}\) into \(\mathbf{p} + \mathbf{q}\), with \(\mathbf{p}\) in the column space and \(\mathbf{q}\) perpendicular to that space. which of the four subspaces contains \(\mathbf{q}\)?",orthogonality
"find the projection matrix \(\mathbf{p}\) onto the space spanned by \(\mathbf{a_1} = (1, 0, 1)\) and \(\mathbf{a_2} = (1, 1, -1)\).",orthogonality
"if \(\mathbf{p}\) is the projection matrix onto a \(k\)-dimensional subspace \(s\) of the whole space \(\mathbb{r}^n\), what is the column space of \(\mathbf{p}\) and what is its rank?",orthogonality
"(a) if \(\mathbf{p} = \mathbf{p}^t\), show that \(\mathbf{p}\) is a projection matrix.
    (b) what subspace does the matrix \(\mathbf{p} = 0\) project onto?",orthogonality
"if the vectors \(\mathbf{a_1}\), \(\mathbf{a_2}\), and \(\mathbf{b}\) are orthogonal, what are \(\mathbf{a}^t \mathbf{a}\) and \(\mathbf{a}^t \mathbf{b}\)? what is the projection of \(\mathbf{b}\) onto the plane of \(\mathbf{a_1}\) and \(\mathbf{a_2}\)?",orthogonality
suppose \(\mathbf{p}\) is the projection matrix onto the subspace \(\mathbf{s}\) and \(\mathbf{q}\) is the projection onto the orthogonal complement \(\mathbf{s}^\perp\). what are \(\mathbf{p} + \mathbf{q}\) and \(\mathbf{p}\mathbf{q}\)? show that \(\mathbf{p} - \mathbf{q}\) is its own inverse.,orthogonality
"if \(\mathbf{v}\) is the subspace spanned by \((1, 1, 0, 1)\) and \((0, 0, 1, 0)\), find:
    \begin{enumerate}",orthogonality
a basis for the orthogonal complement \(\mathbf{v}^\perp\).,orthogonality
the projection matrix \(\mathbf{p}\) onto \(\mathbf{v}\).,orthogonality
"the vector in \(\mathbf{v}\) closest to the vector \(\mathbf{b} = (0, 1, 0, -1)\) in \(\mathbf{v}^\perp\).",orthogonality
"find the best straight-line fit (least squares) to the measurements \(\mathbf{b} = 4\) at \(t = -2\), \(\mathbf{b} = 3\) at \(t = -1\), \(\mathbf{b} = 1\) at \(t = 0\), and \(\mathbf{b} = 0\) at \(t = 2\). then, find the projection of \(\mathbf{b} = (4, 3, 1, 0)\) onto the column space of
    \[
    \mathbf{a} = \begin{bmatrix} 1 & -2 \\ 1 & -1 \\ 1 & 0 \\ 1 & 2 \end{bmatrix}.
    \]",orthogonality
"the vectors \(\mathbf{a_1} = (1, 1, 0)\) and \(\mathbf{a_2} = (1, 1, 1)\) span a plane in \(\mathbb{r}^3\). find the projection matrix \(\mathbf{p}\) onto the plane, and find a nonzero vector \(\mathbf{b}\) that is projected to zero.",orthogonality
"if \(\mathbf{p}\) is the projection matrix onto a line in the x-y plane, draw a figure to describe the effect of the “reflection matrix” \(\mathbf{h} = \mathbf{i} - 2\mathbf{p}\). explain both geometrically and algebraically why \(\mathbf{h}^2 = \mathbf{i}\).",orthogonality
"show that if \(\mathbf{u}\) has unit length, then the rank-1 matrix \(\mathbf{p} = \mathbf{u}\mathbf{u}^t\) is a projection matrix. it has properties (i) and (ii) in 3n. by choosing \(\mathbf{u} = \frac{\mathbf{a}}{\|\mathbf{a}\|}\), \(\mathbf{p}\) becomes the projection onto the line through \(\mathbf{a}\), and \(\mathbf{p}\mathbf{b}\) is the point \(\mathbf{p} = x_{ab}\). rank-1 projections correspond exactly to least-squares problems in one unknown.",orthogonality
what 2 by 2 matrix projects the x-y plane onto the \(-45^\circ\) line \(x + y = 0\)?,orthogonality
"we want to fit a plane \(y = c + dt + ez\) to the four points:
    \[
    y = 3 \text{ at } t = 1, z = 1, \quad y = 6 \text{ at } t = 0, z = 3, \quad y = 5 \text{ at } t = 2, z = 1, \quad y = 0 \text{ at } t = 0, z = 0.
    \]
    \begin{enumerate}",orthogonality
find 4 equations in 3 unknowns to pass a plane through the points (if there is such a plane).,orthogonality
find 3 equations in 3 unknowns for the best least-squares solution.,orthogonality
"if \(\mathbf{p_c} = \mathbf{a}(\mathbf{a}^t\mathbf{a})^{-1}\mathbf{a}^t\) is the projection onto the column space of \(\mathbf{a}\), what is the projection \(\mathbf{p_r}\) onto the row space? (it is not \(\mathbf{p_c}^t\)!)",orthogonality
"if \(\mathbf{p}\) is the projection onto the column space of \(\mathbf{a}\), what is the projection onto the left nullspace?",orthogonality
"suppose \(l_1\) is the line through the origin in the direction of \(\mathbf{a_1}\) and \(l_2\) is the line through \(\mathbf{b}\) in the direction of \(\mathbf{a_2}\). to find the closest points \(\mathbf{x_1a_1}\) and \(\mathbf{b} + \mathbf{x_2a_2}\) on the two lines, write the two equations for \(\mathbf{x_1}\) and \(\mathbf{x_2}\) that minimize \(\|\mathbf{x_1a_1} - \mathbf{x_2a_2} - \mathbf{b}\|\). solve for \(\mathbf{x}\) if \(\mathbf{a_1} = (1, 1, 0)\), \(\mathbf{a_2} = (0, 1, 0)\), and \(\mathbf{b} = (2, 1, 4)\).",orthogonality
"find the best line \(c + dt\) to fit \(\mathbf{b} = (4, 2, -1, 0, 0)\) at times \(t = -2, -1, 0, 1, 2\).",orthogonality
"show that the best least-squares fit to a set of measurements \(y_1, \dots, y_m\) by a horizontal line (a constant function \(y = c\)) is their average
    \[
    c = \frac{y_1 + \dots + y_m}{m}.
    \]",orthogonality
"find the best straight-line fit to the following measurements, and sketch your solution:
    \[
    y = 2 \text{ at } t = -1, \quad y = 0 \text{ at } t = 0, \quad y = -3 \text{ at } t = 1, \quad y = -5 \text{ at } t = 2.
    \]",orthogonality
"suppose that instead of a straight line, we fit the data in problem 24 by a parabola: \(y = c + dt + et^2\). in the inconsistent system \(\mathbf{a}\mathbf{x} = \mathbf{b}\) that comes from the four measurements, what are the coefficient matrix \(\mathbf{a}\), the unknown vector \(\mathbf{x}\), and the data vector \(\mathbf{b}\)? you need not compute \(\mathbf{x_b}\).",orthogonality
"a middle-aged man was stretched on a rack to lengths \(l = 5, 6, 7\) feet under applied forces of \(f = 1, 2, 4\) tons. assuming hooke’s law \(l = a + bf\), find his normal length \(a\) by least squares.",orthogonality
"(recommended) this problem projects \(\mathbf{b} = (b_1, \dots, b_m)\) onto the line through \(\mathbf{a} = (1, \dots, 1)\). we solve \(m\) equations \(\mathbf{a}^t \mathbf{x} = \mathbf{b}\) in one unknown (by least squares).
    \begin{enumerate}",orthogonality
solve \(\mathbf{a}^t \mathbf{a} \mathbf{x_b} = \mathbf{a}^t \mathbf{b}\) to show that \(\mathbf{x_b}\) is the mean (the average) of the \(b\)’s.,orthogonality
"find \(\mathbf{e} = \mathbf{b} - \mathbf{a} \mathbf{x_b}\), the variance \(\|\mathbf{e}\|^2\), and the standard deviation \(\|\mathbf{e}\|\).",orthogonality
"the horizontal line \(\mathbf{b_b} = 3\) is closest to \(\mathbf{b} = (1, 2, 6)\). check that \(\mathbf{p} = (3, 3, 3)\) is perpendicular to \(\mathbf{e}\) and find the projection matrix \(\mathbf{p}\).",orthogonality
"first assumption behind least squares: each measurement error has mean zero. multiply the 8 error vectors \(\mathbf{b} - \mathbf{a}\mathbf{x} = (\pm1, \pm1, \pm1)\) by \((\mathbf{a}^t \mathbf{a})^{-1} \mathbf{a}^t\) to show that the 8 vectors \(\mathbf{x_b} - \mathbf{x}\) also average to zero. the estimate \(\mathbf{x_b}\) is unbiased.",orthogonality
"second assumption behind least squares: the \(m\) errors \(e_i\) are independent with variance \(\sigma^2\), so the average of \((\mathbf{b} - \mathbf{a}\mathbf{x})(\mathbf{b} - \mathbf{a}\mathbf{x})^t\) is \(\sigma^2 \mathbf{i}\). multiply on the left by \((\mathbf{a}^t \mathbf{a})^{-1} \mathbf{a}^t\) and on the right by \(\mathbf{a} (\mathbf{a}^t \mathbf{a})^{-1}\) to show that the average of \((\mathbf{x_b} - \mathbf{x})(\mathbf{x_b} - \mathbf{x})^t\) is \(\sigma^2 (\mathbf{a}^t \mathbf{a})^{-1}\). this is the all-important covariance matrix for the error in \(\mathbf{x_b}\).",orthogonality
"a doctor takes four readings of your heart rate. the best solution to \(\mathbf{x} = \mathbf{b_1}, \dots, \mathbf{x} = \mathbf{b_4}\) is the average \(\mathbf{x_b}\) of \(\mathbf{b_1}, \dots, \mathbf{b_4}\). the matrix \(\mathbf{a}\) is a column of 1s. problem 29 gives the expected error \((\mathbf{x_b} - \mathbf{x})^2\) as \(\sigma^2 (\mathbf{a}^t \mathbf{a})^{-1} = \dots\). by averaging, the variance drops from \(\sigma^2\) to \(\sigma^2 / 4\).",orthogonality
"if you know the average \(\mathbf{x_b_9}\) of 9 numbers \(\mathbf{b_1}, \dots, \mathbf{b_9}\), how can you quickly find the average \(\mathbf{x_b_{10}}\) with one more number \(\mathbf{b_{10}}\)? the idea of recursive least squares is to avoid adding 10 numbers. what coefficient of \(\mathbf{x_b_9}\) correctly gives \(\mathbf{x_b_{10}}\)?
    \[
    \mathbf{x_b_{10}} = \frac{1}{10} \left( b_1 + \dots + b_{10} \right) = \frac{1}{10} \mathbf{b_{10}} + \mathbf{x_b_9}.
    \]",orthogonality
"with \(\mathbf{b} = (0, 8, 8, 20)\) at \(t = 0, 1, 3, 4\), set up and solve the normal equations \(\mathbf{a}^t \mathbf{a} \mathbf{x_b} = \mathbf{a}^t \mathbf{b}\). for the best straight line as in figure 3.9a, find its four heights \(\mathbf{p_i}\) and four errors \(\mathbf{e_i}\). what is the minimum value \(e^2 = e_1^2 + e_2^2 + e_3^2 + e_4^2\)?",orthogonality
"(line \(c + dt\) does go through \(\mathbf{p}\)) with \(\mathbf{b} = (0, 8, 8, 20)\) at times \(t = 0, 1, 3, 4\), write the four equations \(\mathbf{a} \mathbf{x} = \mathbf{b}\) (unsolvable). change the measurements to \(\mathbf{p} = (1, 5, 13, 17)\) and find an exact solution to \(\mathbf{a} \mathbf{x} = \mathbf{p}\).",orthogonality
"check that \(\mathbf{e} = \mathbf{b} - \mathbf{p} = (-1, 3, -5, 3)\) is perpendicular to both columns of \(\mathbf{a}\). what is the shortest distance \(\|\mathbf{e}\|\) from \(\mathbf{b}\) to the column space of \(\mathbf{a}\)?",orthogonality
"for the closest parabola \(\mathbf{b} = c + dt + et^2\) to the same four points, write the unsolvable equations \(\mathbf{a} \mathbf{x} = \mathbf{b}\) in three unknowns \(\mathbf{x} = (c, d, e)\). set up the three normal equations \(\mathbf{a}^t \mathbf{a} \mathbf{x_b} = \mathbf{a}^t \mathbf{b}\) (solution not required). you are now fitting a parabola to four points—what is happening in figure 3.9b?",orthogonality
"for the closest cubic \(\mathbf{b} = c + dt + et^2 + ft^3\) to the same four points, write the four equations \(\mathbf{a} \mathbf{x} = \mathbf{b}\). solve them by elimination. this cubic now goes exactly through the points. what are \(\mathbf{p}\) and \(\mathbf{e}\)?",orthogonality
"the average of the four times is \(\mathbf{b_t} = \frac{1}{4} (0 + 1 + 3 + 4) = 2\). the average of the four \(\mathbf{b}\)’s is \(\mathbf{b_b} = \frac{1}{4} (0 + 8 + 8 + 20) = 9\).
    \begin{enumerate}",orthogonality
"verify that the best line goes through the center point \((\mathbf{b_t}, \mathbf{b_b}) = (2, 9)\).",orthogonality
explain why \(c + d \mathbf{b_t} = \mathbf{b_b}\) comes from the first equation in \(\mathbf{a}^t \mathbf{a} \mathbf{x_b} = \mathbf{a}^t \mathbf{b}\).,orthogonality
what happens to the weighted average \(\mathbf{x_b_w} = \frac{w_1^2 \mathbf{b_1} + w_2^2 \mathbf{b_2}}{w_1^2 + w_2^2}\) if the first weight \(w_1\) approaches zero? the measurement \(\mathbf{b_1}\) is totally unreliable.,orthogonality
"from \( m \) independent measurements \( b_1, \dots, b_m \) of your pulse rate, weighted by \( w_1, \dots, w_m \), what is the weighted average that replaces equation (9)? it is the best estimate when the statistical variances are \( \sigma_i^2 \equiv \frac{1}{w_i^2} \).",orthogonality
"if \( w = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \), find the \( w \)-inner product of \( \mathbf{x} = (2, 3) \) and \( \mathbf{y} = (1, 1) \), and the \( w \)-length of \( \mathbf{x} \). what line of vectors is \( w \)-perpendicular to \( \mathbf{y} \)?",orthogonality
"find the weighted least-squares solution \( \mathbf{x_b_w} \) to \( \mathbf{a} \mathbf{x} = \mathbf{b} \):
    \[
    \mathbf{a} = \begin{pmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 2 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}, \quad \mathbf{w} = \begin{pmatrix} 2 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}.
    \]
    check that the projection \( \mathbf{a} \mathbf{x_b_w} \) is still perpendicular (in the \( w \)-inner product) to the error \( \mathbf{b} - \mathbf{a} \mathbf{x_b_w} \).",orthogonality
"(a) suppose you guess your professor’s age, making errors \( e = -2, -1, 5 \) with probabilities \( \frac{1}{2}, \frac{1}{4}, \frac{1}{4} \). check that the expected error \( e(e) \) is zero and find the variance \( e(e^2) \).
    
    (b) if the professor guesses too (or tries to remember), making errors \( -1, 0, 1 \) with probabilities \( \frac{1}{8}, \frac{6}{8}, \frac{1}{8} \), what weights \( w_1 \) and \( w_2 \) give the reliability of your guess and the professor’s guess?",orthogonality
"(a) write the four equations for fitting \( y = c + dt \) to the data 
    \[
    y = -4 \text{ at } t = -2, \quad y = -3 \text{ at } t = -1, \quad y = -1 \text{ at } t = 1, \quad y = 0 \text{ at } t = 2.
    \]
    show that the columns are orthogonal.
    
    (b) find the optimal straight line, draw its graph, and write \( e^2 \).
    
    (c) interpret the zero error in terms of the original system of four equations in two unknowns: the right-hand side \( (-4, -3, -1, 0) \) is in the space.",orthogonality
"project \( b = (0,3,0) \) onto each of the orthonormal vectors \( a_1 = \left( \frac{2}{3}, \frac{2}{3}, -\frac{1}{3} \right) \) and \( a_2 = \left( -\frac{1}{3}, \frac{2}{3}, \frac{2}{3} \right) \), and then find its projection \( p \) onto the plane of \( a_1 \) and \( a_2 \).",orthogonality
"find also the projection of \( b = (0,3,0) \) onto \( a_3 = \left( \frac{2}{3}, -\frac{1}{3}, \frac{2}{3} \right) \), and add the three projections. why is \( p = a_1 a_1^t + a_2 a_2^t + a_3 a_3^t \) equal to \( i \)?",orthogonality
"if \( q_1 \) and \( q_2 \) are orthogonal matrices, so that \( q^t q = i \), show that \( q_1 q_2 \) is also orthogonal. if \( q_1 \) is rotation through \( \theta \), and \( q_2 \) is rotation through \( \phi \), what is \( q_1 q_2 \)? can you find the trigonometric identities for \( \sin(\theta + \phi) \) and \( \cos(\theta + \phi) \) in the matrix multiplication \( q_1 q_2 \)?",orthogonality
"if \( \mathbf{u} \) is a unit vector, show that \( q = i - 2 \mathbf{u} \mathbf{u}^t \) is a symmetric orthogonal matrix. (it is a reflection, also known as a householder transformation.) compute \( q \) when
    \[
    \mathbf{u}^t = \begin{pmatrix} \frac{1}{2} & \frac{1}{2} & -\frac{1}{2} & -\frac{1}{2} \end{pmatrix}.
    \]",orthogonality
"find a third column so that the matrix
    \[
    q = \begin{pmatrix} \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{14}} & \frac{1}{\sqrt{3}} & \frac{2}{\sqrt{14}} & \frac{1}{\sqrt{3}} & -\frac{3}{\sqrt{14}} \end{pmatrix}
    \]
    is orthogonal. it must be a unit vector that is orthogonal to the other columns; how much freedom does this leave? verify that the rows automatically become orthonormal at the same time.",orthogonality
"show, by forming \( b^t b \) directly, that pythagoras’s law holds for any combination
    \[
    b = x_1 q_1 + \dots + x_n q_n
    \]
    of orthonormal vectors: \( \| b \|^2 = x_1^2 + \dots + x_n^2 \). in matrix terms, \( b = qx \), so this again proves that lengths are preserved: \( \| qx \|^2 = \| x \|^2 \).",orthogonality
"project the vector \( b = (1,2) \) onto two vectors that are not orthogonal, \( a_1 = (1,0) \) and \( a_2 = (1,1) \). show that, unlike the orthogonal case, the sum of the two one-dimensional projections does not equal \( b \).",orthogonality
"if the vectors \( q_1, q_2, q_3 \) are orthonormal, what combination of \( q_1 \) and \( q_2 \) is closest to \( q_3 \)?",orthogonality
"if \( q_1 \) and \( q_2 \) are the outputs from gram-schmidt, what were the possible input vectors \( a \) and \( b \)?",orthogonality
show that an orthogonal matrix that is upper triangular must be diagonal.,orthogonality
what multiple of \( a_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix} \) should be subtracted from \( a_2 = \begin{pmatrix} 4 \\ 0 \end{pmatrix} \) to make the result orthogonal to \( a_1 \)? factor \( \begin{pmatrix} 1 & 4 \\ 1 & 0 \end{pmatrix} \) into qr with orthonormal vectors in \( q \).,orthogonality
"apply the gram-schmidt process to 
    \[
    a = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}, \quad b = \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}, \quad c = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}
    \]
    and write the result in the form \( a = qr \).",orthogonality
"from the nonorthogonal \( a, b, c \), find orthonormal vectors \( q_1, q_2, q_3 \):
    \[
    a = \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix}, \quad b = \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}, \quad c = \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}.
    \]",orthogonality
"find an orthonormal set \( q_1, q_2, q_3 \) for which \( q_1, q_2 \) span the column space of
    \[
    a = \begin{pmatrix} 1 & 1 \\ 2 & -1 \\ -2 & 4 \end{pmatrix}.
    \]
    which fundamental subspace contains \( q_3 \)? what is the least-squares solution of \( ax = b \) if \( b = \begin{pmatrix} 1 \\ 2 \\ 7 \end{pmatrix} \)?",orthogonality
"express the gram-schmidt orthogonalization of \( a_1, a_2 \) as \( a = qr \):
    \[
    a_1 = \begin{pmatrix} 1 \\ 2 \\ 2 \end{pmatrix}, \quad a_2 = \begin{pmatrix} 1 \\ 3 \\ 1 \end{pmatrix}.
    \]
    given \( n \) vectors \( a_i \) with \( m \) components, what are the shapes of \( a \), \( q \), and \( r \)?",orthogonality
"with the same matrix \( a \) as in problem 16, and with \( b = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} \), use \( a = qr \) to solve the least-squares problem \( ax = b \).",orthogonality
"if \( a = qr \), find a simple formula for the projection matrix \( p \) onto the column space of \( a \).",orthogonality
"show that these modified gram-schmidt steps produce the same \( c \) as in equation (10):
    \[
    c^* = c - (q_1^t c) q_1 \quad \text{and} \quad c = c^* - (q_2^t c^*) q_2.
    \]
    this is much more stable, to subtract the projections one at a time.",orthogonality
"in hilbert space, find the length of the vector \( v = \left( \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{4}}, \frac{1}{\sqrt{8}}, \dots \right) \) and the length of the function \( f(x) = e^x \) (over the interval \( 0 \leq x \leq 1 \)). what is the inner product over this interval of \( e^x \) and \( e^{-x} \)?",orthogonality
what is the closest function \( a \cos x + b \sin x \) to the function \( f(x) = \sin 2x \) on the interval from \( -\pi \) to \( \pi \)? what is the closest straight line \( c + dx \)?,orthogonality
"by setting the derivative to zero, find the value of \( b_1 \) that minimizes
    \[
    \| b_1 \sin x - \cos x \|^2 = \int_0^{2\pi} \left( b_1 \sin x - \cos x \right)^2 dx.
    \]
    compare with the fourier coefficient \( b_1 \).",orthogonality
"find the fourier coefficients \( a_0, a_1, b_1 \) of the step function \( y(x) \), which equals 1 on the interval \( 0 \leq x \leq \pi \) and 0 on the remaining interval \( \pi < x < 2\pi \):
    \[
    a_0 = \frac{(y, 1)}{(1, 1)}, \quad a_1 = \frac{(y, \cos x)}{(\cos x, \cos x)}, \quad b_1 = \frac{(y, \sin x)}{(\sin x, \sin x)}.
    \]",orthogonality
"find the fourth legendre polynomial. it is a cubic \( x^3 + ax^2 + bx + c \) that is orthogonal to 1, \( x \), and \( x^2 - \frac{1}{3} \) over the interval \( -1 \leq x \leq 1 \).",orthogonality
what is the closest straight line to the parabola \( y = x^2 \) over \( -1 \leq x \leq 1 \)?,orthogonality
"in the gram-schmidt formula (10), verify that \( c \) is orthogonal to \( q_1 \) and \( q_2 \).",orthogonality
"find an orthonormal basis for the subspace spanned by
    \[
    a_1 = (1, -1, 0, 0), \quad a_2 = (0, 1, -1, 0), \quad a_3 = (0, 0, 1, -1).
    \]",orthogonality
"apply gram-schmidt to \( (1, -1, 0), (0, 1, -1), (1, 0, -1) \), to find an orthonormal basis on the plane \( x_1 + x_2 + x_3 = 0 \). what is the dimension of this subspace, and how many nonzero vectors come out of gram-schmidt?",orthogonality
"(recommended) find orthogonal vectors \( a, b, c \) by gram-schmidt from \( a, b, c \):
    \[
    a = (1, -1, 0, 0), \quad b = (0, 1, -1, 0), \quad c = (0, 0, 1, -1).
    \]
    \( a, b, c \) and \( a, b, c \) are bases for the vectors perpendicular to \( d = (1, 1, 1, 1) \).",orthogonality
"if \( a = qr \), then \( a^t a = r^t r \) (triangular times triangular). gram-schmidt on \( a \) corresponds to elimination on \( a^t a \). compare
    \[
    a = \begin{bmatrix}
        1 & 0 & 0 \\
        -1 & 1 & 0 \\
        0 & -1 & 1 \\
        0 & 0 & -1
    \end{bmatrix}
    \quad \text{with} \quad 
    a^t a = \begin{bmatrix}
        2 & -1 & 0 \\
        -1 & 2 & -1 \\
        0 & -1 & 2
    \end{bmatrix}.
    \]
    for \( a^t a \), the pivots are \( 2, \frac{3}{2}, \frac{4}{3} \), and the multipliers are \( -\frac{1}{2} \) and \( -\frac{2}{3} \).
    \begin{enumerate}",orthogonality
"using those multipliers in \( a \), show that column 1 of \( a \) and \( b = \text{column 2} - \frac{1}{2} (\text{column 1}) \) and \( c = \text{column 3} - \frac{2}{3} (\text{column 2}) \) are orthogonal.",orthogonality
"check that \( \| \text{column 1} \|_2 = 2 \), \( \| b \|_2 = \frac{3}{2} \), and \( \| c \|_2 = \frac{4}{3} \), using the pivots.",orthogonality
"true or false (give an example in either case):
    \begin{enumerate}",orthogonality
\( q^{-1} \) is an orthogonal matrix when \( q \) is an orthogonal matrix.,orthogonality
"if \( q \) (3 by 2) has orthonormal columns, then \( \| qx \| \) always equals \( \| x \| \).",orthogonality
"find a basis for the subspace \( s \) in \( \mathbb{r}^4 \) spanned by all solutions of
        \[
        x_1 + x_2 + x_3 - x_4 = 0.
        \]",orthogonality
find a basis for the orthogonal complement \( s^\perp \).,orthogonality
"find \( b_1 \in s \) and \( b_2 \in s^\perp \) so that \( b_1 + b_2 = b = (1, 1, 1, 1) \).",orthogonality
what are \( f_2 \) and \( f_4 \) for the \( 4 \times 4 \) fourier matrix \( f \)?,orthogonality
find a permutation \( p \) of the columns of \( f \) that produces \( fp = f \) (for \( n \times n \)). combine with \( ff = ni \) to find \( f^2 \) and \( f^4 \) for the \( n \times n \) fourier matrix.,orthogonality
"if you form a \( 3 \times 3 \) submatrix of the \( 6 \times 6 \) matrix \( f_6 \), keeping only the entries in its first, third, and fifth rows and columns, what is that submatrix?",orthogonality
"mark all the sixth roots of \( 1 \) in the complex plane. what is the primitive root \( \omega_6 \)? (find its real and imaginary parts.) which power of \( \omega_6 \) is equal to \( \frac{1}{\omega_6} \)? what is
    \[
    1 + \omega + \omega^2 + \omega^3 + \omega^4 + \omega^5?
    \]",orthogonality
"find all solutions to the equation \( e^{ix} = -1 \), and all solutions to \( e^{i\theta} = i \).",orthogonality
"what are the square and the square root of \( \omega_{128} \), the primitive 128th root of 1?",orthogonality
"solve the \( 4 \times 4 \) system if the right-hand sides are \( y_0 = 2, y_1 = 0, y_2 = 2, y_3 = 0 \). in other words, solve \( f_4c = y \).",orthogonality
"solve the same system with \( y = (2,0,-2,0) \) by knowing \( f_4^{-1} \) and computing \( c = f_4^{-1} y \). verify that 
    \[
    c_0 + c_1 e^{ix} + c_2 e^{2ix} + c_3 e^{3ix}
    \]
    takes the values \( 2, 0, -2, 0 \) at the points \( x = 0, \frac{\pi}{2}, \pi, \frac{3\pi}{2} \).",orthogonality
"if \( y = (1,1,1,1) \), show that \( c = (1,0,0,0) \) satisfies \( f_4c = y \).",orthogonality
"now suppose \( y = (1,0,0,0) \), and find \( c \).",orthogonality
"for \( n = 2 \), write \( y_0 \) from the first line of equation (13) and \( y_1 \) from the second line.",orthogonality
"for \( n = 4 \), use the first line to find \( y_0 \) and \( y_1 \), and the second to find \( y_2 \) and \( y_3 \), all in terms of \( y_0 \) and \( y'' \).",orthogonality
"compute \( y = f_4c \) by the three steps of the fast fourier transform if \( c = (1,0,1,0) \).",orthogonality
"compute \( y = f_8c \) by the three steps of the fast fourier transform if \( c = (1,0,1,0,1,0,1,0) \). repeat the computation with \( c = (0,1,0,1,0,1,0,1) \).",orthogonality
"for the \( 4 \times 4 \) matrix, write out the formulas for \( c_0, c_1, c_2, c_3 \) and verify that if \( f \) is odd then \( c \) is odd. the vector \( f \) is odd if \( f_{n-j} = -f_j \); for \( n = 4 \) that means \( f_0 = 0, f_3 = -f_1, f_2 = 0 \) as in \( \sin 0, \sin \frac{\pi}{2}, \sin \pi, \sin \frac{3\pi}{2} \). this is copied by \( c \) and it leads to a fast sine transform.",orthogonality
multiply the three matrices in equation (16) and compare with \( f \). in which six entries do you need to know that \( i^2 = -1 \)?,orthogonality
invert the three factors in equation (14) to find a fast factorization of \( f^{-1} \).,orthogonality
\( f \) is symmetric. so transpose equation (14) to find a new fast fourier transform!,orthogonality
"all entries in the factorization of \( f_6 \) involve powers of \( \omega \), the sixth root of 1:
    \[
    f_6 =
    \begin{bmatrix}
        i & d \\
        i & -d
    \end{bmatrix}
    \begin{bmatrix}
        f_3 & 0 \\
        0 & f_3
    \end{bmatrix}
    p.
    \]
    write these factors with \( 1, \omega, \omega^2 \) in \( d \) and \( 1, \omega^2, \omega^4 \) in \( f_3 \). multiply!",orthogonality
"the columns of the fourier matrix $f$ are the eigenvectors of the cyclic permutation $p$. multiply $pf$ to find the eigenvalues $\lambda_0$ to $\lambda_3$:
    \begin{equation*}
        \begin{bmatrix}
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 1 \\
            1 & 0 & 0 & 0
        \end{bmatrix}
        \begin{bmatrix}
            1 & 1 & 1 & 1 \\
            1 & i & i^2 & i^3 \\
            1 & i^2 & i^4 & i^6 \\
            1 & i^3 & i^6 & i^9
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 1 & 1 & 1 \\
            1 & i & i^2 & i^3 \\
            1 & i^2 & i^4 & i^6 \\
            1 & i^3 & i^6 & i^9
        \end{bmatrix}
        \begin{bmatrix}
            \lambda_0 \\
            \lambda_1 \\
            \lambda_2 \\
            \lambda_3
        \end{bmatrix}.
    \end{equation*}
    this is $pf = f\lambda$ or $p = f\lambda f^{-1}$.",orthogonality
"two eigenvectors of this circulant matrix $c$ are $(1,1,1,1)$ and $(1,i,i^2,i^3)$. what are the eigenvalues $e_0$ and $e_1$?
    \begin{equation*}
        \begin{bmatrix}
            c_0 & c_1 & c_2 & c_3 \\
            c_3 & c_0 & c_1 & c_2 \\
            c_2 & c_3 & c_0 & c_1 \\
            c_1 & c_2 & c_3 & c_0
        \end{bmatrix}
        \begin{bmatrix}
            1 \\
            1 \\
            1 \\
            1
        \end{bmatrix}
        = e_0
        \begin{bmatrix}
            1 \\
            1 \\
            1 \\
            1
        \end{bmatrix}
    \end{equation*}
    and
    \begin{equation*}
        c
        \begin{bmatrix}
            1 \\
            i \\
            i^2 \\
            i^3
        \end{bmatrix}
        = e_1
        \begin{bmatrix}
            1 \\
            i \\
            i^2 \\
            i^3
        \end{bmatrix}.
    \end{equation*}",orthogonality
"find the eigenvalues of the “periodic” $-1, 2, -1$ matrix $c$. the $-1$s in the corners of $c$ make it periodic (a circulant matrix):
    \begin{equation*}
        c =
        \begin{bmatrix}
            2 & -1 & 0 & -1 \\
            -1 & 2 & -1 & 0 \\
            0 & -1 & 2 & -1 \\
            -1 & 0 & -1 & 2
        \end{bmatrix}
    \end{equation*}
    has $c_0 = 2, c_1 = -1, c_2 = 0, c_3 = -1$.",orthogonality
"to multiply $c$ times $x$, when $c = fef^{-1}$, we can multiply $f(e(f^{-1}x))$ instead. the direct $cx$ uses $n^2$ separate multiplications. knowing $e$ and $f$, the second way uses only $n \log_2 n + n$ multiplications. how many of those come from $e$, how many from $f$, and how many from $f^{-1}$?",orthogonality
"how could you quickly compute these four components of $fc$ starting from $c_0 + c_2$, $c_0 - c_2$, $c_1 + c_3$, $c_1 - c_3$? you are finding the fast fourier transform!
    \begin{equation*}
        fc =
        \begin{bmatrix}
            c_0 + c_1 + c_2 + c_3 \\
            c_0 + i c_1 + i^2 c_2 + i^3 c_3 \\
            c_0 + i^2 c_1 + i^4 c_2 + i^6 c_3 \\
            c_0 + i^3 c_1 + i^6 c_2 + i^9 c_3
        \end{bmatrix}
    \end{equation*}",orthogonality
"find the length of $a = (2,-2,1)$, and write two independent vectors that are perpendicular to $a$.",orthogonality
"find all vectors that are perpendicular to $(1,3,1)$ and $(2,7,2)$, by making those the rows of $a$ and solving $ax = 0$.",orthogonality
"what is the angle between $a = (2,-2,1)$ and $b = (1,2,2)$?",orthogonality
"what is the projection $p$ of $b = (1,2,2)$ onto $a = (2,-2,1)$?",orthogonality
"find the cosine of the angle between the vectors $(3,4)$ and $(4,3)$.",orthogonality
"where is the projection of $b = (1,1,1)$ onto the plane spanned by $(1,0,0)$ and $(1,1,0)$?",orthogonality
the system $ax = b$ has a solution if and only if $b$ is orthogonal to which of the four fundamental subspaces?,orthogonality
"which straight line gives the best fit to the following data: $b = 0$ at $t = 0$, $b = 0$ at $t = 1$, $b = 12$ at $t = 3$?",orthogonality
"construct the projection matrix $p$ onto the space spanned by $(1,1,1)$ and $(0,1,3)$.",orthogonality
which constant function is closest to $y = x^4$ (in the least-squares sense) over the interval $0 \leq x \leq 1$?,orthogonality
"if $q$ is orthogonal, is the same true of $q^3$?",orthogonality
find all $3 \times 3$ orthogonal matrices whose entries are zeros and ones.,orthogonality
"what multiple of $a_1$ should be subtracted from $a_2$, to make the result orthogonal to $a_1$? sketch a figure.",orthogonality
"factor 
    \[
    \begin{bmatrix}
        \cos\theta & \sin\theta \\
        \sin\theta & 0
    \end{bmatrix}
    \]
    into qr, recognizing that the first column is already a unit vector.",orthogonality
"if every entry in an orthogonal matrix is either $\frac{1}{4}$ or $-\frac{1}{4}$, how big is the matrix?",orthogonality
"suppose the vectors $q_1, \dots, q_n$ are orthonormal. if $b = c_1q_1 + \dots + c_nq_n$, give a formula for the first coefficient $c_1$ in terms of $b$ and the $q$'s.",orthogonality
"what words describe the equation $a^t a x = a^t b$, the vector $p = ax = pb$, and the matrix $p = a(a^t a)^{-1} a^t$?",orthogonality
"given the orthonormal vectors \( q_1 = \left(\frac{2}{3}, \frac{2}{3}, -\frac{1}{3} \right) \) and \( q_2 = \left(-\frac{1}{3}, \frac{2}{3}, \frac{2}{3} \right) \) as columns of \( q \):
    \begin{itemize}",orthogonality
find the matrices \( q^tq \) and \( qq^t \).,orthogonality
"show that \( qq^t \) is a projection matrix onto the plane of \( q_1 \) and \( q_2 \).
    \end{itemize}",orthogonality
"prove that if \( v_1, \dots, v_n \) is an orthonormal basis for \( \mathbb{r}^n \), then:
    \[
    v_1 v_1^t + \dots + v_n v_n^t = i.
    \]",orthogonality
"true or false: if the vectors \( x \) and \( y \) are orthogonal, and \( p \) is a projection, then \( px \) and \( py \) are orthogonal.",orthogonality
"fit a line \( b = c + dt \) through the points \( (t,b) = (2,0) \) and \( (2,6) \), and show that the normal equations break down. sketch all the optimal lines minimizing the sum of squared errors.",orthogonality
"find the point on the plane \( x+y-z=0 \) that is closest to \( b = (2,1,0) \).",orthogonality
"find an orthonormal basis for \( \mathbb{r}^3 \) starting with the vector \( (1,1,1) \).",orthogonality
"ct scanners reconstruct density matrices from projections. in the \( 2 \times 2 \) case, can you recover the matrix \( a \) if you know the sum along each row and column?",orthogonality
"can you recover a \( 3 \times 3 \) matrix if you know its row sums, column sums, and also the sums along the main diagonal and the four parallel diagonals?",orthogonality
"find an orthonormal basis for the plane \( x - y + z = 0 \), and determine the projection matrix \( p \) that projects onto the plane. what is the nullspace of \( p \)?",orthogonality
"let \( a = [3 \quad 1 \quad 1] \), and let \( v \) be the nullspace of \( a \):
    \begin{itemize}",orthogonality
"use gram-schmidt to construct an orthonormal pair \( q_1, q_2 \) from:
    \[
    a_1 = (4,5,2,2), \quad a_2 = (1,2,0,0).
    \]
    express \( a_1 \) and \( a_2 \) as combinations of \( q_1 \) and \( q_2 \), and find the triangular \( r \) in \( a = qr \).",orthogonality
"show that:
    \begin{itemize}",orthogonality
"is there a matrix whose row space contains \( (1,1,0) \) and whose nullspace contains \( (0,1,1) \)?",orthogonality
"find the distance from the plane \( x_1 + x_2 - x_3 - x_4 = 8 \) to the origin, and find the closest point on the plane.",orthogonality
"in a parallelogram with corners at \( 0, v, w, v + w \), show that the sum of the squared lengths of the four sides equals the sum of the squared lengths of the two diagonals.",orthogonality
"given:
    \[
    a =
    \begin{bmatrix}
    1 & -6 \\
    3 & 6 \\
    4 & 8 \\
    5 & 0 \\
    7 & 8
    \end{bmatrix}
    \]
    \begin{itemize}",orthogonality
"given the weighting matrix:
    \[
    w =
    \begin{bmatrix}
    2 & 1 \\
    1 & 0
    \end{bmatrix}
    \]
    find the \( w \)-inner product of \( (1,0) \) with \( (0,1) \).",orthogonality
"to solve a rectangular system $ax = b$, we replace $a^{-1}$ (which doesn’t exist) by $(a^ta)^{-1}a^t$ (which exists if $a$ has independent columns). show that this is a left-inverse of $a$ but not a right-inverse. on the left of $a$ it gives the identity; on the right it gives the projection $p$.",orthogonality
"find the straight line $c + dt$ that best fits the measurements $b = 0,1,2,5$ at times $t = 0,1,3,4$.",orthogonality
"find the curve $y = c + d t^2$ which gives the best least-squares fit to the measurements $y = 6$ at $t = 0$, $y = 4$ at $t = 1$, $y = 0$ at $t = 2$. write the three equations that are solved if the curve goes through the three points, and find the best $c$ and $d$.",orthogonality
"if the columns of $a$ are orthogonal to each other, what can you say about the form of $a^ta$? if the columns are orthonormal, what can you say then?",orthogonality
under what condition on the columns of $a$ (which may be rectangular) is $a^ta$ invertible?,orthogonality
"consider the fourier matrix \( f_n \) of order \( n \), whose entries are given by \( (f_n)_{jk} = \frac{1}{\sqrt{n}} e^{-2\pi i jk/n} \) for \( j, k = 0, 1, \dots, n-1 \). prove that \( f_n \) is a unitary matrix, i.e., show that \( f_n^\dagger f_n = i \), where \( i \) is the identity matrix of order \( n \). discuss the significance of this result in the context of fourier transforms.",orthogonality
compute the inverse of the fourier matrix \( f_4 \) explicitly and verify that \( f_4^{-1} = f_4^\dagger \). explain why this property generalizes to fourier matrices of any order \( n \) and describe its implications for signal processing and orthogonality.,orthogonality
"the discrete fourier transform (dft) of an \( n \)-dimensional vector \( x \) is given by \( x = f_n x \), where \( f_n \) is the fourier matrix. show that the inverse transform is given by \( x = f_n^\dagger x \). using this property, prove that applying the fourier transform twice results in a time-reversal operation, i.e., \( f_n^2 x = x^* \) (where \( x^* \) represents a permutation of \( x \) corresponding to time-reversal).",orthogonality
"let \( a \) be an \( n \times n \) circulant matrix, which means its rows are cyclic permutations of the first row. show that any circulant matrix can be diagonalized using the fourier matrix \( f_n \), i.e., prove that \( a = f_n d f_n^\dagger \), where \( d \) is a diagonal matrix whose entries are the eigenvalues of \( a \). compute the eigenvalues of a given circulant matrix \( a \) explicitly using the fast fourier transform (fft).",orthogonality
"show that the fourier matrix \( f_n \) forms an orthonormal basis for \( \mathbb{c}^n \) and explain how this relates to parseval's theorem. given a function sampled at \( n \) equally spaced points, demonstrate how the fourier basis can be used to expand the function in terms of complex exponentials.",orthogonality
"consider a function \( f(x) \) defined on a discrete set of points \( x_k = k/n \) for \( k = 0, 1, \dots, n-1 \). define the fourier coefficients as \( c_k = \sum_{j=0}^{n-1} f(x_j) e^{-2\pi i jk/n} \). show that the set of exponentials \( e^{2\pi i jk/n} \) forms an orthonormal basis with respect to the inner product \( \langle f, g \rangle = \sum_{k=0}^{n-1} f(x_k) \overline{g(x_k)} \).",orthogonality
"the fast fourier transform (fft) reduces the computational complexity of the discrete fourier transform (dft) from \( o(n^2) \) to \( o(n \log n) \). derive the recursive formula for the fft algorithm by splitting an \( n \)-point dft into two \( n/2 \)-point dfts, and show how this leads to an efficient divide-and-conquer approach.",orthogonality
consider the fourier matrix \( f_8 \). compute its eigenvalues and eigenvectors explicitly. show that the eigenvalues are roots of unity and discuss the significance of the eigenvectors in terms of harmonic decomposition.,orthogonality
"given the fourier matrix \( f_n \), show that its powers satisfy the property \( f_n^4 = i \) when \( n \) is a power of 2. use this result to construct an efficient algorithm for computing the fft of a sequence by breaking it into subproblems recursively.",orthogonality
"prove that the fourier matrix \( f_n \) diagonalizes any toeplitz matrix whose entries depend only on the difference \( i - j \). explain why such matrices appear frequently in signal processing and image analysis, and derive the spectral decomposition of a given toeplitz matrix using the fourier transform.",orthogonality
"consider the space of continuous functions on the interval \( [0,1] \) with the inner product defined by 
    \[
    \langle f, g \rangle = \int_0^1 f(x) g(x) \,dx.
    \]
    use the gram-schmidt process to construct an orthonormal basis for the space spanned by the functions \( \{1, x, x^2\} \).",orthogonality
"let \( v \) be the space of real polynomials of degree at most \( n \) with inner product 
    \[
    \langle f, g \rangle = \int_{-1}^{1} f(x) g(x) w(x) \,dx,
    \]
    where \( w(x) \) is a weight function. apply the gram-schmidt process to derive the first three orthogonal polynomials when \( w(x) = 1 \), leading to the legendre polynomials.",orthogonality
"consider the space of square-integrable functions \( l^2([0, \pi]) \) with the inner product 
    \[
    \langle f, g \rangle = \int_0^{\pi} f(x) g(x) \,dx.
    \]
    apply the gram-schmidt process to the functions \( \{1, \sin x, \sin 2x\} \) to construct an orthonormal basis.",orthogonality
"the set of functions \( \{e^{ix}, e^{2ix}, e^{3ix}\} \) is not orthogonal under the usual \( l^2([0,2\pi]) \) inner product. use the gram-schmidt process to construct an orthonormal basis from this set and compare it to the standard fourier basis.",orthogonality
"given a symmetric matrix \( a \) of size \( n \times n \), the eigenvectors of \( a \) form a basis for \( \mathbb{r}^n \). show that if the eigenvectors are not initially orthonormal, applying the gram-schmidt process to them yields an orthonormal set that diagonalizes \( a \).",orthogonality
"let \( m \) be an \( m \times n \) matrix with independent column vectors. explain why applying the gram-schmidt process to the column vectors of \( m \) results in an orthonormal basis for the column space of \( m \), and derive the qr decomposition of \( m \) using this method.",orthogonality
"consider the space of continuous periodic functions on \( [-\pi, \pi] \) with inner product 
    \[
    \langle f, g \rangle = \int_{-\pi}^{\pi} f(x) g(x) \,dx.
    \]
    apply gram-schmidt to the trigonometric polynomials \( \{1, \cos x, \cos 2x\} \) to construct an orthonormal set and interpret the result in terms of fourier series.",orthogonality
"let \( \{f_1, f_2, \dots, f_n\} \) be a linearly independent set of functions in an inner product space. show that the gram-schmidt process transforms this set into an orthonormal set while preserving the span of the original functions.",orthogonality
the matrix representation of a linear transformation \( t: v \to v \) in an orthonormal basis is unitary if \( v \) is a complex inner product space. use the gram-schmidt process to construct such an orthonormal basis from an arbitrary basis and explain why this simplifies the analysis of \( t \).,orthogonality
"in the vector space of polynomials with the inner product 
    \[
    \langle p, q \rangle = \int_{0}^{1} p(x) q(x) e^{-x} \,dx,
    \]
    apply the gram-schmidt process to the set \( \{1, x, x^2\} \) to obtain the first three orthonormal polynomials associated with the laguerre polynomials.",orthogonality
"given a set of data points \( (x_i, y_i) \) for \( i = 1, 2, \dots, n \), the best straight-line approximation in the least squares sense minimizes the sum of squared errors. derive the normal equations for finding the best-fitting line \( y = mx + c \) using the concept of orthogonality in linear algebra. prove that the residual vector is orthogonal to the column space of the design matrix.",orthogonality
"consider the problem of fitting a straight line to a given dataset using orthogonal projections. let \( a \) be the matrix whose columns are the basis vectors \( [1, x] \) for the space of linear functions, and let \( b \) be the vector of observed values. show that the least squares solution corresponds to the orthogonal projection of \( b \) onto the column space of \( a \). determine the conditions under which the solution is unique.",orthogonality
"the best straight-line approximation in the sense of least squares can be obtained using the qr decomposition of the design matrix. given a set of points \( (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n) \), construct the matrix equation for least squares regression and apply the qr factorization to solve for the coefficients of the best-fit line. show explicitly how the orthogonality of \( q \) simplifies the computation.",orthogonality
"consider a set of \( n \) points \( (x_i, y_i) \) that are subject to measurement errors. instead of minimizing the vertical distances (standard least squares), one can minimize the perpendicular distances from the points to the line, leading to the total least squares approach. formulate the problem using singular value decomposition (svd) and show how the best straight-line fit corresponds to the right singular vector associated with the smallest singular value of a transformed data matrix.",orthogonality
"the best straight-line approximation problem is a special case of the more general problem of finding the best affine subspace approximation. given a dataset in \( \mathbb{r}^m \), formulate the problem of finding the best \( k \)-dimensional affine subspace using the concept of orthogonality and singular value decomposition. prove that the principal components of the data provide the optimal directions for such a subspace and explain how this reduces to the best straight-line fit when \( k = 1 \).",orthogonality
"consider the space of square-integrable functions on the interval \( [-\pi, \pi] \), denoted as \( l^2([-\pi, \pi]) \). the fourier basis consists of the functions \( \{1, \cos(nx), \sin(nx) \}_{n=1}^{\infty} \). prove that this set forms an orthogonal basis under the inner product 
    \[
    \langle f, g \rangle = \int_{-\pi}^{\pi} f(x) g(x) \,dx.
    \]
    then, derive the explicit fourier series expansion of an arbitrary function \( f(x) \in l^2([-\pi, \pi]) \) in terms of these basis functions.",orthogonality
"the fourier series representation of a function \( f(x) \) is given by
    \[
    f(x) = \frac{a_0}{2} + \sum_{n=1}^{\infty} \left( a_n \cos(nx) + b_n \sin(nx) \right).
    \]
    using the orthogonality of the fourier basis functions, derive explicit expressions for the fourier coefficients \( a_n \) and \( b_n \). show that the inner product structure leads to these coefficients being projections of \( f(x) \) onto the basis functions.",orthogonality
"consider a function \( f(x) \) defined on \( [-\pi, \pi] \) with a discontinuity at \( x = x_0 \). use the orthogonality of the fourier basis to show that the fourier series of \( f(x) \) converges to the average of the left-hand and right-hand limits at \( x_0 \) (gibbs phenomenon). provide a rigorous proof that the fourier series satisfies this property.",orthogonality
"the fourier matrix \( f_n \) of order \( n \) is given by
    \[
    f_n = \frac{1}{\sqrt{n}} \left[ e^{2\pi i jk/n} \right]_{j,k=0}^{n-1}.
    \]
    show that \( f_n \) is unitary, meaning \( f_n^* f_n = i_n \), and explain its connection to the discrete fourier transform (dft). prove that the columns of \( f_n \) form an orthonormal set with respect to the standard inner product in \( \mathbb{c}^n \).",orthogonality
"the fourier series expansion of a function \( f(x) \) on \( [-\pi, \pi] \) is known to converge in the \( l^2 \)-norm due to the completeness of the trigonometric basis. prove that the sequence of partial sums of the fourier series forms a best approximation to \( f(x) \) in the sense of the \( l^2 \)-norm, i.e., it minimizes the error 
    \[
    \int_{-\pi}^{\pi} \left| f(x) - s_n(x) \right|^2 dx,
    \]
    where \( s_n(x) \) is the \( n \)th partial sum of the fourier series. use the concept of orthogonal projections in hilbert spaces to justify this result.",orthogonality
"let \( h \) be a hilbert space with an inner product \( \langle \cdot, \cdot \rangle \). prove that every cauchy sequence in \( h \) converges to a unique limit in \( h \). use this result to show that any closed subspace of \( h \) is also a hilbert space.",orthogonality
"given a hilbert space \( h \) and an orthonormal basis \( \{e_n\}_{n=1}^{\infty} \), prove that for any vector \( v \in h \), the series
    \[
    v = \sum_{n=1}^{\infty} \langle v, e_n \rangle e_n
    \]
    converges in the norm induced by the inner product. show that this expansion minimizes the squared error among all finite approximations of \( v \) using a fixed number of basis elements.",orthogonality
"prove that if \( h \) is a hilbert space, then every bounded linear functional \( f: h \to \mathbb{r} \) (or \( \mathbb{c} \)) can be represented as an inner product with a fixed element of \( h \), i.e., there exists a unique \( g \in h \) such that
    \[
    f(v) = \langle v, g \rangle \quad \forall v \in h.
    \]
    this is known as the riesz representation theorem. provide a constructive proof of this result.",orthogonality
"consider the space \( l^2([a, b]) \) of square-integrable functions with the inner product
    \[
    \langle f, g \rangle = \int_{a}^{b} f(x) g(x) \,dx.
    \]
    show that this space is a hilbert space. furthermore, prove that the set of legendre polynomials forms an orthonormal basis for this space on \( [-1,1] \).",orthogonality
"define the notion of an orthogonal projection onto a closed subspace \( v \) of a hilbert space \( h \). show that for any vector \( x \in h \), there exists a unique vector \( v \in v \) such that \( x - v \) is orthogonal to \( v \). prove that this projection is a linear operator and is idempotent.",orthogonality
"prove that in a hilbert space \( h \), the gram-schmidt process applied to a linearly independent sequence produces an orthonormal sequence that spans the same subspace. use this result to construct an explicit orthonormal basis for the space of polynomials up to degree \( n \) in \( l^2([-1,1]) \) using the gram-schmidt process.",orthogonality
"let \( h \) be a hilbert space, and let \( t: h \to h \) be a bounded linear operator. prove that if \( t \) is self-adjoint, then all of its eigenvalues are real. show that if \( t \) is compact and self-adjoint, then it has an orthonormal basis of eigenvectors.",orthogonality
"suppose that \( h \) is an infinite-dimensional hilbert space. show that every sequence \( \{x_n\} \) in \( h \) has a weakly convergent subsequence, i.e., there exists \( x \in h \) such that
    \[
    \langle x_n, y \rangle \to \langle x, y \rangle
    \]
    for all \( y \in h \). provide an example where strong convergence fails but weak convergence holds.",orthogonality
"given a self-adjoint operator \( t \) on a hilbert space \( h \), prove that there exists an orthonormal basis of eigenvectors if and only if \( t \) is compact. use this to explain why infinite-dimensional self-adjoint operators may not have an eigenvalue decomposition.",orthogonality
"consider a bounded sequence \( \{x_n\} \) in a hilbert space \( h \). prove that if the sequence satisfies the condition
    \[
    \|x_{n+1} - x_n\| \to 0,
    \]
    then \( \{x_n\} \) is weakly convergent. discuss why this property is crucial in the study of iterative algorithms in numerical linear algebra.",orthogonality
"let \( a \) be an \( m \times n \) matrix with full column rank. show that there exists an orthogonal matrix \( q \) of size \( m \times m \) and an upper triangular matrix \( r \) of size \( m \times n \) such that \( a = qr \). prove that if \( a \) has full column rank, then \( r \) has full row rank and its leading \( n \times n \) submatrix is invertible.",orthogonality
"suppose that \( a \) is an \( m \times n \) matrix where \( m > n \) and has full column rank. derive an explicit algorithm for computing the qr decomposition using the gram-schmidt process. explain why this method may suffer from numerical instability, and suggest an alternative method for computing the qr factorization.",orthogonality
"consider the householder transformation-based approach to qr factorization. given an arbitrary matrix \( a \), show how householder reflectors can be used to iteratively reduce \( a \) to an upper triangular form. prove that the product of householder reflectors forms an orthogonal matrix \( q \).",orthogonality
"the qr decomposition can be used to solve the least squares problem \( ax = b \) for an overdetermined system \( a \in \mathbb{r}^{m \times n} \) with \( m > n \). show that using qr factorization, the normal equation solution can be rewritten as \( rx = q^tb \). prove that this formulation avoids issues related to ill-conditioning that arise in normal equations.",orthogonality
"prove that for any invertible square matrix \( a \), the qr factorization can be used to compute the eigenvalues of \( a \) iteratively using the qr algorithm. show that if \( a \) is symmetric, the iterates \( a_k = q_k^t a_{k-1} q_k \) converge to a diagonal matrix containing the eigenvalues of \( a \).",orthogonality
let \( a \) be an \( m \times n \) matrix with full column rank. show that the qr decomposition is unique if and only if the diagonal elements of \( r \) are all positive. provide an example where the qr decomposition is not unique and explain why.,orthogonality
consider the modified gram-schmidt algorithm as a variation of the classical gram-schmidt process for computing the qr decomposition. show that the modified gram-schmidt process produces the same result as the classical approach but with improved numerical stability.,orthogonality
"the qr decomposition can be applied to computing the singular value decomposition (svd) of a matrix. suppose \( a \) is an \( m \times n \) matrix of full rank. show that by first computing the qr factorization of \( a \), followed by applying the svd to \( r \), one can derive the singular values and singular vectors of \( a \).",orthogonality
let \( a \) be a symmetric positive definite matrix. show that the qr factorization of \( a \) can be used in iterative methods such as the conjugate gradient method to solve the linear system \( ax = b \). explain how qr factorization improves numerical stability in such iterative methods.,orthogonality
consider an ill-conditioned matrix \( a \). prove that the qr factorization provides a numerically stable method for computing the solution to the least squares problem compared to normal equations. provide an explicit example where the use of qr factorization significantly improves numerical accuracy over the direct normal equation approach.,orthogonality
"let \( v \) be an inner product space, and let \( \{ v_1, v_2, \dots, v_n \} \) be a linearly independent set in \( v \). prove that the gram-schmidt process generates an orthonormal basis \( \{ q_1, q_2, \dots, q_n \} \) such that each \( q_i \) is a linear combination of \( v_1, \dots, v_i \). show that the transformation from \( \{ v_1, \dots, v_n \} \) to \( \{ q_1, \dots, q_n \} \) is unique up to sign.",orthogonality
"given an \( m \times n \) matrix \( a \) with full column rank, show that the qr decomposition \( a = qr \) provides an efficient way to determine an orthonormal basis for the column space of \( a \). prove that the first \( n \) columns of \( q \) form such a basis and discuss the geometric significance.",orthogonality
consider the classical gram-schmidt process and the modified gram-schmidt process for constructing an orthonormal basis. show that both methods produce the same orthonormal basis in exact arithmetic but differ in numerical stability. provide an example where the classical gram-schmidt process fails due to numerical instability.,orthogonality
"prove that if \( a \) is an \( m \times n \) matrix with full column rank, then the qr factorization \( a = qr \) is unique if and only if the diagonal elements of \( r \) are positive. provide an explicit example where qr decomposition is not unique.",orthogonality
"the qr decomposition is widely used in solving least squares problems. prove that if \( ax = b \) is an overdetermined system where \( a \) has full column rank, then the least squares solution can be written as \( x = r^{-1} q^t b \). explain why this method is preferred over solving the normal equations.",orthogonality
"show that the qr decomposition can be applied iteratively to compute the eigenvalues of a symmetric matrix using the qr algorithm. prove that if \( a \) is symmetric, then the sequence \( a_k = q_k^t a_{k-1} q_k \) converges to a diagonal matrix whose diagonal entries are the eigenvalues of \( a \).",orthogonality
"let \( a \) be an \( m \times n \) matrix with full column rank, and let \( a = qr \) be its qr decomposition. show that the columns of \( q \) form an orthonormal basis for the column space of \( a \), and explain how this decomposition can be used to find the best approximation of a vector \( b \) in the column space of \( a \).",orthogonality
"prove that for any invertible matrix \( a \), the qr factorization can be used to compute the singular value decomposition (svd). show that if \( a = qr \), then applying svd to \( r \) provides the singular values and singular vectors of \( a \).",orthogonality
consider an \( n \times n \) symmetric matrix \( a \) with distinct eigenvalues. prove that the sequence \( a_k = q_k^t a_{k-1} q_k \) generated by the qr algorithm converges to a diagonal matrix whose entries are the eigenvalues of \( a \). justify why this holds for symmetric matrices.,orthogonality
"let \( a \) be an \( m \times n \) matrix with full column rank. show that the gram-schmidt process applied to the columns of \( a \) results in a qr decomposition \( a = qr \). prove that if \( a \) has linearly dependent columns, then the gram-schmidt process fails to produce a full set of orthonormal vectors.",orthogonality
"consider a unitary matrix \( u \) of size \( n \times n \). prove that its qr decomposition satisfies \( u = qr \) where \( q = u \) and \( r \) is an upper triangular matrix with unit diagonal entries. show that in this case, \( r \) is necessarily diagonal.",orthogonality
"given an ill-conditioned matrix \( a \), explain why using qr decomposition is numerically stable for solving \( ax = b \) in least squares problems compared to solving the normal equations \( a^t a x = a^t b \). provide an explicit example illustrating this.",orthogonality
"show that for any rectangular matrix \( a \), the qr decomposition can be used to compute the pseudo-inverse \( a^+ \). prove that if \( a = qr \), then \( a^+ = r^{-1} q^t \) when \( a \) has full column rank.",orthogonality
"prove that if \( a \) is an \( n \times n \) symmetric matrix with distinct eigenvalues, then the qr algorithm converges to an orthonormal basis of eigenvectors of \( a \). explain why the shift technique is introduced to accelerate convergence.",orthogonality
let \( a \) be an \( m \times n \) matrix of full rank and let \( a = qr \) be its qr factorization. show that the projection of any vector \( b \) onto the column space of \( a \) can be written as \( \text{proj}_{\text{col}(a)} b = qq^t b \). interpret this result geometrically.,orthogonality
"let \( a \) be an \( m \times n \) matrix with full column rank, and let \( w \) be a positive definite diagonal matrix of size \( m \times m \). derive the normal equations for the weighted least squares problem 
    \[
    \min_x \| w^{1/2} (ax - b) \|_2^2.
    \]
    show that the solution is given by \( x = (a^t w a)^{-1} a^t w b \) and discuss under what conditions this solution is unique.",orthogonality
consider a dataset consisting of \( m \) observations with varying levels of reliability. explain how a weighted least squares approach can be used to account for different levels of confidence in the observations. show that choosing the weight matrix \( w \) as the inverse of the variance-covariance matrix of the errors minimizes the variance of the estimator \( x \).,orthogonality
"given a system of linear equations \( ax = b \) where \( a \) is an \( m \times n \) matrix and the measurement errors have known variances \( \sigma_1^2, \sigma_2^2, \dots, \sigma_m^2 \), explain how the choice \( w = \text{diag}(\sigma_1^{-2}, \sigma_2^{-2}, \dots, \sigma_m^{-2}) \) leads to an optimal solution in the sense of minimizing the variance of \( x \). prove that the weighted least squares estimator is unbiased.",orthogonality
"suppose \( a \) is an \( m \times n \) matrix with full column rank and \( w \) is a symmetric positive definite weight matrix. show that the residual \( r = b - ax \) in the weighted least squares solution is orthogonal to the weighted column space of \( a \), i.e.,
    \[
    a^t w r = 0.
    \]
    interpret this result geometrically.",orthogonality
"consider the problem of fitting a polynomial \( p(x) = a_0 + a_1 x + \dots + a_n x^n \) to a given set of data points \( (x_i, y_i) \) for \( i = 1, \dots, m \). derive the weighted least squares normal equations for determining the coefficients \( a_0, a_1, \dots, a_n \) when each observation \( y_i \) has an associated weight \( w_i \). discuss conditions under which the solution is unique.",orthogonality
"show that if \( a \) is an \( m \times n \) matrix with full column rank, then the matrix \( (a^t w a) \) in the weighted least squares solution is symmetric and positive definite, ensuring that the normal equations have a unique solution. give an example where dropping the full column rank assumption leads to non-uniqueness.",orthogonality
"consider an iterative refinement approach to solving the weighted least squares problem, where the weights are updated dynamically based on residual errors. formulate an iterative algorithm that adjusts the weights as \( w_{k+1} = \text{diag}(|r_k|^{-1}) \), where \( r_k \) is the residual at step \( k \). analyze the convergence of this approach and its relationship to robust regression methods.",orthogonality
"the qr decomposition is often used for solving least squares problems. given the weighted least squares system \( \min_x \| w^{1/2} (ax - b) \|_2^2 \), show that applying a qr decomposition to \( w^{1/2} a \) provides a stable way to compute \( x \). compare this method with the normal equations approach in terms of numerical stability.",orthogonality
"let \( a \) be an \( m \times n \) matrix with full column rank, and let \( w \) be a symmetric positive definite weight matrix. consider a perturbation in the data \( b \), given by \( b' = b + \delta b \). show that the sensitivity of the weighted least squares solution \( x \) to changes in \( b \) depends on the condition number of \( a^t w a \). provide an interpretation of this result in practical applications.",orthogonality
"show that if the weight matrix \( w \) in a weighted least squares problem is chosen optimally based on the error distribution, the resulting estimator is the best linear unbiased estimator (blue) according to the gauss-markov theorem. prove this result formally and discuss its implications in statistics and machine learning.",orthogonality
"let \( a \) be an \( m \times n \) matrix with full column rank, and let \( b \in \mathbb{r}^m \). derive the least squares solution \( x \) that minimizes \( \| ax - b \|_2^2 \) and show that the error vector is orthogonal to the column space of \( a \).",orthogonality
prove that the least squares solution \( x = (a^t a)^{-1} a^t b \) exists and is unique when \( a \) has full column rank. what happens when \( a \) does not have full column rank?,orthogonality
"define the projection matrix \( p \) that projects any vector \( b \in \mathbb{r}^m \) onto the column space of \( a \). show that \( p \) is symmetric and idempotent, i.e., \( p^2 = p \).",orthogonality
"show that for any \( m \times n \) matrix \( a \) with full column rank, the projection matrix onto \( \text{col}(a) \) is given by \( p = a (a^t a)^{-1} a^t \). interpret this result geometrically.",orthogonality
"consider the problem of fitting a linear model \( y = a_0 + a_1 x \) to given data points \( (x_i, y_i) \) for \( i = 1, \dots, m \). derive the normal equations for the least squares estimates of \( a_0 \) and \( a_1 \).",orthogonality
"given a data set of points \( (x_i, y_i) \) in \( \mathbb{r}^2 \), derive the best quadratic polynomial fit \( y = a_0 + a_1 x + a_2 x^2 \) using least squares. discuss conditions for uniqueness of the solution.",orthogonality
let \( a \) be an \( m \times n \) matrix with full column rank. prove that the residual vector \( r = b - ax \) is orthogonal to every column of \( a \).,orthogonality
"show that if \( a \) is an orthogonal matrix, then the least squares solution simplifies to \( x = a^t b \). interpret this result.",orthogonality
consider a subspace \( s \) of \( \mathbb{r}^m \) spanned by the columns of an \( m \times n \) matrix \( a \). define the orthogonal complement of \( s \) and explain why any vector in \( \mathbb{r}^m \) can be uniquely decomposed into a sum of a vector in \( s \) and a vector in \( s^\perp \).,orthogonality
"given a set of data points \( (x_i, y_i) \) in \( \mathbb{r}^2 \), explain how the normal equations for least squares fitting can be obtained from an orthogonal projection argument.",orthogonality
show that the projection matrix \( p = a (a^t a)^{-1} a^t \) satisfies \( p^t = p \) and \( p^2 = p \). interpret these properties in terms of orthogonality and projection onto a subspace.,orthogonality
"compute the least squares solution for the overdetermined system
    \[
    \begin{bmatrix} 
    1 & 1 \\ 
    1 & 2 \\ 
    1 & 3 
    \end{bmatrix} 
    \begin{bmatrix} 
    x_1 \\ 
    x_2 
    \end{bmatrix} 
    = 
    \begin{bmatrix} 
    1 \\ 
    2 \\ 
    2 
    \end{bmatrix}.
    \]
    find the corresponding projection matrix and verify that the residual is orthogonal to the column space of \( a \).",orthogonality
"show that for a square invertible matrix \( a \), the projection matrix onto the column space of \( a \) is the identity matrix \( i \). interpret this result in terms of least squares solutions.",orthogonality
consider a matrix \( a \) whose columns form an orthonormal basis for \( \mathbb{r}^m \). show that the least squares solution simplifies to \( x = a^t b \) and compare it to the general case where \( a \) is not orthonormal.,orthogonality
"let \( a \) be an \( m \times n \) matrix with full column rank, and let \( p \) be the projection matrix onto \( \text{col}(a) \). prove that for any vector \( b \in \mathbb{r}^m \), the projection \( pb \) minimizes the distance from \( b \) to \( \text{col}(a) \).",orthogonality
show that the determinant of the projection matrix \( p = a (a^t a)^{-1} a^t \) is zero when \( a \) is not square. explain why this must be the case.,orthogonality
discuss how gram-schmidt orthogonalization can be used to derive an alternative formula for the least squares solution when \( a \) has full column rank. compare this with the normal equations approach.,orthogonality
show that the eigenvalues of the projection matrix \( p = a (a^t a)^{-1} a^t \) are either 0 or 1. explain why this is expected based on the geometric interpretation of projections.,orthogonality
"let \( u \) and \( w \) be subspaces of an inner product space \( v \) such that \( u \cap w = \{0\} \). show that \( u \) and \( w \) are orthogonal if and only if for all \( u \in u \) and \( w \in w \), the inner product satisfies \( \langle u, w \rangle = 0 \). further, prove that if \( v = u \oplus w \), then every vector in \( v \) can be uniquely decomposed as the sum of vectors from \( u \) and \( w \).",orthogonality
"consider an \( m \times n \) matrix \( a \) with full column rank. define the projection matrix \( p = a(a^t a)^{-1} a^t \) that projects vectors onto the column space of \( a \). show that for any \( b \in \mathbb{r}^m \), the vector \( pb \) is the closest vector to \( b \) in the column space of \( a \). additionally, prove that the residual \( (i - p)b \) is orthogonal to the column space of \( a \).",orthogonality
"given a nonzero vector \( a \in \mathbb{r}^n \), prove that the projection of a vector \( b \) onto the line spanned by \( a \) is given by
    \[
    \text{proj}_a (b) = \frac{\langle b, a \rangle}{\langle a, a \rangle} a.
    \]
    show that the residual vector \( r = b - \text{proj}_a (b) \) is orthogonal to \( a \), and use this to prove the cauchy-schwarz inequality.",orthogonality
"let \( p \) be a symmetric matrix such that \( p^2 = p \). prove that the eigenvalues of \( p \) are either 0 or 1, and interpret this result in terms of projections onto subspaces. further, show that the rank of \( p \) equals the dimension of the subspace onto which \( p \) projects.",orthogonality
"define the cosine of the angle \( \theta \) between two nonzero vectors \( a, b \in \mathbb{r}^n \) using the inner product:
    \[
    \cos \theta = \frac{\langle a, b \rangle}{\|a\| \|b\|}.
    \]
    prove that \( -1 \leq \cos \theta \leq 1 \) and characterize the cases when \( \cos \theta = 1, 0, -1 \). additionally, derive an expression for the projection of \( b \) onto \( a \) in terms of \( \cos \theta \).",orthogonality
"consider the problem of projecting a point \( p \) onto a plane in \( \mathbb{r}^3 \) defined by a normal vector \( n \). derive a formula for the projection of \( p \) onto the plane and show that the distance from \( p \) to the plane is given by \( \frac{|\langle p, n \rangle|}{\|n\|} \). apply this result to find the projection of the point \( (3, 4, 5) \) onto the plane \( x + 2y + 3z = 6 \).",orthogonality
"the quadratic \( f = x^2 + 4xy + 2y^2 \) has a saddle point at the origin, despite the fact that its coefficients are positive. write \( f \) as a difference of two squares.",positive definite matrices
"decide for or against the positive definiteness of these matrices, and write out the corresponding \( f = x^t a x \):
    \begin{enumerate}",positive definite matrices
\( \begin{bmatrix} 1 & 3 \\ 3 & 5 \end{bmatrix} \),positive definite matrices
\( \begin{bmatrix} 1 & -1 \\ -1 & 1 \end{bmatrix} \),positive definite matrices
\( \begin{bmatrix} 2 & 3 \\ 3 & 5 \end{bmatrix} \),positive definite matrices
\( \begin{bmatrix} -1 & 2 \\ 2 & -8 \end{bmatrix} \),positive definite matrices
"if a 2 by 2 symmetric matrix passes the tests \( a > 0 \), \( ac > b^2 \), solve the quadratic equation \( \det(a - \lambda i) = 0 \) and show that both eigenvalues are positive.",positive definite matrices
"decide between a minimum, maximum, or saddle point for the following functions:
    \begin{enumerate}",positive definite matrices
\( f = -1 + 4(e^x - x) - 5x \sin y + 6y^2 \) at the point \( x = y = 0 \).,positive definite matrices
"\( f = (x^2 - 2x) \cos y \), with a stationary point at \( x = 1, y = \pi \).",positive definite matrices
\begin{enumerate},positive definite matrices
for which numbers \( b \) is the matrix \( a = \begin{bmatrix} 1 & b \\ b & 9 \end{bmatrix} \) positive definite?,positive definite matrices
factor \( a = ldl^t \) when \( b \) is in the range for positive definiteness.,positive definite matrices
find the minimum value of \( \frac{1}{2}(x^2 + 2bxy + 9y^2) - y \) for \( b \) in this range.,positive definite matrices
what is the minimum if \( b = 3 \)?,positive definite matrices
"suppose the positive coefficients \( a \) and \( c \) dominate \( b \) in the sense that \( a + c > 2b \). find an example that has \( ac < b^2 \), so the matrix is not positive definite.",positive definite matrices
"what 3 by 3 symmetric matrices \( a_1 \) and \( a_2 \) correspond to \( f_1 \) and \( f_2 \)?
        \[
        f_1 = x_1^2 + x_2^2 + x_3^2 - 2x_1x_2 - 2x_1x_3 + 2x_2x_3
        \]
        \[
        f_2 = x_1^2 + 2x_2^2 + 11x_3^2 - 2x_1x_2 - 2x_1x_3 - 4x_2x_3
        \]",positive definite matrices
show that \( f_1 \) is a single perfect square and not positive definite. where is \( f_1 \) equal to 0?,positive definite matrices
factor \( a_2 \) into \( ll^t \). write \( f_2 = x^t a_2 x \) as a sum of three squares.,positive definite matrices
"if \( a = \begin{bmatrix} a & b \\ b & c \end{bmatrix} \) is positive definite, test \( a^{-1} = \begin{bmatrix} p & q \\ q & r \end{bmatrix} \) for positive definiteness.",positive definite matrices
"the quadratic \( f(x_1, x_2) = 3(x_1 + 2x_2)^2 + 4x_2^2 \) is positive. find its matrix \( a \), factor it into \( ldl^t \), and connect the entries in \( d \) and \( l \) to 3, 2, 4 in \( f \).",positive definite matrices
"if \( r = \begin{bmatrix} p & q \\ q & r \end{bmatrix} \), write out \( r^2 \) and check that it is positive definite unless \( r \) is singular.",positive definite matrices
"if \( a = \begin{bmatrix} a & b \\ b & c \end{bmatrix} \) is hermitian (complex \( b \)), find its pivots and determinant.",positive definite matrices
"complete the square for \( x^h a x \). now \( x^h = [x_1 \, x_2] \) can be complex
        \[
        a |x_1|^2 + 2\text{re}(b)x_1 x_2 + c |x_2|^2 = a \left| x_1 + \frac{b}{a} x_2 \right|^2 + |x_2|^2.
        \]",positive definite matrices
show that \( a > 0 \) and \( ac > |b|^2 \) ensure that \( a \) is positive definite.,positive definite matrices
are the matrices \( \begin{bmatrix} 1 & 1+i \\ 1-i & 2 \end{bmatrix} \) and \( \begin{bmatrix} 3 & 4+i \\ 4-i & 6 \end{bmatrix} \) positive definite?,positive definite matrices
decide whether \( f = x^2 y^2 - 2x - 2y \) has a minimum at the point \( x = y = 1 \) (after showing that the first derivatives are zero at that point).,positive definite matrices
"under what conditions on \( a, b, c \) is \( ax^2 + 2bxy + cy^2 > x^2 + y^2 \) for all \( x, y \)?",positive definite matrices
"which of \( a_1, a_2, a_3, a_4 \) has two positive eigenvalues? test \( a > 0 \) and \( ac > b^2 \), don’t compute the eigenvalues. find an \( x \) so that \( x^t a_1 x < 0 \).
    \[
    a_1 = \begin{bmatrix} 5 & 6 \\ 6 & 7 \end{bmatrix}, 
    a_2 = \begin{bmatrix} -1 & -2 \\ -2 & -5 \end{bmatrix}, 
    a_3 = \begin{bmatrix} 1 & 10 \\ 10 & 100 \end{bmatrix}, 
    a_4 = \begin{bmatrix} 1 & 10 \\ 10 & 101 \end{bmatrix}.
    \]",positive definite matrices
"what is the quadratic \( f = ax^2 + 2bxy + cy^2 \) for each of these matrices? complete the square to write \( f \) as a sum of one or two squares \( d_1(\cdot)^2 + d_2(\cdot)^2 \).
    \[
    a = \begin{bmatrix} 1 & 2 \\ 2 & 9 \end{bmatrix}, 
    a = \begin{bmatrix} 1 & 3 \\ 3 & 9 \end{bmatrix}.
    \]",positive definite matrices
"show that \( f(x,y) = x^2 + 4xy + 3y^2 \) does not have a minimum at \( (0,0) \) even though it has positive coefficients. write \( f \) as a difference of squares and find a point \( (x,y) \) where \( f \) is negative.",positive definite matrices
"(important) if \( a \) has independent columns, then \( a^t a \) is square and symmetric and invertible (section 4.2). rewrite \( x^t a^t a x \) to show why it is positive except when \( x = 0 \). then \( a^t a \) is positive definite.",positive definite matrices
"test to see if \( a^t a \) is positive definite in each case:
    \[
    a = \begin{bmatrix} 1 & 2 \\ 0 & 3 \end{bmatrix}, 
    a = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 2 & 1 \end{bmatrix}, 
    a = \begin{bmatrix} 1 & 1 & 2 \\ 1 & 2 & 1 \end{bmatrix}.
    \]",positive definite matrices
"\textbf{find the 3 by 3 matrix \( a \) and its pivots, rank, eigenvalues, and determinant:}
    \[
    \begin{bmatrix}
    x_1 & x_2 & x_3
    \end{bmatrix}
    \begin{bmatrix}
    a
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3
    \end{bmatrix}
    = 4(x_1 - x_2 + 2x_3)
    \]",positive definite matrices
"\textbf{for \( f_1(x,y) = \frac{1}{4}x^4 + x^2y + y^2 \) and \( f_2(x,y) = x^3 + xy - x \), find the second derivative matrices \( a_1 \) and \( a_2 \):}
    \[
    a = \begin{bmatrix}
    \frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
    \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
    \end{bmatrix}
    \]
    \( a_1 \) is positive definite, so \( f_1 \) is concave up (convex). find the minimum point of \( f_1 \) and the saddle point of \( f_2 \) (look where first derivatives are zero).",positive definite matrices
"\textbf{the graph of \( z = x^2 + y^2 \) is a bowl opening upward. the graph of \( z = x^2 - y^2 \) is a saddle. the graph of \( z = -x^2 - y^2 \) is a bowl opening downward. what is a test on \( f(x,y) \) to have a saddle at (0,0)?}",positive definite matrices
\textbf{which values of \( c \) give a bowl and which give a saddle point for the graph of \( z = 4x^2 + 12xy + cy^2 \)? describe this graph at the borderline value of \( c \).},positive definite matrices
"\textbf{for what range of numbers \( a \) and \( b \) are the matrices \( a \) and \( b \) positive definite?}
    \[
    a = \begin{bmatrix}
    a & 2 & 2 \\
    2 & a & 2 \\
    2 & 2 & a
    \end{bmatrix}, \quad
    b = \begin{bmatrix}
    1 & 2 & 4 \\
    2 & b & 8 \\
    4 & 8 & 7
    \end{bmatrix}
    \]",positive definite matrices
"\textbf{decide for or against the positive definiteness of}
    \[
    a = \begin{bmatrix}
    2 & -1 & -1 \\
    -1 & 2 & -1 \\
    -1 & -1 & 2
    \end{bmatrix}, \quad
    b = \begin{bmatrix}
    2 & -1 & -1 \\
    -1 & 2 & 1 \\
    -1 & 1 & 2
    \end{bmatrix}, \quad
    c = \begin{bmatrix}
    0 & 1 & 2 \\
    1 & 0 & 1 \\
    2 & 1 & 0
    \end{bmatrix}
    \]",positive definite matrices
"\textbf{construct an indefinite matrix with its largest entries on the main diagonal:}
    \[
    a = \begin{bmatrix}
    1 & b & -b \\
    b & 1 & b \\
    -b & b & 1
    \end{bmatrix}
    \]
    with \( |b| < 1 \) can have \( \det(a) < 0 \).",positive definite matrices
"\textbf{show from the eigenvalues that if \( a \) is positive definite, so is \( a^2 \) and so is \( a^{-1} \).}",positive definite matrices
"\textbf{if \( a \) and \( b \) are positive definite, then \( a + b \) is positive definite. pivots and eigenvalues are not convenient for \( a + b \). much better to prove \( x^t (a + b) x > 0 \).}",positive definite matrices
"\textbf{from the pivots, eigenvalues, and eigenvectors of}
    \[
    a = \begin{bmatrix}
    5 & 4 \\
    4 & 5
    \end{bmatrix}
    \]
    write \( a \) as \( r^t r \) in three ways: \( (l \sqrt{d})(\sqrt{d} l^t) \), \( (q \sqrt{\lambda})(\sqrt{\lambda} q^t) \), and \( (q \sqrt{\lambda} q^t)(q \sqrt{\lambda} q^t) \).",positive definite matrices
"\textbf{if \( a = q \lambda q^t \) is symmetric positive definite, then \( r = q \sqrt{\lambda} q^t \) is its symmetric positive definite square root. why does \( r \) have positive eigenvalues? compute \( r \) and verify \( r^2 = a \) for}
    \[
    a = \begin{bmatrix}
    10 & 6 \\
    6 & 10
    \end{bmatrix}, \quad
    a = \begin{bmatrix}
    10 & -6 \\
    -6 & 10
    \end{bmatrix}
    \]",positive definite matrices
"\textbf{if \( a \) is symmetric positive definite and \( c \) is nonsingular, prove that \( b = c^t a c \) is also symmetric positive definite.}",positive definite matrices
\textbf{if \( a = r^t r \) prove the generalized schwarz inequality \( |x^t a y|^2 \leq (x^t a x)(y^t a y) \).},positive definite matrices
"\textbf{the ellipse \( u^2 + 4v^2 = 1 \) corresponds to}
    \[
    a = \begin{bmatrix}
    1 & 0 \\
    0 & 4
    \end{bmatrix}
    \]
    write the eigenvalues and eigenvectors, and sketch the ellipse.",positive definite matrices
"\textbf{reduce the equation \( 3u^2 - 2\sqrt{2} uv + 2v^2 = 1 \) to a sum of squares by finding the eigenvalues of the corresponding \( a \), and sketch the ellipse.}",positive definite matrices
"\textbf{in three dimensions, \( \lambda_1 y_1^2 + \lambda_2 y_2^2 + \lambda_3 y_3^2 = 1 \) represents an ellipsoid when all \( \lambda_i > 0 \). describe all the different kinds of surfaces that appear in the positive semidefinite case when one or more of the eigenvalues is zero.}",positive definite matrices
\textbf{write down the five conditions for a 3 by 3 matrix to be negative definite (\( -a \) is positive definite) with special attention to condition iii: how is \( \det(-a) \) related to \( \det(a) \)?},positive definite matrices
"\textbf{decide whether the following matrices are positive definite, negative definite, semidefinite, or indefinite:}
    \[
    a = \begin{bmatrix}
    1 & 2 & 3 \\
    2 & 5 & 4 \\
    3 & 4 & 9
    \end{bmatrix}, \quad
    b = \begin{bmatrix}
    1 & 2 & 0 & 0 \\
    2 & 6 & -2 & 0 \\
    0 & -2 & 5 & -2 \\
    0 & 0 & -2 & 3
    \end{bmatrix}, \quad
    c = -b, \quad
    d = a^{-1}
    \]
    is there a real solution to \( -x^2 - 5y^2 - 9z^2 - 4xy - 6xz - 8yz = 1 \)?",positive definite matrices
"\textbf{suppose \( a \) is symmetric positive definite and \( q \) is an orthogonal matrix. true or false:}
    \begin{enumerate}",positive definite matrices
\( q^t a q \) is a diagonal matrix.,positive definite matrices
\( q^t a q \) is symmetric positive definite.,positive definite matrices
\( q^t a q \) has the same eigenvalues as \( a \).,positive definite matrices
\( e^{-a} \) is symmetric positive definite.,positive definite matrices
"\textbf{if \( a \) is positive definite and \( a_{11} \) is increased, prove from cofactors that the determinant is increased. show by example that this can fail if \( a \) is indefinite.}",positive definite matrices
"\textbf{from \( a = r^t r \), show for positive definite matrices that \( \det(a) \leq a_{11} a_{22} \cdots a_{nn} \). (the length squared of column \( j \) of \( r \) is \( a_{jj} \). use determinant = volume.)}",positive definite matrices
"\textbf{(lyapunov test for stability of \( m \)) suppose \( a m + m^t a = -i \) with positive definite \( a \). if \( m x = \lambda x \), show that \( \text{re}(\lambda) < 0 \). (hint: multiply the first equation by \( x^t \) and \( x \).)}",positive definite matrices
"\textbf{which 3 by 3 symmetric matrices \( a \) produce these functions \( f = x^t a x \)? why is the first matrix positive definite but not the second one?}
    \begin{enumerate}",positive definite matrices
\( f = 2(x_1^2 + x_2^2 + x_3^2 - x_1 x_2 - x_2 x_3) \),positive definite matrices
\( f = 2(x_1^2 + x_2^2 + x_3^2 - x_1 x_2 - x_1 x_3 - x_2 x_3) \),positive definite matrices
"\textbf{compute the three upper left determinants to establish positive definiteness. verify that their ratios give the second and third pivots.}
    \[
    a = \begin{bmatrix}
    2 & 2 & 0 \\
    2 & 5 & 3 \\
    0 & 3 & 8
    \end{bmatrix}
    \]",positive definite matrices
"\textbf{a positive definite matrix cannot have a zero (or even worse, a negative number) on its diagonal. show that this matrix fails to have \( x^t a x > 0 \):}
    \[
    a = \begin{bmatrix}
    4 & 1 & 1 \\
    1 & 0 & 2 \\
    1 & 2 & 5
    \end{bmatrix}
    \]",positive definite matrices
"\textbf{a diagonal entry \( a_{jj} \) of a symmetric matrix cannot be smaller than all \( \lambda \)'s. if it were, then \( a - a_{jj} i \) would have eigenvalues and would be positive definite. but \( a - a_{jj} i \) has a on the main diagonal.}",positive definite matrices
"\textbf{give a quick reason why each of these statements is true:}
    \begin{enumerate}",positive definite matrices
every positive definite matrix is invertible.,positive definite matrices
the only positive definite projection matrix is \( p = i \).,positive definite matrices
a diagonal matrix with positive diagonal entries is positive definite.,positive definite matrices
a symmetric matrix with a positive determinant might not be positive definite!,positive definite matrices
"\textbf{for which \( s \) and \( t \) do \( a \) and \( b \) have all \( \lambda > 0 \) (and are therefore positive definite)?}
    \[
    a = \begin{bmatrix}
    s & -4 & -4 \\
    -4 & s & -4 \\
    -4 & -4 & s
    \end{bmatrix}, \quad
    b = \begin{bmatrix}
    t & 3 & 0 \\
    3 & t & 4 \\
    0 & 4 & t
    \end{bmatrix}
    \]",positive definite matrices
\textbf{you may have seen the equation for an ellipse as \( \left( \frac{x}{a} \right)^2 + \left( \frac{y}{b} \right)^2 = 1 \). what are \( a \) and \( b \) when the equation is written as \( \lambda_1 x^2 + \lambda_2 y^2 = 1 \)? the ellipse \( 9x^2 + 16y^2 = 1 \) has half-axes with lengths \( a = \) and \( b = \).},positive definite matrices
\textbf{draw the tilted ellipse \( x^2 + xy + y^2 = 1 \) and find the half-lengths of its axes from the eigenvalues of the corresponding \( a \).},positive definite matrices
"\textbf{with positive pivots in \( d \), the factorization \( a = ldl^t \) becomes \( l \sqrt{d} \sqrt{d} l^t \). (square roots of the pivots give \( d = \sqrt{d} \sqrt{d} \).) then \( c = l \sqrt{d} \) yields the cholesky factorization \( a = c c^t \), which is ""symmetrized lu"":}
    \begin{enumerate}",positive definite matrices
"from \( c = \begin{bmatrix} 3 & 0 \\ 1 & 2 \end{bmatrix} \), find \( a \).",positive definite matrices
"from \( a = \begin{bmatrix} 4 & 8 \\ 8 & 25 \end{bmatrix} \), find \( c \).",positive definite matrices
"\textbf{in the cholesky factorization \( a = c c^t \), with \( c = l \sqrt{d} \), the square roots of the pivots are on the diagonal of \( c \). find \( c \) (lower triangular) for}
    \[
    a = \begin{bmatrix}
    9 & 0 & 0 \\
    0 & 1 & 2 \\
    0 & 2 & 8
    \end{bmatrix}, \quad
    a = \begin{bmatrix}
    1 & 1 & 1 \\
    1 & 2 & 2 \\
    1 & 2 & 7
    \end{bmatrix}
    \]",positive definite matrices
"\textbf{the symmetric factorization \( a = ldl^t \) means that \( x^t a x = x^t l d l^t x \):}
    \[
    \begin{bmatrix} 
    x & y 
    \end{bmatrix} \begin{bmatrix} 
    a & b \\
    b & c 
    \end{bmatrix} \begin{bmatrix} 
    x \\
    y
    \end{bmatrix} 
    = \begin{bmatrix} 
    x & y
    \end{bmatrix} \begin{bmatrix} 
    1 & 0 \\
    \frac{b}{a} & 1
    \end{bmatrix} \begin{bmatrix} 
    a & 0 \\
    0 & \frac{ac-b^2}{a}
    \end{bmatrix} \begin{bmatrix} 
    1 & \frac{b}{a} \\
    0 & 1
    \end{bmatrix} \begin{bmatrix} 
    x \\
    y
    \end{bmatrix}
    \]
    the left-hand side is \( ax^2 + 2bxy + cy^2 \). the right-hand side is \( a(x + \frac{b}{a} y)^2 + y^2 \). the second pivot completes the square! test with \( a = 2 \), \( b = 4 \), \( c = 10 \).",positive definite matrices
"\textbf{without multiplying} 
    \[
    a = \begin{bmatrix}
    \cos\theta & -\sin\theta \\
    \sin\theta & \cos\theta
    \end{bmatrix} \begin{bmatrix}
    2 & 0 \\
    0 & 5
    \end{bmatrix} \begin{bmatrix}
    \cos\theta & \sin\theta \\
    -\sin\theta & \cos\theta
    \end{bmatrix}
    \]
    find:
    \begin{enumerate}",positive definite matrices
(a) the determinant of \( a \).,positive definite matrices
(b) the eigenvalues of \( a \).,positive definite matrices
(c) the eigenvectors of \( a \).,positive definite matrices
(d) a reason why \( a \) is symmetric positive definite.,positive definite matrices
"\textbf{for the semidefinite matrices}
    \[
    a = \begin{bmatrix}
    2 & -1 & -1 \\
    -1 & 2 & -1 \\
    -1 & -1 & 2
    \end{bmatrix} \quad \text{(rank 2)} \quad \text{and} \quad b = \begin{bmatrix}
    1 & 1 & 1 \\
    1 & 1 & 1 \\
    1 & 1 & 1
    \end{bmatrix} \quad \text{(rank 1)},
    \]
    write \( x^t a x \) as a sum of two squares and \( x^t b x \) as one square.",positive definite matrices
"\textbf{apply any three tests to each of the matrices}
    \[
    a = \begin{bmatrix}
    1 & 1 & 1 \\
    1 & 1 & 1 \\
    1 & 1 & 0
    \end{bmatrix} \quad \text{and} \quad b = \begin{bmatrix}
    2 & 1 & 2 \\
    1 & 1 & 1 \\
    2 & 1 & 2
    \end{bmatrix},
    \]
    to decide whether they are positive definite, positive semidefinite, or indefinite.",positive definite matrices
"\textbf{for} 
    \[
    c = \begin{bmatrix}
    2 & 0 \\
    0 & -1
    \end{bmatrix} \quad \text{and} \quad a = \begin{bmatrix}
    1 & 1 \\
    1 & 1
    \end{bmatrix},
    \]
    confirm that \( c^t a c \) has eigenvalues of the same signs as \( a \). construct a chain of nonsingular matrices \( c(t) \) linking \( c \) to an orthogonal matrix \( q \). why is it impossible to construct a nonsingular chain linking \( c \) to the identity matrix?",positive definite matrices
"\textbf{if the pivots of a matrix are all greater than 1, are the eigenvalues all greater than 1? test on the tridiagonal \( -1, 2, -1 \) matrices.}",positive definite matrices
"\textbf{use the pivots of \( a - \frac{1}{2}i \) to decide whether \( a \) has an eigenvalue smaller than \( \frac{1}{2} \):}
    \[
    a - \frac{1}{2} i = \begin{bmatrix}
    2.5 & 3 & 0 \\
    3 & 9.5 & 7 \\
    0 & 7 & 7.5
    \end{bmatrix}
    \]",positive definite matrices
"\textbf{an algebraic proof of the law of inertia starts with the orthonormal eigenvectors \( x_1, \dots, x_p \) of \( a \) corresponding to eigenvalues \( \lambda_i > 0 \), and the orthonormal eigenvectors \( y_1, \dots, y_q \) of \( c^t a c \) corresponding to eigenvalues \( \mu_i < 0 \).}
    \begin{enumerate}",positive definite matrices
"(a) to prove that the \( p + q \) vectors \( x_1, \dots, x_p, cy_1, \dots, cy_q \) are independent, assume that some combination gives zero:
        \[
        a_1 x_1 + \dots + a_p x_p = b_1 c y_1 + \dots + b_q c y_q = z \quad (\text{say}).
        \]
        show that
        \[
        z^t a z = \lambda_1 a_1^2 + \dots + \lambda_p a_p^2 \geq 0 \quad \text{and} \quad z^t a z = \mu_1 b_1^2 + \dots + \mu_q b_q^2 \leq 0.
        \]",positive definite matrices
"(b) deduce that the \( a_i \)'s and \( b_i \)'s are zero (proving linear independence). from that, deduce \( p + q \leq n \).",positive definite matrices
"(c) the same argument for the \( n - p \) negative \( \lambda \)'s and the \( n - q \) positive \( \mu \)'s gives
        \[
        n - p + n - q \leq n.
        \]
        (we again assume no zero eigenvalues, which are handled separately). show that \( p + q = n \), so the number \( p \) of positive \( \lambda \)'s equals the number \( n - q \) of positive \( \mu \)'s — which is the law of inertia.",positive definite matrices
"\textbf{if \( c \) is nonsingular, show that \( a \) and \( c^t a c \) have the same rank. thus they have the same number of zero eigenvalues.}",positive definite matrices
"\textbf{find by experiment the number of positive, negative, and zero eigenvalues of}
    \[
    a = \begin{bmatrix} 
    i & b \\
    b^t & 0
    \end{bmatrix}
    \]
    when the block \( b \) (of order \( \frac{1}{2} n \)) is nonsingular.",positive definite matrices
\textbf{do \( a \) and \( c^t a c \) always satisfy the law of inertia when \( c \) is not square?},positive definite matrices
"\textbf{in equation (9) with \( m_1 = 1 \) and \( m_2 = 2 \), verify that the normal modes are orthogonal:}
    \[
    x_1^t m x_2 = 0.
    \]",positive definite matrices
"\textbf{find the eigenvalues and eigenvectors of} 
    \[
    a x = \lambda m x:
    \quad a = \begin{bmatrix} 
    6 & -3 \\
    -3 & 6
    \end{bmatrix}, \quad m = \begin{bmatrix}
    4 & 1 \\
    1 & 4
    \end{bmatrix}.
    \]",positive definite matrices
"\textbf{if the symmetric matrices \( a \) and \( m \) are indefinite, \( a x = \lambda m x \) might not have real eigenvalues. construct a 2 by 2 example.}",positive definite matrices
"\textbf{a group of nonsingular matrices includes \( ab \) and \( a^{-1} \) if it includes \( a \) and \( b \). “products and inverses stay in the group.” which of these sets are groups?}
    \begin{enumerate}",positive definite matrices
"positive definite symmetric matrices \( a \),",positive definite matrices
"orthogonal matrices \( q \),",positive definite matrices
"all exponentials \( e^{ta} \) of a fixed matrix \( a \),",positive definite matrices
"matrices \( p \) with positive eigenvalues,",positive definite matrices
matrices \( d \) with determinant 1.,positive definite matrices
"\textbf{compute \( a^t a \) and its eigenvalues \( \sigma_1^2, 0 \) and unit eigenvectors \( v_1, v_2 \):}
    \[
    a = \begin{bmatrix}
    1 & 4 \\
    2 & 8
    \end{bmatrix}.
    \]",positive definite matrices
"\textbf{(a) compute \( a a^t \) and its eigenvalues \( \sigma_1^2, 0 \) and unit eigenvectors \( u_1, u_2 \).}
    
    \textbf{(b) choose signs so that \( a v_1 = \sigma_1 u_1 \) and verify the svd:}
    \[
    a = \begin{bmatrix}
    1 & 4 \\
    2 & 8
    \end{bmatrix} = \begin{bmatrix} 
    u_1 & u_2
    \end{bmatrix}
    \begin{bmatrix}
    \sigma_1 & 0
    \end{bmatrix}
    \begin{bmatrix}
    v_1 & v_2
    \end{bmatrix}^t.
    \]
    
    \textbf{(c) which four vectors give orthonormal bases for \( c(a), n(a), c(a^t), n(a^t) \)?}",positive definite matrices
"\textbf{find the svd from the eigenvectors \( v_1, v_2 \) of \( a^t a \) and \( a v_i = \sigma_i u_i \):}
    \[
    \text{fibonacci matrix } a = \begin{bmatrix}
    1 & 1 \\
    1 & 0
    \end{bmatrix}.
    \]",positive definite matrices
\textbf{use the svd part of the matlab demo eigshow (or java on the course page \texttt{web.mit.edu/18.06}) to find the same vectors \( v_1 \) and \( v_2 \) graphically.},positive definite matrices
"\textbf{compute \( a^t a \) and \( a a^t \), and their eigenvalues and unit eigenvectors, for}
    \[
    a = \begin{bmatrix}
    1 & 1 & 0 \\
    0 & 1 & 1
    \end{bmatrix}.
    \]
    multiply the three matrices \( u \sigma v^t \) to recover \( a \).",positive definite matrices
"\textbf{suppose \( u_1, \dots, u_n \) and \( v_1, \dots, v_n \) are orthonormal bases for \( \mathbb{r}^n \). construct the matrix \( a \) that transforms each \( v_j \) into \( u_j \) to give \( a v_1 = u_1, \dots, a v_n = u_n \).}",positive definite matrices
"\textbf{construct the matrix with rank 1 that has \( a v = 12 u \) for}
    \[
    v = \frac{1}{2} (1,1,1,1), \quad u = \frac{1}{3} (2,2,1).
    \]
    its only singular value is \( \sigma_1 = \cdots \).",positive definite matrices
"\textbf{find \( u \sigma v^t \) if \( a \) has orthogonal columns \( w_1, \dots, w_n \) of lengths \( \sigma_1, \dots, \sigma_n \).}",positive definite matrices
"\textbf{explain how \( u \sigma v^t \) expresses \( a \) as a sum of \( r \) rank-1 matrices in equation (3):}
    \[
    a = \sigma_1 u_1 v_1^t + \cdots + \sigma_r u_r v_r^t.
    \]",positive definite matrices
"\textbf{suppose \( a \) is a 2 by 2 symmetric matrix with unit eigenvectors \( u_1 \) and \( u_2 \). if its eigenvalues are \( \lambda_1 = 3 \) and \( \lambda_2 = -2 \), what are \( u \), \( \sigma \), and \( v^t \)?}",positive definite matrices
"\textbf{suppose \( a \) is invertible (with \( \sigma_1 > \sigma_2 > 0 \)). change \( a \) by as small a matrix as possible to produce a singular matrix \( a' \). hint: \( u \) and \( v \) do not change. find \( a' \) from}
    \[
    a = \begin{bmatrix} 
    u_1 & u_2 
    \end{bmatrix}
    \begin{bmatrix}
    \sigma_1 & 0 \\
    0 & \sigma_2
    \end{bmatrix}
    \begin{bmatrix}
    v_1 & v_2
    \end{bmatrix}^t.
    \]",positive definite matrices
"\textbf{(a) if \( a \) changes to \( 4a \), what is the change in the svd?}
    
    \textbf{(b) what is the svd for \( a^t \) and for \( a^{-1} \)?}",positive definite matrices
\textbf{why doesn’t the svd for \( a + i \) just use \( \sigma + i \)?},positive definite matrices
\textbf{find the svd and the pseudoinverse \( a^+ \) of the \( m \times n \) zero matrix.},positive definite matrices
"\textbf{find the svd and the pseudoinverse \( v \sigma^+ u^t \) of}
    \[
    a = \begin{bmatrix} 
    1 & 1 & 1 & 1
    \end{bmatrix}, \quad b = \begin{bmatrix}
    0 & 1 & 0 \\
    1 & 0 & 0
    \end{bmatrix}, \quad c = \begin{bmatrix}
    1 & 1 \\
    0 & 0
    \end{bmatrix}.
    \]",positive definite matrices
"\textbf{if an \( m \times n \) matrix \( q \) has orthonormal columns, what is \( q^+ \)?}",positive definite matrices
"\textbf{diagonalize \( a^t a \) to find its positive definite square root \( s = v \sigma^{1/2} v^t \) and its polar decomposition \( a = qs \):}
    \[
    a = \frac{1}{\sqrt{10}} \begin{bmatrix}
    10 & 6 \\
    0 & 8
    \end{bmatrix}.
    \]",positive definite matrices
"\textbf{what is the minimum-length least-squares solution \( x^+ = a^+ b \) to the following?}
    \[
    ax = \begin{bmatrix}
    1 & 0 & 0 \\
    1 & 0 & 0 \\
    1 & 1 & 1
    \end{bmatrix}
    \begin{bmatrix}
    c \\
    d \\
    e
    \end{bmatrix}
    = \begin{bmatrix}
    0 \\
    2 \\
    2
    \end{bmatrix}.
    \]
    you can compute \( a^+ \), or find the general solution to \( a^t a x = a^t b \) and choose the solution that is in the row space of \( a \). this problem fits the best plane \( c + dt + ez \) to \( b = 0 \) and also \( b = 2 \) at \( t = z = 0 \) (and \( b = 2 \) at \( t = z = 1 \)).
    
    \textbf{(a) if \( a \) has independent columns, its left-inverse \( (a^t a)^{-1} a^t \) is \( a^+ \).}
    
    \textbf{(b) if \( a \) has independent rows, its right-inverse \( a^t (a a^t)^{-1} \) is \( a^+ \).}
    
    in both cases, verify that \( x^+ = a^+ b \) is in the row space, and \( a^t a x^+ = a^t b \).",positive definite matrices
\textbf{split \( a = u \sigma v^t \) into its reverse polar decomposition \( q s_0 \).},positive definite matrices
\textbf{is \( (ab)^+ = b^+ a^+ \) always true for pseudoinverses? i believe not.},positive definite matrices
"\textbf{removing zero rows of \( u \) leaves \( a = lu \), where the \( r \) columns of \( l \) span the column space of \( a \) and the \( r \) rows of \( u \) span the row space. then \( a^+ \) has the explicit formula}
    \[
    a^+ = u^t (u u^t)^{-1} (l^t l)^{-1} l^t.
    \]
    \textbf{why is \( a^+ b \) in the row space with \( u^t \) at the front? why does \( a^t a a^+ b = a^t b \), so that \( x^+ = a^+ b \) satisfies the normal equation as it should?}",positive definite matrices
\textbf{explain why \( a a^+ \) and \( a^+ a \) are projection matrices (and therefore symmetric). what fundamental subspaces do they project onto?},positive definite matrices
"\textbf{consider the system \( a\mathbf{x} = \mathbf{b} \) given by}
    \[
    a = \begin{pmatrix}
    2 & -1 & 0 \\
    -1 & 2 & -1 \\
    0 & -1 & 2
    \end{pmatrix}, \quad
    \mathbf{b} = \begin{pmatrix}
    4 \\
    0 \\
    4
    \end{pmatrix}.
    \]
    \textbf{construct the corresponding quadratic function \( p(x_1, x_2, x_3) \), compute its partial derivatives \( \frac{\partial p}{\partial x_i} \), and verify that they vanish exactly at the desired solution.}",positive definite matrices
"\textbf{complete the square in \( p = \frac{1}{2} \mathbf{x}^t a \mathbf{x} - \mathbf{x}^t \mathbf{b} = \frac{1}{2} (\mathbf{x} - a^{-1} \mathbf{b})^t a (\mathbf{x} - a^{-1} \mathbf{b}) + \text{constant}.} \\
    \textbf{this constant equals \( p_{\text{min}} \) because the term before it is never negative. (why?)}",positive definite matrices
"\textbf{find the minimum, if there is one, of \( p_1 = \frac{1}{2} x^2 + xy + y^2 - 3y \) and \( p_2 = \frac{1}{2} x^2 - 3y \). what matrix \( a \) is associated with \( p_2 \)?}",positive definite matrices
"\textbf{(review) another quadratic that certainly has its minimum at \( a\mathbf{x} = \mathbf{b} \) is}
    \[
    q(\mathbf{x}) = \frac{1}{2} \| a\mathbf{x} - \mathbf{b} \|^2 = \frac{1}{2} \mathbf{x}^t a^t a \mathbf{x} - \mathbf{x}^t a^t \mathbf{b} + \frac{1}{2} \mathbf{b}^t \mathbf{b}.
    \]
    \textbf{comparing \( q \) with \( p \), and ignoring the constant \( \frac{1}{2} \mathbf{b}^t \mathbf{b} \), what system of equations do we get at the minimum of \( q \)?}",positive definite matrices
"\textbf{for any symmetric matrix \( a \), compute the ratio \( r(x) \) for the special choice \( x = (1, \ldots, 1) \). how is the sum of all entries \( a_{ij} \) related to \( \lambda_1 \) and \( \lambda_n \)?}",positive definite matrices
"\textbf{with \( a = \begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix} \), find a choice of \( x \) that gives a smaller \( r(x) \) than the bound \( \lambda_1 \leq 2 \) that comes from the diagonal entries. what is the minimum value of \( r(x) \)?}",positive definite matrices
"\textbf{if \( b \) is positive definite, show from the rayleigh quotient that the smallest eigenvalue of \( a + b \) is larger than the smallest eigenvalue of \( a \).}",positive definite matrices
"\textbf{if \( \lambda_1 \) and \( \mu_1 \) are the smallest eigenvalues of \( a \) and \( b \), show that the smallest eigenvalue \( \theta_1 \) of \( a + b \) is at least as large as \( \lambda_1 + \mu_1 \). (try the corresponding eigenvector \( x \) in the rayleigh quotients.)}",positive definite matrices
"\textbf{if \( b \) is positive definite, show from the minimax principle that the second smallest eigenvalue is increased by adding \( b \): \( \lambda_2(a + b) > \lambda_2(a) \).}",positive definite matrices
"\textbf{if you throw away two rows and columns of \( a \), what inequalities do you expect between the smallest eigenvalue \( \mu \) of the new matrix and the original \( \lambda \)'s?}",positive definite matrices
"\textbf{find the minimum values of}
    \[
    r(x) = \frac{x_1^2 - x_1 x_2 + x_2^2}{x_1^2 + x_2^2}
    \]
    and
    \[
    r(x) = \frac{x_1^2 - x_1 x_2 + x_2^2}{2x_1^2 + x_2^2}.
    \]",positive definite matrices
\textbf{prove from equation (11) that \( r(x) \) is never larger than the largest eigenvalue \( \lambda_n \).},positive definite matrices
"\textbf{the minimax principle for \( \lambda_j \) involves \( j \)-dimensional subspaces \( s_j \):}
    \[
    \lambda_j = \min_{s_j} \max_{x \in s_j} r(x).
    \]
    \begin{enumerate}",positive definite matrices
"if \( \lambda_j \) is positive, infer that every \( s_j \) contains a vector \( x \) with \( r(x) > 0 \).",positive definite matrices
deduce that \( s_j \) contains a vector \( y = c^{-1} x \) with \( \frac{y^t c^t a c y}{y^t y} > 0 \).,positive definite matrices
"conclude that the \( j \)-th eigenvalue of \( c^t a c \), from its minimax principle, is also positive—proving again the law of inertia in section 6.2.",positive definite matrices
\textbf{show that the smallest eigenvalue \( \lambda_1 \) of \( a x = \lambda m x \) is not larger than the ratio \( \frac{a_{11}}{m_{11}} \) of the corner entries.},positive definite matrices
"\textbf{which particular subspace \( s_2 \) in problem 13 gives the minimum value \( \lambda_2 \)? in other words, over which \( s_2 \) is the maximum of \( r(x) \) equal to \( \lambda_2 \)?}",positive definite matrices
"\textbf{16.} from the zero submatrix, determine the signs of the \( n \) eigenvalues of the matrix \( a \):
    \[
    a = \begin{pmatrix}
    0 & \cdot & 0 & 1 \\
    \cdot & \cdot & 0 & 2 \\
    0 & 0 & 0 & \cdot \\
    1 & 2 & \cdot & n
    \end{pmatrix}
    \]",positive definite matrices
\textbf{17.} (constrained minimum) suppose the unconstrained minimum \( x = a^{-1}b \) happens to satisfy the constraint \( cx = d \). verify that equation (5) correctly gives \( p_{\text{min}} = p_{\text{min}} \); the correction term is zero.,positive definite matrices
"\textbf{use three hat functions, with \( h = \frac{1}{4} \), to solve \( -u'' = 2 \) with \( u(0) = u(1) = 0 \). verify that the approximation \( u \) matches \( u = x - x^2 \) at the nodes.}",positive definite matrices
\textbf{solve \( -u'' = x \) with \( u(0) = u(1) = 0 \). then solve approximately with two hat functions and \( h = \frac{1}{3} \). where is the largest error?},positive definite matrices
"\textbf{suppose \( -u'' = 2 \), with the boundary condition \( u(1) = 0 \) changed to \( u'(1) = 0 \). this “natural” condition on \( u' \) need not be imposed on the trial functions \( v \). with \( h = \frac{1}{3} \), there is an extra half-hat \( v_3 \), which goes from 0 to 1 between \( x = \frac{2}{3} \) and \( x = 1 \). compute \( a_{33} = \int (v_3')^2 \, dx \) and \( f_3 = \int 2v_3 \, dx \). solve \( a y = f \) for the finite element solution \( y_1 v_1 + y_2 v_2 + y_3 v_3 \).}",positive definite matrices
"\textbf{solve \( -u'' = 2 \) with a single hat function, but place its node at \( x = \frac{1}{4} \) instead of \( x = \frac{1}{2} \). (sketch this function \( v_1 \).) with boundary conditions \( u(0) = u(1) = 0 \), compare the finite element approximation with the true \( u = x - x^2 \).}",positive definite matrices
"\textbf{suppose \( -u'' = 2 \), with the boundary condition \( u(1) = 0 \) changed to \( u'(1) = 0 \). this ""natural"" condition on \( u' \) need not be imposed on the trial functions \( v \). with \( h = \frac{1}{3} \), there is an extra half-hat \( v_3 \), which goes from 0 to 1 between \( x = \frac{2}{3} \) and \( x = 1 \). compute \( a_{33} = \int (v_3')^2 \, dx \) and \( f_3 = \int 2v_3 \, dx \). solve \( a y = f \) for the finite element solution \( y_1 v_1 + y_2 v_2 + y_3 v_3 \).}",positive definite matrices
"\textbf{galerkin’s method starts with the differential equation (say \( -u'' = f(x) \)) instead of the energy \( p \). the trial solution is still \( u = y_1 v_1 + y_2 v_2 + \cdots + y_n v_n \), and the \( y \)'s are chosen to make the difference between \( -u'' \) and \( f \) orthogonal to every \( v_j \):}
    \[
    \int \left( -y_1 v_1'' - y_2 v_2'' - \cdots - y_n v_n'' \right) v_j \, dx = \int f(x) v_j(x) \, dx.
    \]
    \textbf{integrate the left side by parts to reach \( a y = f \), proving that galerkin gives the same \( a \) and \( f \) as rayleigh-ritz for symmetric problems.}",positive definite matrices
"\textbf{a basic identity for quadratics shows \( y = a^{-1} b \) as minimizing:}
    \[
    p(y) = \frac{1}{2} y^t a y - y^t b = \frac{1}{2} (y - a^{-1} b)^t a (y - a^{-1} b) - \frac{1}{2} b^t a^{-1} b.
    \]
    \textbf{the minimum over a subspace of trial functions is at the \( y \) nearest to \( a^{-1} b \). (that makes the first term on the right as small as possible; it is the key to convergence of \( u \) to \( u \).) if \( a = i \) and \( b = (1,0,0) \), which multiple of \( v = (1,1,1) \) gives the smallest value of \( p(y) = \frac{1}{2} y^t y - y_1 \)?}",positive definite matrices
"\textbf{for a single hat function \( v(x) \) centered at \( x = \frac{1}{2} \), compute \( a = \int (v')^2 \, dx \) and \( m = \int v^2 \, dx \). in the 1 by 1 eigenvalue problem, is \( \lambda = \frac{a}{m} \) larger or smaller than the true eigenvalue \( \lambda = \pi^2 \)?}",positive definite matrices
"\textbf{for the hat functions \( v_1 \) and \( v_2 \) centered at \( x = h = \frac{1}{3} \) and \( x = 2h = \frac{2}{3} \), compute the 2 by 2 mass matrix \( m_{ij} = \int v_i v_j \, dx \), and solve the eigenvalue problem \( a x = \lambda m x \).}",positive definite matrices
"\textbf{what is the mass matrix \( m_{ij} = \int v_i v_j \, dx \) for \( n \) hat functions with \( h = \frac{1}{n+1} \)?}",positive definite matrices
"define a local minimum, local maximum, and saddle point. provide an example of each for a function of two variables.",positive definite matrices
discuss the necessary conditions for a point to be classified as a local maximum. include a detailed explanation of the role of the gradient and hessian matrix in this classification.,positive definite matrices
"what is the significance of the second-derivative test in multivariable optimization? derive the conditions under which the second-derivative test can be used to classify critical points as local minima, maxima, or saddle points.",positive definite matrices
"explain how the hessian matrix is used to classify critical points of a function. provide the necessary conditions for a critical point to be a local minimum, local maximum, or saddle point based on the eigenvalues of the hessian matrix.",positive definite matrices
"prove that if the hessian matrix of a function at a critical point is positive definite, then that point is a local minimum.",positive definite matrices
derive the second derivative test for a function of two variables. provide an example and use this test to classify the critical points of the given function.,positive definite matrices
"consider the function \( f(x, y) = x^2 + y^2 - 4x - 6y + 13 \). find the critical points and classify them as minima, maxima, or saddle points using the second-derivative test.",positive definite matrices
"determine whether the point \( (1, 1) \) is a local minimum, maximum, or saddle point for the function \( f(x, y) = x^2 + y^2 + 2xy - 6x - 4y \). use the second-derivative test and discuss the results.",positive definite matrices
"for the function \( f(x, y) = x^3 + y^3 - 3x^2 - 3y^2 \), find the critical points and classify them as local minima, maxima, or saddle points. use the second-derivative test.",positive definite matrices
"find the critical points of the function \( f(x, y) = 4x^2 + 4y^2 - 16x - 8y + 18 \). classify them as minima, maxima, or saddle points using the second-derivative test.",positive definite matrices
"given the function \( f(x, y) = x^2 + 2y^2 - 4x + 4y \), find the critical points and determine whether each point is a minimum, maximum, or saddle point using the second-derivative test.",positive definite matrices
"solve for the critical points of the function \( f(x, y) = x^2y + y^3 - 4x \). use the second-derivative test to classify the critical points.",positive definite matrices
"consider the function \( f(x, y) = x^2y + 3xy^2 - 6x - 5y \). find and classify the critical points using the second-derivative test.",positive definite matrices
"for the function \( f(x, y, z) = x^2 + y^2 + z^2 - 4x - 2y + 2z + 3 \), find the critical points and classify them as minima, maxima, or saddle points using the second-derivative test.",positive definite matrices
"consider the function \( f(x, y, z) = x^2 + y^2 + z^2 + 2xy - 4xz \). find and classify the critical points as minima, maxima, or saddle points using the second-derivative test.",positive definite matrices
"given the function \( f(x, y, z) = x^2y + y^2z + z^2x \), find and classify the critical points using the second-derivative test.",positive definite matrices
"let \( f(x, y, z) = x^3 + y^3 + z^3 - 6x^2 - 6y^2 - 6z^2 \). find the critical points and classify them using the second-derivative test.",positive definite matrices
"determine the nature of the critical points for the function \( f(x, y) = 4x^2 - 3xy + 2y^2 - 6x + 5y \) using the second-derivative test.",positive definite matrices
"for the function \( f(x, y) = 3x^2 + 2y^2 - 8x - 6y \), find the critical points and classify them using the second-derivative test.",positive definite matrices
"consider the function \( f(x, y) = 2x^2 + 3y^2 + 4xy - 6x + 5y \). find the critical points and use the second-derivative test to classify them.",positive definite matrices
"determine whether the function \( f(x, y) = x^3 - 3xy + y^3 \) has any saddle points. if so, classify them.",positive definite matrices
"find the critical points of the function \( f(x, y) = x^2y + y^2 - 6x - 4y + 9 \). use the second-derivative test to classify the points as minima, maxima, or saddle points.",positive definite matrices
"given the function \( f(x, y) = x^2 + y^2 - 6xy \), find the critical points and classify them using the second-derivative test.",positive definite matrices
define a positive definite matrix and explain its significance in optimization problems. provide examples of how positive definite matrices are used in quadratic forms.,positive definite matrices
"state and explain sylvester’s criterion for testing positive definiteness. apply this criterion to the matrix 
    \[
    a = \begin{bmatrix} 4 & 2 \\ 2 & 3 \end{bmatrix}
    \]
    and determine if it is positive definite.",positive definite matrices
what are the necessary and sufficient conditions for a matrix to be positive definite? discuss the role of eigenvalues and principal minors in these conditions.,positive definite matrices
"derive sylvester’s criterion for testing whether a matrix is positive definite. use this to test the positive definiteness of the matrix
    \[
    a = \begin{bmatrix} 2 & 1 & 0 \\ 1 & 2 & 1 \\ 0 & 1 & 2 \end{bmatrix}.
    \]",positive definite matrices
"prove that if a matrix is positive definite, all its eigenvalues are positive. provide an example of a positive definite matrix and demonstrate how the eigenvalues can be computed.",positive definite matrices
"derive the cholesky decomposition of a positive definite matrix. use the matrix
    \[
    a = \begin{bmatrix} 6 & 3 \\ 3 & 6 \end{bmatrix}
    \]
    and find its cholesky decomposition.",positive definite matrices
"consider the matrix
    \[
    a = \begin{bmatrix} 5 & 2 \\ 2 & 5 \end{bmatrix}.
    \]
    use sylvester’s criterion to determine if the matrix is positive definite.",positive definite matrices
"solve the quadratic form \( q(x) = x^t a x \) where 
    \[
    a = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}
    \]
    and determine if the matrix is positive definite.",positive definite matrices
"determine if the matrix
    \[
    a = \begin{bmatrix} 10 & 2 & 3 \\ 2 & 10 & 4 \\ 3 & 4 & 10 \end{bmatrix}
    \]
    is positive definite by checking its eigenvalues and principal minors.",positive definite matrices
"given the matrix 
    \[
    a = \begin{bmatrix} 3 & 1 & 1 \\ 1 & 3 & 1 \\ 1 & 1 & 3 \end{bmatrix}
    \]
    verify whether it is positive definite by computing the eigenvalues.",positive definite matrices
"prove that if the principal minors of a matrix are all positive, then the matrix is positive definite. use this to verify the positive definiteness of the matrix 
    \[
    a = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}.
    \]",positive definite matrices
"consider the matrix
    \[
    a = \begin{bmatrix} 1 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 1 \end{bmatrix}.
    \]
    use sylvester’s criterion to check if it is positive definite.",positive definite matrices
"let
    \[
    a = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}.
    \]
    show that this matrix is positive definite using both sylvester’s criterion and eigenvalue computation.",positive definite matrices
"consider the matrix 
    \[
    a = \begin{bmatrix} 4 & 2 & 1 \\ 2 & 3 & 1 \\ 1 & 1 & 2 \end{bmatrix}.
    \]
    determine if it is positive definite by calculating the eigenvalues and principal minors.",positive definite matrices
"given the matrix 
    \[
    a = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}
    \]
    verify that it is positive definite by checking its eigenvalues and principal minors.",positive definite matrices
"consider the matrix
    \[
    a = \begin{bmatrix} 6 & 1 & 2 \\ 1 & 6 & 3 \\ 2 & 3 & 6 \end{bmatrix}.
    \]
    use sylvester’s criterion to determine if the matrix is positive definite.",positive definite matrices
"solve the quadratic form \( q(x) = x^t a x \) where 
    \[
    a = \begin{bmatrix} 5 & 4 \\ 4 & 5 \end{bmatrix}.
    \]
    show whether the matrix is positive definite by checking the eigenvalues and principal minors.",positive definite matrices
"find the cholesky decomposition of the matrix
    \[
    a = \begin{bmatrix} 4 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 2 & 4 \end{bmatrix}.
    \]",positive definite matrices
"prove that if all eigenvalues of a matrix are positive, the matrix is positive definite. use this theorem to prove that the matrix
    \[
    a = \begin{bmatrix} 8 & 6 \\ 6 & 8 \end{bmatrix}
    \]
    is positive definite.",positive definite matrices
"consider the matrix 
    \[
    a = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}.
    \]
    check whether it is positive definite by computing the eigenvalues.",positive definite matrices
"find the eigenvalues of the matrix
    \[
    a = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix}
    \]
    and determine if the matrix is positive definite.",positive definite matrices
"consider the matrix 
    \[
    a = \begin{bmatrix} 4 & 1 & 2 \\ 1 & 4 & 1 \\ 2 & 1 & 4 \end{bmatrix}.
    \]
    verify if the matrix is positive definite using sylvester’s criterion.",positive definite matrices
"prove that if a matrix is positive definite, then it is symmetric. use this result to verify if the matrix 
    \[
    a = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}
    \]
    is positive definite.",positive definite matrices
"use sylvester’s criterion to verify whether the matrix
    \[
    a = \begin{bmatrix} 3 & 4 \\ 4 & 6 \end{bmatrix}
    \]
    is positive definite.",positive definite matrices
"show that the matrix 
    \[
    a = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}
    \]
    is not positive definite by computing its eigenvalues.",positive definite matrices
"verify if the matrix 
    \[
    a = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}
    \]
    is positive definite by using sylvester’s criterion.",positive definite matrices
"consider the matrix 
    \[
    a = \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}.
    \]
    determine if the matrix is positive definite using both sylvester’s criterion and eigenvalue computation.",positive definite matrices
"given the matrix 
    \[
    a = \begin{bmatrix} 7 & 3 & 4 \\ 3 & 7 & 5 \\ 4 & 5 & 7 \end{bmatrix}
    \]
    determine if it is positive definite by checking the eigenvalues and principal minors.",positive definite matrices
define singular value decomposition (svd) and discuss its properties. explain how the singular values provide insight into the structure of a matrix and its rank.,positive definite matrices
"how is singular value decomposition (svd) related to the eigenvalue decomposition of a matrix? in particular, relate the singular values of \( a \) to the eigenvalues of \( a^t a \) and \( a a^t \).",positive definite matrices
explain the importance of the singular values in dimensionality reduction. how do they contribute to reducing the rank and improving computational efficiency in applications such as principal component analysis (pca)?,positive definite matrices
"derive the singular value decomposition of a matrix. show the steps involved in decomposing a matrix \( a \) into \( a = u \sigma v^t \), where \( u \), \( \sigma \), and \( v \) have specific properties.",positive definite matrices
"prove that the singular values of a matrix \( a \) are the square roots of the eigenvalues of \( a^t a \). use this result to find the singular values of the matrix
    \[
    a = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}.
    \]",positive definite matrices
"show that the columns of the matrix \( u \) in the svd of \( a \) are orthonormal. that is, prove that \( u^t u = i \) where \( i \) is the identity matrix.",positive definite matrices
"given the matrix
    \[
    a = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 0 \end{bmatrix},
    \]
    compute its singular value decomposition (svd).",positive definite matrices
"use the singular value decomposition (svd) of the matrix 
    \[
    a = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}
    \]
    to compute the rank and nullity of the matrix.",positive definite matrices
"apply svd to solve the least squares problem \( ax = b \), where
    \[
    a = \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix}, \quad b = \begin{bmatrix} 7 \\ 8 \\ 9 \end{bmatrix}.
    \]
    find the least squares solution \( x \).",positive definite matrices
show that the matrix \( a \) has the same singular values as the matrix \( a^t \). prove this by computing the svd of both \( a \) and \( a^t \).,positive definite matrices
"compute the singular value decomposition (svd) of the matrix
    \[
    a = \begin{bmatrix} 3 & 4 \\ 4 & 3 \end{bmatrix}.
    \]
    discuss how the singular values can be interpreted geometrically.",positive definite matrices
"given the matrix
    \[
    a = \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix},
    \]
    use svd to compute its rank, and then use this information to describe the nullity of \( a \).",positive definite matrices
"prove that the svd provides a unique factorization up to sign. specifically, show that if \( a = u \sigma v^t \), then the decomposition is unique except for possible signs of the singular values and the columns of \( u \) and \( v \).",positive definite matrices
"apply the singular value decomposition (svd) to the matrix
    \[
    a = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}
    \]
    and find its approximate rank using the first two singular values.",positive definite matrices
"show that the singular values of a matrix \( a \) are invariant under orthogonal transformations. that is, for any orthogonal matrix \( q \), the singular values of \( a \) and \( qa \) are the same.",positive definite matrices
"given a matrix \( a \), define and explain the relationship between the rank of \( a \) and its singular values. prove that the rank of \( a \) is equal to the number of non-zero singular values.",positive definite matrices
"show how the svd can be used to solve an overdetermined system of equations \( ax = b \) where \( a \) is a non-square matrix. solve the system using the svd of the matrix
    \[
    a = \begin{bmatrix} 2 & 1 \\ 1 & 2 \\ 2 & 2 \end{bmatrix}, \quad b = \begin{bmatrix} 5 \\ 6 \\ 7 \end{bmatrix}.
    \]",positive definite matrices
"explain the connection between the svd and the spectral theorem for symmetric matrices. specifically, describe how the eigenvalue decomposition of a symmetric matrix relates to the svd.",positive definite matrices
"compute the svd of the matrix
    \[
    a = \begin{bmatrix} 3 & 4 & 0 \\ 4 & 3 & 0 \\ 0 & 0 & 5 \end{bmatrix}.
    \]
    discuss how the singular values reflect the geometry of the matrix.",positive definite matrices
"use the svd to compute the pseudoinverse of the matrix
    \[
    a = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}.
    \]
    verify that \( a^+ a = i \) where \( a^+ \) is the pseudoinverse.",positive definite matrices
"show that the frobenius norm of a matrix \( a \), defined as \( \|a\|_f = \sqrt{\sum_{i} \sigma_i^2} \), is equal to the square root of the sum of the squares of the singular values.",positive definite matrices
"apply the svd to perform dimensionality reduction on the matrix
    \[
    a = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}
    \]
    by keeping only the first singular value. compare the original matrix with the reduced matrix.",positive definite matrices
"prove that for a diagonal matrix \( d \), the svd of \( d \) is simply \( d = u \sigma v^t \) where \( u = v = i \) and \( \sigma = d \).",positive definite matrices
"given the matrix
    \[
    a = \begin{bmatrix} 2 & 3 \\ 2 & 2 \\ 3 & 2 \end{bmatrix},
    \]
    compute the svd of \( a \) and discuss the significance of the singular values in relation to the matrix's rank.",positive definite matrices
"use the svd of the matrix
    \[
    a = \begin{bmatrix} 1 & 3 \\ 2 & 2 \end{bmatrix}
    \]
    to perform dimensionality reduction, reducing the matrix to a rank-1 approximation. compare this approximation to the original matrix.",positive definite matrices
"given the matrix
    \[
    a = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix},
    \]
    compute its svd and explain the relationship between its singular values and the rank.",positive definite matrices
"compute the svd of the matrix
    \[
    a = \begin{bmatrix} 5 & 4 \\ 4 & 5 \end{bmatrix}.
    \]
    discuss how the singular values and vectors relate to the transformation properties of the matrix.",positive definite matrices
"find the rank of the matrix
    \[
    a = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}
    \]
    using its singular values.",positive definite matrices
"show how the svd can be used to obtain an optimal low-rank approximation of a matrix. use this to approximate the matrix
    \[
    a = \begin{bmatrix} 4 & 3 \\ 3 & 4 \end{bmatrix}
   ]
    to rank-1.",positive definite matrices
"define the minimum principle in optimization. discuss its significance in solving constrained optimization problems, and provide an example where the minimum principle is used to determine the optimal solution to a problem.",positive definite matrices
how does the minimum principle relate to positive definite matrices in optimization? discuss the importance of positive definiteness and explain how it ensures the uniqueness of the solution in certain optimization problems.,positive definite matrices
explain the application of the minimum principle in variational problems. illustrate this by providing a classical example such as the brachistochrone problem or the problem of finding the shortest path under certain constraints.,positive definite matrices
"derive the minimum principle for a functional \( j(y) = \int_{a}^{b} f(x, y(x), y'(x)) \, dx \), and explain how the conditions for optimality are derived using the euler-lagrange equation. discuss the role of the boundary conditions in this derivation.",positive definite matrices
"prove that a function \( f(x) \) that satisfies the minimum principle must have a positive definite hessian matrix. use the second-order necessary condition for optimality to demonstrate this property, and provide an example where this condition is verified.",positive definite matrices
"use the minimum principle to solve the variational problem of finding the function \( y(x) \) that minimizes the functional
    \[
    j(y) = \int_{0}^{1} \left( (y')^2 + y^2 \right) \, dx.
    \]
    solve this problem by deriving the euler-lagrange equation and solving for \( y(x) \).",positive definite matrices
"apply the minimum principle in optimization by considering the quadratic objective function
    \[
    f(x) = x^t a x + b^t x + c,
    \]
    where \( a \) is a symmetric matrix. show how positive definiteness of \( a \) ensures that the function has a unique minimum. solve for the optimal \( x \).",positive definite matrices
derive the minimum principle in the context of constrained optimization. consider a constrained optimization problem with an equality constraint \( g(x) = 0 \). use the method of lagrange multipliers to derive the necessary conditions for optimality.,positive definite matrices
"discuss the connection between the minimum principle and the concept of convexity in optimization. prove that if the hessian of the objective function is positive definite, the function is strictly convex, and hence the minimum principle guarantees a unique minimum.",positive definite matrices
"consider the functional
    \[
    j(y) = \int_0^1 \left( y''(x)^2 + (y'(x))^2 \right) dx.
    \]
    use the minimum principle to derive the euler-lagrange equation for this variational problem and solve for \( y(x) \).",positive definite matrices
prove that a function \( f(x) \) is convex if its second derivative is positive definite. use this result to explain why positive definite matrices guarantee a unique minimum in optimization problems.,positive definite matrices
"apply the minimum principle to solve the following variational problem:
    \[
    j(y) = \int_0^1 (y''(x)^2 - y'(x)^2) \, dx,
    \]
    where \( y(x) \) is a function subject to boundary conditions. solve for the function \( y(x) \) that minimizes the functional.",positive definite matrices
"derive the minimum principle for a multi-variable function and use it to find the minimum of the objective function
    \[
    f(x_1, x_2) = x_1^2 + x_2^2 + 2x_1 x_2.
    \]
    solve for the values of \( x_1 \) and \( x_2 \) that minimize this function.",positive definite matrices
"discuss the role of the hessian matrix in the minimum principle. specifically, show that the hessian matrix of a twice-differentiable function is positive definite at a local minimum.",positive definite matrices
"consider the variational problem
    \[
    j(y) = \int_0^1 \left( y'(x)^2 + 3y(x)^2 \right) dx.
    \]
    use the minimum principle to solve for the function \( y(x) \) that minimizes this functional subject to boundary conditions \( y(0) = 0 \) and \( y(1) = 0 \).",positive definite matrices
"derive the minimum principle in the context of functional optimization. use this principle to determine the optimal value of the integral
    \[
    \int_0^1 (y''(x)^2 + y(x)^2) dx,
    \]
    where \( y(x) \) is a function with boundary conditions \( y(0) = 0 \) and \( y(1) = 0 \).",positive definite matrices
"prove that for a convex optimization problem, the minimum principle guarantees that any local minimum is also a global minimum. use an example to illustrate this property.",positive definite matrices
"use the minimum principle to find the optimal solution for the quadratic functional
    \[
    j(y) = \int_0^1 (y'(x)^2 + 2y(x)^2) \, dx.
    \]
    solve for the function \( y(x) \) that minimizes the functional subject to boundary conditions \( y(0) = 0 \) and \( y(1) = 1 \).",positive definite matrices
show that a positive definite hessian matrix implies a strictly convex function. use this result to explain why the minimum principle guarantees a unique minimum for a strictly convex function.,positive definite matrices
"apply the minimum principle to the following constrained optimization problem:
    \[
    \min_{x} \, f(x) = x_1^2 + x_2^2 \quad \text{subject to} \quad g(x) = x_1 + x_2 - 1 = 0.
    \]
    solve for the optimal values of \( x_1 \) and \( x_2 \) using lagrange multipliers.",positive definite matrices
"consider the variational problem where the functional is given by
    \[
    j(y) = \int_0^1 \left( (y'(x))^2 + (y(x))^3 \right) dx.
    \]
    use the minimum principle to derive the euler-lagrange equation and solve for \( y(x) \).",positive definite matrices
"derive the necessary conditions for optimality using the minimum principle for the functional
    \[
    j(y) = \int_0^1 (y'(x)^2 - y(x)^2) \, dx,
    \]
    subject to boundary conditions \( y(0) = 0 \) and \( y(1) = 0 \).",positive definite matrices
prove that the second derivative of a convex function is positive definite. use this result to demonstrate the application of the minimum principle in optimization problems with convex objective functions.,positive definite matrices
"use the minimum principle to solve the variational problem
    \[
    j(y) = \int_0^1 \left( 2y'(x)^2 + y(x)^2 \right) \, dx.
    \]
    solve for the function \( y(x) \) that minimizes this functional subject to boundary conditions \( y(0) = 0 \) and \( y(1) = 1 \).",positive definite matrices
show that the first-order condition of the minimum principle gives a necessary condition for a local minimum of a quadratic functional.,positive definite matrices
discuss the connection between the minimum principle and the method of steepest descent in optimization. provide an example illustrating how the gradient of a function leads to the minimum principle.,positive definite matrices
"consider the objective function
    \[
    f(x) = x^t a x + b^t x + c,
    \]
    where \( a \) is a positive definite matrix. show that this function has a unique minimum at \( x = -a^{-1}b \).",positive definite matrices
"solve the following variational problem using the minimum principle:
    \[
    j(y) = \int_0^1 \left( y'(x)^2 + y(x)^4 \right) dx,
    \]
    subject to boundary conditions \( y(0) = 0 \) and \( y(1) = 1 \).",positive definite matrices
"derive the necessary and sufficient conditions for optimality in the minimum principle for the problem of minimizing the functional
    \[
    j(y) = \int_0^1 (y'(x)^2 - y(x)^3) \, dx.
    \]",positive definite matrices
"use the minimum principle in the context of a constrained optimization problem. for the objective function \( f(x) = x^2 + 2x \), solve the optimization problem subject to the constraint \( x = 1 \).",positive definite matrices
"apply the minimum principle to a multi-variable optimization problem. given the function
    \[
    f(x, y) = x^2 + y^2 + 2xy,
    \]
    find the critical points and determine whether they correspond to a minimum using the second-order conditions.",positive definite matrices
"derive the minimum principle for a functional involving higher-order derivatives, such as \( j(y) = \int_0^1 \left( y^{(3)}(x)^2 + y(x)^2 \right) dx \). solve for the function \( y(x) \) that minimizes this functional.",positive definite matrices
"use the minimum principle to solve the optimization problem for the function
    \[
    f(x) = x_1^2 + x_2^2 - 2x_1x_2,
    \]
    and find the point where this function achieves its minimum.",positive definite matrices
"show that if the hessian matrix of a function is positive definite, the function has a unique minimum. apply this to the function \( f(x) = x^2 + 3x + 2 \) to find its minimum.",positive definite matrices
"derive the euler-lagrange equation for the functional
    \[
    j(y) = \int_0^1 \left( y'(x)^2 + 3y(x)^2 \right) dx.
    \]",positive definite matrices
define the finite element method and explain its significance in solving partial differential equations (pdes). provide a detailed discussion on how the finite element method discretizes the domain and how it translates a continuous problem into a solvable system of algebraic equations.,positive definite matrices
how does the concept of positive definiteness relate to the stiffness matrix in the finite element method? discuss the implications of positive definite matrices for the existence and uniqueness of the solution in the context of finite element analysis.,positive definite matrices
"discuss the application of the finite element method in structural engineering. illustrate how it is used to analyze stress, strain, and displacement in structures like beams, trusses, and plates under various loading conditions.",positive definite matrices
"derive the weak form of a boundary value problem for the poisson equation:
    \[
    -\delta u(x) = f(x), \quad u(x) = 0 \text{ on } \partial\omega.
    \]
    show how the weak formulation leads to a system of linear equations in the finite element method.",positive definite matrices
prove that the stiffness matrix in the finite element method is positive definite. assume a linear finite element for a 1d structural problem and show that the associated stiffness matrix is positive definite.,positive definite matrices
"solve a simple finite element problem involving the solution of a 1d poisson equation:
    \[
    -\frac{d^2u}{dx^2} = f(x), \quad u(0) = 0, \quad u(1) = 0.
    \]
    use linear finite elements to discretize the problem and assemble the stiffness matrix. solve the resulting system of equations for \( u(x) \) assuming a given source function \( f(x) \).",positive definite matrices
"given the stiffness matrix for a 1d finite element problem:
    \[
    k = \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix},
    \]
    analyze its positive definiteness by calculating its eigenvalues and discuss the implications for the solution of the corresponding system of equations.",positive definite matrices
"derive the weak form of a boundary value problem for the heat equation:
    \[
    \frac{\partial u}{\partial t} - \alpha \delta u = 0, \quad u = 0 \text{ on } \partial\omega.
    \]
    show how the weak formulation leads to a time-stepping scheme for finite element analysis.",positive definite matrices
"discuss the process of discretization in the finite element method. how does the choice of element type (e.g., linear, quadratic) affect the accuracy of the solution? provide examples comparing different element types.",positive definite matrices
"prove that the stiffness matrix \( k \) is symmetric and positive definite for a linear finite element problem. use the variational principle to show that the associated functional is minimized by the solution, and explain the role of symmetry and positive definiteness in this context.",positive definite matrices
"consider the finite element formulation for the bending of a beam under an applied load. derive the governing equations for the system, the stiffness matrix, and boundary conditions for a beam subjected to both bending and axial forces.",positive definite matrices
"explain the concept of ""degrees of freedom"" in the context of finite element analysis. how does the number of degrees of freedom affect the size of the stiffness matrix and the computational complexity of the problem?",positive definite matrices
"show how the stiffness matrix for a 1d linear finite element problem can be derived using the galerkin method. assume the following linear trial function \( \hat{u}(x) \) for the approximation of the displacement field:
    \[
    \hat{u}(x) = \lambda_1 u_1 + \lambda_2 u_2.
    \]
    calculate the stiffness matrix for this element.",positive definite matrices
"in a 2d finite element problem, derive the stiffness matrix for a triangular element using linear interpolation. assume the element has nodes at \( (x_1, y_1), (x_2, y_2), (x_3, y_3) \), and derive the matrix form of the element's stiffness matrix.",positive definite matrices
"prove that for a linear finite element in 1d, the stiffness matrix is invertible and hence provides a unique solution. use the properties of the bilinear form and the positive definiteness of the stiffness matrix to argue this point.",positive definite matrices
"for a 1d beam subjected to a uniform load \( w(x) = 1 \) and simply supported at both ends, derive the stiffness matrix for the beam element and compute the displacements at the nodes.",positive definite matrices
show how boundary conditions (dirichlet and neumann) are incorporated into the finite element formulation. provide a numerical example to demonstrate how these conditions affect the final stiffness matrix.,positive definite matrices
discuss the concept of mesh refinement in the finite element method. how does refining the mesh improve the accuracy of the solution? explain the trade-off between computational cost and accuracy.,positive definite matrices
"derive the weak form of the boundary value problem for the wave equation:
    \[
    \frac{\partial^2 u}{\partial t^2} - c^2 \delta u = 0, \quad u = 0 \text{ on } \partial\omega.
    \]
    show how the weak formulation leads to a system of equations for the finite element method.",positive definite matrices
"given the stiffness matrix
    \[
    k = \begin{bmatrix} 4 & -2 & 0 \\ -2 & 4 & -2 \\ 0 & -2 & 4 \end{bmatrix},
    \]
    solve the corresponding system of equations for \( u = [u_1, u_2, u_3]^t \) assuming boundary conditions \( u_1 = 0 \) and \( u_3 = 1 \).",positive definite matrices
"explain the role of the mass matrix in finite element analysis. how does it differ from the stiffness matrix, and how is it used in dynamic problems such as vibration analysis?",positive definite matrices
"derive the weak form of a boundary value problem for the elasticity equation in 2d:
    \[
    \nabla \cdot (\sigma) = f, \quad \sigma = c \varepsilon, \quad u = 0 \text{ on } \partial\omega.
    \]
    explain how this weak form leads to a finite element formulation for the displacement field \( u(x) \).",positive definite matrices
"show how the stiffness matrix for a 2d structural problem can be assembled using a 2d quadrilateral element. derive the shape functions and element stiffness matrix, and discuss the boundary conditions that are applied.",positive definite matrices
"for a 1d finite element model, derive the system of equations for a structure subjected to both internal and external forces. explain how the stiffness matrix is assembled and solve for the displacements at the nodes.",positive definite matrices
"explain the process of solving the system of equations resulting from the finite element method. how does the assembly of the global stiffness matrix and force vector work, and what methods are used to solve the resulting linear system?",positive definite matrices
"for a 1d linear finite element with two nodes, derive the global stiffness matrix for a structure subjected to a uniform distributed load. assume that the element has young's modulus \( e = 210 \, \text{gpa} \) and the element length \( l = 2 \, \text{m} \).",positive definite matrices
discuss the impact of mesh size and element type on the convergence of the finite element solution. how does refining the mesh lead to a more accurate solution in the context of the finite element method?,positive definite matrices
"given the stiffness matrix
    \[
    k = \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix},
    \]
    calculate the displacement vector for a system subjected to a force vector \( f = [1, 1]^t \). solve for the displacements and discuss the implications for the structure's deformation.",positive definite matrices
"derive the stiffness matrix for a 2d finite element problem using quadratic shape functions. explain the differences between linear and quadratic shape functions, and show how they affect the accuracy of the solution.",positive definite matrices
"for a 1d finite element problem, derive the stiffness matrix for a triangular element. discuss how this element is used to discretize a domain and solve a boundary value problem.",positive definite matrices
prove that the global stiffness matrix in the finite element method is symmetric. discuss the implications of this symmetry for the efficiency of solving the system of equations.,positive definite matrices
"given a 2d triangular element, derive the stiffness matrix using the galerkin method. assume the element has nodes at \( (x_1, y_1), (x_2, y_2), (x_3, y_3) \), and show how the shape functions are used to derive the matrix.",positive definite matrices
show that the finite element stiffness matrix is always positive semi-definite. discuss how positive definiteness ensures the stability of the numerical solution.,positive definite matrices
derive the weak form of a boundary value problem for a system of nonlinear equations. explain how the finite element method can be extended to solve nonlinear pdes.,positive definite matrices
"for a 1d bar with constant cross-sectional area subjected to an axial force, derive the stiffness matrix and solve for the displacement at each node using finite element analysis.",positive definite matrices
prove that the stiffness matrix for a 1d bar element is positive definite. show that the stiffness matrix leads to a unique solution for the displacement field.,positive definite matrices
derive the global stiffness matrix for a 1d finite element model consisting of two elements. discuss how the element stiffness matrices are assembled to form the global system.,positive definite matrices
"derive the finite element formulation for a 1d elasticity problem. assume the material is isotropic, and the structure is subjected to a uniform tensile force.",positive definite matrices
explain how the finite element method is applied to problems in heat transfer. derive the weak form of the heat conduction equation and show how it leads to a finite element solution for temperature distribution.,positive definite matrices
discuss how finite element methods are used in fluid dynamics. derive the weak form for the navier-stokes equations and explain how the method can be applied to fluid flow problems.,positive definite matrices
"given the stiffness matrix for a 2d finite element problem, explain how boundary conditions are incorporated and how the global system of equations is solved for the displacements.",positive definite matrices
derive the element stiffness matrix for a 2d rectangular finite element. assume the element has uniform material properties and discuss how the stiffness matrix is derived using shape functions.,positive definite matrices
discuss the numerical solution of the finite element method using direct and iterative solvers. compare the advantages and disadvantages of both approaches in terms of computational cost and accuracy.,positive definite matrices
"define a matrix norm in the context of linear algebra. discuss its role in matrix inequalities and provide examples of commonly used norms such as the frobenius norm and the spectral norm. how do these norms relate to the structure of optimization problems, and what are the implications for matrix analysis? given the matrix \( a = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix} \), compute its frobenius norm and spectral norm.",positive definite matrices
"state and explain ky fan’s inequalities in the context of matrix theory. discuss their significance in optimization, particularly in the context of matrix eigenvalues. how can these inequalities be used to establish bounds in convex optimization problems involving eigenvalues of symmetric matrices? apply ky fan's inequalities to the matrix \( a = \begin{pmatrix} 4 & 1 \\ 1 & 3 \end{pmatrix} \) and determine its eigenvalue bounds.",positive definite matrices
"what is the löwner-heinz inequality, and how is it applied in matrix analysis? provide a detailed explanation of its geometric interpretation and discuss its importance in the context of matrix functions, particularly when applied to positive semidefinite matrices. prove the löwner-heinz inequality for the matrices \( a = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \) and \( b = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \).",positive definite matrices
"discuss the concept of matrix majorization and its relationship with matrix inequalities. how does matrix majorization serve as a tool in convex optimization? provide examples of matrix inequalities that are related to matrix majorization, and explain their significance in optimization theory. for the matrix \( a = \begin{pmatrix} 3 & 1 \\ 0 & 2 \end{pmatrix} \), determine whether \( a \) is majorized by \( b = \begin{pmatrix} 2 & 2 \\ 1 & 1 \end{pmatrix} \).",positive definite matrices
"derive ky fan's inequalities for the case of general matrices, not necessarily symmetric, and demonstrate how they can be applied to establish bounds on eigenvalues of non-symmetric matrices in optimization problems. discuss the relationship between these inequalities and other matrix inequalities in convex optimization. use ky fan’s inequality to find eigenvalue bounds for the matrix \( a = \begin{pmatrix} 1 & 2 & 3 \\ 2 & 1 & 4 \\ 3 & 4 & 1 \end{pmatrix} \).",positive definite matrices
"prove the löwner-heinz inequality for self-adjoint operators and discuss its significance in the context of convexity. how does this inequality relate to the optimization of convex functions over self-adjoint matrices or operators? provide an example where this inequality is used to solve an optimization problem. let \( a = \begin{pmatrix} 2 & 1 \\ 1 & 3 \end{pmatrix} \) and \( b = \begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix} \), prove the löwner-heinz inequality and find its application in an optimization problem.",positive definite matrices
"apply ky fan’s inequalities to solve a semidefinite programming problem. given the semidefinite program \( \max \text{tr}(a x) \), subject to \( x \succeq 0 \) and \( \text{tr}(x) = 1 \), where \( a = \begin{pmatrix} 3 & 2 \\ 2 & 1 \end{pmatrix} \), use ky fan’s inequalities to determine the optimal solution.",positive definite matrices
"solve an optimization problem using matrix inequalities, such as maximizing a convex function subject to matrix constraints. discuss the use of ky fan’s inequalities and löwner-heinz inequality in obtaining optimal solutions for problems involving semidefinite matrices. given \( a = \begin{pmatrix} 5 & 4 \\ 4 & 5 \end{pmatrix} \), solve the optimization problem \( \max \text{tr}(a x) \) subject to \( x \succeq 0 \) and \( \text{tr}(x) = 1 \).",positive definite matrices
"derive a set of inequalities involving matrix norms, and discuss their application to optimization problems. show how these inequalities can be used to analyze the stability of solutions in optimization problems involving matrix constraints. for the matrix \( a = \begin{pmatrix} 2 & 3 \\ 3 & 4 \end{pmatrix} \), find its frobenius norm and spectral norm, and use them to analyze the stability of a related optimization problem.",positive definite matrices
"discuss how matrix inequalities are used in convex optimization to derive duality theorems. provide a detailed explanation of the duality gap in semidefinite programming and how matrix inequalities play a crucial role in its analysis. for the semidefinite program \( \max \text{tr}(a x) \), subject to \( x \succeq 0 \), compute the duality gap for the given matrix \( a = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \).",positive definite matrices
"explain the application of ky fan's inequalities in the context of low-rank approximation problems. how can these inequalities be used to derive bounds on the rank of matrix approximations and to solve optimization problems involving low-rank matrices? given a matrix \( a = \begin{pmatrix} 6 & 2 \\ 2 & 3 \end{pmatrix} \), use ky fan’s inequalities to find the rank of a low-rank approximation.",positive definite matrices
"derive an explicit formula for the eigenvalues of a matrix function involving a positive semidefinite matrix. use this formula to show how matrix inequalities can be applied to optimization problems involving matrix functions, such as maximizing the trace of a matrix function subject to constraints. for the matrix \( a = \begin{pmatrix} 4 & 2 \\ 2 & 4 \end{pmatrix} \), compute its eigenvalues and use them to solve a related optimization problem.",positive definite matrices
"prove that the löwner-heinz inequality is a special case of a more general matrix inequality involving matrix monotonicity. discuss the implications of this more general form for optimization problems, particularly in the context of semidefinite programming and convex optimization. use the inequality to solve an optimization problem with the matrix \( a = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} \) and \( b = \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} \).",positive definite matrices
show how matrix inequalities can be applied to the analysis of the complexity of algorithms in semidefinite programming. discuss how the matrix inequalities associated with the dual problem can be used to derive bounds on the computational complexity of solving semidefinite programs. consider the matrix \( a = \begin{pmatrix} 2 & 1 \\ 1 & 3 \end{pmatrix} \) and analyze its computational complexity using matrix inequalities.,positive definite matrices
"explain how the concept of matrix majorization can be used to establish bounds in optimization problems involving eigenvalues of matrix functions. provide an example where matrix majorization is used to optimize a function over a set of matrices with specified eigenvalue constraints. use matrix majorization to solve the problem where \( a = \begin{pmatrix} 3 & 1 \\ 1 & 2 \end{pmatrix} \) and the eigenvalue constraints are \( \lambda_1 = 4, \lambda_2 = 1 \).",positive definite matrices
"discuss the relationship between matrix inequalities and the optimization of matrix-valued functions. show how ky fan’s inequalities and the löwner-heinz inequality can be used to derive conditions for optimality in matrix optimization problems, and explain how these conditions lead to the solution of matrix optimization problems in practice. given \( a = \begin{pmatrix} 4 & 1 \\ 1 & 4 \end{pmatrix} \), solve the matrix optimization problem \( \max \text{tr}(a x) \) subject to \( x \succeq 0 \).",positive definite matrices
"show how ky fan's inequalities can be used to derive bounds on the eigenvalues of the sum of two positive semidefinite matrices. provide a detailed proof and discuss the implications for optimization problems involving matrix sums. given \( a = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \) and \( b = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \), compute the bounds on the eigenvalues of \( a + b \).",positive definite matrices
"discuss the concept of schur-convex functions in matrix optimization and show how matrix inequalities are used to establish conditions for the convexity of matrix-valued functions. provide an example where schur-convex functions are used to optimize matrix functions under matrix inequality constraints. given a schur-convex function \( f(x) = \text{tr}(a x^2) \), find the optimal matrix \( x \) given the constraint \( x \succeq 0 \).",positive definite matrices
"solve an optimization problem involving a matrix inequality constraint. consider a problem where the objective is to maximize a convex function subject to a matrix constraint such as \( a \succeq b \). apply the theory of matrix inequalities to derive the optimal solution and discuss its significance in the context of matrix analysis and convex optimization. given \( a = \begin{pmatrix} 4 & 1 \\ 1 & 3 \end{pmatrix} \) and \( b = \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} \), solve the optimization problem \( \max \text{tr}(a x) \) subject to \( x \succeq b \).",positive definite matrices
"define a positive semidefinite matrix and explain the difference between a positive semidefinite matrix and a positive definite matrix. provide examples of both types of matrices, and describe how their eigenvalues influence their classification. given the matrix \( a = \begin{pmatrix} 4 & 2 \\ 2 & 3 \end{pmatrix} \), determine whether it is positive semidefinite and explain your reasoning.",positive definite matrices
state schur's theorem and explain how it applies to positive semidefinite matrices. what does schur’s theorem imply about the eigenvalues of a positive semidefinite matrix? use schur's theorem to prove that the matrix \( a = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \) is positive semidefinite.,positive definite matrices
"discuss the role of positive semidefinite matrices in quadratic forms. how can you express a quadratic form \( x^t a x \) as a convex function when \( a \) is positive semidefinite? given \( a = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} \) and \( x = \begin{pmatrix} 1 \\ 2 \end{pmatrix} \), compute the quadratic form \( x^t a x \) and discuss its implications.",positive definite matrices
"derive the conditions under which a matrix is positive semidefinite. what are the key criteria that a matrix must satisfy to be classified as positive semidefinite? given the matrix \( a = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \), show that it is positive semidefinite by verifying its conditions.",positive definite matrices
prove that a matrix is positive semidefinite if and only if all of its eigenvalues are non-negative. use the matrix \( a = \begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix} \) to verify this property by finding its eigenvalues and checking whether they are non-negative.,positive definite matrices
explain the concept of cholesky factorization and its application to positive semidefinite matrices. how does cholesky decomposition help in solving systems of linear equations where the coefficient matrix is positive semidefinite? perform the cholesky factorization of the matrix \( a = \begin{pmatrix} 4 & 2 \\ 2 & 3 \end{pmatrix} \) and verify that the resulting matrix is lower triangular.,positive definite matrices
"apply the properties of positive semidefinite matrices to optimization problems in machine learning. how are these matrices used in the formulation of cost functions and constraints in machine learning algorithms? consider the following optimization problem: minimize \( f(x) = x^t a x \), subject to \( a = \begin{pmatrix} 5 & 1 \\ 1 & 5 \end{pmatrix} \) and \( x = \begin{pmatrix} 1 \\ 1 \end{pmatrix} \). compute the minimum value of \( f(x) \).",positive definite matrices
"given the matrix \( a = \begin{pmatrix} 1 & 2 & 3 \\ 2 & 4 & 5 \\ 3 & 5 & 6 \end{pmatrix} \), determine whether it is positive semidefinite by examining its eigenvalues. show your work by finding the eigenvalues of the matrix and explaining why the matrix is positive semidefinite if all the eigenvalues are non-negative.",positive definite matrices
"discuss the significance of the spectral properties of positive semidefinite matrices in optimization. how can eigenvalues and eigenvectors be used to analyze the stability and optimality of solutions to optimization problems involving positive semidefinite matrices? given \( a = \begin{pmatrix} 6 & 2 \\ 2 & 6 \end{pmatrix} \), find the eigenvalues and eigenvectors, and interpret them in the context of optimization.",positive definite matrices
"apply the concept of positive semidefinite matrices to the analysis of covariance matrices in statistics. how does the positive semidefiniteness of a covariance matrix influence its interpretation and use in multivariate analysis? given the covariance matrix \( a = \begin{pmatrix} 1 & 0.8 \\ 0.8 & 1 \end{pmatrix} \), verify its positive semidefiniteness and discuss its implications for multivariate normal distributions.",positive definite matrices
"explain how positive semidefinite matrices are used in signal processing, particularly in the context of filtering and estimation problems. provide an example where the matrix \( a = \begin{pmatrix} 1 & 0.5 \\ 0.5 & 1 \end{pmatrix} \) is used as a covariance matrix in a kalman filter and discuss how the positive semidefiniteness of \( a \) ensures the stability of the filter.",positive definite matrices
"investigate the relationship between positive semidefinite matrices and convex optimization. how do positive semidefinite matrices play a role in formulating convex optimization problems, and what properties do these matrices provide for solution methods such as gradient descent? given \( a = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} \), solve the convex optimization problem \( \min_x \, x^t a x \), subject to \( x_1 + x_2 = 1 \).",positive definite matrices
"derive the conditions under which a symmetric matrix is positive semidefinite. how can you use the cholesky factorization and eigenvalue decomposition to test if a symmetric matrix is positive semidefinite? given \( a = \begin{pmatrix} 4 & 1 \\ 1 & 4 \end{pmatrix} \), check if \( a \) is positive semidefinite by performing both cholesky factorization and eigenvalue decomposition.",positive definite matrices
"discuss the applications of positive semidefinite matrices in machine learning, particularly in the context of support vector machines (svms). how do positive semidefinite kernels lead to optimal hyperplane classification? consider the kernel matrix \( k = \begin{pmatrix} 1 & 0.5 \\ 0.5 & 1 \end{pmatrix} \), and demonstrate how its positive semidefiniteness ensures the feasibility of svm optimization.",positive definite matrices
"given the matrix \( a = \begin{pmatrix} 2 & 1 & 0 \\ 1 & 2 & 1 \\ 0 & 1 & 2 \end{pmatrix} \), determine whether it is positive semidefinite. if so, explain why this matrix is useful in a machine learning application such as kernel methods for regression or classification.",positive definite matrices
"solve an optimization problem involving positive semidefinite matrices using a lagrange multiplier approach. given the matrix \( a = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \), solve the optimization problem \( \max \text{tr}(a x) \), subject to \( x \succeq 0 \) and \( \text{tr}(x) = 1 \).",positive definite matrices
"show that if a matrix is positive semidefinite, its inverse is also positive semidefinite, provided the matrix is invertible. given \( a = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} \), find \( a^{-1} \) and verify that it is positive semidefinite.",positive definite matrices
"construct a subset of the $x$-$y$ plane $\mathbb{r}^2$ that is
    \begin{enumerate}",vector spaces
"closed under vector addition and subtraction, but not scalar multiplication.",vector spaces
closed under scalar multiplication but not under vector addition.,vector spaces
"which of the following subsets of $\mathbb{r}^3$ are actually subspaces?
    \begin{enumerate}",vector spaces
"the plane of vectors $(b_1, b_2, b_3)$ with the first component $b_1 = 0$.",vector spaces
the plane of vectors $\mathbf{b}$ with $b_1 = 1$.,vector spaces
"the vectors $\mathbf{b}$ with $b_2 b_3 = 0$ (this is the union of two subspaces, the plane $b_2 = 0$ and the plane $b_3 = 0$).",vector spaces
"all combinations of two given vectors $(1, 1, 0)$ and $(2, 0, 1)$.",vector spaces
"the plane of vectors $(b_1, b_2, b_3)$ that satisfy $b_3 - b_2 + 3b_1 = 0$.",vector spaces
"describe the column space and the nullspace of the matrices
    \[
    a = \begin{pmatrix}
        1 & -1 \\
        0 & 0
    \end{pmatrix}, \quad
    b = \begin{pmatrix}
        0 & 0 & 3 \\
        1 & 2 & 3
    \end{pmatrix}, \quad
    c = \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 0 & 0
    \end{pmatrix}.
    \]",vector spaces
what is the smallest subspace of $3 \times 3$ matrices that contains all symmetric matrices and all lower triangular matrices? what is the largest subspace that is contained in both of those subspaces?,vector spaces
"addition and scalar multiplication are required to satisfy these eight rules:
    \begin{enumerate}",vector spaces
$x + y = y + x$.,vector spaces
$x + (y + z) = (x + y) + z$.,vector spaces
"there is a unique ""zero vector"" such that $x + 0 = x$ for all $x$.",vector spaces
for each $x$ there is a unique vector $-x$ such that $x + (-x) = 0$.,vector spaces
$1x = x$.,vector spaces
$(c_1 c_2)x = c_1 (c_2 x)$.,vector spaces
$c(x + y) = cx + cy$.,vector spaces
$(c_1 + c_2)x = c_1 x + c_2 x$.,vector spaces
"suppose addition in $\mathbb{r}^2$ adds an extra 1 to each component, so that $(3,1) + (5,0)$ equals $(9,2)$ instead of $(8,1)$. with scalar multiplication unchanged, which rules are broken?",vector spaces
"show that the set of all positive real numbers, with $x+y$ and $cx$ redefined to equal the usual $xy$ and $x^c$, is a vector space. what is the ""zero vector""?",vector spaces
"suppose $(x_1, x_2) + (y_1, y_2)$ is defined to be $(x_1 + y_2, x_2 + y_1)$. with the usual $cx = (cx_1, cx_2)$, which of the eight conditions are not satisfied?",vector spaces
let $p$ be the plane in $\mathbb{r}^3$ with equation $x + 2y + z = 6$. what is the equation of the plane $p_0$ through the origin parallel to $p$? are $p$ and $p_0$ subspaces of $\mathbb{r}^3$?,vector spaces
"which of the following are subspaces of $\mathbb{r}^{\infty}$?
    \begin{enumerate}",vector spaces
"all sequences like $(1,0,1,0,...)$ that include infinitely many zeros.",vector spaces
"all sequences $(x_1, x_2, ...)$ with $x_j = 0$ from some point onward.",vector spaces
all decreasing sequences: $x_{j+1} \leq x_j$ for each $j$.,vector spaces
all convergent sequences: the $x_j$ have a limit as $j \to \infty$.,vector spaces
all arithmetic progressions: $x_{j+1} - x_j$ is the same for all $j$.,vector spaces
"all geometric progressions $(x_1, kx_1, k^2x_1, ...)$ allowing all $k$ and $x_1$.",vector spaces
"which of the following descriptions are correct? the solutions $x$ of
    \[
    a x = \begin{pmatrix} 1 & 1 & 1 \\ 1 & 0 & 2 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
    \]
    form
    \begin{enumerate}",vector spaces
a plane.,vector spaces
a line.,vector spaces
a point.,vector spaces
a subspace.,vector spaces
the nullspace of $a$.,vector spaces
the column space of $a$.,vector spaces
show that the set of nonsingular $2 \times 2$ matrices is not a vector space. show also that the set of singular $2 \times 2$ matrices is not a vector space.,vector spaces
"the matrix $a = \begin{pmatrix} 2 & -2 \\ 2 & -2 \end{pmatrix}$ is a ""vector"" in the space $m$ of all $2 \times 2$ matrices. write the zero vector in this space, the vector $\frac{1}{2} a$, and the vector $-a$. what matrices are in the smallest subspace containing $a$?",vector spaces
\begin{enumerate},vector spaces
describe a subspace of $m$ that contains $a = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ but not $b = \begin{bmatrix} 0 & 0 \\ 0 & -1 \end{bmatrix}$.,vector spaces
"if a subspace of $m$ contains $a$ and $b$, must it contain $i$?",vector spaces
describe a subspace of $m$ that contains no nonzero diagonal matrices.,vector spaces
the functions $f(x) = x^2$ and $g(x) = 5x$ are ``vectors'' in the vector space $f$ of all real functions. the combination $3 f(x)-4g(x)$ is the function $h(x) =$. which rule is broken if multiplying $f(x)$ by $c$ gives the function $f(cx)$?,vector spaces
"if the sum of the ``vectors'' $f(x)$ and $g(x)$ in $f$ is defined to be $f(g(x))$, then the ``zero vector'' is $g(x) = x$. keep the usual scalar multiplication $c f(x)$, and find two rules that are broken.",vector spaces
"describe the smallest subspace of the $2 \times 2$ matrix space $m$ that contains
    \begin{enumerate}",vector spaces
$\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ and $\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$.,vector spaces
$\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ and $\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.,vector spaces
$\begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$.,vector spaces
"$\begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$, $\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$, $\begin{bmatrix} 0 & 1 \\ 0 & 1 \end{bmatrix}$.",vector spaces
"let $p$ be the plane in $\mathbb{r}^3$ with equation $x + y - 2z = 4$. the origin $(0,0,0)$ is not in $p$! find two vectors in $p$ and check that their sum is not in $p$.",vector spaces
"$p_0$ is the plane through $(0,0,0)$ parallel to the plane $p$ in problem 15. what is the equation for $p_0$? find two vectors in $p_0$ and check that their sum is in $p_0$.",vector spaces
"the four types of subspaces of $\mathbb{r}^3$ are planes, lines, $\mathbb{r}^3$ itself, or $z$ containing only $(0,0,0)$.
    \begin{enumerate}",vector spaces
describe the three types of subspaces of $\mathbb{r}^2$.,vector spaces
describe the five types of subspaces of $\mathbb{r}^4$.,vector spaces
"the intersection of two planes through $(0,0,0)$ is probably a \underline{ } but it could be a \underline{ }. it can’t be the zero vector $z$!",vector spaces
"the intersection of a plane through $(0,0,0)$ with a line through $(0,0,0)$ is probably a \underline{ } but it could be a \underline{ }.",vector spaces
"if $s$ and $t$ are subspaces of $\mathbb{r}^5$, their intersection $s \cap t$ (vectors in both subspaces) is a subspace of $\mathbb{r}^5$. check the requirements on $x+y$ and $cx$.",vector spaces
"suppose $p$ is a plane through $(0,0,0)$ and $l$ is a line through $(0,0,0)$. the smallest vector space containing both $p$ and $l$ is either \underline{ } or \underline{ }.",vector spaces
"true or false for $m =$ all $3 \times 3$ matrices (check addition using an example)?
    \begin{enumerate}",vector spaces
the skew-symmetric matrices in $m$ (with $a^t = -a$) form a subspace.,vector spaces
the unsymmetric matrices in $m$ (with $a^t \neq a$) form a subspace.,vector spaces
"the matrices that have $(1,1,1)$ in their nullspace form a subspace.",vector spaces
"describe the column spaces (lines or planes) of these particular matrices:
    \begin{align*}
        a &= \begin{bmatrix} 1 & 2 \\ 0 & 0 \\ 0 & 0 \end{bmatrix}, \quad 
        b = \begin{bmatrix} 1 & 0 \\ 0 & 2 \\ 0 & 0 \end{bmatrix}, \quad 
        c = \begin{bmatrix} 1 & 0 \\ 2 & 0 \\ 0 & 0 \end{bmatrix}.
    \end{align*}",vector spaces
"for which right-hand sides (find a condition on $b_1, b_2, b_3$) are these systems solvable?
    \begin{enumerate}",vector spaces
$\begin{bmatrix} 1 & 4 & 2 \\ 2 & 8 & 4 \\ -1 & -4 & -2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}.$,vector spaces
$\begin{bmatrix} 1 & 4 \\ 2 & 9 \\ -1 & -4 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}.$,vector spaces
"adding row 1 of $a$ to row 2 produces $b$. adding column 1 to column 2 produces $c$.
    a combination of the columns of one matrix is also a combination of the columns of another.
    which two matrices have the same column space?
    \[
    a = \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}, \quad
    b = \begin{bmatrix} 1 & 2 \\ 3 & 6 \end{bmatrix}, \quad
    c = \begin{bmatrix} 1 & 3 \\ 2 & 6 \end{bmatrix}.
    \]",vector spaces
"for which vectors $(b_1, b_2, b_3)$ do these systems have a solution?
    \begin{enumerate}",vector spaces
"\[
        \begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix}
        \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} =
        \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}.
        \]",vector spaces
"\[
        \begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 0 \end{bmatrix}
        \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} =
        \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}.
        \]",vector spaces
"(recommended) if we add an extra column $b$ to a matrix $a$, then the column space
    gets larger unless . give an example in which the column space gets larger and
    an example in which it doesn’t. why is $ax = b$ solvable exactly when the column
    space doesn’t get larger by including $b$?",vector spaces
"describe the smallest subspace of the $2 \times 2$ matrix space $m$ that contains:
    \begin{enumerate}",vector spaces
let $p$ be the plane in $\mathbb{r}^3$ given by $x + y - 2z = 4$. find two vectors in $p$ and verify their sum is not in $p$.,vector spaces
"find the equation of $p_0$, the plane through $(0,0,0)$ parallel to $p$. find two vectors in $p_0$ and check that their sum is in $p_0$.",vector spaces
"describe the types of subspaces:
    \begin{enumerate}",vector spaces
three types of subspaces in $\mathbb{r}^2$.,vector spaces
five types of subspaces in $\mathbb{r}^4$.,vector spaces
"describe the column spaces of these matrices:
    \begin{align*}
        a &= \begin{bmatrix} 1 & 2 \\ 0 & 0 \\ 0 & 0 \end{bmatrix}, \quad
        b = \begin{bmatrix} 1 & 0 \\ 0 & 2 \\ 0 & 0 \end{bmatrix}, \quad
        c = \begin{bmatrix} 1 & 0 \\ 2 & 0 \\ 0 & 0 \end{bmatrix}.
    \end{align*}",vector spaces
"for which right-hand sides $(b_1, b_2, b_3)$ are these systems solvable?
    \begin{align*}
        \begin{bmatrix} 1 & 4 & 2 \\ 2 & 8 & 4 \\ -1 & -4 & -2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} &= \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix},
    \end{align*}",vector spaces
provide an example where the column spaces of $a$ and $ab$ are not equal.,vector spaces
"if $a$ is an invertible $8 \times 8$ matrix, explain why its column space is $\mathbb{r}^8$.",vector spaces
"true or false (provide counterexamples if false):
    \begin{enumerate}",vector spaces
the vectors $b$ not in $c(a)$ form a subspace.,vector spaces
"if $c(a)$ contains only the zero vector, then $a$ is the zero matrix.",vector spaces
the column space of $2a$ equals the column space of $a$.,vector spaces
the column space of $a-i$ equals the column space of $a$.,vector spaces
"construct a $3 \times 3$ matrix whose column space contains $(1,1,0)$ and $(1,0,1)$ but not $(1,1,1)$. construct another matrix whose column space is only a line.",vector spaces
"if the $9 \times 12$ system $ax = b$ is solvable for every $b$, what is $c(a)$?",vector spaces
why isn’t $\mathbb{r}^2$ a subspace of $\mathbb{r}^3$?,vector spaces
"construct a system with more unknowns than equations, but no solution. change the right-hand side to zero and find all solutions $x_n$.",vector spaces
"reduce $a$ and $b$ to echelon form, to find their ranks. which variables are free?
    \[
    a = \begin{bmatrix}
        1 & 2 & 0 & 1 \\
        0 & 1 & 1 & 0 \\
        1 & 2 & 0 & 1
    \end{bmatrix}, \quad
    b = \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6 \\
        7 & 8 & 9
    \end{bmatrix}
    \]
    find the special solutions to $ax = 0$ and $bx = 0$. find all solutions.",vector spaces
"find the echelon form $u$, the free variables, and the special solutions:
    \[
    a = \begin{bmatrix} 0 & 1 & 0 & 3 \\ 0 & 2 & 0 & 6 \end{bmatrix}, \quad
    b = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix}.
    \]
    the system $ax = b$ is consistent (has a solution) when $b$ satisfies $b_2 = \; ?$ find the complete solution.
find two vectors in the nullspace of a, and the complete solution to ax = b.
9. (a) find the special solutions to ux = 0. reduce u to r and repeat:
ux =



1 2 3 4
0 0 1 2
0 0 0 0








x1
x2
x3
x4





=



0
0
0


.
(b) if the right-hand side is changed from (0,0,0) to (a,b,0), what are all solutions?",vector spaces
"reduce $a$ and $b$ to echelon form, to find their ranks. which variables are free?
    \[
    a = \begin{bmatrix}
        1 & 2 & 0 & 1 \\
        0 & 1 & 1 & 0 \\
        1 & 2 & 0 & 1
    \end{bmatrix}, \quad
    b = \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6 \\
        7 & 8 & 9
    \end{bmatrix}.
    \]
    find the special solutions to $ax = 0$ and $bx = 0$. find all solutions.",vector spaces
"find the echelon form $u$, the free variables, and the special solutions:
    \[
    a = \begin{bmatrix} 0 & 1 & 0 & 3 \\ 0 & 2 & 0 & 6 \end{bmatrix}, \quad
    b = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix}.
    \]
    $ax = b$ is consistent (has a solution) when $b$ satisfies $b_2 = \ldots$. find the complete solution in the same form as equation (4).",vector spaces
"carry out the same steps as in the previous problem to find the complete solution of $mx = b$:
    \[
    m = \begin{bmatrix} 0 & 0 \\ 1 & 2 \\ 0 & 0 \\ 3 & 6 \end{bmatrix}, \quad
    b = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ b_4 \end{bmatrix}.
    \]",vector spaces
"write the complete solutions $x = x_p + x_n$ to these systems, as in equation (4):
    \[
    \begin{bmatrix} 1 & 2 & 2 \\ 2 & 4 & 5 \end{bmatrix}
    \begin{bmatrix} u \\ v \\ w \end{bmatrix} =
    \begin{bmatrix} 1 \\ 4 \end{bmatrix},
    \]
    \[
    \begin{bmatrix} 1 & 2 & 2 \\ 2 & 4 & 4 \end{bmatrix}
    \begin{bmatrix} u \\ v \\ w \end{bmatrix} =
    \begin{bmatrix} 1 \\ 4 \end{bmatrix}.
    \]",vector spaces
"describe the set of attainable right-hand sides $b$ (in the column space) for
    \[
    \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 2 & 3 \end{bmatrix}
    \begin{bmatrix} u \\ v \end{bmatrix} =
    \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix},
    \]
    by finding the constraints on $b$ that turn the third equation into $0 = 0$ (after elimination). what is the rank, and a particular solution?",vector spaces
"find the value of $c$ that makes it possible to solve $ax = b$, and solve it:
    \begin{align*}
    u + v + 2w &= 2, \\
    2u + 3v - w &= 5, \\
    3u + 4v + w &= c.
    \end{align*}",vector spaces
"under what conditions on $b_1$ and $b_2$ (if any) does $ax = b$ have a solution?
    \[
    a = \begin{bmatrix} 1 & 2 & 0 & 3 \\ 2 & 4 & 0 & 7 \end{bmatrix}, \quad
    b = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix}.
    \]",vector spaces
"find a $2 \times 3$ system $ax = b$ whose complete solution is
    \begin{equation*}
        x = \begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix} + w \begin{bmatrix} 1 \\ 3 \\ 1 \end{bmatrix}.
    \end{equation*}
    find a $3 \times 3$ system with these solutions exactly when $b_1 + b_2 = b_3$.",vector spaces
"write a $2 \times 2$ system $ax = b$ with many solutions $x_n$ but no solution $x_p$. (therefore, the system has no solution.) which $b$'s allow an $x_p$?",vector spaces
"which of these rules give a correct definition of the rank of $a$?
    \begin{enumerate}",vector spaces
the number of nonzero rows in $r$.,vector spaces
the number of columns minus the total number of rows.,vector spaces
the number of columns minus the number of free columns.,vector spaces
the number of 1s in $r$.,vector spaces
"find the reduced row echelon forms $r$ and the rank of these matrices:
    \begin{enumerate}",vector spaces
the $3 \times 4$ matrix of all 1s.,vector spaces
the $4 \times 4$ matrix with $a_{ij} = (-1)^{ij}$.,vector spaces
the $3 \times 4$ matrix with $a_{ij} = (-1)^j$.,vector spaces
"find $r$ for each of these (block) matrices, and the special solutions:
    \begin{equation*}
        a = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 3 \\ 2 & 4 & 6 \end{bmatrix}, \quad
        b = \begin{bmatrix} a & a \end{bmatrix}, \quad
        c = \begin{bmatrix} a & a \\ a & 0 \end{bmatrix}.
    \end{equation*}",vector spaces
"if the $r$ pivot variables come first, the reduced $r$ must look like
    \begin{equation*}
        r = \begin{bmatrix} i & f \\ 0 & 0 \end{bmatrix},
    \end{equation*}
    where $i$ is $r \times r$ and $f$ is $r \times (n-r)$. what is the nullspace matrix $n$ containing the special solutions?",vector spaces
"suppose all $r$ pivot variables come last. describe the four blocks in the $m \times n$ reduced echelon form (the block $b$ should be $r \times r$):
    \begin{equation*}
        r = \begin{bmatrix} a & b \\ c & d \end{bmatrix}.
    \end{equation*}
    what is the nullspace matrix $n$ of special solutions? what is its shape?",vector spaces
"(silly problem) describe all $2 \times 3$ matrices $a_1$ and $a_2$ with row echelon forms $r_1$ and $r_2$, such that $r_1 + r_2$ is the row echelon form of $a_1 + a_2$. is it true that $r_1 = a_1$ and $r_2 = a_2$ in this case?",vector spaces
"if $a$ has $r$ pivot columns, then $a^t$ has $r$ pivot columns. give a $3 \times 3$ example for which the column numbers are different for $a$ and $a^t$.",vector spaces
"reduce $a$ and $b$ to echelon form, to find their ranks. which variables are free?
    \[
    a = \begin{bmatrix} 1 & 2 & 0 & 1 \\ 0 & 1 & 1 & 0 \\ 1 & 2 & 0 & 1 \end{bmatrix}, \quad
    b = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}.
    \]
    find the special solutions to $ax = 0$ and $bx = 0$. find all solutions.",vector spaces
"find the echelon form $u$, the free variables, and the special solutions:
    \[
    a = \begin{bmatrix} 0 & 1 & 0 & 3 \\ 0 & 2 & 0 & 6 \end{bmatrix}, \quad b = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix}.
    \]",vector spaces
$ax = b$ is consistent (has a solution) when $b$ satisfies $b_2 = $. find the complete solution in the same form as equation (4).,vector spaces
"carry out the same steps as in the previous problem to find the complete solution of $mx = b$:
    \[
    m = \begin{bmatrix} 0 & 0 \\ 1 & 2 \\ 0 & 0 \\ 3 & 6 \end{bmatrix}, \quad b = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ b_4 \end{bmatrix}.
    \]",vector spaces
"find a $2 \times 3$ system $ax = b$ whose complete solution is:
    \[
    x = \begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix} + w \begin{bmatrix} 1 \\ 3 \\ 1 \end{bmatrix}.
    \]",vector spaces
find a $3 \times 3$ system with these solutions exactly when $b_1 + b_2 = b_3$.,vector spaces
write a $2 \times 2$ system $ax = b$ with many solutions $x_n$ but no solution $x_p$. which $b$’s allow an $x_p$?,vector spaces
"find the ranks of $ab$ and $am$ (rank 1 matrix times rank 1 matrix):
    \[
    a = \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}, \quad
    b = \begin{bmatrix} 2 & 1 & 4 \\ 3 & 1.5 & 6 \end{bmatrix}, \quad
    m = \begin{bmatrix} 1 & b \\ c & bc \end{bmatrix}.
    \]",vector spaces
suppose $a$ and $b$ have the same reduced-row echelon form $r$. explain how to change $a$ to $b$ by elementary row operations.,vector spaces
"every $m \times n$ matrix of rank $r$ reduces to $(m \times r)$ times $(r \times n)$:
    \[
    a = (\text{pivot columns of } a)(\text{first } r \text{ rows of } r) = (\text{col})(\text{row}).
    \]",vector spaces
"write the $3 \times 4$ matrix $a$ as the product of a $3 \times 2$ matrix from the pivot columns and a $2 \times 4$ matrix from $r$:
    \[
    a = \begin{bmatrix} 1 & 3 & 3 & 2 \\ 2 & 6 & 9 & 7 \\ -1 & -3 & 3 & 4 \end{bmatrix}.
    \]",vector spaces
suppose $a$ is an $m \times n$ matrix of rank $r$. its reduced echelon form is $r$. describe exactly the reduced row echelon form of $r^t$ (not $a^t$).,vector spaces
"execute the six steps following equation (6) to find the column space and null space of $a$ and the solution to $ax = b$:
    \[
    a = \begin{bmatrix} 2 & 4 & 6 & 4 \\ 2 & 5 & 7 & 6 \\ 2 & 3 & 5 & 2 \end{bmatrix}, \quad b = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix} = \begin{bmatrix} 4 \\ 3 \\ 5 \end{bmatrix}.
    \]",vector spaces
"for every $c$, find $r$ and the special solutions to $ax = 0$:
    \[
    a = \begin{bmatrix} 1 & 1 & 2 & 2 \\ 2 & 2 & 4 & 4 \\ 1 & c & 2 & 2 \end{bmatrix}, \quad a = \begin{bmatrix} 1-c & 2 \\ 0 & 2-c \end{bmatrix}.
    \]",vector spaces
"what is the null space matrix $n$ (of special solutions) for $a$, $b$, $c$?
    \[
    a = \begin{bmatrix} i & i \end{bmatrix}, \quad b = \begin{bmatrix} i & i \\ 0 & 0 \end{bmatrix}, \quad c = \begin{bmatrix} i & i & i \end{bmatrix}.
    \]",vector spaces
"find the complete solutions of:
    \[
    x + 3y + 3z = 1, \quad 2x + 6y + 9z = 5, \quad -x - 3y + 3z = 5.
    \]
    \[
    \begin{bmatrix} 1 & 3 & 1 & 2 \\ 2 & 6 & 4 & 8 \\ 0 & 0 & 2 & 4 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \\ t \end{bmatrix} = \begin{bmatrix} 1 \\ 3 \\ 1 \end{bmatrix}.
    \]",vector spaces
"under what condition on $b_1, b_2, b_3$ is the following system solvable? include $b$ as a fourth column in $[a | b]$. find all solutions when that condition holds:
    \[
    \begin{aligned}
    x + 2y - 2z &= b_1 \\
    2x + 5y - 4z &= b_2 \\
    4x + 9y - 8z &= b_3.
    \end{aligned}
    \]",vector spaces
"what conditions on $b_1, b_2, b_3, b_4$ make each system solvable? solve for $x$:
    \[
    \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 2 & 5 \\ 3 & 9 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ b_4 \end{bmatrix}, \quad \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \\ 2 & 5 & 7 \\ 3 & 9 & 12 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ b_4 \end{bmatrix}.
    \]",vector spaces
"find the special solutions to $rx = 0$ and $r^t y = 0$ for:
    \begin{align*}
        r &= \begin{bmatrix} 1 & 0 & 2 & 3 \\ 0 & 1 & 4 & 5 \\ 0 & 0 & 0 & 0 \end{bmatrix},
        &r &= \begin{bmatrix} 0 & 1 & 2 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}.
    \end{align*}",vector spaces
"find an $r \times r$ invertible submatrix $s$ from the pivot rows and columns of each $a$:
    \begin{align*}
        a &= \begin{bmatrix} 1 & 2 & 3 \\ 1 & 2 & 4 \end{bmatrix},
        &a &= \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \end{bmatrix},
        &a &= \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}.
    \end{align*}",vector spaces
explain why the pivot rows and columns always give an invertible submatrix.,vector spaces
"find the ranks of $ab$ and $am$ for:
    \begin{align*}
        a &= \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix},
        &b &= \begin{bmatrix} 2 & 1 & 4 \\ 3 & 1.5 & 6 \end{bmatrix},
        &m &= \begin{bmatrix} 1 & b \\ c & bc \end{bmatrix}.
    \end{align*}",vector spaces
prove that $\text{rank}(ab) \leq \text{rank}(b)$.,vector spaces
suppose $a$ and $b$ are $n \times n$ matrices and $ab = i$. prove that $a$ is invertible.,vector spaces
"if $a$ is $2 \times 3$ and $c$ is $3 \times 2$, show that $ca \neq i$. give an example where $ac = i$.",vector spaces
suppose $a$ and $b$ have the same reduced row echelon form $r$. explain how $a$ can be transformed into $b$.,vector spaces
"find the complete solution to:
    \begin{align*}
        x + 3y + 3z &= 1 \\
        2x + 6y + 9z &= 5 \\
        -x - 3y + 3z &= 5.
    \end{align*}",vector spaces
"for what conditions on $b_1, b_2, b_3$ is the following system solvable?
    \begin{align*}
        x + 2y - 2z &= b_1 \\
        2x + 5y - 4z &= b_2 \\
        4x + 9y - 8z &= b_3.
    \end{align*}",vector spaces
"which vectors $(b_1, b_2, b_3)$ are in the column space of $a$?
    \begin{align*}
        a &= \begin{bmatrix} 1 & 2 & 1 \\ 2 & 6 & 3 \\ 0 & 2 & 5 \end{bmatrix},
        &a &= \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 4 \\ 2 & 4 & 8 \end{bmatrix}.
    \end{align*}",vector spaces
"explain why the following statements are false:
    \begin{enumerate}",vector spaces
the complete solution is any linear combination of $x_p$ and $x_n$.,vector spaces
a system $ax = b$ has at most one particular solution.,vector spaces
the solution $x_p$ with all free variables zero is the shortest solution.,vector spaces
"if $a$ is invertible, there is no solution $x_n$ in the null space.",vector spaces
suppose column 5 of $u$ has no pivot. determine if the zero vector is the only solution to $ax = 0$.,vector spaces
"find $x_p$ and all special solutions for:
    \begin{align*}
        ax &= 2b, & \begin{bmatrix} a & a \end{bmatrix} \begin{bmatrix} x \\ x \end{bmatrix} &= b.
    \end{align*}",vector spaces
"find the values of $q$ for which the rank conditions hold:
    \begin{align*}
        a &= \begin{bmatrix} 6 & 4 & 2 \\ -3 & -2 & -1 \\ 9 & 6 & q \end{bmatrix},
        &b &= \begin{bmatrix} 3 & 1 & 3 \\ q & 2 & q \end{bmatrix}.
    \end{align*}",vector spaces
"give examples of matrices $a$ for which the number of solutions to $ax = b$ is:
    \begin{enumerate}",vector spaces
"0 or 1, depending on $b$.",vector spaces
"infinite, regardless of $b$.",vector spaces
"0 or infinite, depending on $b$.",vector spaces
"1, regardless of $b$.",vector spaces
"write all known relations between $r$, $m$, and $n$ if $ax = b$ has
    \begin{enumerate}",vector spaces
no solution for some $b$.,vector spaces
infinitely many solutions for every $b$.,vector spaces
"exactly one solution for some $b$, no solution for other $b$.",vector spaces
exactly one solution for every $b$.,vector spaces
"apply gauss-jordan elimination (right-hand side becomes extra column) to $ux = 0$ and $ux = c$. reach $rx = 0$ and $rx = d$:
    \[
    \begin{bmatrix} 1 & 2 & 3 & 0 \\ 0 & 0 & 4 & 0 \end{bmatrix},
    \quad \begin{bmatrix} 1 & 2 & 3 & 5 \\ 0 & 0 & 4 & 8 \end{bmatrix}
    \]
    solve $rx = 0$ to find $x_n$ (free variable $x_2 = 1$). solve $rx = d$ to find $x_p$ (free variable $x_2 = 0$).",vector spaces
"apply elimination with the extra column to reach $rx = 0$ and $rx = d$:
    \[
    \begin{bmatrix} 3 & 0 & 6 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix},
    \quad \begin{bmatrix} 3 & 0 & 6 & 9 \\ 0 & 0 & 2 & 4 \\ 0 & 0 & 0 & 5 \end{bmatrix}
    \]
    solve $rx = 0$ (free variable = 1). what are the solutions to $rx = d$?",vector spaces
"reduce to $ux = c$ (gaussian elimination) and then $rx = d$:
    \[
    \begin{bmatrix} 1 & 0 & 2 & 3 \\ 1 & 3 & 2 & 0 \\ 2 & 0 & 4 & 9 \end{bmatrix}
    \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix} =
    \begin{bmatrix} 2 \\ 5 \\ 10 \end{bmatrix} = b
    \]
    find a particular solution $x_p$ and all nullspace solutions $x_n$.",vector spaces
"find $a$ and $b$ with the given property or explain why you can’t.
    \begin{enumerate}",vector spaces
the only solution to $ax = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ is $x = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$.,vector spaces
the only solution to $bx = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ is $x = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$.,vector spaces
the complete solution to $ax = \begin{bmatrix} 1 \\ 3 \end{bmatrix}$ is $x = \begin{bmatrix} 1 \\ 0 \end{bmatrix} + c \begin{bmatrix} 0 \\ 1 \end{bmatrix}$. find $a$.,vector spaces
"the nullspace of a $3 \times 4$ matrix $a$ is the line through $(2,3,1,0)$.
    \begin{enumerate}",vector spaces
what is the rank of $a$ and the complete solution to $ax = 0$?,vector spaces
what is the exact row reduced echelon form $r$ of $a$?,vector spaces
"reduce these matrices $a$ and $b$ to their ordinary echelon forms $u$:
\begin{enumerate}",vector spaces
$a = \begin{bmatrix} 1 & 2 & 2 & 4 & 6 \\ 1 & 2 & 3 & 6 & 9 \\ 0 & 0 & 1 & 2 & 3 \end{bmatrix}$,vector spaces
$b = \begin{bmatrix} 2 & 4 & 2 \\ 0 & 4 & 4 \\ 0 & 8 & 8 \end{bmatrix}$,vector spaces
"true or false? (give reason if true, or counterexample to show it is false.)
\begin{enumerate}",vector spaces
a square matrix has no free variables.,vector spaces
an invertible matrix has no free variables.,vector spaces
an $m \times n$ matrix has no more than $n$ pivot variables.,vector spaces
an $m \times n$ matrix has no more than $m$ pivot variables.,vector spaces
is there a $3 \times 3$ matrix with no zero entries for which $u = r = i$?,vector spaces
"put as many $1$s as possible in a $4 \times 7$ echelon matrix $u$ and in a reduced form $r$ whose pivot columns are $2, 4, 5$.",vector spaces
suppose column 4 of a $3 \times 5$ matrix is all 0s. then $x_4$ is certainly a variable. the special solution for this variable is the vector $x = \begin{bmatrix} ? \\ ? \\ ? \\ 1 \\ 0 \end{bmatrix}$.,vector spaces
suppose the first and last columns of a $3 \times 5$ matrix are the same (nonzero). then $x_5$ is a free variable. find the special solution for this variable.,vector spaces
"the equation $x -3y - z = 0$ determines a plane in $\mathbb{r}^3$. what is the matrix $a$ in this equation? which are the free variables? the special solutions are $(3,1,0)$ and $(-1,0,1)$. the parallel plane $x-3y-z = 12$ contains the particular point $(12,0,0)$. all points on this plane have the following form:
\[
\begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} 12 \\ 0 \\ 0 \end{bmatrix} + y \begin{bmatrix} 3 \\ 1 \\ 0 \end{bmatrix} + z \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}.
\]
  % question 59",vector spaces
"suppose column 1 + column 3 + column 5 = 0 in a $4 \times 5$ matrix with four pivots. 
which column is sure to have no pivot (and which variable is free)? what is the special solution? what is the nullspace?

% question 60 - 66",vector spaces
construct a matrix whose nullspace consists of all combinations of $\begin{bmatrix}2 \\ 2 \\ 1 \\ 0 \end{bmatrix}$ and $\begin{bmatrix}3 \\ 1 \\ 0 \\ 1 \end{bmatrix}$.,vector spaces
construct a matrix whose nullspace consists of all multiples of $\begin{bmatrix}4 \\ 3 \\ 2 \\ 1 \end{bmatrix}$.,vector spaces
construct a matrix whose column space contains $\begin{bmatrix}1 \\ 1 \\ 5 \end{bmatrix}$ and $\begin{bmatrix}0 \\ 3 \\ 1 \end{bmatrix}$ and whose nullspace contains $\begin{bmatrix}1 \\ 1 \\ 2 \end{bmatrix}$.,vector spaces
construct a matrix whose column space contains $\begin{bmatrix}1 \\ 1 \\ 0 \end{bmatrix}$ and $\begin{bmatrix}0 \\ 1 \\ 1 \end{bmatrix}$ and whose nullspace contains $\begin{bmatrix}1 \\ 0 \\ 1 \end{bmatrix}$ and $\begin{bmatrix}0 \\ 0 \\ 1 \end{bmatrix}$.,vector spaces
construct a matrix whose column space contains $\begin{bmatrix}1 \\ 1 \\ 1 \end{bmatrix}$ and whose nullspace is the line of multiples of $\begin{bmatrix}1 \\ 1 \\ 1 \\ 1 \end{bmatrix}$.,vector spaces
construct a $2 \times 2$ matrix whose nullspace equals its column space.,vector spaces
"why does no $3 \times 3$ matrix have a nullspace that equals its column space?

% question 67",vector spaces
"the reduced form $r$ of a $3 \times 3$ matrix with randomly chosen entries is almost sure to be\dots what $r$ is virtually certain if the random $a$ is $4 \times 3$?

% question 68",vector spaces
"show by example that these three statements are generally false:
    \begin{enumerate}",vector spaces
$a$ and $a^t$ have the same nullspace.,vector spaces
$a$ and $a^t$ have the same free variables.,vector spaces
if $r$ is the reduced form $\text{rref}(a)$ then $r^t$ is $\text{rref}(a^t)$.,vector spaces
"if the special solutions to $rx = 0$ are in the columns of these $n$, go backward to find the nonzero rows of the reduced matrices $r$:
    \[ n = \begin{bmatrix} 2 & 3 \\ 1 & 0 \\ 0 & 1 \end{bmatrix}, \quad n = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, \quad n = \begin{bmatrix} \end{bmatrix} \text{ (empty } 3 \times 1 \text{)}. \]

% question 70",vector spaces
"explain why $a$ and $-a$ always have the same reduced echelon form $r$.


   question 1",vector spaces
"show that the vectors $v_1$, $v_2$, and $v_3$ are independent, but the vectors $v_1$, $v_2$, $v_3$, and $v_4$ are dependent. solve the equation $c_1 v_1 + c_2 v_2 + c_3 v_3 + c_4 v_4 = 0$ (or equivalently, $ac = 0$), where the vectors are the columns of the matrix $a$.

\[
v_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad 
v_2 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}, \quad
v_3 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, \quad
v_4 = \begin{bmatrix} 2 \\ 3 \\ 4 \end{bmatrix}
\]

% question 2",vector spaces
"find the largest possible number of independent vectors among the following:

\[
v_1 = \begin{bmatrix} 1 \\ -1 \\ 0 \\ 0 \end{bmatrix}, \quad
v_2 = \begin{bmatrix} 1 \\ 0 \\ -1 \\ 0 \end{bmatrix}, \quad
v_3 = \begin{bmatrix} 1 \\ 0 \\ 0 \\ -1 \end{bmatrix}, \quad
v_4 = \begin{bmatrix} 0 \\ 1 \\ -1 \\ 0 \end{bmatrix}, \quad
v_5 = \begin{bmatrix} 0 \\ 1 \\ 0 \\ -1 \end{bmatrix}, \quad
v_6 = \begin{bmatrix} 0 \\ 0 \\ 1 \\ -1 \end{bmatrix}
\]

this number is the dimension of the space spanned by these vectors.

% question 3",vector spaces
"prove that if $a = 0$, $d = 0$, or $f = 0$ (in the three cases), the columns of the matrix $u$ are dependent. given:

\[
u = \begin{bmatrix} 
a & b & c \\
0 & d & e \\
0 & 0 & f 
\end{bmatrix}
\]
    % question 4",vector spaces
"if $a$, $d$, and $f$ in problem 3 are all nonzero, show that the only solution to $u x = 0$ is $x = 0$. then $u$ has independent columns.",vector spaces
"decide the dependence or independence of the following sets of vectors:
    \begin{enumerate}",vector spaces
"the vectors $(1, 3, 2)$, $(2, 1, 3)$, and $(3, 2, 1)$.",vector spaces
"the vectors $(1, -3, 2)$, $(2, 1, -3)$, and $(-3, 2, 1)$.",vector spaces
"choose three independent columns of $u$. then make two other choices. do the same for $a$. you have found bases for which spaces?
\[
u = \begin{bmatrix}
2 & 3 & 4 & 1 \\
0 & 6 & 7 & 0 \\
0 & 0 & 0 & 9 \\
0 & 0 & 0 & 0
\end{bmatrix}, \quad
a = \begin{bmatrix}
2 & 3 & 4 & 1 \\
0 & 6 & 7 & 0 \\
0 & 0 & 0 & 9 \\
4 & 6 & 8 & 2
\end{bmatrix}
\]

% question 7",vector spaces
"if $w_1$, $w_2$, $w_3$ are independent vectors, show that the differences $v_1 = w_2 - w_3$, $v_2 = w_1 - w_3$, and $v_3 = w_1 - w_2$ are dependent. find a combination of the $v$'s that gives zero.

% question 8",vector spaces
"if $w_1$, $w_2$, $w_3$ are independent vectors, show that the sums $v_1 = w_2 + w_3$, $v_2 = w_1 + w_3$, and $v_3 = w_1 + w_2$ are independent. (write $c_1 v_1 + c_2 v_2 + c_3 v_3 = 0$ in terms of the $w$'s. find and solve equations for the $c$'s.)

% question 9",vector spaces
"suppose $v_1$, $v_2$, $v_3$, and $v_4$ are vectors in $\mathbb{r}^3$:
    \begin{enumerate}",vector spaces
these four vectors are dependent because \underline{\hspace{5cm}}.,vector spaces
the two vectors $v_1$ and $v_2$ will be dependent if \underline{\hspace{5cm}}.,vector spaces
"the vectors $v_1$ and $(0, 0, 0)$ are dependent because \underline{\hspace{5cm}}.",vector spaces
"find two independent vectors on the plane $x + 2y - 3z - t = 0$ in $\mathbb{r}^4$. then find three independent vectors. why not four? this plane is the nullspace of what matrix?

% problems 11–18 are about the space spanned by a set of vectors.

% question 11",vector spaces
"describe the subspace of $\mathbb{r}^3$ (is it a line, a plane, or $\mathbb{r}^3$?) spanned by the following vectors:
    \begin{enumerate}",vector spaces
"the two vectors $(1, 1, -1)$ and $(-1, -1, 1)$.",vector spaces
"the three vectors $(0, 1, 1)$, $(1, 1, 0)$, and $(0, 0, 0)$.",vector spaces
the columns of a $3 \times 5$ echelon matrix with 2 pivots.,vector spaces
all vectors with positive components.,vector spaces
"the vector $b$ is in the subspace spanned by the columns of $a$ when there is a solution to \underline{\hspace{5cm}}. the vector $c$ is in the row space of $a$ when there is a solution to \underline{\hspace{5cm}}. true or false: if the zero vector is in the row space, the rows are dependent. \underline{\hspace{5cm}}

% question 13",vector spaces
"find the dimensions of the following spaces:
    \begin{enumerate}",vector spaces
the column space of $u$.,vector spaces
the row space of $a$.,vector spaces
the row space of $u$.,vector spaces
"choose \( x = (x_1, x_2, x_3, x_4) \) in \( \mathbb{r}^4 \). it has 24 rearrangements like \( (x_2, x_1, x_3, x_4) \) and \( (x_4, x_3, x_1, x_2) \). those 24 vectors, including \( x \) itself, span a subspace \( s \). find specific vectors \( x \) so that the dimension of \( s \) is:
    \begin{enumerate}",vector spaces
0,vector spaces
1,vector spaces
3,vector spaces
4,vector spaces
"\( v + w \) and \( v - w \) are combinations of \( v \) and \( w \). write \( v \) and \( w \) as combinations of \( v + w \) and \( v - w \). the two pairs of vectors span the same space. when are they a basis for the same space?

% question 16",vector spaces
"decide whether or not the following vectors are linearly independent, by solving \( c_1 v_1 + c_2 v_2 + c_3 v_3 + c_4 v_4 = 0 \):
    \[
    v_1 = \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}, 
    v_2 = \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, 
    v_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \end{bmatrix}, 
    v_4 = \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \end{bmatrix}.
    \]
    decide also if they span \( \mathbb{r}^4 \), by trying to solve \( c_1 v_1 + \dots + c_4 v_4 = (0, 0, 0, 1) \).

% question 17",vector spaces
"suppose the vectors to be tested for independence are placed into the rows instead of the columns of \( a \). how does the elimination process from \( a \) to \( u \) decide for or against independence?

% question 18",vector spaces
"to decide whether \( b \) is in the subspace spanned by \( w_1, \dots, w_n \), let the vectors \( w \) be the columns of \( a \) and try to solve \( a x = b \). what is the result for:
    \begin{enumerate}",vector spaces
"\( w_1 = (1, 1, 0), w_2 = (2, 2, 1), w_3 = (0, 0, 2), b = (3, 4, 5) \)?",vector spaces
"\( w_1 = (1, 2, 0), w_2 = (2, 5, 0), w_3 = (0, 0, 2), w_4 = (0, 0, 0), \) and any \( b \)?",vector spaces
"if \( v_1, \dots, v_n \) are linearly independent, the space they span has dimension \_\_\_. these vectors are a \_\_\_ for that space. if the vectors are the columns of an \( m \times n \) matrix, then \( m \) is \_\_\_ than \( n \).

% question 20",vector spaces
"find a basis for each of these subspaces of \( \mathbb{r}^4 \):
    \begin{enumerate}",vector spaces
all vectors whose components are equal.,vector spaces
all vectors whose components add to zero.,vector spaces
"all vectors that are perpendicular to \( (1,1,0,0) \) and \( (1,0,1,1) \).",vector spaces
"the column space (in \( \mathbb{r}^2 \)) and null space (in \( \mathbb{r}^5 \)) of  
        \[
        u =
        \begin{bmatrix}
        1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 0 & 1 & 0
        \end{bmatrix}.
        \]",vector spaces
"find three different bases for the column space of \( u \) above. then find two different bases for the row space of \( u \).

% question 22",vector spaces
"suppose \( v_1, v_2, \dots, v_6 \) are six vectors in \( \mathbb{r}^4 \).
    \begin{enumerate}",vector spaces
those vectors (do)(do not)(might not) span \( \mathbb{r}^4 \).,vector spaces
those vectors (are)(are not)(might be) linearly independent.,vector spaces
any four of those vectors (are)(are not)(might be) a basis for \( \mathbb{r}^4 \).,vector spaces
"if those vectors are the columns of \( a \), then \( ax = b \) (has) (does not have) (might not have) a solution.",vector spaces
"the columns of \( a \) are \( n \) vectors from \( \mathbb{r}^m \). if they are linearly independent, what is the rank of \( a \)? if they span \( \mathbb{r}^m \), what is the rank? if they are a basis for \( \mathbb{r}^m \), what then?

% question 24",vector spaces
"find a basis for the plane \( x - 2y + 3z = 0 \) in \( \mathbb{r}^3 \). then find a basis for the intersection of that plane with the \( xy \)-plane. then find a basis for all vectors perpendicular to the plane.

% question 25",vector spaces
"suppose the columns of a \( 5 \times 5 \) matrix \( a \) are a basis for \( \mathbb{r}^5 \).
    \begin{enumerate}",vector spaces
the equation \( ax = 0 \) has only the solution \( x = 0 \) because \_\_\_.,vector spaces
"if \( b \) is in \( \mathbb{r}^5 \), then \( ax = b \) is solvable because \_\_\_.",vector spaces
"suppose \( s \) is a five-dimensional subspace of \( \mathbb{r}^6 \). true or false?
    \begin{enumerate}",vector spaces
every basis for \( s \) can be extended to a basis for \( \mathbb{r}^6 \) by adding one more vector.,vector spaces
every basis for \( \mathbb{r}^6 \) can be reduced to a basis for \( s \) by removing one vector.,vector spaces
"\( u \) comes from \( a \) by subtracting row 1 from row 3:
    \[
    a =
    \begin{bmatrix}
    1 & 3 & 2 \\
    0 & 1 & 1 \\
    1 & 3 & 2
    \end{bmatrix},
    \quad
    u =
    \begin{bmatrix}
    1 & 3 & 2 \\
    0 & 1 & 1 \\
    0 & 0 & 0
    \end{bmatrix}.
    \]
    find bases for:
    \begin{itemize}",vector spaces
the two column spaces.,vector spaces
the two row spaces.,vector spaces
"the two null spaces.
    \end{itemize}

% question 28",vector spaces
"true or false (give a good reason)?
    \begin{enumerate}",vector spaces
"if the columns of a matrix are dependent, so are the rows.",vector spaces
the column space of a \( 2 \times 2 \) matrix is the same as its row space.,vector spaces
the column space of a \( 2 \times 2 \) matrix has the same dimension as its row space.,vector spaces
the columns of a matrix are a basis for the column space.,vector spaces
"for which numbers \( c \) and \( d \) do these matrices have rank 2?
    \[
    a =
    \begin{bmatrix}
    1 & 2 & 5 & 0 & 5 \\
    0 & 0 & c & 2 & 2 \\
    0 & 0 & 0 & d & 2
    \end{bmatrix}
    \]
    and
    \[
    b =
    \begin{bmatrix}
    c & d \\
    d & c
    \end{bmatrix}.
    \]
  % question 30",vector spaces
"by locating the pivots, find a basis for the column space of
    \[
    u =
    \begin{bmatrix}
    0 & 5 & 4 & 3 \\
    0 & 0 & 2 & 1 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0
    \end{bmatrix}.
    \]
    express each column that is not in the basis as a combination of the basic columns. find also a matrix \( a \) with this echelon form \( u \), but a different column space.

% question 31",vector spaces
"find a counterexample to the following statement: if \( v_1, v_2, v_3, v_4 \) is a basis for the vector space \( \mathbb{r}^4 \), and if \( w \) is a subspace, then some subset of the \( v \)'s is a basis for \( w \).

% question 32",vector spaces
"find the dimensions of these vector spaces:
    \begin{enumerate}",vector spaces
the space of all vectors in \( \mathbb{r}^4 \) whose components add to zero.,vector spaces
the nullspace of the \( 4 \times 4 \) identity matrix.,vector spaces
the space of all \( 4 \times 4 \) matrices.,vector spaces
"suppose \( v \) is known to have dimension \( k \). prove that:
    \begin{enumerate}",vector spaces
any \( k \) independent vectors in \( v \) form a basis.,vector spaces
any \( k \) vectors that span \( v \) form a basis.,vector spaces
"prove that if \( v \) and \( w \) are three-dimensional subspaces of \( \mathbb{r}^5 \), then \( v \) and \( w \) must have a nonzero vector in common. 

\textbf{hint}: start with bases for the two subspaces, making six vectors in all.  
    % question 35",vector spaces
"\textbf{true or false?}
    \begin{enumerate}",vector spaces
"if the columns of \( a \) are linearly independent, then \( ax = b \) has exactly one solution for every \( b \).",vector spaces
a \( 5 \times 7 \) matrix never has linearly independent columns.,vector spaces
"if \( a \) is a \( 64 \times 17 \) matrix of rank 11, how many independent vectors satisfy \( ax = 0 \)? how many independent vectors satisfy \( a^t y = 0 \)?

% question 37",vector spaces
"find a basis for each of these subspaces of \( 3 \times 3 \) matrices:
    \begin{enumerate}",vector spaces
all diagonal matrices.,vector spaces
all symmetric matrices (\( a^t = a \)).,vector spaces
all skew-symmetric matrices (\( a^t = -a \)).,vector spaces
"\textbf{problems about function spaces:}
    \begin{enumerate}",vector spaces
find all functions that satisfy \( \frac{dy}{dx} = 0 \).,vector spaces
choose a particular function that satisfies \( \frac{dy}{dx} = 3 \).,vector spaces
find all functions that satisfy \( \frac{dy}{dx} = 3 \).,vector spaces
"the cosine space \( f_3 \) contains all combinations \( y(x) = a\cos x + b\cos 2x + c\cos 3x \). find a basis for the subspace that has \( y(0) = 0 \).

% question 40",vector spaces
"find a basis for the space of functions that satisfy:
    \begin{enumerate}",vector spaces
\( \frac{dy}{dx} - 2y = 0 \).,vector spaces
\( \frac{dy}{dx} - \frac{y}{x} = 0 \).,vector spaces
"suppose \( y_1(x), y_2(x), y_3(x) \) are three different functions of \( x \). the vector space they span could have dimension 1, 2, or 3. give an example of \( y_1, y_2, y_3 \) to show each possibility.

% question 42",vector spaces
"find a basis for the space of polynomials \( p(x) \) of degree \( \leq 3 \). find a basis for the subspace with \( p(1) = 0 \).

% question 43",vector spaces
"write the \( 3 \times 3 \) identity matrix as a combination of the other five permutation matrices. then show that those five matrices are linearly independent. (assume a combination gives zero, and check entries to prove each term is zero.) the five permutations are a basis for the subspace of \( 3 \times 3 \) matrices with row and column sums all equal.
    % question 44",vector spaces
"\textbf{review: which of the following are bases for \( \mathbb{r}^3 \)?}
    \begin{enumerate}",vector spaces
"\( (1,2,0) \) and \( (0,1,-1) \).",vector spaces
"\( (1,1,-1) \), \( (2,3,4) \), \( (4,1,-1) \), \( (0,1,-1) \).",vector spaces
"\( (1,2,2) \), \( (-1,2,1) \), \( (0,8,0) \).",vector spaces
"\( (1,2,2) \), \( (-1,2,1) \), \( (0,8,6) \).",vector spaces
"\textbf{review: suppose \( a \) is \( 5 \times 4 \) with rank 4.}  
show that \( ax = b \) has no solution when the \( 5 \times 5 \) matrix \( [a \; b] \) is invertible.  
show that \( ax = b \) is solvable when \( [a \; b] \) is singular.

    % question 1",vector spaces
"\textbf{true or false:}  
if \( m = n \), then the row space of \( a \) equals the column space.  
if \( m < n \), then the null space has a larger dimension than \(\dots\).

% question 2",vector spaces
"\textbf{find the dimension and construct a basis for the four subspaces associated with the matrices:}
\[
a =
\begin{bmatrix}
0 & 1 & 4 & 0 \\
0 & 2 & 8 & 0
\end{bmatrix}
\]
\[
u =
\begin{bmatrix}
0 & 1 & 4 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}
\]

% question 3",vector spaces
"\textbf{find the dimension and a basis for the four fundamental subspaces for:}
\[
a =
\begin{bmatrix}
1 & 2 & 0 & 1 \\
0 & 1 & 1 & 0 \\
1 & 2 & 0 & 1
\end{bmatrix}
\]
\[
u =
\begin{bmatrix}
1 & 2 & 0 & 1 \\
0 & 1 & 1 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}
\]

% question 4",vector spaces
"\textbf{describe the four subspaces in three-dimensional space associated with:}
\[
a =
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{bmatrix}
\]

% question 5",vector spaces
"\textbf{if the product \( ab \) is the zero matrix, \( ab = 0 \), show that the column space of \( b \) is contained in the nullspace of \( a \).}  
(also, the row space of \( a \) is in the left nullspace of \( b \), since each row of \( a \) multiplies \( b \) to give a zero row.)

% question 6",vector spaces
"\textbf{suppose \( a \) is an \( m \times n \) matrix of rank \( r \). under what conditions do these hold?}
    \begin{enumerate}",vector spaces
\( a \) has a two-sided inverse: \( aa^{-1} = a^{-1}a = i \).,vector spaces
\( ax = b \) has infinitely many solutions for every \( b \).,vector spaces
"\textbf{why is there no matrix whose row space and nullspace both contain \( (1,1,1) \)?}

% question 8",vector spaces
"\textbf{suppose the only solution to \( ax = 0 \) (with \( m \) equations in \( n \) unknowns) is \( x = 0 \). what is the rank and why?}  
the columns of \( a \) are linearly \(\dots\).

% question 9",vector spaces
"\textbf{find a \( 1 \times 3 \) matrix whose nullspace consists of all vectors in \( \mathbb{r}^3 \) such that \( x_1 + 2x_2 + 4x_3 = 0 \).}  
find a \( 3 \times 3 \) matrix with that same nullspace.

% question 10",vector spaces
"\textbf{if \( ax = b \) always has at least one solution, show that the only solution to \( a^t y = 0 \) is \( y = 0 \).}  
\textit{hint: what is the rank?}
    % question 11",vector spaces
"\textbf{if \( ax = 0 \) has a nonzero solution, show that \( a^t y = f \) fails to be solvable for some right-hand sides \( f \).}  
construct an example of \( a \) and \( f \).

% question 12",vector spaces
"\textbf{find the rank of \( a \) and write the matrix as \( a = uv^t \):}
\[
a =
\begin{bmatrix}
1 & 0 & 0 & 3 \\
0 & 0 & 0 & 0 \\
2 & 0 & 0 & 6
\end{bmatrix}
\]
\[
a =
\begin{bmatrix}
2 & -2 \\
6 & -6
\end{bmatrix}
\]

% question 13",vector spaces
"\textbf{if \( a, b, c \) are given with \( a \neq 0 \), choose \( d \) so that:}
\[
a =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
= uv^t
\]
has rank 1. what are the pivots?

% question 14",vector spaces
"\textbf{find a left-inverse and/or a right-inverse (when they exist) for:}
\[
a =
\begin{bmatrix}
1 & 1 & 0 \\
0 & 1 & 1
\end{bmatrix}
\]
\[
m =
\begin{bmatrix}
1 & 0 \\
1 & 1 \\
0 & 1
\end{bmatrix}
\]
\[
t =
\begin{bmatrix}
a & b \\
0 & a
\end{bmatrix}
\]

% question 15",vector spaces
"\textbf{if the columns of \( a \) are linearly independent (\( a \) is \( m \times n \)), then:}
\begin{itemize}",vector spaces
the rank is \(\dots\).,vector spaces
the nullspace is \(\dots\).,vector spaces
the row space is \(\dots\).,vector spaces
"there exists a \(\dots\)-inverse.
\end{itemize}

% question 16",vector spaces
"\textbf{(a paradox) suppose \( a \) has a right-inverse \( b \). then \( ab = i \) leads to \( a^t ab = a^t \) or}
\[
b (a^t a)^{-1} a^t
\]
but that satisfies \( ba = i \); it is a left-inverse. which step is not justified?

% question 17",vector spaces
"\textbf{find a matrix \( a \) that has \( v \) as its row space, and a matrix \( b \) that has \( v \) as its nullspace, if \( v \) is the subspace spanned by:}
\[
\begin{bmatrix}
1 \\ 1 \\ 0
\end{bmatrix},
\quad
\begin{bmatrix}
1 \\ 2 \\ 0
\end{bmatrix},
\quad
\begin{bmatrix}
1 \\ 5 \\ 0
\end{bmatrix}
\]
    % question 18",vector spaces
"\textbf{find a basis for each of the four subspaces of}
\[
a =
\begin{bmatrix}
0 & 1 & 2 & 3 & 4 \\
0 & 1 & 2 & 4 & 6 \\
0 & 0 & 0 & 1 & 2
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 \\
1 & 1 & 0 \\
0 & 1 & 1
\end{bmatrix}
\begin{bmatrix}
0 & 1 & 2 & 3 & 4 \\
0 & 0 & 0 & 1 & 2 \\
0 & 0 & 0 & 0 & 0
\end{bmatrix}.
\]

% question 19",vector spaces
"\textbf{if \( a \) has the same four fundamental subspaces as \( b \), does \( a = cb \)?}

% question 20",vector spaces
"\textbf{(a) if a \( 7 \times 9 \) matrix has rank 5, what are the dimensions of the four subspaces?}  
what is the sum of all four dimensions?  
\textbf{(b) if a \( 3 \times 4 \) matrix has rank 3, what are its column space and left nullspace?}

% question 21",vector spaces
"\textbf{construct a matrix with the required property, or explain why you can’t.}
\begin{enumerate}",vector spaces
"column space contains \( \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} \), \( \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \),
    row space contains \( \begin{bmatrix} 1 & 2 \end{bmatrix} \), \( \begin{bmatrix} 2 & 5 \end{bmatrix} \).",vector spaces
"column space has basis \( \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} \), nullspace has basis \( \begin{bmatrix} 3 \\ 2 \\ 1 \end{bmatrix} \).",vector spaces
dimension of nullspace = 1 + dimension of left nullspace.,vector spaces
"left nullspace contains \( \begin{bmatrix} 1 & 3 \end{bmatrix} \), row space contains \( \begin{bmatrix} 3 & 1 \end{bmatrix} \).",vector spaces
"row space = column space, nullspace \( \neq \) left nullspace.",vector spaces
"\textbf{without elimination, find dimensions and bases for the four subspaces for}
\[
a =
\begin{bmatrix}
0 & 3 & 3 & 3 \\
0 & 0 & 0 & 0 \\
0 & 1 & 0 & 1
\end{bmatrix}
\quad \text{and} \quad
b =
\begin{bmatrix}
1 & 1 \\
4 & 4 \\
5 & 5
\end{bmatrix}.
\]

% question 23",vector spaces
"\textbf{suppose the \( 3 \times 3 \) matrix \( a \) is invertible. write bases for the four subspaces for \( a \), and also for the \( 3 \times 6 \) matrix \( b = [a \ a] \).}

% question 24",vector spaces
"\textbf{what are the dimensions of the four subspaces for \( a \), \( b \), and \( c \), if \( i \) is the \( 3 \times 3 \) identity matrix and 0 is the \( 3 \times 2 \) zero matrix?}
\[
a =
\begin{bmatrix}
i & 0
\end{bmatrix}, \quad
b =
\begin{bmatrix}
i & i \\
0^t & 0^t
\end{bmatrix}, \quad
c =
\begin{bmatrix}
0
\end{bmatrix}.
\]

    % question 25",vector spaces
"\textbf{which subspaces are the same for these matrices of different sizes?}
\begin{enumerate}",vector spaces
\( a \) and \( \begin{bmatrix} a \\ a \end{bmatrix} \).,vector spaces
\( \begin{bmatrix} a \\ a \end{bmatrix} \) and \( \begin{bmatrix} a & a \\ a & a \end{bmatrix} \).,vector spaces
"\textbf{if the entries of a \( 3 \times 3 \) matrix are chosen randomly between 0 and 1, what are the most likely dimensions of the four subspaces?}  
what if the matrix is \( 3 \times 5 \)?

% question 27",vector spaces
"\textbf{(important) a is an \( m \times n \) matrix of rank \( r \). suppose there are right-hand sides \( b \) for which \( ax = b \) has no solution.}
\begin{enumerate}",vector spaces
"what inequalities (\(<\) or \(\leq\)) must be true between \( m, n, \) and \( r \)?",vector spaces
how do you know that \( a^t y = 0 \) has a nonzero solution?,vector spaces
"\textbf{construct a matrix with \( (1,0,1) \) and \( (1,2,0) \) as a basis for its row space and its column space. why can’t this be a basis for the row space and nullspace?}

% question 29",vector spaces
"\textbf{without computing \( a \), find bases for the four fundamental subspaces:}
\[
a =
\begin{bmatrix}
1 & 0 & 0 \\
6 & 1 & 0 \\
9 & 8 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 2 & 3 & 4 \\
0 & 1 & 2 & 3 \\
0 & 0 & 1 & 2
\end{bmatrix}.
\]

% question 30",vector spaces
"\textbf{if you exchange the first two rows of a matrix \( a \), which of the four subspaces stay the same?}  
if \( y = (1,2,3,4) \) is in the left nullspace of \( a \), write down a vector in the left nullspace of the new matrix.
    % question 31",vector spaces
"\textbf{explain why \( v = (1,0,-1) \) cannot be a row of \( a \) and also be in the nullspace.}

% question 32",vector spaces
"\textbf{describe the four subspaces of \( \mathbb{r}^3 \) associated with}
\[
a =
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{bmatrix}
\quad \text{and} \quad
i + a =
\begin{bmatrix}
1 & 1 & 0 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{bmatrix}.
\]

% question 33",vector spaces
"\textbf{(left nullspace) add the extra column \( b \) and reduce \( a \) to echelon form:}
\[
[a \, | \, b] =
\begin{bmatrix}
1 & 2 & 3 & b_1 \\
4 & 5 & 6 & b_2 \\
7 & 8 & 9 & b_3
\end{bmatrix}
\rightarrow
\begin{bmatrix}
1 & 2 & 3 & b_1 \\
0 & -3 & -6 & b_2 - 4b_1 \\
0 & 0 & 0 & b_3 - 2b_2 + b_1
\end{bmatrix}.
\]
a combination of the rows of \( a \) has produced the zero row. what combination is it?  
(look at \( b_3 - 2b_2 + b_1 \) on the right-hand side.)  
which vectors are in the nullspace of \( a^t \) and which are in the nullspace of \( a \)?

% question 34",vector spaces
"\textbf{following the method of problem 33, reduce \( a \) to echelon form and look at zero rows. the \( b \) column tells which combinations you have taken of the rows:}
\begin{enumerate}",vector spaces
"\(
    \begin{bmatrix}
    1 & 2 & b_1 \\
    3 & 4 & b_2 \\
    4 & 6 & b_3
    \end{bmatrix}
    \).",vector spaces
"\(
    \begin{bmatrix}
    1 & 2 & b_1 \\
    2 & 3 & b_2 \\
    2 & 4 & b_3 \\
    2 & 5 & b_4
    \end{bmatrix}
    \).",vector spaces
"\textbf{suppose \( a \) is the sum of two matrices of rank one: \( a = uv^t + wz^t \).}
\begin{enumerate}",vector spaces
which vectors span the column space of \( a \)?,vector spaces
which vectors span the row space of \( a \)?,vector spaces
the rank is less than 2 if or if \dots,vector spaces
"compute \( a \) and its rank if \( u = z = (1,0,0) \) and \( v = w = (0,0,1) \).",vector spaces
"\textbf{without multiplying matrices, find bases for the row and column spaces of \( a \):}
\[
a =
\begin{bmatrix}
1 & 2 \\
4 & 5 \\
2 & 7
\end{bmatrix}
\begin{bmatrix}
3 & 0 & 3 \\
1 & 1 & 2
\end{bmatrix}.
\]
how do you know from these shapes that \( a \) is not invertible?

% question 37",vector spaces
"\textbf{true or false (with a reason or a counterexample)?}
\begin{enumerate}",vector spaces
\( a \) and \( a^t \) have the same number of pivots.,vector spaces
\( a \) and \( a^t \) have the same left nullspace.,vector spaces
"if the row space equals the column space, then \( a^t = a \).",vector spaces
"if \( a^t = -a \), then the row space of \( a \) equals the column space.",vector spaces
"\textbf{if \( ab = 0 \), the columns of \( b \) are in the nullspace of \( a \). if those vectors are in \( \mathbb{r}^n \), prove that}
\[
\text{rank}(a) + \text{rank}(b) \leq n.
\]

% question 39",vector spaces
"\textbf{can tic-tac-toe be completed (5 ones and 4 zeros in \( a \)) so that \( \text{rank}(a) = 2 \) but neither side passed up a winning move?}

% question 40",vector spaces
"\textbf{construct any \( 2 \times 3 \) matrix of rank 1. copy figure 2.5 and put one vector in each subspace (two in the nullspace). which vectors are orthogonal?}

% question 41",vector spaces
"\textbf{redraw figure 2.5 for a \( 3 \times 2 \) matrix of rank \( r = 2 \). which subspace is \( z \) (zero vector only)? the nullspace part of any vector \( x \) in \( \mathbb{r}^2 \) is \( x_n = \) \dots}
    % question 1",vector spaces
"\textbf{for the 3-node triangular graph in the figure below, write the \(3 \times 3\) incidence matrix \( a \).}  
find a solution to \( ax = 0 \) and describe all other vectors in the nullspace of \( a \).  
find a solution to \( a^t y = 0 \) and describe all other vectors in the left nullspace of \( a \).

\begin{center}
\textbf{graph representation:}\\
\begin{tabular}{c}
\(\bullet\) \hspace{0.5cm} --- edge 1 --- \hspace{0.5cm} \(\bullet\) \\
| \\
\(\bullet\) --- edge 2 --- \(\bullet\) --- edge 3 --- \(\bullet\) \\
\end{tabular}
\end{center}

% question 2",vector spaces
"\textbf{for the same \(3 \times 3\) matrix, show directly from the columns that every vector \( b \) in the column space will satisfy}  
\[
b_1 + b_2 - b_3 = 0.
\]
derive the same thing from the three rows—the equations in the system \( ax = b \).  
what does that mean about potential differences around a loop?

% question 3",vector spaces
"\textbf{show directly from the rows that every vector \( f \) in the row space will satisfy}  
\[
f_1 + f_2 + f_3 = 0.
\]
derive the same thing from the three equations \( a^t y = f \).  
what does that mean when the \( f \)'s are currents into the nodes?

% question 4",vector spaces
"\textbf{compute the \( 3 \times 3 \) matrix \( a^t a \), and show that it is symmetric but singular.}  
what vectors are in its nullspace?  
removing the last column of \( a \) (and last row of \( a^t \)) leaves the \( 2 \times 2 \) matrix in the upper left corner; show that it is not singular.

% question 5",vector spaces
"\textbf{put the diagonal matrix \( c \) with entries \( c_1, c_2, c_3 \) in the middle and compute \( a^t c a \).}  
show again that the \( 2 \times 2 \) matrix in the upper left corner is invertible.

% question 6",vector spaces
"\textbf{write the \( 6 \times 4 \) incidence matrix \( a \) for the second graph in the figure.}  
the vector \( (1,1,1,1) \) is in the nullspace of \( a \), but now there will be \( m - n + 1 = 3 \) independent vectors that satisfy \( a^t y = 0 \).  
find three vectors \( y \) and connect them to the loops in the graph.
% question 7",vector spaces
"\textbf{if the second graph represents six games between four teams, and the score differences are \( b_1, \dots, b_6 \), when is it possible to assign potentials \( x_1, \dots, x_4 \) so that the potential differences agree with the \( b \)'s?}  
you are finding (from kirchhoff or from elimination) the conditions that make \( ax = b \) solvable.

% question 8",vector spaces
"\textbf{write down the dimensions of the four fundamental subspaces for this \( 6 \times 4 \) incidence matrix, and a basis for each subspace.}

% question 9",vector spaces
"\textbf{compute \( a^t a \) and \( a^t c a \), where the \( 6 \times 6 \) diagonal matrix \( c \) has entries \( c_1, \dots, c_6 \).}  
how can you tell from the graph where the \( c \)'s will appear on the main diagonal of \( a^t c a \)?

% question 10",vector spaces
"\textbf{draw a graph with numbered and directed edges (and numbered nodes) whose incidence matrix is:}
\[
a =
\begin{bmatrix}
-1 & 1 & 0 & 0 \\
-1 & 0 & 1 & 0 \\
0 & 1 & 0 & -1 \\
0 & 0 & -1 & 1
\end{bmatrix}.
\]
is this graph a tree? (are the rows of \( a \) independent?)  
show that removing the last edge produces a spanning tree. then the remaining rows are a basis for which subspace?

% question 11",vector spaces
"\textbf{with the last column removed from the preceding \( a \), and with the numbers \( 1, 2, 2, 1 \) on the diagonal of \( c \), write out the \( 7 \times 7 \) system:}
\[
c^{-1} y + ax = 0
\]
\[
a^t y = f.
\]
eliminating \( y_1, y_2, y_3, y_4 \) leaves three equations:
\[
a^t c a x = -f.
\]
solve the equations when \( f = (1,1,6) \).  
with those currents entering nodes \( 1,2,3 \) of the network, what are the potentials at the nodes and currents on the edges?
    % question 12",vector spaces
"\textbf{if \( a \) is a \( 12 \times 7 \) incidence matrix from a connected graph, what is its rank?}  
how many free variables are there in the solution to \( ax = b \)?  
how many free variables are there in the solution to \( a^t y = f \)?  
how many edges must be removed to leave a spanning tree?

% question 13",vector spaces
"\textbf{in the graph above with 4 nodes and 6 edges, find all 16 spanning trees.}

% question 14",vector spaces
"\textbf{if mit beats harvard 35-0, yale ties harvard, and princeton beats yale 7-6, what score differences in the other 3 games (h-p, mit-p, mit-y) will allow potential differences that agree with the score differences?}  
if the score differences are known for the games in a spanning tree, they are known for all games.

% question 15",vector spaces
"\textbf{in our method for football rankings, should the strength of the opposition be considered — or is that already built in?}

% question 16",vector spaces
"\textbf{if there is an edge between every pair of nodes (a complete graph), how many edges are there?}  
the graph has \( n \) nodes, and edges from a node to itself are not allowed.

% question 17",vector spaces
"\textbf{for both graphs drawn below, verify euler’s formula:}
\[
\text{(# of nodes)} - \text{(# of edges)} + \text{(# of loops)} = 1.
\]

% question 18",vector spaces
"\textbf{multiply matrices to find \( a^t a \), and guess how its entries come from the graph:}
\begin{enumerate}",vector spaces
"\textbf{why does the nullspace of \( a^t a \) contain \( (1,1,1,1) \)? what is its rank?}

% question 20",vector spaces
"\textbf{why does a complete graph with \( n = 6 \) nodes have \( m = 15 \) edges?}  
a spanning tree connecting all six nodes has \( n-1 \) edges.  
there are 
\[
t = n^{(n-2)} = 6^{(4)}
\]
spanning trees!

% question 21",vector spaces
"\textbf{the adjacency matrix of a graph has \( m_{ij} = 1 \) if nodes \( i \) and \( j \) are connected by an edge (otherwise \( m_{ij} = 0 \)).}  
for the graph in problem 6 with 6 nodes and 4 edges, write down \( m \) and also \( m^2 \).  
why does \( (m^2)_{ij} \) count the number of 2-step paths from node \( i \) to node \( j \)?
    % question 1",vector spaces
"\textbf{what matrix has the effect of rotating every vector through 90° and then projecting the result onto the x-axis?}  
what matrix represents projection onto the x-axis followed by projection onto the y-axis?

% question 2",vector spaces
"\textbf{does the product of 5 reflections and 8 rotations of the x-y plane produce a rotation or a reflection?}

% question 3",vector spaces
"\textbf{the matrix}
\[
a = \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix}
\]
\textbf{produces a stretching in the x-direction. draw the circle \(x^2 + y^2 = 1\) and sketch around it the points \((2x, y)\) that result from multiplication by \(a\).}  
what shape is that curve?

% question 4",vector spaces
"\textbf{every straight line remains straight after a linear transformation.}  
if \( z \) is halfway between \( x \) and \( y \), show that \( az \) is halfway between \( ax \) and \( ay \).

% question 5",vector spaces
"\textbf{the matrix}
\[
a = \begin{bmatrix} 1 & 0 \\ 3 & 1 \end{bmatrix}
\]
\textbf{yields a shearing transformation, which leaves the y-axis unchanged. sketch its effect on the x-axis, by indicating what happens to \( (1,0) \), \( (2,0) \), and \( (-1,0) \)—and how the whole axis is transformed.}

% question 6",vector spaces
"\textbf{what \( 3 \times 3 \) matrices represent the transformations that:}
\begin{enumerate}",vector spaces
"\textbf{on the space \( p_3 \) of cubic polynomials, what matrix represents \( \frac{d^2}{dt^2} \)?}  
construct the \( 4 \times 4 \) matrix from the standard basis \( 1, t, t^2, t^3 \).  
find its null space and column space. what do they mean in terms of polynomials?

% question 8",vector spaces
"\textbf{from the cubic polynomials \( p_3 \) to the fourth-degree polynomials \( p_4 \), what matrix represents multiplication by \( 2 +3t \)?}  
the columns of the \( 5 \times 4 \) matrix \( a \) come from applying the transformation to \( 1, t, t^2, t^3 \).

% question 9",vector spaces
"\textbf{the solutions to the linear differential equation \( \frac{d^2u}{dt^2} = u \) form a vector space (since combinations of solutions are still solutions).}  
find two independent solutions to give a basis for that solution space.

% question 10",vector spaces
"\textbf{with initial values \( u = x \) and \( \frac{du}{dt} = y \) at \( t = 0 \), what combination of basis vectors in problem 9 solves \( u'' = u \)?}  
this transformation from initial values to solution is linear. what is its \( 2 \times 2 \) matrix (using \( x = 1, y = 0 \) and \( x = 0, y = 1 \) as basis for \( v \), and your basis for \( w \))?

% question 11",vector spaces
"\textbf{verify directly from \( c^2 + s^2 = 1 \) that reflection matrices satisfy \( h^2 = i \).}

% question 12",vector spaces
"\textbf{suppose \( a \) is a linear transformation from the x-y plane to itself. why does \( a^{-1} (x + y) = a^{-1} x + a^{-1} y \)?}  
if \( a \) is represented by the matrix \( m \), explain why \( a^{-1} \) is represented by \( m^{-1} \).

% question 13",vector spaces
"\textbf{the product \( (ab)c \) of linear transformations starts with a vector \( x \) and produces \( u = cx \). then rule 2v applies \( ab \) to \( u \) and reaches \( (ab)cx \).}
\begin{enumerate}",vector spaces
"\textbf{prove that \( t^2 \) is a linear transformation if \( t \) is linear (from \( \mathbb{r}^3 \) to \( \mathbb{r}^3 \)).}

% question 15",vector spaces
"\textbf{the space of all \( 2 \times 2 \) matrices has the four basis ``vectors''}
\[
\begin{bmatrix}1 & 0 \\ 0 & 0 \end{bmatrix}, \quad
\begin{bmatrix}0 & 1 \\ 0 & 0 \end{bmatrix}, \quad
\begin{bmatrix}0 & 0 \\ 1 & 0 \end{bmatrix}, \quad
\begin{bmatrix}0 & 0 \\ 0 & 1 \end{bmatrix}.
\]
for the linear transformation of transposing, find its matrix \( a \) with respect to this basis. why is \( a^2 = i \)?

% question 16",vector spaces
"\textbf{find the \( 4 \times 4 \) cyclic permutation matrix:}  
\[
(x_1, x_2, x_3, x_4) \text{ is transformed to } a x = (x_2, x_3, x_4, x_1).
\]
what is the effect of \( a^2 \)? show that \( a^3 = a^{-1} \).

% question 17",vector spaces
"\textbf{find the \( 4 \times 3 \) matrix \( a \) that represents a right shift:}  
\[
(x_1, x_2, x_3) \text{ is transformed to } (0, x_1, x_2, x_3).
\]
find also the left shift matrix \( b \) from \( \mathbb{r}^4 \) back to \( \mathbb{r}^3 \), transforming  
\[
(x_1, x_2, x_3, x_4) \text{ to } (x_2, x_3, x_4).
\]
what are the products \( ab \) and \( ba \)?

% question 18",vector spaces
"\textbf{in the vector space \( p_3 \) of all polynomials \( p(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 \), let \( s \) be the subset of polynomials satisfying:}  
\[
\int_0^1 p(x) \, dx = 0.
\]
verify that \( s \) is a subspace and find a basis.

% question 19",vector spaces
"\textbf{a nonlinear transformation is invertible if \( t(x) = b \) has exactly one solution for every \( b \).}  
the example \( t(x) = x^2 \) is not invertible because \( x^2 = b \) has two solutions for positive \( b \) and no solution for negative \( b \).  
which of the following transformations (from \( \mathbb{r} \) to \( \mathbb{r} \)) are invertible? none are linear, not even (c).
\begin{enumerate}",vector spaces
"\textbf{what is the axis and the rotation angle for the transformation that takes \( (x_1, x_2, x_3) \) into \( (x_2, x_3, x_1) \)?}

% question 21",vector spaces
"\textbf{a linear transformation must leave the zero vector fixed: \( t(0) = 0 \).}  
prove this from \( t(v+w) = t(v) + t(w) \) by choosing \( w = 0 \).  
prove it also from the requirement \( t(cv) = ct(v) \) by choosing \( c = 0 \).

% question 22",vector spaces
"\textbf{which of these transformations is not linear? the input is \( v = (v_1, v_2) \).}
\begin{enumerate}",vector spaces
"\textbf{if \( s \) and \( t \) are linear with \( s(v) = t(v) = v \), then does \( s(t(v)) = v \) or \( v^2 \)?}

% question 24",vector spaces
"\textbf{suppose \( t(v) = v \), except that \( t(0, v_2) = (0,0) \).}  
show that this transformation satisfies \( t(cv) = ct(v) \) but not \( t(v+w) = t(v) + t(w) \).

% question 25",vector spaces
"\textbf{which of these transformations satisfy \( t(v+w) = t(v) + t(w) \), and which satisfy \( t(cv) = ct(v) \)?}
\begin{enumerate}",vector spaces
"\textbf{for these transformations of \( v = \mathbb{r}^2 \) to \( w = \mathbb{r}^2 \), find \( t(t(v)) \).}
\begin{enumerate}",vector spaces
"\textbf{the ``cyclic'' transformation \( t \) is defined by}
\[
t(v_1, v_2, v_3) = (v_2, v_3, v_1).
\]
what is \( t(t(t(v))) \)? what is \( t^{100}(v) \)?

% question 27",vector spaces
"\textbf{the ``cyclic'' transformation \( t \) is defined by}
\[
t(v_1, v_2, v_3) = (v_2, v_3, v_1).
\]
what is \( t(t(t(v))) \)? what is \( t^{100}(v) \)?

% question 28",vector spaces
"\textbf{find the range and kernel (these are the column space and null space) of \( t \).}
\begin{enumerate}",vector spaces
"\textbf{a linear transformation from \( v \) to \( w \) has an inverse from \( w \) to \( v \) when the range is all of \( w \) and the kernel contains only \( v = 0 \). why are these transformations not invertible?}
\begin{enumerate}",vector spaces
"\textbf{suppose a linear transformation \( t \) transforms \( (1, 1) \) to \( (2, 2) \) and \( (2, 0) \) to \( (0, 0) \). find \( t(v) \) when:}
\begin{enumerate}",vector spaces
"\textbf{m is any 2 by 2 matrix and \( a = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix} \). the linear transformation \( t \) is defined by \( t(m) = am \). what rules of matrix multiplication show that \( t \) is linear?}

% question 32",vector spaces
"\textbf{suppose \( a = \begin{pmatrix} 1 & 2 \\ 3 & 6 \end{pmatrix} \).}  
show that the identity matrix \( i \) is not in the range of \( t \). find a nonzero matrix \( m \) such that \( t(m) = am \) is zero.
% question 33",vector spaces
"\textbf{suppose \( t \) transposes every matrix \( m \). try to find a matrix \( a \) that gives \( am = m^t \) for every \( m \). show that no matrix \( a \) will do it.}  
to professors: is this a linear transformation that doesn’t come from a matrix?

% question 34",vector spaces
"\textbf{the transformation \( t \) that transposes every matrix is definitely linear. which of these extra properties are true?}
\begin{enumerate}",vector spaces
"\textbf{suppose \( t(m) = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} m \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix} \).}  
find a matrix with \( t(m) \neq 0 \). describe all matrices with \( t(m) = 0 \) (the kernel of \( t \)) and all output matrices \( t(m) \) (the range of \( t \)).

% question 36",vector spaces
"\textbf{problems 36–40 are about changing the basis.}
\begin{enumerate}",vector spaces
"(a) what matrix $m$ transforms $(1,0)$ and $(0,1)$ to $(r,t)$ and $(s,u)$? \\
    (b) what matrix $n$ transforms $(a,c)$ and $(b,d)$ to $(1,0)$ and $(0,1)$? \\
    (c) what condition on $a$, $b$, $c$, $d$ will make part (b) impossible?",vector spaces
"(a) how do $m$ and $n$ in problem 37 yield the matrix that transforms $(a,c)$ to $(r,t)$ and $(b,d)$ to $(s,u)$? \\
    (b) what matrix transforms $(2,5)$ to $(1,1)$ and $(1,3)$ to $(0,2)$?",vector spaces
"if you keep the same basis vectors but put them in a different order, the change-of-basis matrix $m$ is a matrix. if you keep the basis vectors in order but change their lengths, $m$ is a matrix.",vector spaces
"the matrix that transforms $(1,0)$ and $(0,1)$ to $(1,4)$ and $(1,5)$ is $m = \cdots$. the combination $a(1,4) + b(1,5)$ that equals $(1,0)$ has $(a,b) = (\cdots)$. how are those new coordinates of $(1,0)$ related to $m$ or $m^{-1}$?",vector spaces
"what are the three equations for $a$, $b$, $c$ if the parabola $y = a + bx + cx^2$ equals $4$ at $x = a$, $5$ at $x = b$, and $6$ at $x = c$? find the determinant of the $3 \times 3$ matrix. for which numbers $a$, $b$, $c$ will it be impossible to find this parabola $y$?",vector spaces
"suppose $v_1$, $v_2$, $v_3$ are eigenvectors for $t$. this means $t(v_i) = \lambda_i v_i$ for $i = 1,2,3$. what is the matrix for $t$ when the input and output bases are the $v_i$'s?",vector spaces
every invertible linear transformation can have $i$ as its matrix. for the output basis just choose $w_i = t(v_i)$. why must $t$ be invertible?,vector spaces
"suppose $t$ is reflection across the x-axis and $s$ is reflection across the y-axis. the domain $v$ is the x-y plane. if $v = (x,y)$ what is $s(t(v))$? find a simpler description of the product $st$.",vector spaces
"suppose $t$ is reflection across the $45^\circ$ line, and $s$ is reflection across the y-axis. if $v = (2,1)$ then $t(v) = (1,2)$. find $s(t(v))$ and $t(s(v))$. this shows that generally $st \neq ts$.",vector spaces
"show that the product $st$ of two reflections is a rotation. multiply these reflection matrices to find the rotation angle:
    \[
    \begin{bmatrix}
    \cos 2\theta & \sin 2\theta \\
    \sin 2\theta & -\cos 2\theta
    \end{bmatrix}
    \begin{bmatrix}
    \cos 2\alpha & \sin 2\alpha \\
    \sin 2\alpha & -\cos 2\alpha
    \end{bmatrix}.
    \]",vector spaces
"the $4 \times 4$ hadamard matrix is entirely $+1$ and $-1$:
    \[
    h = \begin{bmatrix}
    1 & 1 & 1 & 1 \\
    1 & -1 & 1 & -1 \\
    1 & 1 & -1 & -1 \\
    1 & -1 & -1 & 1
    \end{bmatrix}.
    \]
    find $h^{-1}$ and write $v = (7,5,3,1)$ as a combination of the columns of $h$.",vector spaces
"suppose we have two bases $v_1, \dots, v_n$ and $w_1, \dots, w_n$ for $\mathbb{r}^n$. if a vector has coefficients $b_i$ in one basis and $c_i$ in the other basis, what is the change-of-basis matrix in $b = mc$? start from:
    \[
    b_1 v_1 + \dots + b_n v_n = vb = c_1 w_1 + \dots + c_n w_n = wc.
    \]
    your answer represents $t(v) = v$ with the input basis of $v$'s and output basis of $w$'s. because of different bases, the matrix is not $i$.",vector spaces
"true or false: if we know $t(v)$ for $n$ different nonzero vectors in $\mathbb{r}^2$, then we know $t(v)$ for every vector in $\mathbb{r}^n$.",vector spaces
"(recommended) suppose all vectors $x$ in the unit square $0 \leq x_1 \leq 1$, $0 \leq x_2 \leq 1$ are transformed to $ax$ ($a$ is $2 \times 2$).
    \begin{enumerate}",vector spaces
what is the shape of the transformed region (all $ax$)?,vector spaces
for which matrices $a$ is that region a square?,vector spaces
for which $a$ is it a line?,vector spaces
for which $a$ is the new area still 1?,vector spaces
"find a basis for the following subspaces of $\mathbb{r}^4$:
    \begin{enumerate}",vector spaces
the vectors for which $x_1 = 2x_4$.,vector spaces
the vectors for which $x_1 + x_2 + x_3 = 0$ and $x_3 + x_4 = 0$.,vector spaces
"the subspace spanned by $(1,1,1,1)$, $(1,2,3,4)$, and $(2,3,4,5)$.",vector spaces
"\textbf{1.2} by giving a basis, describe a two-dimensional subspace of $\mathbb{r}^3$ that contains none of the coordinate vectors $(1,0,0)$, $(0,1,0)$, $(0,0,1)$.",vector spaces
"true or false, with counterexample if false:
    \begin{enumerate}",vector spaces
"if the vectors $x_1, \dots, x_m$ span a subspace $s$, then $\dim s = m$.",vector spaces
the intersection of two subspaces of a vector space cannot be empty.,vector spaces
"if $ax = ay$, then $x = y$.",vector spaces
the row space of $a$ has a unique basis that can be computed by reducing $a$ to echelon form.,vector spaces
"if a square matrix $a$ has independent columns, so does $a^2$.",vector spaces
"what is the echelon form $u$ of $a$?
    \[
    a = \begin{bmatrix}
    1 & 2 & 0 & 2 & 1 \\
    -1 & -2 & 1 & 1 & 0 \\
    1 & 2 & -3 & -7 & -2
    \end{bmatrix}
    \]
    what are the dimensions of its four fundamental subspaces?",vector spaces
"find the rank and the nullspace of
    \[
    a = \begin{bmatrix}
    0 & 0 & 1 \\
    0 & 0 & 1 \\
    1 & 1 & 1
    \end{bmatrix}
    \quad \text{and} \quad
    b = \begin{bmatrix}
    0 & 0 & 1 & 2 \\
    0 & 0 & 1 & 2 \\
    1 & 1 & 1 & 0
    \end{bmatrix}
    \]",vector spaces
"find bases for the four fundamental subspaces associated with
    \[
    a = \begin{bmatrix} 1 & 2 \\ 3 & 6 \end{bmatrix}, \quad
    b = \begin{bmatrix} 0 & 0 \\ 1 & 2 \end{bmatrix}, \quad
    c = \begin{bmatrix} 1 & 1 & 0 & 0 \\ 0 & 1 & 0 & 1 \end{bmatrix}
    \]",vector spaces
"\textbf{1.7} what is the most general solution to the system:
    \[
    u + v + w = 1, \quad u - w = 2?
    \]",vector spaces
"construct a matrix whose nullspace contains the vector $x = (1,1,2)$.",vector spaces
"construct a matrix whose left nullspace contains $y = (1,5)$.",vector spaces
"construct a matrix whose column space is spanned by $(1,1,2)$ and whose row space is spanned by $(1,5)$.",vector spaces
"if you are given any three vectors in $\mathbb{r}^6$ and any three vectors in $\mathbb{r}^5$, is there a $6 \times 5$ matrix whose column space is spanned by the first three and whose row space is spanned by the second three?",vector spaces
"in the vector space of $2 \times 2$ matrices:
    \begin{enumerate}",vector spaces
is the set of rank 1 matrices a subspace?,vector spaces
what subspace is spanned by the permutation matrices?,vector spaces
what subspace is spanned by the positive matrices (all $a_{ij} > 0$)?,vector spaces
what subspace is spanned by the invertible matrices?,vector spaces
invent a vector space that contains all linear transformations from $\mathbb{r}^n$ to $\mathbb{r}^n$. you have to decide on a rule for addition. what is its dimension?,vector spaces
"find the rank of $a$, and give a basis for its nullspace.
        \[
        a = lu = \begin{bmatrix}
        1 & 2 & 1 \\
        2 & 1 & 2 \\
        3 & 2 & 4 & 1
        \end{bmatrix}
        \begin{bmatrix}
        1 & 2 & 0 & 1 & 2 & 1 \\
        0 & 0 & 2 & 2 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 \\
        0 & 0 & 0 & 0 & 0 & 0
        \end{bmatrix}
        \]",vector spaces
the first 3 rows of $u$ are a basis for the row space of $a$—true or false?,vector spaces
"columns 1, 3, 6 of $u$ are a basis for the column space of $a$—true or false?",vector spaces
the four rows of $a$ are a basis for the row space of $a$—true or false?,vector spaces
find as many linearly independent vectors $b$ as possible for which $ax = b$ has a solution.,vector spaces
"in elimination on $a$, what multiple of the third row is subtracted to knock out the fourth row?",vector spaces
"if $a$ is an $n \times (n-1)$ matrix, and its rank is $n-2$, what is the dimension of its nullspace?",vector spaces
"use elimination to find the triangular factors in $a = lu$, if
    \[
    a = \begin{bmatrix}
    a & a & a & a \\
    a & b & b & b \\
    a & b & c & c \\
    a & b & c & d
    \end{bmatrix}
    \]
    under what conditions on the numbers $a$, $b$, $c$, $d$ are the columns linearly independent?",vector spaces
"do the vectors $(1,1,3)$, $(2,3,6)$, and $(1,4,3)$ form a basis for $\mathbb{r}^3$?",vector spaces
"what do you know about $c(a)$ when the number of solutions to $ax = b$ is:
    \begin{enumerate}",vector spaces
"$\infty$, independent of $b$.",vector spaces
"0 or $\infty$, depending on $b$.",vector spaces
"in the previous exercise, how is $r$ related to $m$ and $n$ in each example?",vector spaces
"if $x$ is a vector in $\mathbb{r}^n$, and $x^t y = 0$ for every $y$, prove that $x = 0$.",vector spaces
"if $a$ is an $n \times n$ matrix such that $a^2 = a$ and $\text{rank}(a) = n$, prove that $a = i$.",vector spaces
"what subspace of $3 \times 3$ matrices is spanned by the elementary matrices $e_{ij}$, with 1s on the diagonal and at most one nonzero entry below?",vector spaces
how many $5 \times 5$ permutation matrices are there? are they linearly independent? do they span the space of all $5 \times 5$ matrices? no need to write them all down.,vector spaces
"what is the rank of the $n \times n$ matrix with every entry equal to 1? how about the “checkerboard matrix,” with $a_{ij} = 0$ when $i + j$ is even, $a_{ij} = 1$ when $i + j$ is odd?",vector spaces
"$ax = b$ has a solution under what conditions on $b$, for the following $a$ and $b$?
        \[
        a = \begin{bmatrix}
        1 & 2 & 0 & 3 \\
        0 & 0 & 0 & 0 \\
        2 & 4 & 0 & 1
        \end{bmatrix}, \quad b = \begin{bmatrix}
        b_1 \\
        b_2 \\
        b_3
        \end{bmatrix}
        \]",vector spaces
find a basis for the nullspace of $a$.,vector spaces
"find the general solution to $ax = b$, when a solution exists.",vector spaces
find a basis for the column space of $a$.,vector spaces
what is the rank of $a^t$?,vector spaces
"how can you construct a matrix that transforms the coordinate vectors $e_1, e_2, e_3$ into three given vectors $v_1, v_2, v_3$? when will that matrix be invertible?",vector spaces
"if $e_1, e_2, e_3$ are in the column space of a $3 \times 5$ matrix, does it have a left-inverse? does it have a right-inverse?",vector spaces
"suppose $t$ is the linear transformation on $\mathbb{r}^3$ that takes each point $(u, v, w)$ to $(u+v+w, u+v, u)$. describe what $t^{-1}$ does to the point $(x, y, z)$.",vector spaces
"true or false?
    \begin{enumerate}",vector spaces
every subspace of $\mathbb{r}^4$ is the nullspace of some matrix.,vector spaces
"if $a$ has the same nullspace as $a^t$, the matrix must be square.",vector spaces
the transformation that takes $x$ to $mx + b$ is linear (from $\mathbb{r}^1$ to $\mathbb{r}^1$).,vector spaces
"find bases for the four fundamental subspaces of
    \[
    a_1 = \begin{bmatrix}
    1 & 2 & 0 & 3 \\
    0 & 2 & 2 & 2 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 4
    \end{bmatrix}, \quad a_2 = \begin{bmatrix}
    1 \\
    1 \\
    1
    \end{bmatrix}
    \]",vector spaces
"if the rows of $a$ are linearly independent (where $a$ is $m \times n$), then the rank is \_, the column space is \_, and the left nullspace is \_.",vector spaces
"if $a$ is $8 \times 10$ with a two-dimensional nullspace, show that $ax = b$ can be solved for every $b$.",vector spaces
"describe the linear transformations of the $x$-$y$ plane that are represented with the standard basis $(1,0)$ and $(0,1)$ by the matrices
    \[
    a_1 = \begin{bmatrix}
    1 & 0 \\
    0 & -1
    \end{bmatrix}, \quad a_2 = \begin{bmatrix}
    1 & 0 \\
    2 & 1
    \end{bmatrix}, \quad a_3 = \begin{bmatrix}
    0 & 1 \\
    -1 & 0
    \end{bmatrix}
    \]",vector spaces
"if $a$ is square, show that the nullspace of $a^2$ contains the nullspace of $a$.",vector spaces
when does the rank-1 matrix $a = uv^t$ have $a^2 = 0$?,vector spaces
find a basis for the space of all vectors in $\mathbb{r}^6$ with $x_1 + x_2 = x_3 + x_4 = x_5 + x_6$.,vector spaces
find a matrix with that subspace as its nullspace.,vector spaces
find a matrix with that subspace as its column space.,vector spaces
"suppose the matrices in $pa = lu$ are:
    \[
    p = \begin{bmatrix}
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & 1 & 0
    \end{bmatrix}, \quad
    a = \begin{bmatrix}
    0 & 0 & 1 & -3 & 2 \\
    2 & -1 & 4 & 2 & 1 \\
    4 & -2 & 9 & 1 & 4 \\
    2 & -1 & 5 & -1 & 5
    \end{bmatrix}
    \]
    \[
    l = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 1 & 1 & 0 \\
    2 & 1 & 0 & 1
    \end{bmatrix}, \quad
    u = \begin{bmatrix}
    2 & -1 & 4 & 2 & 1 \\
    0 & 0 & 1 & -3 & 2 \\
    0 & 0 & 0 & 0 & 2 \\
    0 & 0 & 0 & 0 & 0
    \end{bmatrix}
    \]
    \begin{enumerate}",vector spaces
what is the rank of $a$?,vector spaces
what is a basis for the row space of $a$?,vector spaces
"true or false: rows 1, 2, 3 of $a$ are linearly independent.",vector spaces
what is a basis for the column space of $a$?,vector spaces
what is the dimension of the left nullspace of $a$?,vector spaces
what is the general solution to $ax = 0$?,vector spaces
