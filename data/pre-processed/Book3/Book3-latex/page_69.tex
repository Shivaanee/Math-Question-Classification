linear combinations of the \(r\) distinguished coordinates \(b_{\boldsymbol{k}_{i}}\), ..., \(b_{\boldsymbol{k}_{r}}\). One has complete freedom of choice in the coordinates \(b_{\boldsymbol{k}_{i}}\) that is, if \(c_{1}\), ..., \(c_{r}\) are _any_\(r\) scalars, there is one and only one vector \(\beta\) in \(W\) which has \(c_{i}\) as its \(k_{i}\)th coordinate.

The significant point here is this: Given the vectors \(\alpha_{i}\), row-reduction is a straightforward method of determining the integers \(r\), \(k_{i}\), ..., \(k_{r}\) and the scalars \(R_{ij}\) which give the description (2-25) of the subspace spanned by \(\alpha_{1}\), ..., \(\alpha_{m}\). One should observe as we did in Theorem 11 that every subspace \(W\) of \(P^{n}\) has a description of the type (2-25). We should also point out some things about question (2). We have already stated how one can find an invertible \(m\times m\) matrix \(P\) such that \(R=PA\), in Section 1.4. The knowledge of \(P\) enables one to find the scalars \(\boldsymbol{x}_{1}\), ..., \(\boldsymbol{x}_{m}\) such that

\[\beta=x_{1}\alpha_{1}+\,\cdots\,+x_{m}\alpha_{m}\]

when this is possible. For the row vectors of \(R\) are given by

\[\rho_{i}=\sum_{j=1}^{m}P_{ij}\alpha_{j}\]

so that if \(\beta\) is a linear combination of the \(\alpha_{j}\), we have

\[\beta = \sum_{i=1}^{r}\,b_{k_{i}}\rho_{i}\] \ 