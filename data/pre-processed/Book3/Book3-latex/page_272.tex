about \(T\). Two different types of difficulties arise in the computation of such standard forms. One difficulty is, of course, the length of the computations. The other difficulty is that there may not be any method for doing the computations, even if one has the necessary time and patience. The second difficulty arises in, say, trying to find the Jordan form of a complex matrix. There simply is no well-defined method for factoring the characteristic polynomial, and thus one is stopped at the outset. The rational form does not suffer from this difficulty. As we showed in Section 7.4, there is a well-defined method for finding the rational form of a given \(n\times n\) matrix; however, such computations are usually extremely lengthy.

In our summary of the results of these last two chapters, we have not yet mentioned one of the theorems which we proved. This is the theorem which states that if \(T\) is a linear operator on a finite-dimensional vector space over an algebraically closed field, then \(T\) is uniquely expressible as the sum of a diagonalizable operator and a nilpotent operator which commute. This was proved from the primary decomposition theorem and certain information about diagonalizable operators. It is not as deep a theorem as the cyclic decomposition theorem or the existence of the Jordan form, but it does have important and useful applications in certain parts of mathematics. In concluding this chapter, we shall prove an analogous theorem, without assuming that the scalar field is algebraically closed. We begin by defining the operators which will play the role of the diagonalizable operators.

**Definition**.: _Let \(V\) be a finite-dimensional vector space over the field \(F\), and let \(T\) be a linear operator on \(V\). We say that \(T\) is_ **semi-simple** _if every \(T\)-invariant subspace has a complementary \(T\)-invariant subspace._

What we are about to prove is that, with some restriction on the field \(F\), every linear operator \(T\) is uniquely expressible in the form \(T\) = _S_+_N_, where \(S\) is semi-simple, \(N\) is nilpotent, and _SN_ = _NS_. First, we are going to characterize semi-simple operators by means of their minimal polynomials, and this characterization will show us that, when \(F\) is algebraically closed, an operator is semi-simple if and only if it is diagonalizable.

**Lemma**.: _Let \(T\) be a linear operator on the finite-dimensional vector space \(V\), and let \(V=W_{1}\oplus\cdots\oplus W_{k}\) be the primary decomposition for \(T\). In other words, if \(p\) is the minimal polynomial for \(T\) and \(p=p_{1}^{r_{1}}\cdots\,p_{k}^{r_{k}}\) is the prime factorization of \(p\), then \(W_{i}\) is the null space of \(p_{i}(T)^{r_{i}}\). Let \(W\) be any subspace of \(V\) which is invariant under \(T\). Then_

\[W=(W\cap W_{1})\oplus\cdots\oplus(W\cap W_{k})\]

Proof.: For the proof we need to recall a corollary to our proof of the primary decomposition theorem in Section 6.8. If \(E_{1},\ldots,E_{k}\) are 