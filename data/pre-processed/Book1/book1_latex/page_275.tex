\(u=e^{\lambda t}x\); the eigenvalue gives the rate of growth or decay, and the eigenvector \(x\) develops at this rate. The other solutions will be _mixtures_ of these pure solutions, and the mixture is adjusted to fit the initial conditions.

The key equation was \(Ax=\lambda x\). Most vectors \(x\) will not satisfy such an equation. They change direction when multiplied by \(A\), so that \(Ax\) is not a multiple of \(x\). This means that _only certain special numbers are eigenvalues, and only certain special vectors \(x\) are eigenvectors_. We can watch the behavior of each eigenvector, and then combine these "normal modes" to find the solution. To say the same thing in another way, _the underlying matrix can be diagonalized_.

The diagonalization in Section 5.2 will be applied to difference equations, Fibonacci numbers, and Markov processes, and also to differential equations. In every example, we start by computing the eigenvalues and eigenvectors; there is no shortcut to avoid that. Symmetric matrices are especially easy. "Defective matrices" lack a full set of eigenvectors, so they are not diagonalizable. Certainly they have to be discussed, but we will not allow them to take over the book.

We start with examples of particularly good matrices.

**Example 1**.: Everything is clear when \(A\) is a _diagonal matrix_:

\[A=\begin{bmatrix}3&0\\ 0&2\end{bmatrix}\quad\text{has}\quad\lambda_{1}=3\quad\text{with}\quad x_{1}= \begin{bmatrix}1\\ 0\end{bmatrix},\qquad\lambda_{2}=2\quad\text{with}\quad x_{2}=\begin{bmatrix} 0\\ 1\end{bmatrix}.\]

On each eigenvector \(A\) acts like a multiple of the identity: \(Ax_{1}=3x_{1}\) and \(Ax_{2}=2x_{2}\). Other vectors like \(x=(1,5)\) are mixtures \(x_{1}+5x_{2}\) of the two eigenvectors, and when \(A\) multiplies \(x_{1}\) and \(x_{2}\) it produces the eigenvalues \(\lambda_{1}=3\) and \(\lambda_{2}=2\):

\[A\quad\text{times}\quad x_{1}+5x_{2}\quad\text{is}\quad 3x_{1}+10x_{2}= \begin{bmatrix}3\\ 10\end{bmatrix}.\]

This is \(Ax\) for a typical vector \(x\)--not an eigenvector. But the action of \(A\) is determined by its eigenvectors and eigenvalues.

**Example 2**.: The eigenvalues of a _projection matrix_ are \(1\) or \(0\)!

\[P=\begin{bmatrix}\frac{1}{2}&\frac{1}{2}\\ \frac{1}{2}&\frac{1}{2}\end{bmatrix}\quad\text{has}\quad\lambda_{1}=1\quad \text{with}\quad x_{1}=\begin{bmatrix}1\\ 1\end{bmatrix},\qquad\lambda_{2}=0\quad\text{with}\quad x_{2}=\begin{bmatrix} 1\\ -1\end{bmatrix}.\]

We have \(\lambda=1\) when \(x\) projects to itself, and \(\lambda=0\) when \(x\) projects to the zero vector. The column space of \(P\) is filled with eigenvectors, and so is the nullspace. If those spaces have dimension \(r\) and \(n-r\), then \(\lambda=1\) is repeated \(r\) times and \(\lambda=0\) is repeated \(n-r\) times (_always_\(n\)\(\lambda\)'s):

\[\begin{array}{c}\text{\bf Four eigenvalues}\\ \text{\bf allowing repeats}\end{array}\qquad P=\begin{bmatrix}1&0&0&0\\ 0&0&0&0\\ 0&0&0&0\\ 0&0&0&1\end{bmatrix}\quad\text{has}\quad\lambda=1,1,0,0.\] 