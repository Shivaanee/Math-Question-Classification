
Often we can just divide each \(u_{k}\) by its first component \(\alpha_{k}\) before taking the next step. With this simple scaling, the power method \(u_{k+1}=Au_{k}/\alpha_{k}\) converges to a multiple of \(x_{n}\). _The scaling factors \(\alpha_{k}\) will approach \(\lambda_{n}\)_.

**Example 1**.: The \(u_{k}\) approach the eigenvector \(\left[\begin{smallmatrix}2/3\\ 1/3\end{smallmatrix}\right]=\left[\begin{smallmatrix}.667\\ .333\end{smallmatrix}\right]\) when \(A=\left[\begin{smallmatrix}.9&.2\\ .1&.8\end{smallmatrix}\right]\) is the matrix of population shifts in Section 1.3:

\[u_{0}=\begin{bmatrix}1\\ 0\end{bmatrix},\qquad u_{1}=\begin{bmatrix}.9\\ .1\end{bmatrix},\qquad u_{2}=\begin{bmatrix}.83\\ .17\end{bmatrix},\qquad u_{3}=\begin{bmatrix}.781\\ .219\end{bmatrix},\qquad u_{4}=\begin{bmatrix}.747\\ .253\end{bmatrix}.\]

If \(r=|\lambda_{n-1}|/|\lambda_{n}|\) is close to \(1\), then convergence is very slow. In many applications \(r>.9\), which means that more than \(20\) iterations are needed to achieve one more digit. (The example had \(r=.7\), and it was still slow.) If \(r=1\), which means \(|\lambda_{n-1}|=|\lambda_{n}|\), then convergence will probably not occur at all. That happens (in the applet with sound) for a complex conjugate pair \(\lambda_{n-1}=\overline{\lambda}_{n}\). There are several ways to get around this limitation, and we shall describe three of them:

1. The _block power method_ works with several vectors at once, in place of \(u_{k}\). If we multiply \(p\) orthonormal vectors by \(A\), and then apply Gram-Schmidt to orthogonalize them again--that is a single step of the method--the convergence ratio becomes \(r^{\prime}=|\lambda_{n-p}|/|\lambda_{n}|\). We will obtain approximations to \(p\) different eigenvalues and their eigenvectors.
2. The _inverse power method_ operates with \(A^{-1}\) instead of \(A\). A single step is \(v_{k+1}=A^{-1}v_{k}\), which means that we solve the linear system \(Av_{k+1}=v_{k}\) (and save the factors \(L\) and \(U\)!). Now we converge to the _smallest eigenvalue_\(\lambda_{1}\) and its eigenvector \(x_{1}\), provided \(|\lambda_{1}|<|\lambda_{2}|\). Often it is \(\lambda_{1}\) that is wanted in the applications, and then inverse iteration is an automatic choice.
3. The _shifted inverse power method_ is best of all. Replace \(A\) by \(A-\alpha I\). Each eigenvalue is shifted by \(\alpha\), and the convergence factor for the inverse method will change to \(r^{\prime\prime}=|\lambda_{1}-\alpha|/|\lambda_{2}-\alpha|\). If \(\alpha\) is a good approximation to \(\lambda_{1}\), \(r^{\prime\prime}\) will be very small and the convergence is enormously accelerated. Each step of the method solves \((A-\alpha I)w_{k+1}=w_{k}\): \[w_{k}=\frac{c_{1}x_{1}}{(\lambda_{1}-\alpha)^{k}}+\frac{c_{2}x_{2}}{(\lambda _{2}-\alpha)^{k}}+\cdots+\frac{c_{n}x_{n}}{(\lambda_{n}-\alpha)^{k}}.\] When \(\alpha\) is close to \(\lambda_{1}\), the first term dominates after only one or two steps. If \(\lambda_{1}\) has already been computed by another algorithm (such as \(QR\)), then \(\alpha\) is this computed value. One standard procedure is to factor \(A-\alpha I\) into \(LU\) and to solve \(Ux_{1}=(1,1,\ldots,1)\) by back-substitution.

If \(\lambda_{1}\) is not already approximated, the shifted inverse power method has to generate its own choice of \(\alpha\). We can vary \(\alpha=\alpha_{k}\) at every step if we want to, so \((A-\alpha_{k}I)w_{k+1}=w_{k}\) 