When the elimination is down to \(k\) equations, only \(k^{2}-k\) operations are needed to clear out the column below the pivot--by the same reasoning that applied to the first stage, when \(k\) equaled \(n\). Altogether, the total number of operations is the sum of \(k^{2}-k\) over all values of \(k\) from \(1\) to \(n\):

\[\begin{split}\textbf{Left side}\qquad(1^{2}+\cdots+n^{2})-(1+ \cdots+n)&=\frac{n(n+1)(2n+1)}{6}-\frac{n(n+1)}{2}\\ &=\frac{n^{3}-n}{3}.\end{split}\]

Those are standard formulas for the sums of the first \(n\) numbers and the first \(n\) squares. Substituting \(n=1\) and \(n=2\) and \(n=100\) into the formula \(\frac{1}{3}(n^{3}-n)\), forward elimination can take no steps or two steps or about a third of a million steps:

If \(n\) is at all large, _a good estimate for the number of operations is \(\frac{1}{3}n^{3}\)_.

If the size is doubled, and few of the coefficients are zero, the cost is multiplied by \(8\).

Back-substitution is considerably faster. The last unknown is found in only one operation (a division by the last pivot). The second to last unknown requires two operations, and so on. Then the total for back-substitution is \(1+2+\cdots+n\).

Forward elimination also acts on the right-hand side (subtracting the same multiples as on the left to maintain correct equations). This starts with \(n-1\) subtractions of the first equation. Altogether _the right-hand side is responsible for \(n^{2}\) operations_--much less than the \(n^{3}/3\) on the left. The total for forward and back is

\[\textbf{Right side}\qquad[(n-1)+(n-2)+\cdots+1]+[1+2+\cdots+n]=n^{2}.\]

Thirty years ago, almost every mathematician would have guessed that a general system of order \(n\) could not be solved with much fewer than \(n^{3}/3\) multiplications. (There were even theorems to demonstrate it, but they did not allow for all possible methods.) Astonishingly, that guess has been proved wrong. _There now exists a method that requires only \(Cn^{\log_{2}7}\) multiplications!_ It depends on a simple fact: Two combinations of two vectors in two-dimensional space would seem to take \(8\) multiplications, but they can be done in \(7\). That lowered the exponent from \(\log_{2}8\), which is \(3\), to \(\log_{2}7\approx 2.8\). This discovery produced tremendous activity to find the smallest possible power of \(n\). The exponent finally fell (at IBM) below \(2.376\). Fortunately for elimination, the constant \(C\) is so large and the coding is so awkward that the new method is largely (or entirely) of theoretical interest. The newest problem is the cost with _many processors in parallel_.

## Problem Set 1.3

**Problems 1-9 are about elimination on 2 by 2 systems.** 