

## 12 Exercises

* _Approximating a vector as a multiple of another one._ In the special case \(n=1\), the general least squares problem (12.1) reduces to finding a scalar \(x\) that minimizes \(\|ax-b\|^{2}\), where \(a\) and \(b\) are \(m\)-vectors. (We write the matrix \(A\) here in lower case, since it is an \(m\)-vector.) Assuming \(a\) and \(b\) are nonzero, show that \(\|a\hat{x}-b\|^{2}=\|b\|^{2}(\sin\theta)^{2}\), where \(\theta=\angle(a,b)\). This shows that the optimal relative error in approximating one vector by a multiple of another one depends on their angle.
* _Least squares with orthonormal columns._ Suppose the \(m\times n\) matrix \(Q\) has orthonormal columns and \(b\) is an \(m\)-vector. Show that \(\hat{x}=Q^{T}b\) is the vector that minimizes \(\|Qx-b\|^{2}\). What is the complexity of computing \(\hat{x}\), given \(Q\) and \(b\), and how does it compare to the complexity of a general least squares problem with an \(m\times n\) coefficient matrix?
* _Least angle property of least squares._ Suppose the \(m\times n\) matrix \(A\) has linearly independent columns, and \(b\) is an \(m\)-vector. Let \(\hat{x}=A^{\dagger}b\) denote the least squares approximate solution of \(Ax=b\). 1. Show that for any \(n\)-vector \(x\), \((Ax)^{T}b=(Ax)^{T}(A\hat{x})\), _i.e._, the inner product of \(Ax\) and \(b\) is the same as the inner product of \(Ax\) and \(A\hat{x}\). _Hint._ Use \((Ax)^{T}b=x^{T}(A^{T}b)\) and \((A^{T}A)\hat{x}=A^{T}b\). 2. Show that when \(A\hat{x}\) and \(b\) are both nonzero, we have \[\frac{(A\hat{x})^{T}b}{\|A\hat{x}\|\|b\|}=\frac{\|A\hat{x}\|}{\|b\|}.\] The left-hand side is the cosine of the angle between \(A\hat{x}\) and \(b\). _Hint._ Apply part (a) with \(x=\hat{x}\). 3. _Least angle property of least squares._ The choice \(x=\hat{x}\) minimizes the distance between \(Ax\) and \(b\). Show that \(x=\hat{x}\) also minimizes the angle between \(Ax\) and \(b\). (You can assume that \(Ax\) and \(b\) are nonzero.) _Remark._ For any positive scalar \(\alpha\), \(x=\alpha\hat{x}\) also minimizes the angle between \(Ax\) and \(b\).
* _Weighted least squares._ In least squares, the objective (to be minimized) is \[\|Ax-b\|^{2}=\sum_{i=1}^{m}(\tilde{a}_{i}^{T}x-b_{i})^{2},\] where \(\tilde{a}_{i}^{T}\) are the rows of \(A\), and the \(n\)-vector \(x\) is to chosen. In the _weighted least squares problem_, we minimize the objective \[\sum_{i=1}^{m}w_{i}(\tilde{a}_{i}^{T}x-b_{i})^{2},\] where \(w_{i}\) are given positive weights. The weights allow us to assign different weights to the different components of the residual vector. (The objective of the weighted least squares problem is the square of the weighted norm, \(\|Ax-b\|_{w}^{2}\), as defined in exercise 3.28.) 1. Show that the weighted least squares objective can be expressed as \(\|D(Ax-b)\|^{2}\) for an appropriate diagonal matrix \(D\). This allows us to solve the weighted least squares problem as a standard least squares problem, by minimizing \(\|Bx-d\|^{2}\), where \(B=DA\) and \(d=Db\). 2. Show that when \(A\) has linearly independent columns, so does the matrix \(B\). 3. The least squares approximate solution is given by \(\hat{x}=(A^{T}A)^{-1}A^{T}b\). Give a similar formula for the solution of the weighted least squares problem. You might want to use the matrix \(W=\mathbf{diag}(w)\) in your formula.

