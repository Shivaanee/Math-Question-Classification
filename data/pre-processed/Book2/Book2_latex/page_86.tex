* If the group assignments found in step 1 are the same in two successive iterations, the representatives in step 2 will also be the same. It follows that the group assignments and group representatives will never change in future iterations, so we should stop the algorithm. This is what we mean by 'until convergence'. In practice, one often stops the algorithm earlier, as soon as the improvement in successive iterations becomes very small.
* We start the algorithm with a choice of initial group representatives. One simple method is to pick the representatives randomly from the original vectors; another is to start from a random assignment of the original vectors to \(k\) groups, and use the means of the groups as the initial representatives. (There are more sophisticated methods for choosing an initial representatives, but this topic is beyond the scope of this book.)

Convergence.The fact that \(J^{\mathrm{clust}}\) decreases in each step implies that the \(k\)-means algorithm converges in a finite number of steps. However, depending on the initial choice of representatives, the algorithm can, and does, converge to different final partitions, with different objective values.

The \(k\)-means algorithm is a _heuristic_, which means it cannot guarantee that the partition it finds minimizes our objective \(J^{\mathrm{clust}}\). For this reason it is common to run the \(k\)-means algorithm several times, with different initial representatives, and choose the one among them with the smallest final value of \(J^{\mathrm{clust}}\). Despite the fact that the \(k\)-means algorithm is a heuristic, it is very useful in practical applications, and very widely used.

Figure 4.3 shows a few iterations generated by the \(k\)-means algorithm, applied to the example of figure 4.1. We take \(k=3\) and start with randomly chosen group representatives. The final clustering is shown in figure 4.4. Figure 4.5 shows how the clustering objective decreases in each step.

Interpreting the representatives.The representatives \(z_{1},\ldots,z_{k}\) associated with a clustering are quite interpretable. Suppose, for example, that voters in some election can be well clustered into 7 groups, on the basis of a data set that includes demographic data and questionnaire or poll data. If the 4th component of our vectors is the age of the voter, then \((z_{3})_{4}=37.8\) tells us that the average age of voters in group 3 is 37.8. Insight gained from this data can be used to tune campaign messages, or choose media outlets for campaign advertising.

Another way to interpret the group representatives is to find one or a few of the original data points that are closest to each representive. These can be thought of as archetypes for the group.

Choosing \(k\).It is common to run the \(k\)-means algorithm for different values of \(k\), and compare the results. How to choose a value of \(k\) among these depends on how the clustering will be used, which we discuss a bit more in SS4.5. But some general statements can be made. For example, if the value of \(J^{\mathrm{clust}}\) with \(k=7\) is quite a bit smaller than the values of \(J^{\mathrm{clust}}\) for \(k=2,\ldots,6\), and not much larger than the values of \(J^{\mathrm{clust}}\) for \(k=8,9,\ldots\), we could reasonably choose \(k=7\), and conclude that our data (list of vectors) partitions nicely into 7 groups.

 