

**Properties of norm.** Some important properties of the Euclidean norm are given below. Here \(x\) and \(y\) are vectors of the same size, and \(\beta\) is a scalar.

* _Nonnegative homogeneity._\(\|\beta x\|=|\beta|\|x\|\). Multiplying a vector by a scalar multiplies the norm by the absolute value of the scalar.
* _Triangle inequality._\(\|x+y\|\leq\|x\|+\|y\|\). The Euclidean norm of a sum of two vectors is no more than the sum of their norms. (The name of this property will be explained later.) Another name for this inequality is _subadditivity_.
* _Nonnegativity._\(\|x\|\geq 0\).
* _Definiteness._\(\|x\|=0\) only if \(x=0\).

The last two properties together, which state that the norm is always nonnegative, and zero only when the vector is zero, are called _positive definiteness_. The first, third, and fourth properties are easy to show directly from the definition of the norm. As an example, let's verify the definiteness property. If \(\|x\|=0\), then we also have \(\|x\|^{2}=0\), which means that \(x_{1}^{2}+\cdots+x_{n}^{2}=0\). This is a sum of \(n\) nonnegative numbers, which is zero. We can conclude that each of the \(n\) numbers is zero, since if any of them were nonzero the sum would be positive. So we conclude that \(x_{i}^{2}=0\) for \(i=1,\ldots,n\), and therefore \(x_{i}=0\) for \(i=1,\ldots,n\); and thus, \(x=0\). Establishing the second property, the triangle inequality, is not as easy; we will give a derivation on page 57.

General norms.Any real-valued function of an \(n\)-vector that satisfies the four properties listed above is called a (general) norm. But in this book we will only use the Euclidean norm, so from now on, we refer to the Euclidean norm as the norm. (See exercise 3.5, which describes some other useful norms.)

Root-mean-square value.The norm is related to the _root-mean-square_ (RMS) value of an \(n\)-vector \(x\), defined as

\[\mathbf{rms}(x)=\sqrt{\frac{x_{1}^{2}+\cdots+x_{n}^{2}}{n}}=\frac{\|x\|}{ \sqrt{n}}.\]

The argument of the squareroot in the middle expression is called the _mean square_ value of \(x\), denoted \(\mathbf{ms}(x)\), and the RMS value is the squareroot of the mean square value. The RMS value of a vector \(x\) is useful when comparing norms of vectors with different dimensions; the RMS value tells us what a 'typical' value of \(|x_{i}|\) is. For example, the norm of \(\mathbf{1}\), the \(n\)-vector of all ones, is \(\sqrt{n}\), but its RMS value is \(1\), independent of \(n\). More generally, if all the entries of a vector are the same, say, \(\alpha\), then the RMS value of the vector is \(|\alpha|\).

Norm of a sum.A useful formula for the norm of the sum of two vectors \(x\) and \(y\) is

\[\|x+y\|=\sqrt{\|x\|^{2}+2x^{T}y+\|y\|^{2}}.\] (3.1)