Nonlinear least squares examples.Nonlinear least squares problems arise in many of the same settings and applications as linear least squares problems.

* _Location from range measurements._ The 3-vector (or 2-vector) \(x\) represents the location of some object or target in 3-D (or 2-D), which we wish to determine or guess. We are given \(m\)_range measurements_, _i.e._, the distance from \(x\) to some known locations \(a_{1},\ldots,a_{m}\), \[\rho_{i}=\|x-a_{i}\|+v_{i},\quad i=1,\ldots,m,\] where \(v_{i}\) is an unknown measurement error, assumed to be small. Our estimate \(\hat{x}\) of the location is found by minimizing the sum of the squares of the range residuals, \[\sum_{i=1}^{m}\left(\|x-a_{i}\|-\rho_{i}\right)^{2}.\] A similar method is used in GPS devices, where \(a_{i}\) are the known locations of GPS satellites that are in view.
* _Nonlinear model fitting._ We consider a model \(y\approx\hat{f}(x;\theta)\) where \(x\) denotes a feature vector, \(y\) a scalar outcome, \(\hat{f}\) a model of some function relating \(x\) and \(y\), and \(\theta\) is a vector of model parameters that we seek. In chapter 13, \(\hat{f}\) is an affine function of the model parameter \(p\)-vector \(\theta\); but here it need not be. As in chapter 13, we choose the model parameter by minimizing the sum of the squares of the residuals over a data set with \(N\) examples, \[\sum_{i=1}^{N}(\hat{f}(x^{(i)};\theta)-y^{(i)})^{2}.\] (18.4) (As in linear least squares model fitting, we can add a regularization term to this objective function.) This is a nonlinear least squares problem, with variable \(\theta\).

### 18.2 Gauss-Newton algorithm

In this section we describe a powerful heuristic algorithm for the nonlinear least squares problem (18.2) that bears the names of the two famous mathematicians Carl Friedrich Gauss and Isaac Newton. In the next section we will also describe a variation of the Gauss-Newton algorithm known as the _Levenberg-Marquardt algorithm_, which addresses some shortcomings of the basic Gauss-Newton algorithm.

The Gauss-Newton and Levenberg-Marquardt algorithms are iterative algorithms that generate a sequence of points \(x^{(1)},x^{(2)},\ldots\). The vector \(x^{(1)}\) is called the _starting point_ of the algorithm, and \(x^{(k)}\) is called the \(k\)th _iterate_. Moving from \(x^{(k)}\) to \(x^{(k+1)}\) is called an _iteration_ of the algorithm. We judge the iterates by the norm of the associated residuals, \(\|f(x^{(k)})\|\), or its square. The algorithm is terminated when \(\|f(x^{(k)})\|\) is small enough, or \(x^{(k+1)}\) is very near \(x^{(k)}\), or when a maximum number of iterations is reached.

 