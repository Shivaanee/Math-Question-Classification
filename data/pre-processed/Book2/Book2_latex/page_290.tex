The plot gives the number of transistors on a logarithmic scale. Find the least squares straight-line fit of the data using the model

\[\log_{10}N\approx\theta_{1}+\theta_{2}(t-1970),\]

where \(t\) is the year and \(N\) is the number of transistors. Note that \(\theta_{1}\) is the model's prediction of the log of the number of transistors in 1970, and \(10^{\theta_{2}}\) gives the model's prediction of the fractional increase in number of transistors per year.

1. Find the coefficients \(\theta_{1}\) and \(\theta_{2}\) that minimize the RMS error on the data, and give the RMS error on the data. Plot the model you find along with the data points.
2. Use your model to predict the number of transistors in a microprocessor introduced in 2015. Compare the prediction to the IBM Z13 microprocessor, released in 2015, which has around \(4\times 10^{9}\) transistors.
3. Compare your result with Moore's law, which states that the number of transistors per integrated circuit roughly doubles every one and a half to two years.

The computer scientist and Intel corporation co-founder Gordon Moore formulated the law that bears his name in a magazine article published in 1965.

13.4_Asset \(\alpha\) and \(\beta\) and market correlation._ Suppose the \(T\)-vectors \(r^{\rm ind}=(r^{\rm ind}_{1},\ldots,r^{\rm ind}_{T})\) and \(r^{\rm mkt}=(r^{\rm mkt}_{1},\ldots,r^{\rm mkt}_{T})\) are return time series for a specific asset and the whole market, as described on page 251. We let \(r^{\rm rf}\) denote the risk-free interest rate, \(\mu^{\rm mkt}\) and \(\sigma^{\rm mkt}\) the market return and risk (_i.e._, \({\bf avg}(r^{\rm mkt})\) and \({\bf std}(r^{\rm mkt})\)), and \(\mu\) and \(\sigma\) the return and risk of the asset (_i.e._, \({\bf avg}(r^{\rm ind})\) and \({\bf std}(r^{\rm ind})\)). Let \(\rho\) be the correlation coefficient between the market and asset return time series \(r^{\rm mkt}\) and \(r^{\rm ind}\). Express the asset \(\alpha\) and \(\beta\) in terms of \(r^{\rm rf}\), \(\mu\), \(\sigma\), \(\mu^{\rm mkt}\), \(\sigma^{\rm mkt}\), and \(\rho\).
13.5_Polynomial model with multiple features._ The idea of polynomial models can be extended from the case discussed on page 255 where there is only one feature. In this exercise we consider a quadratic (degree two) model with 3 features, _i.e._, \(x\) is a 3-vector. This has the form \[\hat{f}(x)=a+b_{1}x_{1}+b_{2}x_{2}+b_{3}x_{3}+c_{1}x_{1}^{2}+c_{2}x_{2}^{2}+c _{3}x_{3}^{2}+c_{4}x_{1}x_{2}+c_{5}x_{1}x_{3}+c_{6}x_{2}x_{3},\] where the scalar \(a\), 3-vector \(b\), and 6-vector \(c\) are the zeroth, first, and second order coefficients in the model. Put this model into our general linear in the parameters form, by giving \(p\), and the basis functions \(f_{1},\ldots,f_{p}\) (which map 2-vectors to scalars).
13.6_Average prediction error._ Consider a data fitting problem, with first basis function \(\phi_{1}(x)=1\), and data set \(x^{(1)},\ldots,x^{(N)}\), \(y^{(1)},\ldots,y^{(N)}\). Assume the matrix \(A\) in (13.1) has linearly independent columns and let \(\hat{\theta}\) denote the parameter values that minimize the mean square prediction error over the data set. Let the \(N\)-vector \(\hat{r}^{\rm d}\) denote the prediction errors using the optimal model parameter \(\hat{\theta}\). Show that \({\bf avg}(\hat{r}^{\rm d})=0\). In other words: With the least squares fit, the mean of the prediction errors over the data set is zero. _Hint._ Use the orthogonality principle (12.9), with \(z=e_{1}\).
13.7_Data matrix in auto-regressive time series model._ An auto-regressive model with memory \(M\) is fit by minimizing the sum of the squares of the predictions errors on a data set with \(T\) samples, \(z_{1},\ldots,z_{T}\), as described on page 259. Find the matrix \(A\) and vector \(y\) for which \(\|A\beta-y\|^{2}\) gives the sum of the squares of the prediction errors. Show that \(A\) is a Toeplitz matrix (see page 138), _i.e._, entries \(A_{ij}\) with the same value of \(i-j\) are the same.
13.8_Fitting an input-output convolution system._ Let \(u_{1},\ldots,u_{T}\) and \(y_{1},\ldots,y_{T}\) be observed input and output time series for a system that is thought to be an input-output convolution system, meaning \[y_{t}\approx\hat{y}_{t}=\sum_{j=1}^{n}h_{j}u_{t-j+1},\quad t=1,\ldots,T,\] 