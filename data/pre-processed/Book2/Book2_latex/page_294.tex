* 13.20_Fitting with continuous and discontinuous piecewise-linear functions._ Consider a fitting problem with \(n=1\), so \(x^{(1)},\ldots,x^{(N)}\) and \(y^{(1)},\ldots,y^{(N)}\) are numbers. We consider two types of closely related models. The first is a piecewise-linear model with knot points at \(-1\) and \(1\), as described on page 256, and illustrated in figure 13.8. The second is a stratified model (see page 272), with three independent affine models, one for \(x<-1\), one for \(-1\leq x\leq 1\), and one for \(x>1\). (In other words, we stratify on \(x\) taking low, middle, or high values.) Are these two models the same? Is one more general than the other? How many parameters does each model have? _Hint._ See problem title. What can you say about the training set RMS error and test set RMS error that would be achieved using least squares with these two models?
* 13.21_Efficient cross-validation._ The cost of fitting a model with \(p\) basis functions and \(N\) data points (say, using QR factorization) is \(2Np^{2}\) flops. In this exercise we explore the complexity of carrying out 10-fold cross-validation on the same data set. We divide the data set into 10 folds, each with \(N/10\) data points. The naive method is to fit 10 different models, each using 9 of the folds, using the QR factorization, which requires \(10\cdot 2(0.9)Np^{2}=18Np^{2}\) flops. (To evaluate each of these models on the remaining fold requires \(2(N/10)p\) flops, which can be ignored compared to the cost of fitting the models.) So the naive method of carrying out 10-fold cross-validation requires, not surprisingly, around \(10\times\) the number of flops as fitting a single model. The method below outlines another method to carry out 10-fold cross-validation. Give the total flop count for each step, keeping only the dominant terms, and compare the total cost of the method to that of the naive method. Let \(A_{1},\ldots,A_{10}\) denote the \((N/10)\times p\) blocks of the data matrix associated with the folds, and let \(b_{1},\ldots,b_{10}\) denote the right-hand sides in the least squares fitting problem. 1. Form the Gram matrices \(G_{i}=A_{i}^{T}A_{i}\) and the vectors \(c_{i}=A_{i}^{T}b_{i}\). 2. Form \(G=G_{1}+\cdots+G_{10}\) and \(c=c_{1}+\cdots+c_{10}\). 3. For \(k=1,\ldots,10\), compute \(\theta_{k}=(G-G_{k})^{-1}(c-c_{k})\).
* 13.22_Prediction contests._ Several companies have run prediction contests open to the public. Netflix ran the best known contest, offering a $1M prize for the first prediction of user movie rating that beat their existing method RMS prediction error by 10% on a test set. The contests generally work like this (although there are several variations on this format, and most are more complicated). The company posts a public data set, that includes the regressors or features and the outcome for a large number of examples. They also post the features, but not the outcomes, for a (typically smaller) test data set. The contestants, usually teams with obscure names, submit predictions for the outcomes in the test set. Usually there is a limit on how many times, or how frequently, each team can submit a prediction on the test set. The company computes the RMS test set prediction error (say) for each submission. The teams' prediction performance is shown on a _leaderboard_, which lists the 100 or so best predictions in order. Discuss such contests in terms of model validation. How should a team check a set of predictions before submitting it? What would happen if there were no limits on the number of predictions each team can submit? Suggest an obvious method (typically disallowed by the contest rules) for a team to get around the limit on prediction submissions. (And yes, it has been done.) 