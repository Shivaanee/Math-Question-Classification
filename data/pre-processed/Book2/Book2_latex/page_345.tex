* _Bi-objective problems without trade-off._ Suppose \(\mu\) and \(\gamma\) are different positive weights, and \(\hat{x}(\mu)=\hat{x}(\gamma)\). Show that \(\hat{x}(\lambda)\) is constant for all \(\lambda>0\). Therefore the point \((J_{1}^{*}(\lambda),J_{2}^{*}(\lambda))\) is the same for all \(\lambda\) and the trade-off curve collapses to a single point.
* _Effect of weight on objectives in a bi-objective problem._ Suppose \(\hat{x}(\lambda)\) is not constant. Show the following: for \(\lambda<\mu\), we have \[J_{1}^{*}(\lambda)<J_{1}^{*}(\mu),\qquad J_{2}^{*}(\lambda)>J_{2}^{*}(\mu).\] This means that if you increase the weight (on the second objective), the second objective goes down, and the first objective goes up. In other words the trade-off curve slopes downward. _Hint._ Resist the urge to write out any equations or formulas. Use the fact that \(\hat{x}(\lambda)\) is the unique minimizer of \(J_{1}(x)+\lambda J_{2}(x)\), and similarly for \(\hat{x}(\mu)\), to deduce the inequalities \[J_{1}^{*}(\mu)+\lambda J_{2}^{*}(\mu)>J_{1}^{*}(\lambda)+\lambda J_{2}^{*}( \lambda),\qquad J_{1}^{*}(\lambda)+\mu J_{2}^{*}(\lambda)>J_{1}^{*}(\mu)+\mu J _{2}^{*}(\mu).\] Combine these inequalities to show that \(J_{1}^{*}(\lambda)<J_{1}^{*}(\mu)\) and \(J_{2}^{*}(\lambda)>J_{2}^{*}(\mu)\).
* _Slope of the trade-off curve._ The slope of the trade-off curve at the point \((J_{1}^{*}(\lambda),J_{2}^{*}(\lambda))\) is given by \[S=\lim_{\mu\to\lambda}\frac{J_{2}^{*}(\mu)-J_{2}^{*}(\lambda)}{J_{1}^{*}(\mu) -J_{1}^{*}(\lambda)}.\] (This limit is the same if \(\mu\) approaches \(\lambda\) from below or from above.) Show that \(S=-1/\lambda\). This gives another interpretation of the parameter \(\lambda\): \((J_{1}^{*}(\lambda),J_{2}^{*}(\lambda))\) is the point on the trade-off curve where the curve has slope \(-1/\lambda\). _Hint._ First assume that \(\mu\) approaches \(\lambda\) from above (meaning, \(\mu>\lambda\)) and use the inequalities in the hint for part (b) to show that \(S\geq-1/\lambda\). Then assume that \(\mu\) approaches \(\lambda\) from below and show that \(S\leq-1/\lambda\).
* _Least squares with smoothness regularization._ Consider the weighted sum least squares objective \[\|Ax-b\|^{2}+\lambda\|Dx\|^{2},\] where the \(n\)-vector \(x\) is the variable, \(A\) is an \(m\times n\) matrix, \(D\) is the \((n-1)\times n\) difference matrix, with \(i\)th row \((e_{i+1}-e_{i})^{T}\), and \(\lambda>0\). Although it does not matter in this problem, this objective is what we would minimize if we want an \(x\) that satisfies \(Ax\approx b\), and has entries that are smoothly varying. We can express this objective as a standard least squares objective with a stacked matrix of size \((m+n-1)\times n\). Show that the stacked matrix has linearly independent columns if and only if \(A\mathbf{1}\neq 0\), _i.e._, the sum of the columns of \(A\) is not zero.
* _Greedy regulation policy._ Consider a linear dynamical system given by \(x_{t+1}=Ax_{t}+Bu_{t}\), where the \(n\)-vector \(x_{t}\) is the state at time \(t\), and the \(m\)-vector \(u_{t}\) is the input at time \(t\). The goal in regulation is to choose the input so as to make the state small. (In applications, the state \(x_{t}=0\) corresponds to the desired operating point, so small \(x_{t}\) means the state is close to the desired operating point.) One way to achieve this goal is to choose \(u_{t}\) so as to minimize \[\|x_{t+1}\|^{2}+\rho\|u_{t}\|^{2},\] where \(\rho\) is a (given) positive parameter that trades off using a small input versus making the (next) state small. Show that choosing \(u_{t}\) this way leads to a state feedback policy \(u_{t}=Kx_{t}\), where \(K\) is an \(m\times n\) matrix. Give a formula for \(K\) (in terms of \(A\), \(B\), and \(\rho\)). If an inverse appears in your formula, state the conditions under which the inverse exists. _Remark._ This policy is called _greedy_ or _myopic_ since it does not take into account the effect of the input \(u_{t}\) on future states, beyond \(x_{t+1}\). It can work very poorly in practice.

 