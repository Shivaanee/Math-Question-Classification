* _Disease diagnosis._ The labels are a set of diseases (including one label that corresponds to disease-free), and the features are medically relevant values, such as patient attributes and the results of tests. Such a classifier carries out diagnosis (of the diseases corresponding to the labels). The classifier is trained on cases in which a definitive diagnosis has been made.
* _Translation word choice._ A machine translation system translates a word in the source language to one of several possible words in the target language. The label corresponds to a particular choice of translation for the word in the source language. The features contain information about the context around the word, for example, words counts or occurrences in the same paragraph. As an example, the English word 'bank' might be translated into another language one way if the word 'river' appears nearby, and another way if 'financial' or 'reserves' appears nearby. The classifier is trained on data taken from translations carried out by (human) experts.
* _Document topic prediction._ Each example corresponds to a document or article, with the feature vector containing word counts or histograms, and the label corresponding to the topic or category, such as Politics, Sports, Entertainment, and so on.
* _Detection in communications._ Many electronic communications systems transmit messages as a sequence of \(K\) possible _symbols_. The vector \(x\) contains measurements of the received signal. In this context the classifier \(\hat{f}\) is called a _detector_ or _decoder_; the goal is to correctly determine which of the \(K\) symbols was transmitted.

Prediction errors and confusion matrix.For a multi-class classifier \(\hat{f}\) and a given data point \((x,y)\), with predicted outcome \(\hat{y}=\hat{f}(x)\), there are \(K^{2}\) possibilities, corresponding to all the pairs of values of \(y\), the actual outcome, and \(\hat{y}\), the predicted outcome. For a given data set (training or validation set) with \(N\) elements, the numbers of each of the \(K^{2}\) occurrences are arranged into a \(K\times K\) confusion matrix, where \(N_{ij}\) is the number of data points for which \(y=i\) and \(\hat{y}=j\).

The \(K\) diagonal entries \(N_{11},\ldots,N_{KK}\) correspond to the cases when the prediction is correct; the \(K^{2}-K\) off-diagonal entries \(N_{ij}\), \(i\neq j\), correspond to prediction errors. For each \(i\), \(N_{ii}\) is the number of data points with label \(i\) for which we correctly guessed \(\hat{y}=i\). For \(i\neq j\), \(N_{ij}\) is the number of data points for which we have mistaken label \(i\) (its true value) for the label \(j\) (our incorrect guess). For \(K=2\) (Boolean classification) there are only two types of prediction errors, false positive and false negative. For \(K>2\) the situation is more complicated, since there are many more types of errors a predictor can make. From the entries of the confusion matrix we can derive various measures of the accuracy of the predictions. We let \(N_{i}\) (with one index) denote the total number of data points for which \(y=i\), _i.e._, \(N_{i}=N_{i1}+\cdots+N_{iK}\). We have \(N=N_{1}+\cdots+N_{K}\).

The simplest measure is the overall _error rate_, which is the total number of errors (the sum of all off-diagonal entries in the confusion matrix) divided by the 