
Complexity of vector operations.Scalar-vector multiplication \(ax\), where \(x\) is an \(n\)-vector, requires \(n\) multiplications, _i.e._, \(ax_{i}\) for \(i=1,\ldots,n\). Vector addition \(x+y\) of two \(n\)-vectors takes \(n\) additions, _i.e._, \(x_{i}+y_{i}\) for \(i=1,\ldots,n\). Computing the inner product \(x^{T}y=x_{1}y_{1}+\cdots+x_{n}y_{n}\) of two \(n\)-vectors takes \(2n-1\) flops, \(n\) scalar multiplications and \(n-1\) scalar additions. So scalar multiplication, vector addition, and the inner product of \(n\)-vectors require \(n\), \(n\), and \(2n-1\) flops, respectively. We only need an estimate, so we simplify the last to \(2n\) flops, and say that the _complexity_ of scalar multiplication, vector addition, and the inner product of \(n\)-vectors is \(n\), \(n\), and \(2n\) flops, respectively. We can guess that a 1 Gflop/s computer can compute the inner product of two vectors of size one million in around one thousandth of a second, but we should not be surprised if the actual time differs by a factor of 10 from this value.

The _order_ of the computation is obtained by ignoring any constant that multiplies a power of the dimension. So we say that the three vector operations scalar multiplication, vector addition, and inner product have order \(n\). Ignoring the factor of 2 dropped in the actual complexity of the inner product is reasonable, since we do not expect flop counts to predict the running time with an accuracy better than a factor of 2. The order is useful in understanding how the time to execute the computation will scale when the size of the operands changes. An order \(n\) computation should take around 10 times longer to carry out its computation on an input that is 10 times bigger.

Complexity of sparse vector operations.If \(x\) is sparse, then computing \(ax\) requires \(\mathbf{nnz}(x)\) flops. If \(x\) and \(y\) are sparse, computing \(x+y\) requires no more than \(\min\{\mathbf{nnz}(x),\mathbf{nnz}(y)\}\) flops (since no arithmetic operations are required to compute \((x+y)_{i}\) when either \(x_{i}\) or \(y_{i}\) is zero). If the sparsity patterns of \(x\) and \(y\) do not overlap (intersect), then zero flops are needed to compute \(x+y\). The inner product calculation is similar: computing \(x^{T}y\) requires no more than \(2\min\{\mathbf{nnz}(x),\mathbf{nnz}(y)\}\) flops. When the sparsity patterns of \(x\) and \(y\) do not overlap, computing \(x^{T}y\) requires zero flops, since \(x^{T}y=0\) in this case.

 