This leads us to a basic question: How well can we expect a model to predict \(y\) for future or other unknown values of \(x\)? Without some assumptions about the future data, there is no good way to answer this question.

One very common assumption is that the data are described by a formal probability model. With this assumption, techniques from probability and statistics can be used to predict how well a model will work on new, unseen data. This approach has been very successful in many applications, and we hope that you will learn about these methods in another course. In this book, however, we will take a simple intuitive approach to this issue.

If a model predicts the outcomes for new unseen data values as well, or nearly as well, as it predicts the outcomes on the data used to form the model, it is said to have _good generalization ability_. In the opposite case, when the model makes predictions on new unseen data that are much worse than the predictions on the data used to form the model, the model is said to have _poor generalization ability_. So our question is: How can we assess the generalization ability of a model?

Validation on a test set.A simple but effective method for assessing the generalization ability of a model is called _out-of-sample validation_. We divide the data we have into two sets: A _training set_ and a _test set_ (also called a _validation set_). This is often done randomly, with 80% of the data put into the training set and 20% put into the test set. A common way to describe this is to say that '20% of the data were reserved for validation'. Another common choice for the split ratio between the training set and the test set is 90%-10%.

To fit our model, we use _only the data in the training set_. The model that we come up with is based only on the data in the training set; the data in the test set has never been 'seen' by the model. Then we judge the model by its RMS fit _on the test set_. Since the model was developed without any knowledge of the test set data, the test data are effectively data that are new and unseen, and the performance of our model on this data gives us at least an idea of how our model will perform on new, unseen data. If the RMS prediction error on the test set is much larger than the RMS prediction error on the training set, we conclude that our model has poor generalization ability. Assuming that the test data are 'typical' of future data, the RMS prediction error on the test set is what we might guess our RMS prediction error will be on new data.

If the RMS prediction error of the model on the training set is similar to the RMS prediction error on the test set, we have increased confidence that our model has reasonable generalization ability. (A more sophisticated validation method called _cross-validation_, described below, can be used to gain even more confidence.)

For example, if our model achieves an RMS prediction error of 10% (compared to \(\mathbf{rms}(y)\)) on the training set and 11% on the test set, we can _guess_ that it will have a similar RMS prediction error on other unseen data. But there is no guarantee of this, without further assumptions about the data. The basic assumption we are making here is that the future data will 'look like' the test data, or that the test data were 'typical'. Ideas from statistics can make this idea more precise, but we will leave this idea informal and intuitive.

 