Over-fitting.When the RMS prediction error on the training set is much smaller than the RMS prediction error on the test set, we say that the model is _over-fit_. It tells us that, for the purposes of making predictions on new, unseen data, the model is much less valuable than its performance on the training data suggests. Roughly speaking, an over-fit model trusts the data it has seen (_i.e._, the training set) too much; it is too sensitive to the changes in the data that will likely be seen in the future data. One method for avoiding over-fit is to keep the model simple; another technique, called regularization, is discussed in chapter 15. Over-fit can be detected and (one hopes) avoided by validating a model on a test set.

Model prediction quality and generalization ability.Model generalization ability and training set prediction quality are not the same. A model can perform poorly and yet have good generalization ability. As an example, consider the (very simple) model that always makes the prediction \(\hat{y}=0\). This model will (likely) perform poorly on the training set and the test set data, with similar RMS errors, assuming the two data sets are 'similar'. So this model has good generalization ability, but has poor prediction quality. In general, we seek a model that _makes good predictions on the training data set_ and also _makes good predictions on the test data set_. In other words, we seek a model with good performance and generalization ability. We care much more about a model's performance on the test data set than the training data set, since its performance on the test data set is much more likely to predict how the model will do on (other) unseen data.

Choosing among different models.We can use least squares fitting to fit multiple models to the same data. For example, in univariate fitting, we can fit a constant, an affine function, a quadratic, or a higher order polynomial. Which is the best model among these? Assuming that the goal is to make good predictions on new, unseen data, _we should choose the model with the smallest RMS prediction error on the test set_. Since the RMS prediction error on the test set is only a guess about what we might expect for performance on new, unseen data, we can soften this advice to _we should choose a model that has test set RMS error that is near the minimum over the candidates_. If multiple candidates achieve test set performance near the minimum, we should choose the 'simplest' one among these candidates.

We observed earlier that when we add basis functions to a model, our fitting error on the training data can only decrease (or stay the same). But this is not true for the test error. The test error need not decrease when we add more basis functions. Indeed, when we have too many basis functions, we can expect over-fit, _i.e._, larger error on the test set.

If we have a sequence of basis functions \(f_{1},f_{2},\ldots\) we can consider models based on using just \(f_{1}\) (which is often the constant function 1), then \(f_{1}\) and \(f_{2}\), and so on. As we increase \(p\), the number of basis functions, our training error will go down (or stay the same). But the test error typically decreases at first and then starts to increase for larger \(p\). The intuition for this typical behavior is that for \(p\) too small, our model is 'too simple' to fit the data well, and so cannot make good predictions; when \(p\) is too large, our model is 'too complex' and suffers from over-fit, and so makes poor predictions. Somewhere in the middle, where the model achieves near minimum test set performance, is a good choice (or several good choices) of \(p\).

 