

**13.13**: _Interaction model with Boolean features._ Consider a data fitting problem in which all \(n\) original features are Boolean, _i.e._, entries of \(x\) have the value \(0\) or \(1\). These features could be the results of Boolean tests for a patient (or absence or presence of symptoms), or a person's response to a survey with yes/no questions. We wish to use these to predict an outcome, the number \(y\). Our model will include a constant feature \(1\), the original \(n\) Boolean features, and all _interaction terms_, which have the form \(x_{i}x_{j}\) where \(1\leq i<j\leq n\).

1. What is \(p\), the total number of basis functions, in this model? Explicitly list the basis functions for the case \(n=3\). You can decide their order. _Hint._ To count the number of pairs \(i\), \(j\) that satisfy \(1\leq i<j\leq n\), use equation (5.7).
2. Interpret (together) the following three coefficients of \(\theta\): the one associated with the original feature \(x_{3}\); the one associated with the original feature \(x_{5}\); and the one associated with the new product feature \(x_{3}x_{5}\). _Hint._ Consider the four possible values of the pair \(x_{3},x_{5}\).
**13.14**: _Least squares timing._ A computer takes around one second to fit a regression model (using least squares) with \(20\) parameters using \(10^{6}\) data points.

1. About how long do you guess it will take the same computer to fit the same \(20\)-parameter model using \(10^{7}\) data points (_i.e._, \(10\times\) more data points)?
2. About how long do you guess it will take the same computer to fit a \(200\)-parameter model using \(10^{6}\) data points (_i.e._, \(10\times\) more model parameters)?
**13.15**: _Estimating a matrix._ Suppose that the \(n\)-vector \(x\) and the \(m\)-vector \(y\) are thought to be approximately related by a linear function, _i.e._, \(y\approx Ax\), where \(A\) is an \(m\times n\) matrix. We do not know the matrix \(A\), but we do have observed data, \[x^{(1)},\ldots,x^{(N)},\qquad y^{(1)},\ldots,y^{(N)}.\] We can estimate or guess the matrix \(A\) by choosing it to minimize \[\sum_{i=1}^{N}\|Ax^{(i)}-y^{(i)}\|^{2}=\|AX-Y\|^{2},\] where \(X=[x^{(1)}\ \cdots\ x^{(N)}]\) and \(Y=[y^{(1)}\ \cdots\ y^{(N)}]\). We denote this _least squares estimate_ as \(\hat{A}\). (The notation here can be confusing, since \(X\) and \(Y\) are known, and \(A\) is to be found; it is more conventional to have symbols near the beginning of the alphabet, like \(A\), denote known quantities, and symbols near the end, like \(X\) and \(Y\), denote variables or unknowns.)

1. Show that \(\hat{A}=YX^{\dagger}\), assuming the rows of \(X\) are linearly independent. _Hint._ Use \(\|AX-Y\|^{2}=\|X^{T}A^{T}-Y^{T}\|^{2}\), which turns the problem into a matrix least squares problem; see page 233.
2. Suggest a good way to compute \(\hat{A}\), and give the complexity in terms of \(n\), \(m\), and \(N\).
**13.16**: _Relative fitting error and error fitting the logarithm._ (See page 259.) The relative fitting error between a positive outcome \(y\) and its positive prediction \(\hat{y}\) is given by \(\eta=\max\{\hat{y}/y,y/\hat{y}\}-1\). (This is often given as a percentage.) Let \(r\) be the residual between their logarithms, \(r=\log y-\log\hat{y}\). Show that \(\eta=e^{|r|}-1\).
**13.17**: _Fitting a rational function with a polynomial._ Let \(x_{1},\ldots,x_{11}\) be \(11\) points uniformly spaced in the interval \([-1,1]\). (This means \(x_{i}=-1.0+0.2(i-1)\) for \(i=1,\ldots,11\).) Take \(y_{i}=(1+x_{i})/(1+5x_{i}^{2})\), for \(i=1,\ldots,11\). Find the least squares fit of polynomials of degree \(0,1,\ldots,8\) to these points. Plot the fitting polynomials, and the true function \(y=(1+x)/(1+5x^{2})\), over the interval \([-1.1,1.1]\) (say, using \(100\) points). Note that the interval for the plot, \([-1.1,1.1]\), extends a bit outside the range of the data used to fit the polynomials, \([-1,1]\); this gives us an idea of how well the polynomial fits can extrapolate.

