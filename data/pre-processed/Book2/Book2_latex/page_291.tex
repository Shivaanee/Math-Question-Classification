where we interpret \(u_{t}\) as zero for \(t\leq 0\). Here the \(n\)-vector \(h\) is the system impulse response; see page 140. This model of the relation between the input and output time series is also called a _moving average_ (MA) model. Find a matrix \(A\) and vector \(b\) for which

\[\left\|Ah-b\right\|^{2}=(y_{1}-\hat{y}_{1})^{2}+\cdots+(y_{T}-\hat{y}_{T})^{2}.\]

Show that \(A\) is Toeplitz. (See page 138.)
**13.9**: _Conclusions from 5-fold cross-validation._ You have developed a regression model for predicting a scalar outcome \(y\) from a feature vector \(x\) of dimension 20, using a collection of \(N=600\) data points. The mean of the outcome variable \(y\) across the given data is 1.85, and its standard deviation is 0.32. After running 5-fold cross-validation we get the following RMS test errors (based on forming a model based on the data excluding fold \(i\), and testing it on fold \(i\)).

1. How would you expect your model to do on new, unseen (but similar) data? Respond briefly and justify your response.
2. A co-worker observes that the regression model parameters found in the 5 different folds are quite close, but not the same. He says that for the production system, you should use the regression model parameters found when you excluded fold 3 from the original data, since it achieved the best RMS test error. Comment briefly.
**13.10**: _Augmenting features with the average._ You are fitting a regression model \(\hat{y}=x^{T}\beta+v\) to data, computing the model coefficients \(\beta\) and \(v\) using least squares. A friend suggests adding a new feature, which is the average of the original features. (That is, he suggests using the new feature vector \(\tilde{x}=(x,\mathbf{avg}(x))\).) He explains that by adding this new feature, you might end up with a better model. (Of course, you would test the new model using validation.) Is this a good idea?
**13.11**: _Interpreting model fitting results._ Five different models are fit using the same training data set, and tested on the same (separate) test set (which has the same size as the training set). The RMS prediction errors for each model, on the training and test sets, are reported below. Comment briefly on the results for each model. You might mention whether the model's predictions are good or bad, whether it is likely to generalize to unseen data, or whether it is over-fit. You are also welcome to say that you don't believe the results, or think the reported numbers are fishy.

\begin{tabular}{c c c} \hline \hline Model & Train RMS & Test RMS \\ \hline A & 1.355 & 1.423 \\ B & 9.760 & 9.165 \\ C & 5.033 & 0.889 \\ D & 0.211 & 5.072 \\ E & 0.633 & 0.633 \\ \hline \hline \end{tabular}
**13.12**: _Standardizing Boolean features._ (See page 269.) Suppose that the \(N\)-vector \(x\) gives the value of a (scalar) Boolean feature across a set of \(N\) examples. (Boolean means that each \(x_{i}\) has the value 0 or 1. This might represent the presence or absence of a symptom, or whether or not a day is a holiday.) How do we standardize such a feature? Express your answer in terms of \(p\), the fraction of \(x_{i}\) that have the value 1. (You can assume that \(p>0\) and \(p<1\); otherwise the feature is constant.) 