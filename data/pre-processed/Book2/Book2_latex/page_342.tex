

#### Gram caching

We start from the formula (15.3) for the minimizer of the weighted sum objective,

\[\hat{x}=(\lambda_{1}A_{1}^{T}A_{1}+\cdots+\lambda_{k}A_{k}^{T}A_{k})^{-1}( \lambda_{1}A_{1}^{T}b_{1}+\cdots+\lambda_{k}A_{k}^{T}b_{k}).\]

The matrix appearing in the inverse is a weighted sum of the Gram matrices \(G_{i}=A_{i}^{T}A_{i}\) associated with the matrices \(A_{i}\). We can compute \(\hat{x}\) by forming these Gram matrices \(G_{i}\), along with the vectors \(h