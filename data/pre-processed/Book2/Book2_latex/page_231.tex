and both left-invertible, and have multiple left inverses. Do they have a common left inverse? Explain how to find a \(2\times 4\) matrix \(C\) that satisfies \(CA=CB=I\), or determine that no such matrix exists. (You can use numerical computing to find \(C\).) _Hint._ Set up a set of linear equations for the entries of \(C\). _Remark._ There is nothing special about the particular entries of the two matrices \(A\) and \(B\).
* _Checking the computed solution of linear equations._ One of your colleagues says that whenever you compute the solution \(x\) of a square set of \(n\) equations \(Ax=b\) (say, using QR factorization), you should compute the number \(\|Ax-b\|\) and check that it is small. (It is not exactly zero due to the small rounding errors made in floating point computations.) Another colleague says that this would be nice to do, but the additional cost of computing \(\|Ax-b\|\) is too high. Briefly comment on your colleagues' advice. Who is right?
* _Sensitivity of solution of linear equations._ Let \(A\) be an invertible \(n\times n\) matrix, and \(b\) and \(x\) be \(n\)-vectors satisfying \(Ax=b\). Suppose we perturb the \(j\)th entry of \(b\) by \(\epsilon\neq 0\) (which is a traditional symbol for a small quantity), so \(b\) becomes \(\tilde{b}=b+\epsilon e_{j}\). Let \(\tilde{x}\) be the \(n\)-vector that satisfies \(A\tilde{x}=\tilde{b}\), _i.e._, the solution of the linear equations using the perturbed right-hand side. We are interested in \(\|x-\tilde{x}\|\), which is how much the solution changes due to the change in the right-hand side. The ratio \(\|x-\tilde{x}\|/|\epsilon|\) gives the sensitivity of the solution to changes (perturbations) of the \(j\)th entry of \(b\).
* Show that \(\|x-\tilde{x}\|\) does not depend on \(b\); it only depends on the matrix \(A\), \(\epsilon\), and \(j\).
* How would you find the index \(j\) that maximizes the value of \(\|x-\tilde{x}\|\)? By part (a), your answer should be in terms of \(A\) (or quantities derived from \(A\)) and \(\epsilon\) only. _Remark._ If a small change in the right-hand side vector \(b\) can lead to a large change in the solution, we say that the linear equations \(Ax=b\) are _poorly conditioned_ or _ill-conditioned_. As a practical matter it means that unless you are very confident in what the entries of \(b\) are, the solution \(A^{-1}b\) may not be useful in practice.
* _Timing test._ Generate a random \(n\times n\) matrix \(A\) and an \(n\)-vector \(b\), for \(n=500\), \(n=1000\), and \(n=2000\). For each of these, compute the solution \(x=A^{-1}b\) (for example using the backslash operator, if the software you are using supports it), and verify that \(Ax-b\) is (very) small. Report the time it takes to solve each of these three sets of linear equations, and for each one work out the implied speed of your processor in Gflop/s, based on the \(2n^{3}\) complexity of solving equations using the QR factorization.
* _Solving multiple linear equations efficiently._ Suppose the \(n\times n\) matrix \(A\) is invertible. We can solve the system of linear equations \(Ax=b\) in around \(2n^{3}\) flops using algorithm 11.2. Once we have done that (specifically, computed the QR factorization of \(A\)), we can solve an additional set of linear equations with same matrix but different right-hand side, \(Ay=c\), in around \(3n^{2}\) additional flops. Assuming we have solved both of these sets of equations, suppose we want to solve \(Az=d\), where \(d=\alpha b+\beta c\) is a linear combination of \(b\) and \(c\). (We are given the coefficients \(\alpha\) and \(\beta\).) Suggest a method for doing this that is even faster than re-using the QR factorization of \(A\). Your method should have a complexity that is _linear_ in \(n\). Give rough estimates for the time needed to solve \(Ax=b\), \(Ay=c\), and \(Az=d\) (using your method) for \(n=3000\) on a computer capable of carrying out 1 Gflop/s.

 