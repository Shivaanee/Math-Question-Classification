

## 12 Exercises

* _Error in straight-line fit._ Consider the straight-line fit described on page 12, with data given by the \(N\)-vectors \(x^{\mathrm{d}}\) and \(y^{\mathrm{d}}\). Let \(r^{\mathrm{d}}=y^{\mathrm{d}}-\hat{y}^{\mathrm{d}}\) denote the residual or prediction error using the straight-line model (13.3). Show that \(\mathbf{rms}(r^{\mathrm{d}})=\mathbf{std}(y^{\mathrm{d}})\sqrt{1-\rho^{2}}\), where \(\rho\) is the correlation coefficient of \(x^{\mathrm{d}}\) and \(y^{\mathrm{d}}\) (assumed non-constant). This shows that the RMS error with the straight-line fit is a factor \(\sqrt{1-\rho^{2}}\) smaller than the RMS error with a constant fit, which is \(\mathbf{std}(y^{\mathrm{d}})\). It follows that when \(x^{\mathrm{d}}\) and \(y^{\mathrm{d}}\) are highly correlated (\(\rho\approx 1\)) or anti-correlated (\(\rho\approx-1\)), the straight-line fit is much better than the constant fit. _Hint._ From (13.3) we have \[\hat{y}^{\mathrm{d}}-y^{\mathrm{d}}=\rho\frac{\mathbf{std}(y^{\mathrm{d}})}{ \mathbf{std}(x^{\mathrm{d}})}(x^{\mathrm{d}}-\mathbf{avg}(x^{\mathrm{d}}) \mathbf{1})-(y^{\mathrm{d}}-\mathbf{avg}(y^{\mathrm{d}})\mathbf{1}).\] Expand the norm squared of this expression, and use \[\rho=\frac{(x^{\mathrm{d}}-\mathbf{avg}(x^{\mathrm{d}})\mathbf{1})^{T}(y^{ \mathrm{d}}-\mathbf{avg}(y^{\mathrm{d}})\mathbf{1})}{\|x^{\mathrm{d}}- \mathbf{avg}(x^{\mathrm{d}})\mathbf{1}\|\|y^{\mathrm{d}}-\mathbf{avg}(y^{ \mathrm{d}})\mathbf{1}\|}.\]
* _Regression to the mean._ Consider a data set in which the (scalar) \(x^{(i)}\) is the parent's height (average of mother's and father's height), and \(y^{(i)}\) is their child's height. Assume that over the data set the parent and child heights have the same mean value \(\mu\), and the same standard deviation \(\sigma\). We will also assume that the correlation coefficient \(\rho\) between parent and child heights is (strictly) between zero and one. (These assumptions hold, at least approximately, in real data sets that are large enough.) Consider the simple straight-line fit or regression model given by (13.3), which predicts a child's height from the parent's height. Show that this prediction of the child's height lies (strictly) between the parent's height and the mean height \(\mu\) (unless the parent's height happens to be exactly the mean \(\mu\)). For example, if the parents are tall, _i.e._, have height above the mean, we predict that their child will be shorter, but still tall. This phenomenon, called _regression to the mean_, was first observed by the early statistician Sir Francis Galton (who indeed, studied a data set of parent's and child's heights).
* _Moore's law._ The figure and table below show the number of transistors \(N\) in 13 microprocessors, and the year of their introduction.

