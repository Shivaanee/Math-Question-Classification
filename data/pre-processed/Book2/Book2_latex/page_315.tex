

## 12 Exercises

* 1.1. _Chebyshev bound._ Let \(\tilde{f}(x)\) denote the continuous prediction of the Boolean outcome \(y\), and \(\hat{f}(x)=\mathbf{sign}(\tilde{f}(x))\) the actual classifier. Let \(\sigma\) denote the RMS error in the continuous prediction over some set of data, _i.e._, \[\sigma^{2}=\frac{(\tilde{f}(x^{(1)})-y^{(1)})^{2}+\cdots+(\tilde{f}(x^{(N)})- y^{(N)})^{2}}{N}.\] Use the Chebyshev bound to argue that the error rate over this data set, _i.e._, the fraction of data points for which \(\hat{f}(x^{(i)})\neq y^{(i)}\), is no more than \(\sigma^{2}\), assuming \(\sigma<1\). _Remark._ This bound on the error rate is usually quite bad; that is, the actual error rate in often much lower than this bound. But it does show that if the continuous prediction is good, then the classifier must perform well.
* 1.2. _Interpreting the parameters in a regression classifier._ Consider a classifier of the form \(\hat{y}=\mathbf{sign}(x^{T}\beta+v)\), where \(\hat{y}\) is the prediction, the \(n\)-vector \(x\) is the feature vector, and the \(n\)-vector \(\beta\) and scalar \(v\) are the classifier parameters. We will assume here that \(v\neq 0\) and \(\beta\neq 0\). Evidently \(\hat{y}=\mathbf{sign}(v)\) is the prediction when the feature vector \(x\) is zero. Show that when \(\|x\|<|v|/\|\beta\|\), we have \(\hat{y}=\mathbf{sign}(v)\). This shows that when all the features are _small_ (_i.e._, near zero), the classifier makes the prediction \(\hat{y}=\mathbf{sign}(v)\). _Hint_. If two numbers \(a\) and \(b\) satisfy \(|a|<|b|\), then \(\mathbf{sign}(a+b)=\mathbf{sign}(b)\). This means that we can interpret \(\mathbf{sign}(v)\) as the classifier prediction when the features are small. The ratio \(|v|/\|\beta\|\) tells us how big the feature vector must be before the classifier 'changes its mind' and predicts \(\hat{y}=-\mathbf{sign}(v)\).
* 1.3. _Likert classifier._ A response to a question has the options _Strongly Disagree_, _Disagree_, _Neutral_, _Agree_, or _Strongly Agree_, encoded as \(-2,-1,0,1,2\), respectively. You wish to build a multi-class classifier that takes a feature vector \(x\) and predicts the response. A multi-class least squares classifier builds a separate (continuous) predictor for each response versus the others. Suggest a simpler classifier, based on one continuous regression model \(\tilde{f}(x)\) that is fit to the numbers that code the responses, using least squares.
* 1.4. _Multi-class classifier via matrix least squares._ Consider the least squares multi-class classifier described in SS14.3, with a regression model \(\tilde{f}_{k}(x)=x^{T}\beta_{k}\) for the one-versus-others classifiers. (We assume that the offset term is included using a constant feature.) Show that the coefficient vectors \(\beta_{1},\ldots,\beta_{K}\) can be found by solving the matrix least squares problem of minimizing \(\|X^{T}\beta-Y\|^{2}\), where \(\beta\) is the \(n\times K\) matrix with columns \(\beta_{1},\ldots,\beta_{K}\), and \(Y\) is an \(N\times K\) matrix. 1. [label=()]
2. Give \(Y\), _i.e._, describe its entries. What is the \(i\)th row of \(Y\)? 2. Assuming the rows of \(X\) (_i.e._, the data feature vectors) are linearly independent, show that the least squares estimate is given by \(\hat{\beta}=(X^{T})^{\dagger}Y\).
* 1.5. _List classifier._ Consider a multi-class classification problem with \(K\) classes. A standard multi-class classifier is a function \(\hat{f}\) that returns a class (one of the labels \(1,\ldots,K\)), when given a feature \(n\)-vector \(x\). We interpret \(\hat{f}(x)\) as the classifier's guess of the class that corresponds to \(x\). A _list classifier_ returns a list of guesses, typically in order from 'most likely' to 'least likely'. For example, for a specific feature vector \(x\), a list classifier might return \(3,6,2\), meaning (roughly) that its top guess is class \(3\), its next guess is class \(6\), and its third guess is class \(2\). (The lists can have different lengths for different values of the feature vector.) How would you modify the least squares multi-class classifier described in SS14.3.1 to create a list classifier? _Remark._ List classifiers are widely used in electronic communication systems, where the feature vector \(x\) is the received signal, and the class corresponds to which of \(K\) messages was sent. In this context they are called _list decoders_. List decoders produce a list of probable or likely messages, and allow a later processing stage to make the final decision or guess.

