So multiplying a vector \(c\) by the matrix \(A\) is the same as evaluating a polynomial with coefficients \(c\) at \(m\) points. The matrix \(A\) in (6.7) comes up often, and is called a _Vandermonde matrix_ (of degree \(n-1\), at the points \(t_{1},\ldots,t_{m}\)), named for the mathematician Alexandre-Theophile Vandermonde.
* _Total price from multiple suppliers._ Suppose the \(m\times n\) matrix \(P\) gives the prices of \(n\) goods from \(m\) suppliers (or in \(m\) different locations). If \(q\) is an \(n\)-vector of quantities of the \(n\) goods (sometimes called a _basket_ of goods), then \(c=Pq\) is an \(N\)-vector that gives the total cost of the goods, from each of the \(N\) suppliers.
* _Document scoring._ Suppose \(A\) in an \(N\times n\) document-term matrix, which gives the word counts of a corpus of \(N\) documents using a dictionary of \(n\) words, so the rows of \(A\) are the word count vectors for the documents. Suppose that \(w\) in an \(n\)-vector that gives a set of weights for the words in the dictionary. Then \(s=Aw\) is an \(N\)-vector that gives the scores of the documents, using the weights and the word counts. A search engine, for example, might choose \(w\) (based on the search query) so that the scores are predictions of relevance of the documents (to the search).
* _Audio mixing._ Suppose the \(k\) columns of \(A\) are vectors representing audio signals or tracks of length \(T\), and \(w\) is a \(k\)-vector. Then \(b=Aw\) is a \(T\)-vector representing the mix of the audio signals, with track weights given by the vector \(w\).

Inner product.When \(a\) and \(b\) are \(n\)-vectors, \(a^{T}b\) is exactly the inner product of \(a\) and \(b\), obtained from the rules for transposing matrices and forming a matrix-vector product. We start with the (column) \(n\)-vector \(a\), consider it as an \(n\times 1\) matrix, and transpose it to obtain the \(n\)-row-vector \(a^{T}\). Now we multiply this \(1\times n\) matrix by the \(n\)-vector \(b\), to obtain the 1-vector \(a^{T}b\), which we also consider a scalar. So the notation \(a^{T}b\) for the inner product is just a special case of matrix-vector multiplication.

Linear dependence of columns.We can express the concepts of linear dependence and independence in a compact form using matrix-vector multiplication. The columns of a matrix \(A\) are linearly dependent if \(Ax=0\) for some \(x\neq 0\). The columns of a matrix \(A\) are linearly independent if \(Ax=0\) implies \(x=0\).

Expansion in a basis.If the columns of \(A\) are a basis, which means \(A\) is square with linearly independent columns \(a_{1},\ldots,a_{n}\), then for any \(n\)-vector \(b\) there is a unique \(n\)-vector \(x\) that satisfies \(Ax=b\). In this case the vector \(x\) gives the coefficients in the expansion of \(b\) in the basis \(a_{1},\ldots,a_{n}\).

Properties of matrix-vector multiplication.Matrix-vector multiplication satisfies several properties that are readily verified. First, it distributes across the vector argument: For any \(m\times n\) matrix \(A\) and any \(n\)-vectors \(u\) and \(v\), we have

\[A(u+v)=Au+Av.\] 