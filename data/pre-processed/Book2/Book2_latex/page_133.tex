specific order. This reduces the memory requirement by around a factor of two. Sparse matrices are represented by various methods that encode for each nonzero element its row index \(i\) (an integer), its column index \(j\) (an integer) and its value \(A_{ij}\) (a floating point number). When the row and column indices are represented using 4 bytes (which allows \(m\) and \(n\) to range up to around 4.3 billion) this requires a total of around \(16\,\mathbf{nnz}(A)\) bytes.

Complexity of matrix addition, scalar multiplication, and transposition.The addition of two \(m\times n\) matrices or a scalar multiplication of an \(m\times n\) matrix each take \(mn\) flops. When \(A\) is sparse, scalar multiplication requires \(\mathbf{nnz}(A)\) flops. When at least one of \(A\) and \(B\) is sparse, computing \(A+B\) requires at most \(\min\{\mathbf{nnz}(A),\mathbf{nnz}(B)\}\) flops. (For any entry \(i,j\) for which one of \(A_{ij}\) or \(B_{ij}\) is zero, no arithmetic operations are needed to find \((A+B)_{ij}\).) Matrix transposition, _i.e._, computing \(A^{T}\), requires zero flops, since we simply copy entries of \(A\) to those of \(A^{T}\). (Copying the entries does take time to carry out, but this is not reflected in the flop count.)

Complexity of matrix-vector multiplication.A matrix-vector multiplication of an \(m\times n\) matrix \(A\) with an \(n\)-vector \(x\) requires \(m(2n-1)\) flops, which we simplify to \(2mn\) flops. This can be seen as follows. The result \(y=Ax\) of the product is an \(m\)-vector, so there are \(m\) numbers to compute. The \(i\)th element of \(y\) is the inner product of the \(i\)th row of \(A\) and the vector \(x\), which takes \(2n-1\) flops.

If \(A\) is sparse, computing \(Ax\) requires \(\mathbf{nnz}(A)\) multiplies (of \(A_{ij}\) and \(x_{j}\), for each nonzero entry of \(A\)) and a number of additions that is no more than \(\mathbf{nnz}(A)\). Thus, the complexity is between \(\mathbf{nnz}(A)\) and \(2\,\mathbf{nnz}(A)\) flops. As a special example, suppose \(A\) is \(n\times n\) and diagonal. Then \(Ax\) can be computed with \(n\) multiplies (\(A_{ii}\) times \(x_{i}\)) and no additions, a total of \(n=\mathbf{nnz}(A)\) flops.

 