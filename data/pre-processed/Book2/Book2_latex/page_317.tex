

**14.10**: _One-versus-one multi-class classifier._ In SS14.3.1 we construct a \(K\)-class classifier from \(K\) Boolean classifiers that attempt to distinguish each class from the others. In this exercise we describe another method for constructing a \(K\)-class classifier. We first develop a Boolean classifier for every _pair_ of classes \(i\) and \(j\), \(i<j\). There are \(K(K-1)/2\) such pairs of classifiers, called _one-versus-one_ classifiers. Given a feature vector \(x\), we let \(\hat{y}_{ij}\) be the prediction of the \(i\)-versus-\(j\) classifier, with \(\hat{y}_{ij}=1\) meaning that the one-versus-one classifier is guessing that \(y=i\). We consider \(\hat{y}_{ij}=1\) as one 'vote' for class \(i\), and \(\hat{y}_{ij}=-1\) as one 'vote' for class \(j\). We obtain the final estimated class by _majority voting_: We take \(\hat{y}\) as the class that has the most votes. (We can break ties in some simple way, like taking the smallest index that achieves the largest number of votes.)

1. Construct the least squares classifier, and the one-versus-one classifier, for a multi-class (training) data set. Find the confusion matrices, and the error rates, of the two classifiers on both the training data set and a separate test data set.
2. Compare the complexity of computing the one-versus-one multi-class classifier with the complexity of the least squares multi-class classifier (see page 300). Assume the training set contains \(N/K\) examples of each class and that \(N/K\) is much greater than the number of features \(p\). Distinguish two methods for the one-versus-one multi-class classifier. The first, naive, method solves \(K(K-1)/2\) least squares problem with \(N/K\) rows and \(p\) columns. The second, more efficient, method precomputes the Gram matrices \(G_{i}=A_{i}A_{i}^{T}\) for \(i=1,\ldots,K\), where the rows of the \((N/K)\times p\) matrix \(A_{i}\) are the training example for class \(i\), and uses the pre-computed Gram matrices to speed up the solution of the \(K(K-1)/2\) least squares problems.
**14.11**: _Equalizer design from training message._ We consider an electronic communication system, with message to be sent given by an \(N\)-vector \(s\), whose entries are \(-1\) or \(1\), and received signal \(y\), where \(y=c*s\), where \(c\) is an \(n\)-vector, the channel impulse response. The receiver applies equalization to the received signal, which means that it computes \(\tilde{y}=h*y=h*c*s\), where \(h\) is an \(n\)-vector, the equalizer impulse response. The receiver then estimates the original message using \(\hat{s}=\mathbf{sign}(\tilde{y}_{1:N})\). This works well if \(h*c\approx e_{1}\). (See exercise 7.15.) If the channel impulse response \(c\) is known or can be measured, we can design or choose \(h\) using least squares, as in exercise 12.6. In this exercise we explore a method for choosing \(h\) directly, without estimating or measuring \(c\). The sender first sends a message that is _known_ to the receiver, called the _training message_, \(s^{\text{train}}\). (From the point of view of communications, this is wasted transmission, and is called _overhead_.) The receiver receives the signal \(y^{\text{train}}=c*s^{\text{train}}\) from the training message, and then chooses \(h\) to minimize \(\|(h*y^{\text{train}})_{1:N}-s^{\text{train}}\|^{2}\). (In practice, this equalizer is used until the bit error rate increases, which means the channel has changed, at which point another training message is sent.) Explain how this method is the same as least squares classification. What are the training data \(x^{(i)}\) and \(y^{(i)}\)? What is the least squares problem that must be solved to determine the equalizer impulse response \(h\)?